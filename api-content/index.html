{"posts":[{"title":"Differentiated Key-Value Storage Management for Balanced I/O Performance","content":" ATC21 Differentiated Key-Value Storage Management for Balanced I/O Performance https://www.cse.cuhk.edu.hk/~pclee/www/pubs/atc21diffkv.pdf Abstract KV 存储读写放大严峻，现有设计都是在做 trade-off，不能同时实现高性能的读写以及 scan。所以提出了 DiffKV，在 KV 分离的基础上构建并仔细管理 Key Value 的顺序。Key 沿用 LSM-Tree，保持全序，同时以协调的方式管理部分有序的 Value，以保持高扫描性能。进一步提出细粒度 KV 分离，以大小区分 KV 对，实现混合工作负载下的均衡性能。 Introduction 为了减小 compaction 开销，现有方案大致有几个方向。 放松全局有序的要求来缓解 compaction 开销。但常伴随着 scan 性能的下降 Dostoevsky PebblesDB SlimDB 基于 KV 分离，Key 有序，专门的存储区管理 Value。一是更适用于大 KV，二是还是有 scan 的性能损失（毕竟 Value 的顺序不保证了就导致随机读），三是还引入了垃圾回收的开销 WiscKey HashKV BadgerDB Atlas An Efficient Memory-Mapped Key-Value Store for Flash Storage Titan UniKV 简而言之，现有的LSM-tree优化仍然受到读写和扫描之间紧密的性能紧张关系的限制，包括： (i) 键和值的有序程度 (ii) 不同大小的KV对的管理。 Background and Motivation LSM-tree KV Store 基础结构此处省略。 现有优化方案 主要还是分为两类: Relaxing fully-sorted ordering KV separation Relaxing fully-sorted ordering 以 PebblesDB 为例实现了分段 LSM。将一个 Level 分为了几个不相交的 Guards，每个 Guard 里的 SSTables 可以范围重叠。那么你会问了：为啥这样就能减小 compaction 开销呢？ 因为 PebblesDB 只是读取了某一层中的 group 的 SSTables，然后进行了排序创建了新的 SSTables 写到下一层，compaction 过程不需要读取下一层的 SSTables 从而减小 compaction 开销和写放大。但是因为每个 Group 里的 SSTable 是重叠的，所以牺牲了一部分 scan 性能，虽然可以利用多线程来并行读，但是就导致了更多的 CPU 开销，所以提升也是有限的。 KV separation KV 分离则是将 Key 和 Location 信息存储在 LSM-tree，Value 单独地存储在一个日志里，Titan 中以多个 blob 文件形式来组织 Value。对于中大型 Value，因为 Key 和 Location 有着远小于 Value 的大小，在 LSM-tree 里存储的数据量就比较小，compaction 开销和写放大也相应比较小。除此以外，小的 LSM-tree 也减小了读放大，可以提升读性能。 然而，由于 KV 分离将值写入一个附加日志，一个连续范围的键值现在分散在日志的不同位置。scan 操作也就会导致随机读取，因此导致比较差的 scan 性能，特别是小到中型 Value（OLTP 应用超过 90% 的 value 小于 1KB），除此以外，KV 分离还需要 GC 来回收空间，频繁的 GC 操作会导致额外的 I/O 开销。 Trade-off Analysis 性能测试之前，先大概讲一下 Titan Titan 基于 KV 分离实现，此外，采用了多个小的 Blob Files 来代替大的只追加写的日志对 Value 进行管理，使用多线程来减小 GC 开销。 数据组织形式如下： LSM-tree: Key - index(BlobFileID:offset:ValueSize) BlobFile: Value (Value 的存储类似于原本 SSTable 的结构) 每条 BlobRecord 冗余存储了 Value 对应的 Key 以便反向索引，但也引入了写放大 KeyValue 有序存放，为了提升 scan 性能，甚至进行预取 支持 BlobRecord 粒度的 compression，支持多种算法 GC： 监听 LSM-tree 的 compaction 来统计每个 BlobFile 的 discardable 数据大小，触发的 GC 则选择对应 discardable 最大的 File 来作为 candidate GC 选择了一些 candidates，当 discardable size 达到一定比例之后再 GC。使用 Sample 算法，随机取 BlobFile 中的一段数据 A，计其大小为 a，然后遍历 A 中的 key，累加过期的 key 所在的 blob record 的 size 计为 d，最后计算得出 d 占 a 比值 为 r，如果 r &gt;= discardable_ratio 则对该 BlobFile 进行 GC，否则不对其进行 GC。如果 discardable size 占整个 BlobFile 数据大小的比值已经大于或等于 discardable_ratio 则不需要对其进行 Sample Write performance：加载 100GB 数据库，不同 value 大小（128B - 16KB），两种优化方案都能降低写放大，随着 value 大小的增加，写放大降低的越多，相应的写吞吐也就很大提升。证明以往的两种优化方向都是可以改善写放大并提升写性能的，特别是对于大 Value。 Read and scan performance：放松全局有序的要求导致查询性能出现了降级，而 KV 分离因为显著减小了 LSM-tree 的大小，读吞吐比 RocksDB 高很多。而对于 scan 两种方案都比 RocksDB 差很多，做了延迟分解，发现大部分扫描时间花在了 iteratively reading values，随着 value 的增大，scan 延迟的差距变小，因为访问更大的 Value 对应了更小的随机读开销。对于那些小 KV 为主的负载，两种优化方案都会受到很大限制。 总结一下，现有的优化方案其实都是在做 reads/writes 的 trade-off，两种优化方向的本质都是在降低 Value 的有序程度来减小写放大并提升吞吐量，但相应地牺牲了 scan 的性能，特别是对于小到中型的 KV 对。 Design System Overview DiffKV 利用了一个类似于 LSM-tree 的结构 vTree 来组织 Value 保证 Value 的部分有序，也是由多个 Level 组成，每个 level 只能以追加的方式写入，和 LSM-Tree 的不同在于，vTree 只存储那些在每一层中不一定按键完全排序的 Value，即允许部分有序来保证 scan 性能。 为了实现部分有序，vTree 也需要类似 compaction 的操作，称之为 merge，为了减小 merge 的开销，DiffKV 让 LSM-tree 的 compaction 和 vTree 的 merge 协调地执行来减少总体开销。 为了让 DiffKV 和现有设计兼容，内存组件和 WAL 都和原生 LSM 一样，流程是一样的。 Data Orgaization vTree 分层组织，每层由 sorted groups 组成，每个 group 由很多 vTables 组成。（这里仿佛就跟 PebblesDB 的设计是类似的了） vTable 大小固定，8MB 默认。一个 immutable Memtable 的 flush 可以生成很多个 vTables，取决于 value size 和 Memtable Size。 vTable 组成部分： data area: 基于 Key 的顺序存储 values metedata area: 记录必要的元数据，比如 vTable 的数据大小，该 table 中最小最大 Value（和 SSTable 不同的是不要求 BllomFilter，因为直接查索引就能知道）。元数据较小，存储开销也较小。 Sorted Group 所有的 vTables 在 Group 中也是有序的。也就是无重叠范围，LSM-tree 的 SSTables 组成的集合也相当于一个 Sorted Group。DiffKV 一次 flush 对应生成一个 Sorted Group，从而保留每个 immutable Memtable 里顺序性，对应 Groups 数量表示了 vTree 的有序程度，Groups 数量增加，有序性相应下降，在一个极端情况下，如果所有的 sstable/vtable组成一个 Group，那么有序程度最大，因为所有的 KV 对都是完全排序的。 （埋个坑：这里直接用 Groups 数量来表示有序程度会不会有些草率，其实有序与否更多是看重叠范围大小，当然 Groups 越多重叠的概率可能越大，但不确定这个有序程度是不是会对后面造成影响，拭目以待） vTree vTree --- 1:N --- levels --- 1:N --- groups --- 1:N --- vTables --- 1:N --- values 全局有序的最小单元是 group，level 就可能存在具有重叠键范围的 groups 了，merge 操作不需要对 vTree 中连续两层中的所有值进行排序;与 lsm-tree 中的 compaction 相比，这减轻了I/O开销 Compaction-Triggered Merge 首先，为什么要 merge？ 其实就是为了保证部分有序，或者说维持有序性，这样才可能加速 scan。 Merge 会读取一定数量的 vTables，通过查询 LSM-tree 中存储的最新位置信息来检查哪些 value 是有效的，每个 merge 还要更新最新的 location 信息到 LSM-tree 中 为了限制 vTree 中的合并开销，vTree 中的合并操作不是独立执行的，而是和 LSM-tree 中的压缩操作以协调的方式触发的。所以称该操作为 compaction-triggered merge 举例说明： 假设 LSM tree 的 level 和 vTree 的 level 是关联的。 当 LSM-tree 发生 Level i -&gt; i+1 的 compaction 的时候，相应地触发 vTree 上相应的 value 从 vLivL_ivLi​ 到 vLi+1vL_{i+1}vLi+1​ 的 merge。 Merge 有两个主要问题： 选哪些 value 进行 merge？选择那些参加了 compaction 的 keys 对应的 value 进行 merge，称之为 compaction-related value 如何把这些 value 写回到 vLi+1vL_{i+1}vLi+1​？把这些 value 组织起来生成新的 vTables 并把这些 vTables 以追加写的形式写到下一层。如下图所示的棕色就是这些需要 merge 的 values，V33V_{33}V33​, V13V_{13}V13​, V45V_{45}V45​ 在排序后追加写入到下一层。 生成的 vTables 中的所有数据都是有序的，也就是说 Merge 产生的 vTables 形成了一个单独的 sorted group。但是，我们要指出的是，合并操作并不需要重新组织 vTree 中 vLivL_ivLi​ 和 vLi+1vL_{i+1}vLi+1​ 两级的所有vtable。追加写的形式来表面重写，从而减小写放大，老的 vTables 也不会在 merge 过程中删除，因为可能还是包含一些有效的 value，交给 GC 来判断处理。 compaction-triggered merge 带来的好处表现在两个方面：(协同设计的核心) 只合并与 compaction 相关的值可以非常有效地识别哪些值仍然有效，因为在 compaction 期间本身就需要从 lsm-tree 中读出相应的键。相反如果 vTree 的 merge 独立执行，那就需要查询 LSM-tree 并比较 location 信息来判断有效性了，增加了较大的查询开销。 merge 操作过程中新生成的 vTables 伴随着有效 value 的位置信息的变化，LSM-tree 需要被更新来维护对应的最新的 value location 信息，因为只有 compaction-related values 被合并，更新 LSM 树中的值位置可以通过直接更新参与 compaction 的 KV 对来执行。因此，更新值位置的开销可以隐藏在 compaction 操作中，因为 compaction 本身需要重写 lsm 树中的 KV 对 Merge Optimizations Compaction-triggered merge 引入了有限的合并开销，该开销又主要是检查 value 的有效性并写回 value location 信息造成的。但是如果让每一次 compaction 操作都触发 merge 的话可能造成频繁的 merge，比如 vTree 中的每个级别只与 LSM-tree 中的一个级别相关，那么每个 compaction 操作都必须在 vTree 中触发合并操作，为了减小 merge 开销，提出了两个优化方案。 Lazy merge 该策略用于限制 merge 频率和开销。核心思想是聚合多个 lower vTree levels 为单个 level，关联地聚合 LSM-tree 的多个 levels，如下图所示。聚合 vTree 的 0,...,n-2 为一个单独的 level，相应地聚合 LSM Tree 的 0,...,n-2 levels，因此任何发生在 0,...,n-2 之间的 compaction 都不会触发 merge，也就是说 vTree 的 0,...,n-2 Level Merge 操作被延迟到除非需要合并到 vLn−1vL_{n-1}vLn−1​ 的时候才会执行。 该策略显著减少合并次数和合并数据量，但是牺牲了较低层次的 value 的有序程度，然而，我们认为这种牺牲对扫描性能的影响是有限的。因为 LSM 中的大部分数据都是保存在最后几层，不同层次的数据分布不均匀意味着大多数值实际上是从 vTree 的最后两层进行扫描的，所以最后两层的值的有序程度才会更多地影响 scan 性能。也就是说，low level 对 scan 性能影响很小，频繁的在 low level 的合并操作不会对 scan 性能的提升有什么帮助但却引入了较大的 merge 开销。 Scan-optimized merge 该策略用于调整 value 的有序程度，来保证较高的 scan 性能。原本的合并策略中，上层的 value 被重新组织写入到下层，下层的 value 其实是没有参与 merge 的。这种策略减小了写放大，但是导致了太多的 sorted group，即可能重叠的现象更严峻。因此我们的核心思想是找到和其他 vTables 有重叠范围的 vTables，让这些 vTables 参与 merge 而不管其位于哪一层。这样就能保证有序的程度较高。 下图描述了核心思想，在普通的 compaction-triggered merge 之后，首先在下层检查包含 compaction-related values 的 vTables，目标是找出满足如下两个条件的 vTables 集合： 集合中至少一个 vTable 有重叠的键范围 vTables 的数量，也就是 set size，大于预先定义的阈值，max_sorted_run 如果存在上述的 SET，scan 性能势必会降低，因为这些 vTable 没有被排序。所以添加一个 scan optimization tag 给这些 tables，所以他们将能总是参与到下一次 Merge 并增加有序性。 为了找到这样的一组 vTables，首先遍历每个包含 compaction-related values 的 vTable 的起始和终止 Key，对于每个检查的 vTable，统计有重叠键范围的 vTables 的数量，可以通过扫描排序后的键字符串来完成。如下图所示，考虑一个检查过的 vTable 【26-38】，扫描排序后的字符串，可以统计出在 Key 38 之前起始 keys 的个数，本例中有五个，然后结束 Key 在 26 之前的只有一个，然后相减，得到和 【26-38】 重叠的 tables 有四个，也就是该集合的 size 为 4，如果超过了阈值，那么就会给这些 tables 添加 tag，并在下一次合并中对他们进行合并。同时这个 tag 会被持久化到 mainfest file 中，持久化开销可忽略不计。 Scan-optimized merge 该策略是一个原始策略上的 enhancement。原策略只是简单做追加写，而该策略进一步包含了下一层中确定的 tables 来进行合并，从而增加有序性。注意该策略引入了有限的合并开销，有两个原因： 允许每一层有多个 sorted groups（一整层不是全局有序的） （这里有点奇怪） 不是标记了的 table 中的所有 values 都会参与 merge，而只是 compaction-related values 会参与。 GC 为了减小 GC 开销，又提出了基于无效 value 数量的 state-aware lazy approach State awareness DiffKV 在一个哈希表中记录每个 vTable 的无效 KV 的数量，每次当 vTable 参与一个 Merge 的时候，DiffKV 记录从 vTable 中检索到的值的数量，并在哈希表中更新旧 vTable中无效值的数量。它还为哈希表中任何新的 vTable 插入一个条目。对哈希表的更新是在合并操作期间执行的，因此开销是有限的。另外，哈希表中的每个条目只占用几个字节，所以哈希表的内存开销是有限的。 Lazy GC 如果 vTable 有一定比例的无效 Values 且超过阈值 gc_threshold 的时候被选为 GC candidate，需要注意的是 DiffKV 不能立马回收候选的 vTables，相反只是标记一个 GC tag，延迟 GC 到下一次合并。具体的，如果带有 GC tag 的 vTable 被包含到一次合并中，那么该 table 包含的 values 将总是被重写到下一个 level。 该策略避免了查询 LSM 检查有效性的额外开销，也避免了更新 LSM 新的地址信息的开销，被延迟到和合并一起执行，所以查询和更新的开销可以被合并操作给隐藏。 Discussion Optimizing compaction at L0：提出一个简单的优化 selective compaction 来聚合 Level 0 中小的 SSTables，具体而言，我们会触发内部 compaction，简单地在 L0 处合并多个小 sstable 来生成一个新的大的，而不与 L1 处的 sstable 合并，这样的话，L0 中的 SSTables 的大小将和 L1 中的相当，且没有额外的 compaction 开销引入。 Crash consistency：DiffKV 基于 Titan 实现，一致性保证和 Titan 以及 RocskDB 一样，使用了 WAL。同时 DiffKV 还未记录无效数据的 HASHTable 提供了一致性保证，随着 HashTable 在 compaction 之后被更新，DiffKV 将更新信息追加到 manifest 文件中。 Fine-grained KV Separation 对于大KV对，KV分离的好处是显著的，但对于小KV对就不是这样了。然而，不同值大小的混合工作负载也很常见;例如，在广义帕累托分布下，值的大小可能变化很大。在本节中，我们通过 value 大小来区分KV对，通过细粒度的KV分离来进一步增强DiffKV，从而实现混合工作负载下的均衡性能。 Differentiated Value Management 使用两个参数分为三组，small medium large 。小 value 直接写 LSM，中 Value 写 vTree，大 value 写 vLog。 大 value 的在写 Memtable 之前就先分离了 KV，这样做的好处有两个方面： 直接将大 value 刷回到 vLog，并保留小的 Key 和地址信息在 Memtable，可以节省内存消耗，同时因为大的顺序 I/O 保证了较高的写性能。 因为大 value 被写回到了 disk，就不用再写 WAL 了，减小了 I/O 的次数 需要注意的是，对于中小型KV对，以及大型KV对的键和值位置，仍然需要写入WAL中，以保证一致性。 Hotness-aware vLogs Structure of vLogs 就是一个简单的环形只允许追加的日志，由一组无序的 vTables 组成，vTable 和前面提到的 vTable 格式相同，唯一的不同是 value 是追加写入到无序的 vTables 的，所以在每个 table 内也是无序的，我们这样做的原因是因为 KV 分离对于大型 KV 对执行写入MemTable 之前,大型 KV 被立即刷新到磁盘,以免写 WAL,所以没有办法排序每个 table 中的 values。事实上也不需要，因为他们本身就能从大 I/O 中获益了而不用批量写入。 GC for vLogs 为了减小 GC 开销，使用了一个热点感知的设计，采用一种简单而有效的无参数冷热分离方案。如下图所示，使用两个 vLogs，分别对应冷热 vLogs，存储了对应的冷热数据，每个vLog 都有自己的写边界，我们分别称它们为 写头 和 GC 头。为了实现冷热分离，用户写入的数据被追加到 hot vLog 的 write head，而来自于 GC 写的数据（比如有效数据需要在 GC 过程中进行写入）被追加写到 cold vLog 的 GC head。其基本原理是，GC 回收的值通常比最近写入的用户数据访问频率更低，因此可以将它们视为冷数据，这种设计的一个好处是实现简单，因为实现冷热识别不需要参数。显然，我们还可以应用其他的热点感知分类方案。 DiffKV 使用了一个贪心的算法来减小 GC 开销，思想是回收有最大数量的无效值的无序的 vTables。具体的，DiffKV 在 compaction 中监控了每个无序 vTables 的无效数据比例，并维护了一个内存中的 GC 队列来追踪所有候选的 vTables，候选条件即为无效值比例大于阈值。需要注意的是 GC 队列只维护每个无序 vTable 的元数据，它根据无效值的比率按降序进行维护。GC 触发的时候，DiffKV 简单地选择队列头的无序 vTables，如图 9 中的 t1 t2，然后将有效的 values 追加到 cold vLog 的 GC 头。出于性能考虑，DiffKV 使用后台进程多线程来实现 GC。 Evaluation ","link":"https://blog.shunzi.tech/post/DiffKV/"},{"title":"REMIX: Efficient Range Query for LSM-trees","content":" FAST21 REMIX: Efficient Range Query for LSM-trees Abstract LSM 本身是为高速写操作优化的，而范围查询在传统的 LSM-tree 上需要 seek 且归并排序来自多个 Table 的数据，开销是很大的且经常导致较差的读性能。为了提升范围查询的性能，我们提出了一个空间高效的的 KV 索引数据结构 REMIX，记录跨多个表文件的KV数据的全局排序视图。对多个 REMIX 索引数据文件的范围查询可以使用二分搜索快速定位目标键，并在不进行键比较的情况下按排序顺序检索后续键。基于此构建了 RemixDB，一个采用了一个更高效的压缩策略并使用 REMIX 来进行快速的点查询和范围查询的 KV 存储。实验结果表明，在基于写优化 LSM 树的 KV-store 中，REMIX 可以显著提高范围查询性能。 Introduction LSM 代表的是更新开销和读开销的一种权衡，相比于 B+tree 保证了更小的写开销以及更大的读开销。关于读开销的优化就有，驻留在内存中的为每个 Table 维护的 BloomFilters，来减小不必要的 Table 访问。但是 BloomFilter 不能处理范围查询，所以出现了 Range Filters，如 SIGMOD18-SuRF、SIGMOD20-Rosetta 来在范围查询时过滤掉 Tables。但是当范围查询内的 Key 位于很多个候选的 Tables 中时，filtering 就很难提升查询性能了，特别是大范围查询，而且当查询请求可以在缓存中处理时，访问 Filters 的计算开销可能导致性能表现一般，这是真实负载中比较普遍的情况。 LSM 本身是有 Compaction 来减少查询时检索的 SSTable 数量的，选择 Table 的策略又分为了 leveled 和 tiering。 LevelDB、RocksDB 采用的 Leveled 的策略就是把小的 sorted run 合并一个更大的 sorted run 来保证重叠的 Tables 数量小于阈值，该策略却是保证了较好的读性能但是因为归并排序的方式导致写放大比较严峻 Tiered 则是等待多个相近大小的 sorted runs 达到阈值后合并到一个更大的 runs，从而提供更小的写放大以及更高的写吞吐。Cassandra、ScyllaDB 就采用了这样的方式。但是没有限制重叠的 Tables 的数量从而导致较大的查询开销。 SIGMOD18 Dostoevsky 和 SIGMOD19 The Log-Structured Merge-Bush &amp; the Wacky Continuum 提出的 compaction 策略虽然做了一定程度上的读写的平衡，但是还是没能同时实现最好的读和写。 问题的关键其实在于限制 sorted runs 的数量以及 KV 存储不得不归并排序且重写现有的数据。如今的硬件技术使得随机访问的效率也很高了，因此 KV 存储不再说必须保证物理上的有序，而可以只保证逻辑有序同时避免大量的重写。 为此，我们设计了REMIX，现有的范围查询解决方案很难在物理重写数据和动态执行昂贵的排序合并之间进行改进，与此不同的是，REMIX 使用了一个空间效率高的数据结构来记录跨多个表文件的 KV 数据的全局排序视图。使用 REMIX，基于 LSM 树的 KV-store 可以利用高效写压缩策略，而不会牺牲搜索性能。基于此我们还构建了 RemixDB，和高效的 Tiered 压缩策略以及分区的布局集成，同时实现了较低的写放大和快速的查询。 Background 了解几个概念就好： minor compaction：其实就是我们说的 flush 过程，数据从内存持久化到存储设备。这个过程写是比较快的，因为是顺序批量写入且不需要合并存储中的现有数据，但也就意味着查询操作需要检索所有的重叠的 Tables，查询开销较大，所以出现了 major compaction。 major compaction：其实就是把几个重叠的 runs 归并排序排序成更少的的 runs。也就是我们更为熟知的 compaction 操作。常说的 compaction 策略也是指 major compaction 过程中使用的策略。示例如下图 Range Query：范围查询是在 LevelDB/RocksDB 中是通过迭代器来定位多个 Tables 实现的，首先初始化一个迭代器，对一个 key 进行 seek，也就是范围查询的下届，seek 操作则定位对应的迭代器让其指向大于等于该 Key 的最小位置，然后 next 操作则相应地按顺序移动迭代器指向下一个 key，直到遇到了范围查询的边界。由于 sorted run 是按时间顺序生成的，所以目标键可以驻留在任何 runs 中。因此，迭代器必须跟踪所有 sorted runs。如上图所示的查询过程，找到了对应的 Keys 之后构建一个小顶堆进行归并排序，从而得到结果。 REMIX 范围查询其实依赖全局有序视图，而全局有序视图其实本身是从 SSTables 的不变性继承过来的，也就是说该全局有序视图本身可以保证很长一段时间有效，直到 SSTables 结构发生了变化，被删除了重写了之类的操作。现有的方案没有利用到不变性的这个优势，而是在每次范围查询过程中重复构建该全局有序视图，因为大量的计算开销和 I/O 而导致较差的读性能。所以 REMIX 就是想利用 table files 的不变性来维护一个全局有序视图，从而加速后续的查询操作。 为了让 I/O 更高效，LSM 的 KV 存储常使用一些内存高效的元数据格式，譬如稀疏索引和布隆过滤器，如果我们记录了全局有序视图，势必也需要保证内存的空间高效。不能因为存储更多的元数据而导致读写性能损失。 The REMIX Data Structure 图示很好理解，简单解释一下概念。下图所示例子包含了三个 sorted runs，顺序对应地由箭头表示，共计 15 个 Keys，为了构建 REMIX，首先进行了个划分，划分为一定数量的 segments，每个分段包含的 Keys 数量相等。每个 Segment 对应包含一个起始 Key，也就是 Anchor Key，包含一组游标偏移，对应就是记录每个 Sorted Runs 现在的指针位置，也就是大于等于起始锚点的指针位置，还有一个 Run Selectors，包含了真正的顺序，即下一个 Key 所在的 runs 编号。 范围查询的过程就变成了： 首先检索稀疏索引，也就是 Anchor Key，二分查找找到所属的 segment 迭代器对应被 seek 到该 segment 对应的 Anchor Key，然后使用该 segment 的游标偏移来移动迭代器指针，根据 selectors 中的顺序来进行 seek 最后通过在全局有序的视图上找到了对应的目标 Key 例子：图示中找到 Key 17，首先二分找到 Segment2。然后游标从 11 开始移动，根据 Seletors 发现 11 在 R0 上，以及 Offset 的结果，即在 R0 的索引为 1 的地方开始，因为 11 &lt; 17，那么要继续移动游标，直到找到大于等于 17 的 Key，也就是图中的 17，这时候 offset 变成了 2 2 1，根据 Selectors 那么找到了 R1，根据 offset 也就找到了 17。 Efficient Search in a Segment Segement 的划分是一个可调的配置，Size 太大，Anchor Keys 就少了，二分查找更快，但是 Segment 内平均访问次数增多，因此在目标的 Segment 内也使用二分查找。 Binary Search 为了在一个段中执行二分搜索，我们必须能够随机访问段中的每个键。段中的键属于 run，由相应的 run 选择器指示。要访问一个键，我们需要将 run 的游标放在正确的位置。这可以通过计算相同的 run 选择器在键之前的段中出现的次数，并将相应的游标向前移动相同的次数来实现。可以使用现代 CPU 上的 SIMD 指令快速计算出现次数。搜索范围可以通过对段的一些随机访问快速缩小，直到识别出目标键。为了结束查找操作，我们使用每个 run 选择器在目标键之前出现的次数初始化所有游标。 图示很清楚，很好理解就不解释了。 I/O Optimization 执行段内二分查找当然是为了减少比较的次数，但是，搜索路径上的键可能驻留在不同的 runs 中，如果各自的数据块没有被缓存，则必须通过单独的 I/O 请求来检索。上图所示就比较了四次 Key，对应访问了 3 个 runs，但是像 41 43 这两个 Key 其实属于一个 Run，甚至可能属于一个数据块。因此，在进行键比较之后，搜索可以利用相同数据块中的其余键，在必须访问不同的 Run 之前进一步缩小搜索范围。这样，R3 中的每个键都可以在不访问任何其他 Run 的情况下找到。比如为了查找 79，访问 R3 可以将搜索范围缩小到键 43 和键 83 之间，这时候二分查找找到了 71，也就是 R0，然后在 R0 中找到了 79。 Search Efficiency 总结下来，具体的提升体现在下面三个方面 REMIXes find the target key using one binary search：说白了就是执行一次二分查找就能找到 Key（这个一次其实是指完整的一次，包括段内的，因为全局有序视图，就花一次也很好理解） REMIXes move the iterator without key comparisons：这个就是说不用再比较很多个 runs 的 Key 之后再移动了，这个还是因为全局有序，可以跳过一些没必要的比较操作。 REMIXes skip runs that are not on the search path：就是说可以跳过一些 run，但这个其实是看数据重叠的情况的，总是是因为读一个数据块的粒度导致可能读上来的数据块恰好就包含了这个 key，或者减小了查询的范围。 作者表明对点查询也是有优化的。 REMIX Storage Cost 具体公式就不列了，看个数据就行。 其实空间开销不小，特别是那个快 10% 的 USR。 RemixDB RemixDB 使用了 Tiered Compaction 加一个分区的布局。因为有很多研究表明真实负载大多有很强的空间局部性，然后分区存储的话可以很好地降低压缩开销。所以 RemixDB 将键空间划分为不重叠键范围的分区。REMIX 对每个分区中的表文件进行索引，提供分区的排序视图。通过这种方式，RemixDB 本质上是一个使用分层压缩的单层 LSM 树。RemixDB 不仅继承了 Tiered 压缩的写效率，而且在 REMIX 的帮助下实现了高效的读取。RemixDB 的点查询操作(GET)执行一个 seek 操作，如果它与目标键匹配，则返回迭代器下的键。RemixDB 不使用Bloom过滤器。 下图展示了 RemixDB 的结构，内存组件和 LevelDB 这些是一样的。分区中的压缩创建分区的新版本，其中包含新旧表文件的混合以及一个新的 REMIX 文件。旧版本在压缩后进行垃圾收集。 在多级 LSM-tree 设计中，MemTable 的大小通常只有几十 MB，接近于默认的 SSTable 大小。在分区存储布局中，较大的 Memtable 在触发压缩之前可以积累更多的更新，这有助于减少 WA。MemTables 和 WAL 的空间成本几乎不变，考虑到当今数据中心的大内存和存储容量，这是合理的。在 RemixDB 中，MemTable 的最大大小被设置为 4GB。 The Structures of RemixDB Files Table Files 图 6 显示了 RemixDB 中的表文件格式。数据块默认为 4KB。一个大的 KV-pair 如果不能容纳在一个 4KB 的块中，就会独占一个 4K 的倍数的巨型块。每个数据块在块的开始包含一个小数组的 KV 对的块偏移量，用于随机访问单独的 KV 对。 元数据块是一个 8bit value 的数组，每个都记录着一个 4KB block 中的 Keys 的数量。一个块可以包含 255 个 KV，在一个大 Block 中，除了第一个 4KB 外，其余的都将其对应的数字设为0，这样一个非零的数字总是对应于一个块的头部。使用偏移数组和元数据块，搜索可以快速到达任何相邻的块，并跳过任意数量的键，而不访问数据块。因为 KV 对是由 REMIX 索引的，所以表文件不包含索引或过滤器。 REMIX Files 图 7 展示了 REMIX 文件格式，anchor key 是在一个不可变的类似于 B+Tree 的索引中维护的，相当于 LevelDB/RocksDB 的块索引，来辅助二分查找。每个 Anchor Key 和一个 Segment ID 关联，对应关联游标偏移和 run selectors。游标偏移由 16 位的块索引和 8 位的 key 索引组成，即图示中的 blk-id 和 key-id。块索引可以索引 65536 个 4KB 的块，也就是 256MB，每个块可以包含 256 个 KV 对。 一个 Key 的多个版本可以存在于一个分区的不同 table 文件中，一个范围查询操作必须跳过老版本并返回对应的最新的数据，所以 REMIX 构建的全局有序视图按照了从最新到最老的顺序排序，每个 run 选择器的最高位被保留来区分新旧版本。forward scan operation 将总是最先遇到最新版本的 Key，然后通过检查每个 run selector 的保留位来跳过老版本，而不用进行 Key 的比较。 如果一个 Key 有多版本，这些版本可能分布在两个 Segments，查询操作可能需要检索两个 Segments 来获得最新的版本。为了简化查询，当构造一个 REMIX 时，通过在第一个段中插入特殊的 run 选择器作为占位符，我们将键的所有版本向前移动到第二个段。还需要确保一个 Segment 中的 Key 的最大数量等于或大于由 REMIX 索引的 runs 的个数，才能保证每个 Segment 足够大以容纳一个 Key 的所有版本的数据。 为了容纳上面描述的特殊值，每个 run selector 占据了一个 byte，run selector 的第 8 位和第 7 位 (0x80 and 0x40) 分别表示老版本和删除的 Key。特殊值 63 (0x3f) 意味 placeholder，通过这种方式，RemixDB 可以在一个分区中管理多达 63 个 sorted tuns。 Compaction 在每个分区中，compaction 进程基于进入分区的新数据的大小和现有的 tables 的布局来估计 compaction cost，基于估算出来的开销，可能执行下列操作： Abort：取消分区 compaction 并保留 Memtable 和 WAL 中的新数据 Minor Compaction：写新数据到一个或多个 tables 而不用重写现有的 tables Major Compaction：将新数据和一些甚至全部的现有数据合并 Split Compaction：将新数据和所有的现有数据合并，并拆分分区到几个新分区上 Abort：Compaction 之后，一个分区如果有文件将重建该分区的 REMIX 索引，当一个小的表文件因为 minor compacion 在分区中创建，重建 REMIX 可能导致比较大的 I/O 开销。例如，表1 中的 USR 负载就有最高的空间开销比例。写 100MB 新数据到一个 1GB 的分区，将创建一个大约 100M 的索引。为了最小化 I/O 开销，RemixDB 可能丢弃一个分区的 compaction，如果估算出的 I/O 开销超过了阈值。该场景下，新的 KV 数据应该保留在 MemTables 和 WAL 中直到下一次 Compaction。 但是还有一个极端例子，如果一个 uniform 的负载，当所有分区都有 compaction 被Aborted 的时候，compaction 进程不能高效地当移动数据到分区。为了避免这个问题，我们限制了可以驻留在 Memtables 和 WAL 中的新数据的大小，不能超过最大 MemTable 大小的 15%，compaction 进程可以丢弃掉那些 I/O 开销最大的 compactions 如果达到了限制的话。 Minor Compaction：minor compaction 就是从 immutable memtable 中把数据写入到分区中，而不用重写分区中原有的数据，但是需要重建 REMIX 索引。根据新写入的数据大小，minor compaction 创建一个或者多个新的 table files，Minor Compaction 只有在 compaction 后的预期 table 文件数量低于阈值 T 的时候才会执行，我们的实现中该阈值设定为了 10。下图展示了 minor compaction 的例子。 Major Compaction：当一个分区中的预期文件数量达到了阈值 T 将执行 major 或者 split compaction。major compaction 将归并排序现有的 table files 合并成更少的 tables。随着 tables 数量的减少，minor compaction 的效率也就可以根据输入表文件的数量与输出表文件的数量的比率来估计。下图展示了 major compaction 的过程，该例子中，新数据合并到了三个小的 tables，只有一个新的 table 在 compaction 之后被创建，比例为 3/1，如果整个分区被归并排序，compactions 需要重写更多的数据并且仍然输出三个 tables，比例 5/3，因为 table file 大小的限制。相应的，major compaction 选择能够产生最高比率的输入文件的数量。 Split Compaction：major compaction 可能不会很快地减少填满了大 table 的分区中 tables 的数量，这可以通过一个较低的估计输入/输出比率(例如10/9)来预测。该例子下，分区需要被分成多个分区，从而保证每个分区中的 tables 的数量可以持续减少。Split Compaction 归并排序了新数据和现有的 table fils，产生新的 tables 来组成几个新的分区。下图展示了过程。为了避免创建太多的小分区，compaction process 在一个分区里创建了 M 个新的 table files，M 默认为 2，这样来保证创建了 E/M 个分区，E 为新产生的 tables 的数量。 Rebuilding REMIXes 分区存储布局可以通过利用较强的空间局部性来有效最小化真实负载 compaction cost，具体的，RemixDB 可以在几个分区中吸收大多数更新，在接受少量的分区中的 compaction 可以被避免。但是如果负载没有空间局部性，许多分区不可避免地必须执行压缩并进行少量的更新。Tiered Compaction 可以最小化这些分区的写操作，但是在一个分区中重建 REMIX 仍然需要读取已经存在的 tables。在我们的实现中，RemixDB 利用了现有的 REMIX 索引并使用了一个搞笑的合并算法来最小化重建过程的 I/O 开销。 当重建分区的 REMIX，现有的表已经被 REMIX 索引，这些表可以被视为一个 Sorted Run，相应的，重建过程相当于归并排序两个 sorted runs，一个来自现有的数据，另一个来自新数据，当现有的这个 sorted run 比新的大很多的时候，generalized binary merging algorithm 算法通过使用小顶堆实现了更少的 Key 比较，相比于归并排序、算法根据两个 sorted run 之间的 size ratio 估计出了下一次合并点的 location 信息，并在相邻的范围中进行搜索。在RemixDB中，我们通过使用锚键来定位包含合并点的目标段，最后在该段中应用二叉搜索。在这个过程中，访问锚定键不会产生任何I/O，因为它们存储在 REMIX 中。在目标段中进行二分搜索，最多读取 log2Dlog_2Dlog2​D 键来找到合并点。现有表的所有 run 选择器和游标偏移量都可以从现有的 REMIX 中派生出来，而不需要任何 I/O。要为新的分段创建锚定键，我们最多需要在新的排序视图上对每个分段访问一个键。 重建 REMIX 的读开销被分区中的所有表的大小限制，重建过程导致对现有表的读 I/O，为了降低 WA 并提升未来的读性能。构建 REMIX 是否是成本高效的取决于想要节省多少个写 I/O 以及未来想提升多少读性能。实际上，在 SSDs 中的写操作通常比读操作更慢而且可能对设备造成永久的破坏。因此读相比于写来说更经济，特别是有空闲 I/O 带宽的系统。在期望具有弱空间局部性的密集写操作的系统中，采用多层 tiered 压缩策略或延迟在单个分区中重建 REMIX 可以降低以拥有更多级别已排序视图为代价的重建成本。调整 REMIX 与不同的存储布局超出了本文的范围。我们对 RemixDB 在不同工作负载下的重建成本进行了实证评估 Evaluations 实验搭建： 在每个实验中，我们首先创建一组 H 个表文件(1≤H≤16)，它们类似于RemixDB中的一个分区或LSM-tree中的一个级别，使用 tiered compaction。每个表文件包含 64MB 的KV-pairs，其中键和值的大小分别为 16B 和 100B。当 H≥2 时，KV 对可以用两种不同的模式分配到表中: Weak locality：每个键被分配给一个随机选择的表，这提供了弱访问局域性，因为逻辑上连续的键经常驻留在不同的表中 Strong locality：每64个逻辑上连续的键被分配给一个随机选择的表，这提供了强访问局域性，因为一个范围查询可以从几个表中检索多个连续的键 D=32，段内二分查找可以开启或关闭，对应图中的 full/partial 单次 Seek：Merge Iterator 在只有一个 Table 的时候表现更好，因为和 REMIX 需要执行相同次数的二分查找，但是 REMIX 需要动态地计算出现次数，并将迭代器从段的开头移动到一个键进行比较，开销相对更大。随着 Tables 数量增加，REMIX 的性能优势渐渐体现出来了。 Seek+Next50 整体性能低于 Seek 因为要拷贝数据到 Buffer。开启段内二分查找影响不大，因为 next 操作主要影响执行时间，在段中对寻道操作的线性扫描预热了块缓存，这使得未来的操作更快。 点查询：当少于 14 个 tables 的时候，REMIX 都不如带布隆过滤器的点查询，因为搜索可以只需要检查Bloom过滤器来有效地缩小到一个表文件。其次，在SSTable中搜索比在管理更多的键的 REMIX 中要快。在最坏的情况下，REMIX的吞吐量比Bloom过滤器(有3个表)低20%。不出所料，在没有Bloom过滤器的情况下，使用两个以上sstable的搜索速度会慢得多 强局部性的时候在 range query 的结果上差距不大，通常，改进的局部性允许更快的二分搜索，因为在这种情况下，最后几个键比较通常可以使用相同数据块中的键。但是，合并迭代器的吞吐量仍然很低，因为密集的键比较操作占据了搜索时间。带有部分二分搜索的 REMIX 比完全二分搜索的改进更多，是因为局部性的提升减少了在目标 segment 里 scan 的开销，导致更少的缓存缺失。 REMIX 点查询性能也得到了改善，因为强大的局域性加快了底层查找操作的速度。同时，Bloom 过滤器的结果保持不变，因为搜索代价主要由假阳性率和对单个表的搜索代价决定。因此，当包含超过9个表时，remix的性能可以超过Bloom过滤器。 8 个 tables，不同的 segment size，如果关闭了段内二分查找，只 seek 的负载下， D 的大小影响最大，这是因为段中的线性扫描显著增加了较大的 D 值的成本。由于段内的随机访问速度较慢，较大的段大小仍然会导致较高的开销。在 Seek+Next50 实验中，数据复制在执行时间中占主导地位，使用不同的 D 时没有显著差异 不同的 range query 展现出了大约相同的趋势，实验中顺序加载相应的数据，所以在每种存储系统中都不会有重叠的文件，也就意味着 seek 操作只会访问一个文件。然而一个合并迭代器必须检查每个 sorted run 即便他们没有重叠的键范围，所以如果有多个 sorted runs 的话检索每个 run 的操作将占据 seek 时间的很大一部分。具体的，每一个在 LevelDB 和 RocksDB 中的 L0 表都是一个独立的 run，但是每一层 Li 又只包含一个 run。PebblesDB 允许一个 Level 有多个 runs，话虽如此，LevelDB 的性能至少比RocksDB高出2倍，尽管它们都使用了 Leveled 压缩。观察发现 RocksDB 在顺序加载过程中，在 L0 层保留了好几个 tables，总共八个，而没有把这些文件移动到更深的层次。相反，LevelDB 直接把这个 Table 推向了更深的层次 （L2 或 L3），如果这个 Table 和别的 Tables 不重叠的话，从而让 LevelDB 的 L0 层总是为空。因此，RocksDB 中的一个 seek 操作需要动态地排序-合并至少 12 个 sorted run，而这个数字在 LevelDB 中只有 3 或 4。 查找性能对访问的局部性是敏感的。较弱的局部性会增加搜索路径上的CPU和I/O开销。在每个特定 value 大小的实验中，采用 uniform 访问模式的吞吐量比顺序访问低50%左右。同时，顺序访问的性能对值大小不太敏感，因为内存复制成本不显著 另外的实验测试了不同的存储总量大小下 query 长度对范围扫描的性能影响。每个实验以随机的顺序将具有 120B 值大小的固定大小的 KV 数据集加载到存储中，然后使用 Zipfian 访问模式使用四个线程执行范围扫描。REMIX 表现最好，但随着范围查询的长度增加，性能差异逐渐变小，这是因为长范围的查询在每个 sorted run 上展示出了顺序访问的特性，也就意味着在 scan 过程中更多的数据已经被预取，同时内存拷贝也给每个存储增加了固定的开销。 还有观察发现 LevelDB 在 256GB 的时候性能下降到和 RocksDB 一样。因为实验中配置了 4GB 的 block cache，缓存丢失导致大量的 I/O 占据了查询时间。与此同时 RocksDB 展示出了极大的计算开销，因为在数据量较小的时候 L0 层包含了太多 Tables。在更大数据量的存储中，过多的 I/O 掩盖了开销。与此同时 REMIX 保证了最好的访问局部性，因为导致了最少的随机访问和缓存丢失。 单线程随机插入 256GB 的数据，数据集有 20 个 KV 对，value 大小是 120B，负载有 uniform 的访问特征，代表着最差的情况。REMIX 和 PebblesDB 吞吐量最高，因为使用了利于写的 tiered compaction 策略，对应写放大为 4.99、9.26。比 LevelDB/RocksDB 小了很多。RocksDB 和 RemixDB 有更多的读 I/O，因为 RocksDB 使用了四个线程进行 Compaction 来充分利用 SSD 带宽，因为 Block Cache 和 Page Cache 的低效利用导致读 I/O 较多，LevelDB 只支持单线程压缩。尽管 RemixDB 读 I/O 多于 RocksDB 但是总的 I/O 还是小于 RocksDB 的。综上所述，RemixDB 以增加读 I/O 为代价实现了低 WA 和高写吞吐量。 顺序的负载展现出最高的吞吐，因为每一轮压缩只影响很少的分区。写 I/O 主要包含日志和创建新的表，大约是用户写入量的 2 倍。读 I/O 主要是重建 REMIX 和数据本身的大小基本相同。相比之下，两个倾斜负载，在 Memtable 中重复 overwrites 导致写 I/O 大幅减少，但是会创建分散的更新，将导致 Memtable 中更新较慢以及更多的分区被 Compacted。Zipfian-Composite 有更差的空间局部性比 Zipfian，导致了更大的 Compaction I/O 开销。 YCSB 中，RemixDB 除了负载 D 表现都比其他的好。也就是 95% 的读请求访问最近的 5% 的写入，这种访问模式有很强的局部性，大部分请求直接是由存储中的 Memtables 来处理的，单线程压缩导致的缓慢插入阻碍了 LevelDB 的性能(1.1 MOPS)。 即便 REMIX 没有显示出相比于布隆过滤器足够的优势，但是在 YCSB-B，C 中比其他表现好，这两种负载下点查询占据大部分。是因为点查询在多级的 LSM 中在搜索路径上选择对应的 tables 的开销很大，具体而言就是每一个 L0 的 table，大约有两个键比较用于检查 seek 键是否被表覆盖，如果在 L0 中没找到，在每一个更深的层次中执行二分查找，直到 Key 被找到。布隆过滤器的大小大约 600KB，对于一个 64MB 的表而言，访问一个布隆过滤器会导致大约七次内存随机访问，在一个很大的存储中将导致严重的缓存缺失。REMIX 索引的分区形成了一个全局有序视图，基于此的二分查找可以很快得到应答。 ","link":"https://blog.shunzi.tech/post/REMIX/"},{"title":"C++ STL","content":" C++ STL 的相关资料记录 持续更新 STL C++ STL（标准模板库）是一套功能强大的 C++ 模板类，提供了通用的模板类和函数，这些模板类和函数可以实现多种流行和常用的算法和数据结构，如向量、链表、队列、栈。 参考链接 [1] 菜鸟教程：C++ STL 教程 [2] CSDN - C++中STL用法超详细总结 [3] Github - 《C++ Primer中文版（第5版）》笔记 STL 容器 顺序容器 Sequential Containers 概览 类型 特性 vector 可变大小数组。支持快速随机访问。在尾部之外的位置插入/删除元素可能很慢 deque 双端队列。支持快速随机访问。在头尾位置插入/删除速度很快 list 双向链表。只支持双向顺序访问。在任何位置插入/删除速度都很快 forward_list 单向链表。只支持单向顺序访问。在任何位置插入/删除速度都很快 array 固定大小数组。支持快速随机访问。不能添加/删除元素 string 类似vector，但用于保存字符。支持快速随机访问。在尾部插入/删除速度很快 forward_list和array是C++11新增类型。与内置数组相比，array更安全易用。forward_list没有size操作。 容器选择原则 除非有合适的理由选择其他容器，否则应该使用vector。 如果程序有很多小的元素，且空间的额外开销很重要，则不要使用list或forward_list。 如果程序要求随机访问容器元素，则应该使用vector或deque。 如果程序需要在容器头尾位置插入/删除元素，但不会在中间位置操作，则应该使用deque。 如果程序只有在读取输入时才需要在容器中间位置插入元素，之后需要随机访问元素。则： 先确定是否真的需要在容器中间位置插入元素。当处理输入数据时，可以先向vector追加数据，再调用标准库的sort函数重排元素，从而避免在中间位置添加元素。 如果必须在中间位置插入元素，可以在输入阶段使用list。输入完成后将list中的内容拷贝到vector中。 不确定应该使用哪种容器时，可以先只使用vector和list的公共操作：使用迭代器，不使用下标操作，避免随机访问。这样在必要时选择vector或list都很方便。 关联容器 关联容器支持高效的关键字查找和访问操作。2个主要的关联容器（associative-container）类型是map和set。 map中的元素是一些键值对（key-value）：关键字起索引作用，值表示与索引相关联的数据。 set中每个元素只包含一个关键字，支持高效的关键字查询操作：检查一个给定关键字是否在set中。 标准库提供了8个关联容器，它们之间的不同体现在三个方面： 是map还是set类型。 是否允许保存重复的关键字。 是否按顺序保存元素。 允许重复保存关键字的容器名字都包含单词multi；无序保存元素的容器名字都以单词unordered开头。 使用关联容器（Using an Associative Container） map类型通常被称为关联数组（associative array）。 从map中提取一个元素时，会得到一个pair类型的对象。pair是一个模板类型，保存两个名为first和second的公有数据成员。map所使用的pair用first成员保存关键字，用second成员保存对应的值。 // count the number of times each word occurs in the input map&lt;string, size_t&gt; word_count; // empty map from string to size_t string word; while (cin &gt;&gt; word) ++word_count[word]; // fetch and increment the counter for word for (const auto &amp;w : word_count) // for each element in the map // print the results cout &lt;&lt; w.first &lt;&lt; &quot; occurs &quot; &lt;&lt; w.second &lt;&lt; ((w.second &gt; 1) ? &quot; times&quot; : &quot; time&quot;) &lt;&lt; endl; set类型的find成员返回一个迭代器。如果给定关键字在set中，则迭代器指向该关键字，否则返回的是尾后迭代器。 pair pair定义在头文件utility中。一个pair可以保存两个数据成员，分别命名为first和second。 pair&lt;string, string&gt; anon; // holds two strings pair&lt;string, size_t&gt; word_count; // holds a string and an size_t pair&lt;string, vector&lt;int&gt;&gt; line; // holds string and vector&lt;int&gt; pair的默认构造函数对数据成员进行值初始化。 STL 常用算法 搜索 二分查找 upper_bound() //查找[first, last)区域中第一个大于 val 的元素。 ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last, const T&amp; val); //查找[first, last)区域中第一个不符合 comp 规则的元素 ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last, const T&amp; val, Compare comp); lower_bound() //在 [first, last) 区域内查找不小于 val 的元素 ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last, const T&amp; val); //在 [first, last) 区域内查找第一个不符合 comp 规则的元素 ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last, const T&amp; val, Compare comp); ","link":"https://blog.shunzi.tech/post/cpp-std/"},{"title":"HashKV: Enabling Efficient Updates in KV Storage via Hashing","content":" HashKV: Enabling Efficient Updates in KV Storage via Hashing http://adslab.cse.cuhk.edu.hk/software/hashkv/ ATC18 &amp; TOS19 个人总结 Abstract 持久键值(KV)存储主要构建在日志结构的合并树(LSM)上，以获得较高的写性能，但是LSM树存在固有的较高的I/O放大问题。KV分离通过在LSM-tree中只存储键和在单独存储中的值来减轻I/O放大。然而，当前的KV分离设计在更新密集型工作负载下仍然是低效的，因为它在值存储方面的垃圾收集(GC)开销很大。我们提出了HashKV，它的目标是在更新密集型工作负载下，在KV分离的基础上提高更新性能。HashKV使用基于哈希的数据分组，它确定地将值映射到存储空间，从而提高更新和GC的效率。我们通过简单但有用的设计扩展进一步放宽了这种确定性映射的限制。通过大量的实验，我们将HashKV与最先进的KV存储进行了比较，并表明与当前的KV分离设计相比，HashKV实现了4.6×吞吐量和减少了53.4%的写流量。 Introduction 持久性 KV 存储应用广泛，用于存储海量结构化数据：Bigtable, Dynamo, Atlas。虽然现实中的KV存储工作负载主要是读密集型的(例如，在Facebook的Memcached工作负载中，Get/Update的比率可以达到30:1)，但更新密集型的工作负载在许多存储场景中也占主导地位（例如,雅虎报告其低延迟工作负载越来越多地从读转移到写） 现代的针对写操作优化的 KV 存储大多是基于 LSM 树，思想源于最初的 LSF，LSM-tree设计不仅通过避免小的随机更新(这也有害于固态硬盘(SSD)的寿命)来提高写性能，而且通过在每个节点中保留已排序的KV对来提高范围扫描性能。但是写放大严重，读放大更严重。 已有方案：WiscKey 采用 KV 分离来减少 Compaction 操作带来的影响，但是我们发现 KV 分离还是无法在更新密集型工作负载下完全实现高性能。 根本原因在于用于值存储的循环日志需要频繁的垃圾收集(GC)，以从被删除或被新更新取代的KV对中回收空间。然而，由于循环日志的两个限制，GC开销实际上是昂贵的。 首先，循环日志保持严格的GC顺序，因为它总是在日志的开始处执行GC，即最近写入的KV对所在的位置。这可能会导致大量不必要的数据重定位(例如，当最近写的KV对仍然有效时)。 其次，GC 操作需要查询LSM-tree，检查每个KV对的有效性。这些查询具有很高的延迟，特别是当LSM-tree在大工作负载下变得相当大时。 所以提出了 HashKV，为更新密集型工作负载量身定制的高性能KV存储。HashKV建立在KV分离的基础上，并使用一种新的基于哈希的数据分组设计来存储值。其思想是将值存储划分为固定大小的分区，并通过散列其键确定地将每个写入KV对的值映射到一个分区。基于哈希的数据分组支持基于确定性映射的轻量级更新。更重要的是，它显著地减轻了GC开销，因为每个GC操作不仅具有选择一个分区来回收空间的灵活性，而且还消除了为了检查KV对的有效性而对LSM-tree的查询。 另一方面，基于哈希的数据分组的确定性限制了KV对的存储位置。因此，我们提出了三种新的设计扩展来放松基于哈希的数据分组的限制： (i) 动态预留空间分配，在给定大小限制的情况下，动态分配预留空间给额外的写操作 (ii) 热感知，受现有SSD设计的启发，将热KV和冷KV对的存储分开，以提高GC效率 (iii) 有选择性的KV分离，在LSM-tree中保持较小的KV对的完整，以简化查找 基于 LevelDB 实现了 HashKV 的原型，通过测试实验表明，在更新密集型的工作负载下，HashKV 的吞吐量达到 4.6×，与 wisckey 中的循环日志设计相比，写流量减少了 53.4%。此外，在各种情况下，与现代KV存储(如LevelDB和RocksDB)相比，HashKV通常能够实现更高的吞吐量和更少的写流量。 我们的工作只是增加 KV 分离与一个新的Value管理设计的案例。虽然HashKV的密钥和元数据管理现在建立在LevelDB上，但它也可以采用带有新的LSM树设计的其他KV存储。在KV分离条件下，哈希KV如何影响各种基于lsm树的KV存储的性能是未来研究的重点。 Motivation 这一章节主要介绍 LevelDB 的读写放大问题，以及 WiscKey 这种 KV 分离方案带来的改善，同时分析普通的 KV 分离无法在写密集负载中实现高性能的原因。 LevelDB 原理不过多介绍，重点是读写放大问题。 首先，压缩过程不可避免地产生额外的读写。在最坏的情况下，要将一个SSTable从Li−1合并到Li，它需要读取并排序10个SSTable，然后写回所有SSTable。先前的研究表明，LevelDB可以有至少50倍的总体写放大，因为在大的工作负载下，它可能会触发不止一次的压缩，将一个KV对向下移动多个级别。 查找操作可以在多个级别搜索一个KV对，并导致多个磁盘访问。原因是，每个级别的搜索都需要读取相关SSTable中的索引元数据和Bloom过滤器，虽然使用了Bloom过滤器，但它可能会引入误报。在这种情况下，即使KV对实际上不存在，仍然需要从磁盘读取SSTable。因此，每次查找通常会导致多次磁盘访问。这种读放大在大的工作负载下会进一步恶化，因为LSMtree会逐级累积。测量结果表明，在最坏的情况下，读放大可达300倍以上 KV Separation WiscKey 原理 KV分离，由 Wisckey 提出，对键和值的管理进行解耦，以减轻写和读的放大。其基本原理是，在 LSM-tree 中存储值对于索引来说是不必要的。因此，在LSM-tree中，wisckey 只存储键和元数据(例如键/值的大小、值的位置等)，而将值存储在一个单独的仅追加的循环日志vLog 中。KV 分离有效地减轻了LevelDB的写和读放大，因为它显著地减少了 lsm 树的大小，从而同时减少了压缩和查找开销。 由于vLog遵循日志结构设计，KV分离在vLog中实现轻量级的垃圾收集(GC)是至关重要的，即在有限的开销下从无效值中回收空闲空间。具体来说，wisckey跟踪的是vLog头部和vLog尾部，分别对应于vLog的结束和开始。它总是向vLog头插入新的值。当它执行GC操作时，它从vLog尾部读取一大块KV对。它首先查询lsm树，查看每个KV对是否有效。然后丢弃无效KV对的值，并将有效值写回vLog头。它最后更新LSM-tree以获取有效值的最新位置。为了在GC期间支持有效的LSM-tree查询，wisckey还将相关的键和元数据与值一起存储在vLog中。请注意，vLog经常为减少GC开销而提供额外的预留空间。 Limitations 虽然KV分离降低了压缩和查找开销，但我们认为它受到了vLog中大量GC开销的影响。另外，如果预留空间有限，GC开销会变得更加严重。原因有两方面: 首先，由于它的循环日志设计，vLog只能从它的vLog尾部回收空间。这个约束可能会导致不必要的数据移动。特别是，真实世界的KV存储往往表现出很强的局部性，其中一小部分的热KV对经常更新，而其余的冷KV对只接收到很少甚至没有更新。在vLog中保持严格的顺序不可避免地会多次重新定位冷KV对，从而增加GC开销。 此外，每个GC操作都查询LSM-tree，以检查vLog尾部chunk中每个KV对的有效性。由于KV对的键可能分散在整个LSM-tree中，查询开销很高，并增加了GC操作的延迟。虽然KV分离已经减少了LSM-tree的大小，但是LSM-tree在大的工作负载下仍然是相当大的，这加剧了查询成本。 分别总结一下这两个问题： GC 过程中需要对有效数据进行拷贝，热数据进行 GC 是必须的，确保频繁更新的数据的老旧版本的空间能够及时回收，冷数据 GC 其实意义不大，反而因为 GC 过程中的数据迁移引入了开销。GC 最好的情况就是遇到需要删除和回收的数据，因为在 WiscKey 这样的设计背景下，无需进行回收的数据就肯定会进行数据的拷贝来回收对应的这一段空间（为了避免碎片或者说hole 的产生，因为碎片相应地容易引发随机 I/O） GC 操作每个都需要检查数据的有效性，需要去 LSM 树中进行查询，相当于 GC 操作引入了额外的查询开销，并增加了 GC 操作的延迟（这个是因为 LSM 树本身读的一部分原因，造成了读操作需要读很多层，延迟就比较高，本来 LSM 树读性能就不是特别好） 那 KV 分离为啥是用 LSM + Value Log 的方案，而不是 B tree + Value Log 呢？思想就是找一个小写读写性能比 LSM 相对更好的数据结构来代替 LSM 存储 Key。Key 大小甚至可以固定，使用一些定长编码的策略。换一个更高效的索引结构似乎就能解决查询延迟的问题？但仿佛又会牺牲写 实验验证 为了验证KV分离的局限性，我们实现了一个基于vLog的KV存储原型(见§3.8)，并评估其写入放大。我们考虑两个阶段:加载和更新。在加载阶段，将40GiB的1-KiB KV对插入到初始为空的vLog中;在更新阶段，我们基于Zipf分布，Zipfian常数为0.99，发起了40GiB对现有KV对的更新。我们为视频日志提供40GiB的空间，并额外预留30% (12GiB)的空间。我们也在原型中禁用了写缓存(见§3.2)。图2显示了加载和更新阶段vLog的写放大结果，即由于插入或更新导致的总设备写大小与实际写大小的比例。 为了进行比较，我们还考虑两个现代KV存储，LevelDB和RocksDB，基于它们的默认参数。在 Load 阶段，vLog有足够的空间容纳所有KV对，不触发GC，由于KV分离，它的写入放大只有1.6×。但是，在更新阶段，更新会填满保留的空间并开始触发GC。我们可以看到，vLog的写放大倍数为19.7×，接近LevelDB (19.1×)，高于RocksDB (7.9×)。 为了减轻vLog的GC开销，一种方法是将vLog划分成段，并根据成本-收益策略或其变体 [如下参考文献 ] 选择最佳的候选段来减少GC开销。但是，热KV和冷KV对仍然可以在vLog中混合在一起，所以为GC选择的段可能仍然包含冷KV对，并且不必要地移动。 [SOSP1997 Improving the Performance of Log-Structured File Systems with Adaptive Methods] [TOCS1992 The Design and Implementation of a Log-structured File System] [FAST14 Logstructured Memory for DRAM-based Storage] 为了解决热数据和冷数据的混合问题，更好的方法是像SSD设计中那样执行热-冷数据分组，其中，我们将热KV和冷KV的存储分成两个区域，并分别对每个区域应用GC(更多的GC操作将应用于热KV的存储区域)。然而，直接实现热-冷数据分组不可避免地增加了KV分离过程中的更新延迟。由于KV对可能存储在热或冷数据区，每次更新都需要首先查询LSM-tree以获得KV对的确切存储位置。因此，我们工作的一个关键动机是在不使用 LSM 树查找的情况下启用热状态识别。 Design HashKV是一个持久的KV存储，专门针对更新密集型工作负载。在KV分离的基础上改进了 Value 存储的管理，实现了高更新性能。它支持标准的KV操作:PUT(即写入一个KV对)、GET(即检索一个键的值)、DELETE(即删除一个KV对)和SCAN(即检索一个键范围的值)。 Main Idea 在KV分离之上，HashKV引入了几个核心设计元素来实现高效的 Value 存储管理： Hash-based data grouping: 回想一下，vLog在值存储方面会招致大量的GC开销。相反，HashKV通过散列相关的键将值映射到值存储中固定大小的分区中。这种设计实现了: (i)分区隔离，在这种情况下，所有版本的值更新都必须被写入到相同的分区中， (ii)确定性分组，在这种情况下，一个值应该存储在哪个分区中是通过哈希来确定的。我们利用这种设计来实现灵活和轻量级的GC Dynamic reserved space allocation: 由于我们将值映射到固定大小的分区中，一个挑战是一个分区接收的更新可能多于它能容纳的更新。HashKV通过在值存储中分配部分保留空间，允许分区动态增长，超过其大小限制。 Hotness awareness：由于确定性分组，分区可能会被来自热KV对和冷KV对的混合值填满，在这种情况下，GC操作会不必要地读取和回写冷KV对的值。HashKV使用标记方法将冷KV对的值重新定位到不同的存储区域，并将热KV对和冷KV对分开，这样我们就可以只对热KV对使用GC，避免重复复制冷KV对。 Selective KV separation：HashKV通过其值大小来区分KV对，小的KV对可以直接存储在LSM-tree中，而不需要分离KV。这节省了访问小KV对的LSM-tree和值存储的开销，而在LSM-tree中存储小KV对的压缩开销是有限的。 HashKV维护一个单一的LSM-tree来进行索引(而不是像值存储中那样对LSM-tree进行哈希分区)，以保持键的顺序和范围扫描性能。由于基于哈希的数据分组将KV对分散到值存储中，因此会导致随机写操作;相反，vLog使用日志结构的存储布局来维护顺序写入。我们的HashKV原型(见§3.8)利用多线程和批处理写来限制随机写开销。 Storage Management 将 value store 的逻辑地址空间划分成固定大小的单元，称之为 main segments，此外，它还过度规定了保留空间的固定部分，将其再次划分为固定大小的单元，称为 log segments。注意，主段和日志段的大小可能不同;缺省情况下，分别设置为64MiB和1MiB。 对于每一个KV对的插入或更新，HashKV将其密钥散列到一个主要段中。如果主段没有被填满，HashKV会把这个值添加到主段的末尾，以一种日志结构的方式存储这个值;另一方面，如果主段已满，HashKV会动态分配一个空闲的日志段，以日志结构的方式存储额外的值。同样，如果当前日志段已满，它将进一步分配额外的空闲日志段。我们统称一个主段和它的所有关联的日志段为段组。 此外，HashKV更新LSM-tree以获得最新的值位置。为了跟踪段组和段的存储状态，HashKV使用一个内存中的全局段表来存储每个段组的当前结束位置，以供后续的插入或更新，以及与每个段组相关的日志段列表。我们的设计确保了每个插入或更新都可以直接映射到正确的写位置，而无需在写路径上执行LSM-tree查找（直接 HASH 得到对应的分区，然后在分区中进行追加写），从而实现了较高的写性能。另外，与同一个键相关联的值的更新必须转到同一个段组，这简化了GC。为了容错，HashKV checkpoints 段表到持久存储（内存中的段表数据恢复方式）。 为了便于GC, HashKV还存储键和元数据(例如,键/值大小)，和 WiscKey 一样和值存储在一起(参见图3)。这使GC操作能够在 scan value store 时，快速识别关联到某一个值的键。然而，我们的GC设计与 WiscKey 使用的 vLog 有本质上的不同。 为了提高写性能，HashKV在内存中保存了一个写缓存，以降低可靠性为代价来存储最近写的KV对。如果在写缓存中找到了要写的新KV对的键，HashKV就直接就地更新缓存的键的值，而不用向LSM-tree和值存储区发出写操作。它还可以从写缓存中返回KV对用于读取。如果写缓存已满，HashKV将所有缓存的KV对刷新到lsm 树和值存储中。注意，写缓存是一个可选组件，可以出于可靠性考虑禁用它。 HashKV 通过将冷值保存在单独的冷数据日志 cold data log 中来支持热感知(见§3.4)。它还通过在 write journal 和 GC journal 中跟踪更新来解决崩溃一致性问题(见3.7)。 Garbage Collection (GC) HashKV要求GC回收值存储中无效值所占用的空间。在HashKV中，GC以段组为单位进行操作，当保留空间中的空闲日志段用完时将触发GC。在较高的级别上，GC操作首先选择一个候选段组，并识别组中所有有效的KV对(即最新版本的KV对)。然后，它以日志结构的方式将所有有效的KV对写回主段，或者在需要时写到附加的日志段。它还释放任何未使用的日志段，这些日志段以后可以被其他段组使用。最后，它更新LSM-tree中最新的值位置。这里，GC操作需要解决两个问题: (i) 应该为GC选择哪个段组; (ii) GC操作如何快速识别所选段组中的有效KV对。 不同于vLog，它要求GC操作遵循严格的顺序，HashKV可以灵活地选择执行GC的段组。目前采用的是贪婪的方法，选择写量最大的段组。我们的基本原理是，所选的段组通常保存有许多更新，因此有大量写操作的hot KV对。因此，为GC选择这个段组可能会回收最多的空闲空间。为了实现贪婪的方法，HashKV跟踪内存段表中每个段组的写量(见§3.2)，并使用堆 heap 来快速识别哪个段组收到的写量最大 为了检查所选段组中KV对的有效性，HashKV顺序扫描段组中的KV对，而不查询LSM-tree(注意，它还检查写缓存，以查找段组中最新的KV对)。由于KV对以日志结构的方式写入段组，所以必须按照更新的顺序依次放置KV对。对于一个有多个版本更新的KV对，最接近段组末尾的版本必须是最新的版本，并且对应于有效的KV对，而其他版本是无效的。因此，每个GC操作的运行时间只取决于需要扫描的段组的大小。相反，vLog中的GC操作从vLog尾部读取一大块KV对(见§2.2)。它查询LSM-tree(基于与值一起存储的键)以获取每个KV对的最新存储位置，以检查KV对是否为有效的。在大的工作负载下，查询LSM-tree的开销会变得很大。 在段组的GC操作期间，HashKV构造一个临时内存哈希表(按键索引)来缓冲在段组中找到的有效KV对的地址。由于键和地址的大小通常比较小，并且一个段组中的 KV 对数量有限，所以哈希表的大小有限，可以全部存储在内存中。 Hotness Awareness 冷热数据分离提高了日志结构存储中的GC性能。事实上，当前基于散列的数据分组设计实现了某种形式的冷热数据分离，因为对热KV对的更新必须散列到同一段组，而我们当前的GC策略总是选择可能存储热KV对的段组。然而，不可避免的是，一些冷KV对被散列到为GC选择的段组中，导致不必要的数据重写。因此，充分实现冷热数据分离，进一步提高GC性能是一个挑战。 HashKV通过标记方法放松了基于哈希的数据分组的限制(见图4)。具体来说，当HashKV对一个段组执行GC操作时，它将段组中的每个KV对划分为热的或冷的。目前，我们将自最后一次插入以来至少更新过一次的KV对视为热的，否则是冷的(可以使用更精确的热-冷数据识别方法） 对于热KV对，HashKV仍然通过散列将它们的最新版本写回同一个段组。 对于冷KV对，它现在将它们的值写入一个单独的存储区域，并在段组中只保留它们的元数据(即没有值)。此外，它在每个冷KV对的元数据中添加一个标记，以表明它在段组中的存在。 因此，如果一个冷KV对后来被更新，我们直接从标签(不查询LSM-tree)知道冷KV对已经被存储，因此我们可以根据我们的分类策略将其视为热；标记的KV对也将失效。最后，在GC操作结束时，HashKV更新LSM-tree中最新的值位置，这样冷KV对的位置（LSM存储的地址）就指向单独的区域。 通过标记，HashKV避免了将冷KV对的值存储在段组中，并在GC期间重写它们。而且，标记只在GC期间触发，不会给写路径增加额外的开销。目前，我们为冷KV对实现了单独的存储区域，作为值存储中的一个仅追加的日志(称为冷数据日志)，并像vLog一样对冷数据日志执行gc（这里就和 WiscKey 一样了）。如果冷KV对很少被访问，冷数据日志也可以放在容量更大的二级存储中(如硬盘)。 其实这里有三个问题： 热数据特别少，冷数据特别多的时候，空间分配会是个问题。 所有的数据第一次访问的时候都是冷数据，第二次访问就成了热数据了，要是冷数据变热数据之间的时间较长，中间进行了 GC，就会导致数据拷贝（至少一次 segment group 到 cold Value Log），然后再引入一次 Cold Value Log 中的 GC 操作，这时候就退化成了 WiscKey 的 GC，需要检查 LSM Tree。 这种过于简单的冷热数据识别，应对不了热数据变冷的问题，热数据变冷会一直在 segment group 中进行拷贝，还是会一直进行数据的拷贝，还是退化成了 WiscKey 的 GC。（这种简单的冷热数据识别只能对 只访问一次的冷数据为主要组成部分的负载 产生比较好的效果） 本质是将冷数据只进行一次拷贝操作，减少了冷数据的数据迁移，同时验证冷数据的有效性直接通过 tag 验证，不用查询 LSM 树，热数据的有效性的话还是需要通过内存中临时内存哈希表来进行验证。 我们评估了热度感知对HashKV更新性能的影响。我们考虑两个Zipfian常量，0.9和0.99，以捕获工作负载中的不同偏度。图10显示了禁用和启用热感知功能时的结果。当启用热感知功能时，更新吞吐量增加了113.1%和121.3%，而对于Zipfian常数0.9和0.99，写大小分别减少了42.8%和42.5%。 Selective KV Separation HashKV支持具有一般值大小的工作负载。我们的理论基础是，KV分离降低了压缩开销，特别是对于大型KV对，但它对小型KV对的好处是有限的，而且它会导致访问 LSM 树和值存储的额外开销。因此，我们提出了选择KV分离的方法，即对较大值的KV对仍然采用KV分离，而较小值的KV对则全部存储在LSM-tree中。选择KV分离的一个关键挑战是选择区分小尺寸和大尺寸KV对的KV对大小阈值(假设键大小不变)。我们认为选择取决于部署环境。在实践中，我们可以对不同的值大小进行性能测试，看看什么时候选择性KV分离的吞吐量增益显著。 我们观察到，由于在KV分离下存储的小KV对的高更新开销，当小KV对的比例较高时，此时工作负载下，KV分离的性能增益更高。同时 40B-1KB 的组合相比于 40B-4KB 的组合优化效果更明显。 Range Scans 使用LSM-tree进行索引的一个关键原因是它对范围扫描的有效支持。由于LSMtree按键存储和排序KV对，因此它可以通过顺序读取返回一系列键的值。然而，KV分离现在将值存储在单独的存储空间中，因此它会招致额外的值读取。在HashKV中，这些值分散在不同的段组中，因此范围扫描将触发许多随机读取，从而降低性能。HashKV目前利用预读机制通过将值预取到页面缓存中来加速范围扫描。对于每个扫描请求，HashKV遍历LSM-tree中排序键的范围，并(通过posix fadvise)向每个值发出预读请求。然后读取所有值并返回排序的KV对。 在KV对大小上，HashKV具有与vLog相似的扫描性能。然而，对于256-B和1-KiB KV对，HashKV的扫描吞吐量分别比LevelDB低70.0%和36.3%，主要是因为HashKV需要向LSM-tree和值存储发出读操作，而且通过随机读操作从值存储中检索小值的开销也很大。然而，对于4KiB或更大的KV对，HashKV的性能优于LevelDB，例如，4KiB KV对的性能为94.2%。 注意，预读机制(见§3.6)是使HashKV实现高范围扫描性能的关键。例如，与没有预读的情况相比，256-B KV对的HashKV的范围扫描吞吐量增加了81.0% Crash Consistency 当HashKV问题写入持久性存储时，可能会发生崩溃。HashKV基于元数据日志记录解决崩溃一致性问题，主要关注两个方面: (i) 刷新写缓存 (ii) GC操作。 刷新写缓存涉及将KV对写入值存储并更新LSM-tree中的元数据。HashKV维护一个写日志 write journal 来跟踪每次 flush 操作。它在刷新写缓存时执行以下步骤： (i) 将缓存的KV对刷新到值存储中; (ii) 在 write journal 中追加写入元数据更新; (iii) 在 journal end 写入一个提交记录; (iv) 更新 LSM-tree 中的 keys 和元数据; (v) 在日志中标记 flush 操作为 free 状态( free 的日志记录可以稍后回收)。 如果在步骤(iii)完成后发生崩溃，HashKV在写日志中回放更新，并确保LSMtree和值存储是一致的。 GC 操作中崩溃一致性的处理是不同的，因为它们可能会覆盖现有的有效 KV 对。因此，我们还需要保护现有的有效 KV 对，防止在GC 期间崩溃。HashKV 维护一个GC日志来跟踪每个GC操作。 在GC操作中识别出所有有效的KV对后，它将执行以下步骤: (i) 将被覆盖的有效KV对以及元数据更新一起添加到GC日志中; (ii) 将所有有效KV对写回段组; (iii) 更新LSM-tree中的元数据; (iv) 在日志中标记GC操作对应的记录为 free 状态。 我们研究了崩溃一致性机制对HashKV性能的影响。表1显示了结果。当启用了崩溃一致性机制时，P3阶段的HashKV的更新吞吐量减少了6.5%，总写大小增加了4.2%，这表明崩溃一致性机制的影响仍然有限。请注意，我们在运行时通过代码注入和意外终止来使HashKV崩溃，从而验证崩溃一致性机制的正确性。 Implementation Details 基于 LevelDB v1.20 实现 Storage organization: 我们目前将HashKV部署在具有多个ssd的RAID阵列上，以获得更高的I/O性能。我们使用 mdadm 创建一个软件RAID卷，并将RAID卷挂载为Ext4文件系统，在该文件系统上运行LevelDB和值存储。特别地，HashKV将值存储管理为一个大文件。它根据预先配置的段大小，将value store文件划分为两个区域，一个用于主段，另一个用于日志段。所有的段在value store文件中对齐，这样每个 main/log 段的起始偏移量是 main 段大小的倍数。如果启用了热感知功能(见§3.4)，HashKV会在值存储文件中为冷数据日志添加一个单独的区域。此外，为了解决崩溃一致性(见3.7)，HashKV使用单独的文件来存储写和GC日志。 Multi-threading：HashKV通过线程池实现了多线程，当把写缓存中的KV对刷新到不同的段时(见§3.2)，并且在GC(见§3.3）过程中从段组中并行检索段，从而提高I/O性能。 为了减轻确定性分组带来的随机写开销(见§3.1)，HashKV实现了批量写。当HashKV在写缓存中刷新KV对时，它首先识别并缓冲一批被散列到同一个分段组中的KV对，然后(通过线程)发出一个顺序写来刷新批处理。更大的批处理大小减少了随机写开销，但它也降低了并行性。 目前，我们配置了一个批写入阈值，在批中添加KV对后，如果批大小达到或超过批大小阈值，批将被刷新;换句话说，如果一个KV对的大小大于批写阈值，HashKV就直接刷新它。 Evaluation 我们评估了LevelDB (LDB)、RocksDB (RDB)、HyperLevelDB (HDB)、PebblesDB (PDB)、vLog和HashKV (HKV)在更新密集型工作负载下的性能。我们首先比较LevelDB, RocksDB, vLog和HashKV;之后，我们还将HyperLevelDB和pebblesdb纳入比较中。 图5(a)显示了每个阶段的性能。对于vLog和HashKV，加载阶段的吞吐量比更新阶段的吞吐量高，因为更新阶段主要由GC开销控制。在 load 阶段，hashkv的吞吐量分别比LevelDB和RocksDB大17.1×和3.0×。HashKV的吞吐量比vLog慢7.9%，这是由于通过哈希来分发KV对而引入了随机写。在更新阶段，HashKV的吞吐量在LevelDB、RocksDB和vLog上分别为6.3-7.9×、1.3-1.4×和3.7-4.6×。由于压缩开销很大，LevelDB的吞吐量在所有KV存储中是最低的，而vLog的GC开销也很高。 图5(b)和5(c)显示了总写大小和所有加载和更新请求发出后，不同KV存储的大小。HashKV将LevelDB、RocksDB和vLog的总写大小分别减少了71.5%、66.7%和49.6%。而且，它们有非常相似的 KV 存储大小。 对于 HyperLevelDB 和 peblesdb，两者都是如此，由于压缩开销较低，因此具有较高的负载和更新吞吐量。例如，PebblesDB将sstable的片段从较高的级别添加到较低的级别，而不重写较低级别的sstable。HyperLevelDB和PebblesDB都至少实现了HashKV的两倍吞吐量，同时产生的写大小也小于HashKV。 另一方面，它们的存储开销很大，其最终KV存储大小分别为2.2× HashKV和1.7× HashKV。主要原因是HyperLevelDB和PebblesDB都只对选定范围的键进行压缩，以减少写放大，这样在压缩后可能仍然会有很多无效的KV对。它们触发压缩操作的频率也低于LevelDB。这两个因素都会导致较高的存储开销。 在接下来的实验中，我们关注LevelDB、RocksDB、vLog和HashKV，因为它们的存储开销相当。 写放大 PebblesDB/HyperLevelDB 做得更好，但空间放大 HASHKV 更好 Related Work General KV stores: DRAM: Redis, MemC3, NSDI13 Distributed Caching with Memcached, NSDI14 MICA SSDs: FlashStore, SIGMOD11 SkimpyStash, SOSP11 SILT, FAST16 WiscKey NVM: ATC15 NVMKV, ATC17 HiKV LSM-tree-based KV stores bLSM VT-tree LSM-trie LWC-store SkipStore PebblesDB KV separation WiscKey Atlas Cocytus Hash-based data organization Dynamo Kinesis Ceph NVMKV Conclusion 本文提出了HashKV算法，它能够在更新密集型工作负载下对KV存储进行有效的更新。它的新颖之处在于利用基于哈希的数据分组进行确定性数据组织，从而减轻GC开销。我们通过几个扩展进一步增强了HashKV，包括动态预留空间分配、热度感知和选择性KV分离。试验台实验表明，HashKV实现了较高的更新吞吐量，并减少了总写大小。 ","link":"https://blog.shunzi.tech/post/HashKV/"},{"title":"C++ 多线程","content":" C++ 多线程封装，以及一些并发处理机制的资料 持续更新ing 参考链接 [1] CSDB: C++多线程之旅-atomic原子类型 [2] 菜鸟教程：C++ 多线程 [3] cplusplus.com - atomic [4] C++ 并发编程 [5] 知乎：C++11原子操作与无锁编程 [6] 简书：c++11 多线程（3）atomic 总结 [7] 博客园：C++11 并发指南六( 类型详解二 std::atomic ) [8] 华为云：常见原子操作（C语言） [9] 简书：LevelDB 中的跳表实现 atomic 所谓原子操作，就是多线程程序中“最小的且不可并行化的”操作。对于在多个线程间共享的一个资源而言，这意味着同一时刻，多个线程中有且仅有一个线程在对这个资源进行操作，即互斥访问。 而关于原子性，我们应当具有一个基本的认知：高级语言层面，单条语句并不能保证对应的操作具有原子性。 举例说明 在使用 C、C++、Java 等各种高级语言编写代码时，不少人会下意识的认为一条不可再分的单条语句具有原子性，例如常见 i++。 // 伪码 int i = 0; void increase() { i++; } int main() { /* 创建两个线程，每个线程循环进行 100 次 increase */ // 线程 1 Thread thread1 = new Thread( run() { for (int i = 0; i &lt; 100; i++) increase(); } ); // 线程 2 Thread thread2 = new Thread( run() { for (int i = 0; i &lt; 100; i++) increase(); } ); } 如果 i++ 是原子操作，则上述伪码中的 i 最终结果为 200。但实际上每次运行结果可能都不相同，且通常小于 200。 之所以出现这样的情况是因为 i++ 在执行时通常还会继续划分为多条 CPU 指令。以 Java 为例，i++ 编译将形成四条字节码指令，以下四条指令是 Java 虚拟机中的字节码指令，字节码指令是 JVM 执行的指令。实际每条字节码指令还可以继续划分为更底层的机器指令。但字节码指令已经足够演示原子性的含义了。如下所示： CSDN：JVM指令集及各指令的详细使用说明 // Java 字节码指令 0: getstatic # 获取静态属性指令，获取指定类的静态域，并将其值压入栈顶 1: iconst_1 # 当int取值-1~5时，JVM采用iconst指令将常量压入栈中，也就是将 1 压入栈中 2: iadd # 将栈顶两 int 型数值相加并将结果压入栈顶 3: putstatic # 为指定的类的静态域赋值 而上述四条指令的执行并不保证原子性，即执行过程可被打断。考虑如下 CPU 执行序列： 线程 1 执行 getstatic 指令，获得 i = 1 CPU 切换到线程 2，也执行了 getstatic 指令，获得 i = 1。 CPU 切回线程 1 执行剩下指令，此时 i = 2 CPU 切到线程 2，由于步骤 2 读到的是 i = 1，固执行剩下指令最终只会得到 i = 2 C++ std::atomic std::atomic 为C++11封装的原子数据类型。原子数据类型不会发生数据竞争，能直接用在多线程中而不必我们用户对其进行添加互斥资源锁的类型。从实现上，大家可以理解为这些原子类型内部自己加了锁。 原子类型在头文件 中，使用atomic有两套命名模式：一种是使用替代名称，一种是使用atomic的特化。 atomic_bool &lt;=&gt; atomic &lt; bool &gt; atomic_char &lt;=&gt; atomic&lt; char &gt; atomic_int &lt;=&gt; atomic&lt; int &gt; ... 并非所有的类型都能提供原子操作，这是因为原子操作的可行性取决于 CPU 的架构以及所实例化的类型结构是否满足该架构对内存对齐条件的要求，因而我们总是可以通过 std::atomic::is_lock_free来检查该原子类型是否需支持原子操作。这个函数让用户可以查询某原子类型的操作是直接用的原子指令(x.is_lock_free()返回true)， 还是编译器和库内部用了一个锁(x.is_lock_free()返回false)。 template &lt; class T &gt; struct atomic { bool is_lock_free() const volatile; bool is_lock_free() const; void store(T, memory_order = memory_order_seq_cst) volatile; void store(T, memory_order = memory_order_seq_cst); T load(memory_order = memory_order_seq_cst) const volatile; T load(memory_order = memory_order_seq_cst) const; operator T() const volatile; operator T() const; T exchange(T, memory_order = memory_order_seq_cst) volatile; T exchange(T, memory_order = memory_order_seq_cst); bool compare_exchange_weak(T &amp;, T, memory_order, memory_order) volatile; bool compare_exchange_weak(T &amp;, T, memory_order, memory_order); bool compare_exchange_strong(T &amp;, T, memory_order, memory_order) volatile; bool compare_exchange_strong(T &amp;, T, memory_order, memory_order); bool compare_exchange_weak(T &amp;, T, memory_order = memory_order_seq_cst) volatile; bool compare_exchange_weak(T &amp;, T, memory_order = memory_order_seq_cst); bool compare_exchange_strong(T &amp;, T, memory_order = memory_order_seq_cst) volatile; bool compare_exchange_strong(T &amp;, T, memory_order = memory_order_seq_cst); atomic() = default; constexpr atomic(T); atomic(const atomic &amp;) = delete; atomic &amp; operator=(const atomic &amp;) = delete; atomic &amp; operator=(const atomic &amp;) volatile = delete; T operator=(T) volatile; T operator=(T); }; 主要操作 load() 加载原子对象中存入的值，等价于直接使用原子变量。 T load (memory_order sync = memory_order_seq_cst) const volatile noexcept; T load (memory_order sync = memory_order_seq_cst) const noexcept; 读取被封装的值，参数 sync 设置内存序(Memory Order)，可能的取值如下： memory_order_relaxed memory_order_consume memory_order_acquire memory_order_seq_cst store() 存储一个值到原子对象，等价于使用等号。 void store (T val, memory_order sync = memory_order_seq_cst) volatile noexcept; void store (T val, memory_order sync = memory_order_seq_cst) noexcept; 修改被封装的值，std::atomic::store 函数将类型为 T 的参数 val 复制给原子对象所封装的值。T 是 std::atomic 类模板参数。另外参数 sync 指定内存序(Memory Order)，可能的取值: memory_order_relaxed memory_order_release memory_order_seq_cst exchange() 原子操作 返回原来里面存储的值，然后在存储一个新的值，相当于将上面两个 load() 和 store() 合成起来的参数。 T exchange (T val, memory_order sync = memory_order_seq_cst) volatile noexcept; T exchange (T val, memory_order sync = memory_order_seq_cst) noexcept; 读取并修改被封装的值，exchange 会将 val 指定的值替换掉之前该原子对象封装的值，并返回之前该原子对象封装的值，整个过程是原子的(因此exchange 操作也称为 read-modify-write 操作)。sync参数指定内存序(Memory Order)，可能的取值如下： memory_order_relaxed memory_order_consume memory_order_acquire memory_order_release memory_order_acq_rel memory_order_seq_cst compare_exchange_weak() 交换-比较操作是比较原子变量值和所提供的期望值，如果二者相等，则存储提供的期望值，如果不等则将期望值更新为原子变量的实际值，更新成功则返回true反之则返回false。 atomic&lt;bool&gt; b; b.compare_exchange_weak(expected, new_value); 当存储的值和expected相等时则将则更新为new_value，如果不等时则不变。其中expected必须是类型变量，而不能是常量。 compare_exchange_strong() 不像compare_exchange_weak，此版本必须始终true在预期确实与所包含的对象相等时返回，不允许出现虚假故障。但是，在某些计算机上，对于某些在循环中进行检查的算法，compare_exchange_weak 可能会明显改善性能。 其余使用方法和compare_exchange_weak完全一致。 这两个版本的区别是：Weak版本如果数据符合条件被修改，其也可能返回false，就好像不符合修改状态一致；而Strong版本不会有这个问题，但在某些平台上Strong版本比Weak版本慢 [注:在x86平台我没发现他们之间有任何性能差距]；绝大多数情况下，我们应该优先选择使用Strong版本； memory order 在介绍 Memory Order 之前首先介绍 有序性 有序性 上述已经提到 CPU 的一条指令执行时，通常会有多个步骤，如取指IF 即从主存储器中取出指令、ID 译码即翻译指令、EX 执行指令、存储器访问 MEM 取数、WB 写回。 即指令执行将经历：IF、ID、EX、MEM、WB 阶段。 现在考虑 CPU 在执行一条又一条指令时该如何完成上述步骤？最容易想到并是顺序串行，指令 1 依次完成上述五个步骤，完成之后，指令 2 再开始依次完成上述步骤。这种方式简单直接，但执行效率显然存在很大的优化空间。 思考一种流水线工作： 指令1 IF ID EX MEM WB 指令2 IF ID EX MEM WB 指令3 IF ID EX MEM WB 采用这种流水线的工作方式，将避免 CPU 、存储器中各个器件的空闲，从而充分利用每个器件，提升性能。同时注意到由于每条指令执行的情况有所不同，指令执行的先后顺序将会影响到这条流水线的负载情况，而我们的目标则是让整个流水线满载紧凑的运行。 为此 CPU 又实现了「指令重排」技术，CPU 将有选择性的对部分指令进行重排来提高 CPU 执行的性能和效率。例如： x = 100; // #1 y = 200; // #2 z = x + y; // #3 虽然上述高级语言的语句会编译成多条机器指令，多条机器指令还会进行「指令重排」，#1 语句与 #2 语句完全有可能被 CPU 重新排序，所以程序实际运行时可能会先执行 y = 200; 然后再执行 x = 100; 但另一方面，指令重排的前提是不会影响线程内程序的串行语义，CPU 在重排指令时必须保证线程内语义不变，例如： x = 0; // #1 x = 1; // #2 y = x; // #3 上述的 y 一定会按照正常的串行逻辑被赋值为 1。 但不幸的是，CPU 只能保证线程内的串行语义。在多线程的视角下，「指令重排」造成的影响需要程序员自己关注。 // 公共资源 int x = 0; int y = 0; int z = 0; Thread_1: Thread_2: x = 100; while (y != 200); y = 200; print x z = x + y; 如果 CPU 不进行「乱序优化」执行，那么 y = 200 时，x 已经被赋值为 100，此时线程 2 输出 x = 200。但实际运行时，线程 1 可能先执行 y = 200，此时 x 还是初始值 0。线程 2 观察到 y = 200 后，退出循环，输出 x = 0; memory order C++ 提供了 std::atomic 类模板，以保证操作原子性。同时也提供了内存顺序模型 memory_order指定内存访问，以便提供有序性和可见性。 memory_order_relaxed: 只保证原子操作的原子性，不提供有序性的保证 memory_order_consume 当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前 memory_order_acquire 在其影响的内存位置进行获得操作：当前线程中读或写不能被重排到此加载前 memory_order_release 当前线程中的读或写不能被重排到此存储后 memory_order_acq_rel 带此内存顺序的读修改写操作既是获得操作又是释放操作 memory_order_seq_cst 有此内存顺序的加载操作进行获得操作，存储操作进行释放操作，而读修改写操作进行获得操作和释放操作，再加上存在一个单独全序，其中所有线程以同一顺序观测到所有修改 组合出四种顺序： Relaxed ordering 宽松顺序：宽松顺序只保证原子变量的原子性（变量操作的机器指令不进行重排序），但无其他同步操作，不保证多线程的有序性。 Thread1: y.load(std::memory_order_relaxed); Thread2: y.store(h, std::memory_order_relaxed); Release-Acquire ordering 释放获得顺序，store 使用 memory_order_release，load 使用 memory_order_acquire，CPU 将保证如下两点： store 之前的语句不允许被重排序到 store 之后（例子中的 #1 和 #2 语句一定在 store 之前执行） load 之后的语句不允许被重排序到 load 之前（例子中的 #3 和 #4 一定在 load 之后执行） std::atomic&lt;std::string*&gt; ptr; int data; void producer() { std::string* p = new std::string(&quot;Hello&quot;); // #1 data = 42; // #2 ptr.store(p, std::memory_order_release); } void consumer() { std::string* p2; while (!(p2 = ptr.load(std::memory_order_acquire))) ; assert(*p2 == &quot;Hello&quot;); // 绝无问题 #3 assert(data == 42); // 绝无问题 #4 } int main() { std::thread t1(producer); std::thread t2(consumer); t1.join(); t2.join(); } 同时 CPU 将保证 store 之前的语句比 load 之后的语句「先行发生」，即先执行 #1、#2，然后执行 #3、#4。这实际上就意味着线程 1 中 store 之前的读写操作对线程 2 中 load 执行后是可见的。注意是所有操作都同步了，不管 #3 是否依赖了 #1 或 #2 值得关注的是这种顺序模型在一些强顺序系统例如 x86、SPARC TSO、IBM 主框架上是自动进行的。但在另外一些系统如 ARM、Power PC 等需要额外指令来保障。 Release-Consume ordering 释放消费顺序:store 使用 memory_order_release，load 使用 memory_order_consume。其效果与 Release-Acquire ordering 释放获得顺序类似，唯一不同的是并不是所有操作都同步（不够高效），而是只对依赖操作进行同步，保证其有序性上例就是 #3 一定发生在 #1 之后，因为这两个操作依赖于 ptr。但不会保证 #4 一定发生在 #2 之后（注意「释放获得顺序」可以保证这一点）。 std::atomic&lt;std::string*&gt; ptr; int data; void producer() { std::string* p = new std::string(&quot;Hello&quot;); // #1 data = 42; // #2 ptr.store(p, std::memory_order_release); } void consumer() { std::string* p2; while (!(p2 = ptr.load(std::memory_order_consume))) ; assert(*p2 == &quot;Hello&quot;); // #3 绝无出错： *p2 从 ptr 携带依赖 assert(data == 42); // #4 可能也可能不会出错： data 不从 ptr 携带依赖 } int main() { std::thread t1(producer); std::thread t2(consumer); t1.join(); t2.join(); } Sequential consistency 序列一致顺序: 「释放获得顺序」是对某一个变量进行同步，Sequential consistency 序列一致顺序则是对所有变量的所有操作都进行同步。store 和 load 都使用 memory_order_seq_cst，可以理解对每个变量都进行 Release-Acquire 操作。所以这也是最慢的一种顺序模型。 ","link":"https://blog.shunzi.tech/post/cpp-multi-thread/"},{"title":"FloDB: Unlocking Memory in Persistent Key-Value Stores","content":" EuroSys17: FloDB: Unlocking Memory in Persistent Key-Value Stores https://dcl.epfl.ch/site/flodb Abstract 日志结构合并(LSM)数据存储允许存储和处理大量数据，同时保持良好的性能。它们通过吸收内存层中的更新并按顺序分批地将它们传输到磁盘层来缓解I/O瓶颈。然而，LSM体系结构基本上要求元素按排序顺序排列。随着内存中数据量的增长，维护这种排序顺序的成本越来越高。与直觉相反，现有的LSM系统在使用较大的内存组件时实际上可能会损失吞吐量。 在本文中，我们介绍了FloDB，一种LSM内存组件架构，它允许吞吐量在具有充足内存大小的现代多核机器上扩展。FloDB的主要思想本质上是通过在内存组件上添加一个小的内存缓冲层来引导传统的LSM体系结构。该缓冲区提供低延迟操作，屏蔽排序内存组件的写延迟。将这个缓冲区集成到经典的LSM内存组件中以变成 FloDB 并非易事，需要重新访问面向用户的 LSM 操作(搜索、更新、扫描)的算法。FloDB 的两层可以用最先进的、高并发的数据结构来实现。通过这种方式，正如我们在本文中所展示的那样，FloDB 消除了经典 LSM 设计中显著的同步瓶颈，同时提供了一个丰富的 LSM API。 我们实现FloDB作为LevelDB的扩展，谷歌流行的LSM键值存储。我们将FloDB的性能与最先进的LSMs进行比较。简而言之，在各种多线程工作负载下，FloDB的性能比性能仅次于它的竞争对手高出一个数量级。 Introduction 键值存储是许多系统的关键组件，这些系统需要对大量数据进行低延迟的访问。这些存储的特点是扁平的数据组织和简化的接口，这允许有效的实现。然而，键值存储的目标数据量通常大于主存;因此，通常需要持久化存储。由于访问持久存储的速度比 CPU 慢，直接在磁盘上更新数据产生了一个严重的瓶颈。所以很多 KV 存储系统使用了 LSM 结构。LSM 数据存储适用于需要低延迟访问的应用程序，例如需要进行大量更新的消息队列，以及在面向用户的应用程序中维护会话状态。基本上，LSM 体系结构一方面通过缓存读，另一方面通过在内存中吸收写并稍后批量写入磁盘，掩盖了磁盘访问瓶颈。尽管 LSM 键值存储在解决I/O瓶颈带来的挑战方面发挥了很大的作用，但它们的性能不会随着内存组件的大小而扩展，也不会随着线程的数量而扩展。换句话说，可能令人惊讶的是，增加现有lsm的内存部分只能在相对较小的尺寸下提高性能。类似地，添加线程并不能提高许多现有lsm的吞吐量，因为它们使用了全局阻塞同步。 上面描述的两种限制在 LSM 的传统设计中是固定存在的。我们通过引入 FloDB 来规避这些限制，FloDB 是一种新颖的 LSM 内存组件，其设计目的是随线程数量及其在内存中的大小而扩展。传统上，LSMs 一直采用两层存储层次结构，一层在内存中，一层在磁盘上。我们建议增加一个内存级别。换句话说，FloDB 是一个位于磁盘组件之上的两级内存组件(图1)，每个in-memory 级别都是一个并发数据结构。内存顶层是一个小而快速的数据结构，而内存底层是一个更大的、排序的数据结构。在存储到磁盘之前，新条目被插入到顶层，然后在后台被排到底层。 这个方案有几个优点： 首先，它允许扫描和写入并行进行，分别在底层和顶层。 其次，无论内存组件大小如何，使用小型、快速的顶级级别都可以实现低延迟的更新，而现有的 LSM 会随着内存组件大小的增加而性能下降。更大的内存组件可能在峰值吞吐量时造成更长的写突发。 第三，维护底层内存层的排序允许在不进行额外的(昂贵的)排序步骤的情况下对磁盘进行刷新。 我们使用一个小型的高性能并发哈希表来实现 FloDB，并使用一个较大的并发跳表来实现底部的内存级别。乍一看，实现 FloDB 似乎只需要在现有的 LSM 体系结构上添加一个额外的基于哈希表的缓冲区级别。然而，这个看似很小的步骤却带来了微妙的技术挑战。 第一个挑战是确保两个内存级别之间的有效数据流动，以便充分利用额外的缓冲区，同时不耗尽系统资源。为此，我们引入了多插入操作，这是一种用于并发跳表的新操作。其主要思想是在一个操作中在跳跃列表中插入 n 个排序的元素，使用前面插入元素的位置作为插入下一个元素的起点，从而重用已经遍历过的 hops。Skiplist 多插入是独立的，通过增加内存组件更新的吞吐量，它也可以使以前的单写入器 LSM 实现受益。 第二个挑战是确保面向用户的 LSM 操作的一致性，同时在这些操作之间启用高级别的并发性。特别是，FloDB 是第一个同时支持一致扫描和就地更新的LSM系统。 我们的实验表明，FloDB的性能优于当前的键值存储解决方案，特别是在写密集型场景中。例如，在只写的工作负载中，FloDB 可以用一个工作线程来饱和磁盘组件的吞吐量，并且在最多有16个工作线程的情况下，它的性能继续超过性能第二好的系统，平均是后者的2倍。此外，对于倾斜的读写工作负载，FloDB 获得比性能最高的竞争对手高一个数量级的吞吐量： FloDB，一个用于 log-structured merge 内存组件的两级架构。FloDB 可以根据主内存的大小进行扩展，并且具有丰富的API，读取、写入和扫描都可以并发进行。 在Xeon多核机器上，作为LevelDB的扩展，FloDB体系结构的一个公开的c++实现，以及一个高达192GB内存组件大小的实验。 一种用于并行跳过表的新型多插入操作的算法。除了FloDB，多插入还可以使任何使用skiplist 的 LSM 体系结构受益 尽管 FloDB 有很多优点，但我们并不认为它是一剂灵丹妙药;它确实有局限性。 首先，与所有lsm一样，稳态吞吐量受到层次结构中最慢组件的限制:将内存组件写入磁盘。FloDB在内存组件中所做的改进与潜在的磁盘级改进是正交的，这超出了本文的讨论范围。磁盘级别的改进可以与FloDB结合使用，进一步提高LSMs的总体性能。 其次，可以设计出对FloDB扫描有问题的工作负载。例如，在写密集型工作负载和严重争用中，长时间运行扫描的性能会下降。第三，对于比主存大得多的数据集，FloDB提高的读性能要小于写性能。这是因为在LSMs中，读取性能很大程度上取决于缓存的有效性，这也不在我们的论文讨论范围之内。 Shortcomings of Current LSMs LSM Key-Value Store Overview Limitation—Scalability with Number of Threads 多个处理核心的存在可以提高LSM数据存储的性能。虽然现有的LSM系统允许某种程度的并发，但它们仍留有大量的并行机会。例如， leveldb——许多LSM键值存储的基础——支持多个写线程，但通过让线程将其预期的写操作存储在一个并发队列中来序列化写操作;这个队列中的写操作由单个线程一个接一个地应用到键值存储。此外，LevelDB还要求读取器在每次操作中获取全局锁，以便访问或更新元数据 HyperLevelDB 建立在LevelDB之上，通过允许并发更新来提高并发性;然而，为了通过版本号来维护更新的顺序，仍然需要昂贵的同步 RocksDB 也是源自LevelDB的一个键值存储，它通过引入磁盘组件的多线程合并来提高并发性。虽然多线程压缩确实提高了整体性能，但RocksDB仍然保持全局同步来访问内存结构和更新版本号，这与HyperLevelDB类似 cLSM 甚至更进一步，从只读路径上删除了任何阻塞的同步，但是使用全局共享独占锁来协调更新和后台磁盘写操作，仍然削弱了系统的可伸缩性。正如我们在第5节中所展示的，这些可伸缩性瓶颈确实在实践中表现出来 Limitation—Scalability with Memory Size 现有的LSM内存组件可以排序(例如，skiplist)或未排序的(例如，哈希表)。这两种选择都有各自的优点和缺点，但令人惊讶的是，它们都不能扩展到大型内存组件。 一方面，当使用skiplist时，顺序扫描是自然的。而且，压缩阶段只不过是将组件直接复制到磁盘;因此它的开销很低。然而，写需要数据结构大小的对数时间来维持排序顺序(直接从内存组件读取也需要数据结构大小的对数时间。然而，对于大型数据集，大多数读操作都是从磁盘组件上读取的，因此内存组件数据结构的选择对读延迟的影响小于写延迟)。因此，从图3中可以看出，分配更多的内存实际上增加了写延迟。该图显示了作为RocksDB(最流行的最先进的lsm之一)中内存组件大小的函数的读和写延迟的中值。在第99百分位，我们观察到类似的读写延迟趋势。延迟使用RocksDB的读写基准测试，有八个读线程和一个写线程在一个有100万个条目的数据库上运行。密钥大小为8字节，值大小为256字节。延迟被规范化为128 MB内存组件。 另一方面，对于哈希表，写操作在常量时间内完成，但是顺序扫描是不实用的，而且压缩阶段更复杂。压缩在将内存组件写入磁盘之前需要对其进行完全排序，以保存LSM结构。实际上，我们的测量结果表明，基于哈希表的内存组件的平均压缩时间至少比相同大小的基于skiplist的内存组件的平均压缩时间高一个数量级：因为随着哈希表变得越来越大，排序和持久化到磁盘所需的时间也越来越长。在对不可变内存组件进行排序和持久化时，活动(可变)内存组件也可能被填满。在这种情况下，writer 被延迟，因为在内存中没有空间来完成它们的操作。结果，端到端写延迟随着内存大小而增加，如图4所示，图4展示了与图3相同的实验，使用的是哈希表而不是跳过列表。 因此，对于跳过列表和哈希表，端到端系统吞吐量随着内存的增加而趋于稳定，甚至下降。这种限制是传统 LSM 的单级内存组件固有的，限制了 LSM 用户利用现代多核中的丰富内存。在接下来的部分中，我们将展示将哈希表和跳过表的优点结合起来是可能的，从而提高 LSMs的写吞吐量，同时仍然允许 inorder 扫描。 FloDB Design 现有的 LSM 存在固有的可伸缩性问题，包括内存大小和线程数量方面的问题。后者是由可伸缩性瓶颈引起的，而前者则源于大小-延迟的权衡。 这种权衡体现在排序和未排序的内存组件上。有序的组件允许扫描，并且可以直接持久化到磁盘，但是随着内存组件变大，其内存组件对应就会有显著更高的访问次数。未排序组件的速度可能与大小无关，但对于扫描来说并不实用，并且需要在被刷新到磁盘之前排序，需要耗费线性时间，这可能会延迟 writers。FloDB 的内存组件体系结构就是为了避免这些问题而设计的。FloDB 的主要目标是: (1) 根据给定的内存量进行扩展; (2) 使用最小的同步(以扩大规模); (3) 利用内存组件提高写性能，而不牺牲读性能。 下面，我们将概述FloDB的内存组件架构，以及利用这个新结构的主要操作:Get、Put、Delete和Scan。 In-memory Levels 简而言之，FloDB的基本思想是使用两层内存组件，其中一层是小型、快速的数据结构，第二层是大型、排序的数据结构。这种设计允许FloDB打破大小延迟的平衡，并最小化同步。 第一级称为 Membuffer，它又小又快，但不一定要排序。第二级称为Memtable，规模更大，以便捕获更大的工作集，从而更好地屏蔽高I/O延迟。此外，Memtable 保持元素的排序，所以它可以直接刷新到磁盘。这两个级别都是并发的数据结构，使得操作可以并行进行。与其他LSMs 类似，数据从最小的组件(Membuffer)流向最大的组件(磁盘)，因为各个级别都满了。 磁盘级组件不是我们的重点，也不在本文的讨论范围之内。由于磁盘组件和内存中对数据的处理在 LSM 键值存储中是相互正交的，所以我们展示的内存中优化方法可以与任何持久化到磁盘的机制一起使用。例如，FloDB的内存组件可以与类似于 RocksDB 的多线程压缩方案相结合，或者像LSM-trie中那样，可以减少写放大，从而获得更好的磁盘结构。 Operations 我们给出了FloDB主要操作的高层设计。在第4.4节中，我们将描述该设计的具体实现。FloDB的两层内存组件允许非常简单的基本操作(即Get、Put、Delete)，但在扫描的情况下会带来额外的复杂性。 Get 除了LSM数据结构中的同步外，FloDB中的Get操作不需要同步。首先搜索的是Membuffer，然后是Memtable，最后是磁盘。如果在某个级别找到所需的元素，则read可以立即返回。显然，一旦找到了元素，就没有必要在较低的层次中进行搜索，因为层次结构中的较高层次总是包含最新的值。 Update Put和Delete操作本质上是相同的。删除是通过插入一个特殊的tombstone值来完成的。从现在开始，我们将把Put和Delete操作都称为Update。首先，在Membuffer中尝试更新。如果Membuffer中没有空间，则直接在Memtable中进行更新。如果键已经存在于Membuffer或Memtable中，相应的值将就地更新。 就地更新的另一种方法是多版本化:保留相同键的多个版本，仅在压缩阶段丢弃旧版本。所有现有的 LSMs 都使用多版本控制。然而，多版本控制方法不能利用倾斜工作负载的局部性。事实上，不断地更新一个键就足以填满内存组件并触发频繁刷新磁盘。相比之下，在就地更新中，重复写同一个键不会占用额外的内存，因此内存中的存储是按照数据大小的顺序排列的。就地更新，结合一个大型内存组件，允许我们有效地捕获大型的、写密集型的、倾斜的工作负载，如第5节所示。 Scan 我们的扫描算法背后的主要思想是扫描Memtable和磁盘的同时允许在Membuffer中完成并发更新。这种方法的一个挑战是，扫描Memtable 可能返回一个过时的键值，当扫描开始时，这个键仍然在Membuffer 中。我们通过在扫描前清空 Memtable 中的 MemBuffer 来解决这个问题。 另一个挑战是，如果扫描需要很长时间才能完成，如果经常调用扫描，或者在扫描期间有很多线程执行更新，那么吸收更新的 Membuffer 可能会被填满。在这种情况下，我们允许写入器和扫描器在 Memtable 中并行进行。然而，如果允许写入者在扫描 A 期间天真地更新 Memtable，那么在 A 的范围内的一个条目可能会在 A 进行时被修改，导致不一致。我们通过在 Memtable 级别引入每个条目的序列号来解决这个问题。通过这种方式，扫描 A 可以验证自 A 启动以来是否在其范围内写入了 Memtable 中的新值；如果是这种情况，扫描 A 将失效并重新启动。A 的 fallback 机制来确保 liveness(即，扫描不会被 writers 无限期重启)。值得注意的是，我们使用序列号的方式与上面提到的多版本控制不同;在我们的算法中，当 Memtable 中存在的键 k 发生更新时，k 的值和序列号就地更新 （这里感觉讲的不是很清楚，有些含糊） Summary FloDB 的两级设计有几个好处。 首先，很大一部分写操作是在Membuffer 中完成的，因此 FloDB 从快速内存组件(通常为整体未排序的内存组件保留，如哈希表)中获得好处。 第二，排序的底部组件允许扫描，并可以直接刷新到磁盘。 第三，级别的分离使写和读能够与扫描并行进行。 两层层次结构的另一个好处是，可以增加内存组件的总体大小，以获得更好的性能(与现有系统相比)。 最后，我们的设计允许就地更新，同时支持一致扫描 FloDB Implementation 我们在谷歌的 LevelDB 上实现FloDB。LevelDB的内存组件完全被FloDB架构所取代。我们保留了 LevelDB 的持久化和压缩机制。 LevelDB中的原始方法是在内存中保留 thread-local 版本和文件描述符缓存(fd-cache)的一个共享版本，并在必要时获取一个全局锁来访问fd-cache的共享版本。在我们的初步测试中，我们发现这个全局锁是一个主要的可伸缩性瓶颈。为了消除这个瓶颈，作为内存优化的一部分，我们将LevelDB fd-cache实现替换为一个更可伸缩、并发的哈希表 在下面,我们将讨论的关键实现细节FloDB: (1) 对于 Memtable 和 Membuffer 数据结构的选择 (2) 机制将用于 level 之间的数据移动 (3) 我们的新型 multi-insert 操作用于简化在 Memtable 和 Membuffer 之间的数据流 (4) 面向用户的操作的实现 Memory Component Implementation 在实现FloDB体系结构时，需要解决的一个重要问题是，在内存级别上如何选择良好的数据结构。为了使写操作尽可能快，第一级的合适选择是哈希表。如图5所示，一个现代哈希表可以提供超过100 Mops/s的吞吐量，即使有10亿个条目的工作负载。然而，即使哈希表很快，它们也不会对它们的条目进行排序，这意味着不能直接对数据进行顺序迭代。由于这个原因，保持数据有序且易于迭代的数据结构(例如在传统LSM实现中已经用作内存组件的skiplist)是第二级的良好选择。 因此，FloDB的内存层次结构由一个小型的高性能并发哈希表和一个较大的并发跳表组成。哈希表存储键-值元组。skiplist存储用于扫描操作的键-值元组和序列号。 Membuffer和Memtable之间的大小比例是一种权衡。一方面，相对较小的Membuffer会更快地填满，迫使更多的更新在Memtable中完成。这是有问题的，因为Memtable较慢，但也因为Memtable绑定的更新可能会迫使更多的扫描重新启动。另一方面，一个大的Membuffer将花费更长的时间来进入Memtable，这可能会延迟扫描。 Interaction Between Levels Background threads：因为我们不希望数据是静态的(也就是说，数据不断地被插入或更新)，所以FloDB有两种机制来跨层次结构的级别移动数据:持久化和draining。 持久化通过一个专门的后台线程将项从Memtable移动到磁盘——这是LSM实现中的一种既定技术。当Memtable被填满时，持久化被触发。 Draining 将条目从Membuffer移动到Memtable，这是由一个或多个专门的后台线程完成的。Draining 是一个持续不断的过程，因为人们希望保持尽可能低的 Membuffer 占用率。实际上，只有在 Membuffer 中完成写操作时，才会从两级层次结构中获益。 Persisting：在LSMs中，将内存组件持久化到磁盘的一种标准技术是使该组件不可变，安装一个新的、可变的组件，并在后台将旧组件写入磁盘。通过这种方式，当旧组件被持久化时，写入者仍然可以继续处理新组件，并且不可变组件中的数据对读者仍然是可见的。然而，内存组件之间的切换通常是使用锁定完成的。FloDB有一个更高效的RCU（Read-Copy-Update）方法来切换内存组件，它不会阻塞任何更新或读取。当持久化的时候，RCU 是用来确保在后台线程将Memtable复制到磁盘之前，所有对不可变Memtable的待处理的更新都已经完成。第二，在Memtable被复制到磁盘之后，我们使用RCU来确保在后台线程可以继续释放不可变Memtable之前，没有reader线程正在读取它。 Draining：Draining(图6)是由一个或多个专门的后台线程与更新、读取或其他Draining同时进行的，并按照以下步骤进行。要将一个key-value 条目 e 从 Membuffer 移动到 Memtable 中，e 首先被检索并在 Membuffer 中标记。这样做是为了确保没有其他后台线程也试图将 e 移动到 Memtable 中。然后，e 被分配一个序列号(通过原子递增操作获得)，并被后台线程插入到 Memtable 中。最后，e 从 Membuffer 中移除。有一种特殊情况，扫描开始时发生 Draining，为了使扫描能够只在Memtable和磁盘级别上进行，但仍然包括Membuffer中最近的更新，在扫描开始时将Membuffer中所有的内存都 Draining 到Memtable中。这种类型的 Draining 是通过使当前的 Membuffer 不可变，安装一个新的 Membuffer(使用RCU)，然后将旧的 Membuffer 中的所有条目移动到 Memtable 中，类似于Memtable是如何持久化到磁盘上的。 RCU 的本质是转变数据可写状态，然后创建新的空的数据结构 Skiplist Multi-inserts 图5和图7显示了一个并发跳表大约比一个相同大小的并发哈希表慢一到两个数量级。因此，为了使大量的更新可以直接在哈希表中进行，我们需要尽可能快地在各级之间移动项。 Intuition 我们为并发跳过表引入了一种新的多插入操作，以增加 Draining 线程的吞吐量。多插入操作背后的直观感觉很简单。在跳过列表中插入 n 个元素，而不是调用 n 次插入操作，只需在一次多插入操作中插入这些元素。n个元素按升序插入，使用已经完成的进度(即所经过的跃点)插入前一个元素，作为插入下一个元素的起点。 除了增加FloDB中的吞吐量，多插入还可以使使用并发跳过表的其他应用程序受益。例如，像LevelDB这样的lsm使用了来进行更新(第2.2节)，它们可以通过以下方式加速更新。组合线程可以在一个多插入操作中应用所有更新，而不是一个接一个地分别应用它们。更重要的是，几个组合线程可以通过多次插入并发地应用更新。 Pseudocode 多插入操作的伪代码见算法1。操作输入是一个由(键、值)元组组成的数组。首先，输入元组按升序排序。然后，对于每个元组，使用FindFromPreds定位其在跳过列表中的位置。FindFromPreds从前面插入的元素的前身开始搜索当前元素在跳过列表中的位置(第5-8行)。如果当前 level 中存储的前一个元素的键大于要插入的当前元素的键，则可以直接从前一个元素跳转到存储的前一个元素。这是多插入操作的核心，其中应用了路径重用的思想。在跳过列表中找到一个元组的位置后，对该元组的插入操作类似于普通的插入操作。 FindFromPreds(key , preds , succs): // returns true iff key was found // side -effect: updates preds and succs pred = root for level from levelmax downto 0: if (preds[level].key &gt; pred.key): pred = preds[level] curr = pred.next[level] while(true): succ = curr.next[level] if (curr.key &lt; key): pred = curr curr = succ else: break preds[level] = pred succs[level] = curr return (curr.key == key) MultiInsert(keys , values): sortByKey(keys , values) for i from 0 to levelmax: preds[i] = root for each key -value pair (k,v): node = new node(k,v) while(true): if (FindFromPreds(k, preds , succs)): SWAP(succs [0].val , v) break else: for lvl from 0 to node.toplvl: node.next[lvl] = succs[lvl] if (CAS(preds [0]. next[0], succs[0], node)): for lvl from 1 to node.toplvl: while(true): if (CAS(preds[lvl].next[lvl], succs[lvl], node)): break else: FindFromPreds(k, preds , succs) break Concurrency 多个插入相互并发，简单的插入和读取也是如此。然而，多插入的正确性依赖于这样一个事实，即元素不会同时从跳过列表中删除。从设计上讲，这在 FloDB 中不是一个问题;只有当条目持久化到磁盘时，才会从跳过列表中删除它们。此外，虽然多重插入中的每个插入都是原子的，但多重插入本身是不能线性化的，也就是说，整个元素数组不会被视为在单个时间点插入(即中间状态是可见的)。 Neighborhoods 键的邻近性是多插入性能的一个主要因素。直观地说，如果在跳跃列表中多次插入的键最终会在跳跃列表中靠近，路径重用就会最大化。图8描述了一个实验的结果，这个实验比较了简单插入和5个键多插入的吞吐量，这是一个只进行更新的测试中键接近度的函数。在本实验中，如果键范围的邻域大小为n，则一次多插入中的所有键之间的距离最大为2n。可以清楚地看到，随着邻域大小的减小，多插入的效率也会提高。 在FloDB中，我们在哈希表中创建分区，以利用在多次插入中键靠得更近的性能优势(邻域效应)。当一个key-value元组插入到FloDB中时，该键的最有效位 ℓ 将用于确定该元组应该插入到哈希表的哪个分区。然后，键的其余位被散列，以确定分区(即桶)中的位置。因为 ℓ 是一个参数，所以可以很容易地控制邻居的大小。 虽然我们的分区方案利用了多插入的性能优势，但它也使哈希表容易受到数据倾斜的影响。如果存在具有公共前缀的流行键(如果数据倾斜涉及某个键范围)，那么与流行键对应的桶将比与流行度较低的键对应的桶更快地满满。这反过来会导致在 Membuffer 中能够完成的更新比例更小。倾斜工作负载的这种影响将在第5节中讨论。 Implementation of FloDB Operations 算法2给出了Get, Put和Delete操作的伪代码(为了提高可读性，我们省略了进入和退出RCU临界区的代码)。 Get: Get操作简单地在每一层上搜索一个键，顺序如下:Membuffer (MBF)，不可变Membuffer (IMM_MBF)，如果有的话，Memtable (MTB)，不可变Memtable (IMM_MTB)，如果有的话，最后在磁盘上搜索。Get返回它遇到的第一个键，这保证是最新的一个，因为级别是按照与数据流相同的顺序检查的。 Update: 如前所述，Delete操作是一个带有特殊tombstone值的Put操作，因此我们只需要描述后一个操作。实际上，Put操作是通过尝试在Membuffer中插入键值对e来进行的(第10行);如果e的目标哈希表桶没有满，添加到Membuffer成功，操作返回(第11行);否则，e将被插入Memtable中(第20行)。此外，算法2中的完整Put伪代码还包括与持久化线程和并发扫描器同步的机制。首先，如果不成功地将e插入到Membuffer中，并且设置了pauseWriters标志，那么writer将在必要时帮助 Draining 不可变的Membuffer，或者等待标志被取消设置(第12-16行)。正如我们在下面解释的那样，扫描使用pauseWriters标志向写入者发出信号:Membuffer已经被完全清空到memtable中，为扫描做准备，写入者应该等待或者帮助完成清空。第二，写入者会等到Memtable中有空间时才开始执行Put(第17-18行)。这通常是一个非常短的等待，即当前 Memtable 被填满后，持久化线程准备新 Memtable 的时间。 Get(key): for c in MBF IMM_MBF MTB IMM_MTB DISK: if (c != NULL): value = c.search(key) if (value != not_found): return value return not_found Put(key , value): if (MBF.add(key , value) == success): return while pauseWriters: if MBFNeedsDraining (): IMM_MBF.helpDrain () else: wait() while MTB.size &gt; MTB_TARGET_SIZE: wait() seq = globalSeqNumber.fetchAndIncrement() MTB.add(key , value , seq) Delete(key): Put(key , TOMBSTONE) Scan Scan：扫描操作以两个参数作为输入:lowKey和highKey。它返回一个数组，其中包含数据存储中介于低输入键和高输入键之间的所有键和对应值。为清楚起见，我们将扫描算法的介绍分为两部分。首先，我们提出了一种扫描算法，该算法可以并行进行读和写操作，但不能与另一个扫描一起进行。然后，我们介绍必要的额外部分，即允许多线程扫描。 Single-threaded scans, multithreaded reads and writes：单线程扫描的伪代码在算法3中显示(为了更清晰，我们再次忽略了RCU临界区边界)。第一步是冻结对当前Membuffer和Memtable的更新，并将当前Membuffer的内容释放到Memtable中。为此，我们暂停后台 Draining(第4行)，通知写入者停止直接在Memtable中进行更新(第5行)，并等待所有正在进行的Memtable写入完成(第9行)。注意，扫描从来不会完全阻塞写入器;即使写入者不能在扫描的第5行和第13行之间更新Memtable，他们也可以尝试更新Membuffer或帮助处理Draining。帮助确保即使扫描程序线程很慢，也能完成 Draining，从而减少写入程序不允许更新Memtable的时间。 然后，将当前的Membuffer变为不可变的，并创建一个新的Membuffer，用于吸收未来的更新(第6-8行)。然后，发起扫描的线程开始将Membuffer清空到Memtable(第10行)。在Membuffer被清空后，扫描将获得一个序列号(通过原子递增操作)(第12行)。现在，允许后台从新的Membuffer中抽取数据，并允许写入者在Memtable上进行更新(如有必要，第13-14行)是安全的，因为扫描的序列号将用于确保一致性。实际的扫描操作(第15-28行)现在开始，首先是在Memtable和不可变Memtable(如果存在的话)上，最后是在磁盘上。当遇到扫描范围内的键时，将检查其序列号。如果小于扫描序列号，则保存key-value元组。如果遇到已经保存的键，则保留与小于扫描序列号的最大序列号对应的值。否则，如果键序列号高于扫描序列号，扫描将重启，因为序列号高于扫描序列号可能意味着该键对应的值被并发操作覆盖。最后，对保存的键和值数组进行排序并返回。 重启是昂贵的，因为它们需要完全 re-drain Membuffer.。为了避免在写密集型工作负载中任意多次重启扫描，我们添加了一种称为 fallbackScan 的回退机制，当扫描被迫重启过多次时将触发该机制。fallbackScan 的工作原理是阻止 Memtable 的写入者，直到完成扫描。在我们的实验中(第5节)，回退扫描很少被触发，并且不会增加显著的开销。 Scan(lowKey , highKey): restartCount = 0 restart: pauseDrainingThreads = true pauseWriters = true IMM_MBF = MBF MBF = new MemBuffer () MemBufferRCUWait () MemTableRCUWait () IMM_MBF.drain() IMM_MBF.destroy () seq = globalSeqNumber.fetchAndIncrement () pauseWriters = false pauseDrainingThreads = false results = ∅ for dataStructure in MTB IMM_MTB DISK: iter = dataStructure.newIterator () iter.seek(lowKey) while (iter.isValid () and iter.key &lt; highKey): if iter.seq &gt; seq: restartCount += 1 if restartCount &lt; RESTART_THRESHOLD: goto restart else: return fallbackScan(lowKey , highKey) results.add(iter.key , iter.value) iter.next() results.sort() return results Multithreaded scans 如果多个线程同时进行扫描，则需要进行额外的同步，以避免多个线程各自创建一个Membuffer的副本并试图耗尽它的情况。为此，我们区分了两种类型的扫描:主扫描和 piggybacking 扫描。主扫描是在没有其他扫描并发运行时启动的扫描。piggybacking扫描是在其他扫描同时运行时启动的扫描。在任何给定的时间，只有一个主扫描可能正在运行。我们确保主扫描执行算法3的第4-14行，所有扫描执行第2行和第15-30行。piggyback扫描将等待，直到主扫描程序发布第12行获得的序列号，然后继续进行第15-28行中的实际扫描。请注意，如果在主扫描运行时启动的另一个piggyback扫描与主扫描并发，则在主扫描未运行时启动的piggyback扫描也可以启动。这个过程可以自我重复，创建长长的 piggybacking 扫描链，在链的开始处重用相同的主扫描序列号。我们通过一个系统参数来限制这些链的长度，以避免由于使用陈旧的序列号而重新启动的大量扫描。 当多个扫描并发运行时，上述方案会产生良好的性能，这是由于负载机制将耗尽的开销分散到多个扫描调用上，并且负载扫描在重新启动时不会重新耗尽Membuffer。针对低并发情况的一种优化方法是允许主扫描(除了附带扫描之外)重用前一个主扫描的序列号。这样可以避免过于频繁地完全耗尽内存缓冲区。 Correctness 在安全方面，建立一个新的扫描序列号的主扫描对于更新来说是线性化的。线性化点在算法3的第7行，在安装新的可变Membuffer的指令上。Draining 不可变的Membuffer可以确保线性化点之前的所有更新都包含在扫描中，并且第12行中获得的序列号可以确保线性化点之后的更新都不包含在扫描中。然而，piggyback扫描(以及重用现有序列号的主扫描)对于更新是不能线性化的(但它们是可序列化的)，因为它们可能会错过在它们的序列号建立之后发生的更新。如果在应用程序级别需要更严格的扫描一致性，可以指示扫描等待，直到它能够建立一个新的序列号，或者可以完全禁用扫描 piggyback。因此，FloDB中的所有扫描在更新方面都可以线性化，但要以性能为代价(每次扫描在继续之前都必须耗尽Membuffer，这是一个昂贵的操作)。就活性而言，所有的扫描最终都会完成，这是由于 fallbackScan 机制，该机制不能被写入器作废，因此保证会返回。 Implementation Trade-offs 如第5节所示，在各种工作负载下，FloDB比它的竞争对手获得更好的性能。尽管如此，FloDB的性能是有代价的:我们用资源来换取性能。连接两个inmemory级别需要至少一个后台线程(即引流线程)在写密集型工作负载下几乎持续运行。此外，FloDB的扫描操作存在以下局限性。虽然理论上可以调用范围(−∞，+∞)上的扫描(这将返回整个数据库的快照)，但大型扫描可能会重启多次，为了成功完成，会触发针对所有 writer 的 block 操作，开销较大。该扫描算法仅适用于中小型扫描。 Evaluation 测试结果重点是测扩展性，就是说在不同的内存大小或者线程数上性能的提升，因为以往是到了一定程度性能会有所下降的，同时对比其他几个方案表明 FloDB 其实在扩展性这一块确实做的更好，从而把这个故事讲的更完整。 但是看下图结果很容易发现对于只读负载，在线程数大于等于 16 的时候，FloDB 就不如 RocksDB 及其变种 cLSM 了。作者给的解释是 FloDB 比 LevelDB 及其变体要好是因为毕竟采用了简化后的并发性支持更好的 Get 操作，但是比 RocksDB 差是因为在磁盘组件上没有像 RocksDB 那样做这么多优化，而大数据量的读操作很多都是在磁盘组件上最终进行的。这地方其实前面读者也有埋伏笔，就是说内存组件对于写的影响比对读的影响可能更大，所以才有 MemBuffer，然后当时也降到了和其他磁盘组件优化是正交的，所以也算是自圆其说吧。 其他测试就看原文吧，没啥特别想讲的了。 ","link":"https://blog.shunzi.tech/post/FloDB/"},{"title":"C 基础","content":" C 语言基础汇总，主要包括一些容易混淆的、容易忘记的知识点，经常需要查询的。好吧，其实就是菜~ 持续更新ing struct &amp; union 菜鸟教程 - C 结构体 菜鸟教程 - C 共用体 union 定义 union [union tag] { member definition; member definition; ... member definition; } [one or more union variables]; 示例 Data 类型的变量可以存储一个整数、一个浮点数，或者一个字符串。这意味着一个变量（相同的内存位置）可以存储多个多种类型的数据。您可以根据需要在一个共用体内使用任何内置的或者用户自定义的数据类型。 union Data { int i; float f; char str[20]; } data; 共用体占用的内存应足够存储共用体中最大的成员。例如，在上面的实例中，Data 将占用 20 个字节的内存空间，因为在各个成员中，字符串所占用的空间是最大的。下面的实例将显示上面的共用体占用的总内存大小： #include &lt;stdio.h&gt; #include &lt;string.h&gt; union Data { int i; float f; char str[20]; }; int main( ) { union Data data; // Memory size occupied by data : 20 printf( &quot;Memory size occupied by data : %d\\n&quot;, sizeof(data)); data.i = 10; data.f = 220.5; strcpy( data.str, &quot;C Programming&quot;); printf( &quot;data.i : %d\\n&quot;, data.i); printf( &quot;data.f : %f\\n&quot;, data.f); printf( &quot;data.str : %s\\n&quot;, data.str); return 0; } data.i : 1917853763 data.f : 4122360580327794860452759994368.000000 data.str : C Programming 共用体的 i 和 f 成员的值有损坏，因为最后赋给变量的值占用了内存位置，这也是 str 成员能够完好输出的原因。 作用 节省内存，有两个很长的数据结构，不会同时使用，比如一个表示老师，一个表示学生，如果要统计教师和学生的情况用结构体的话就有点浪费了！用共用体的话，只占用最长的那个数据结构所占用的空间，就足够了！ 通信中的数据包会用到共用体:因为不知道对方会发一个什么包过来，用共用体的话就很简单了，定义几种格式的包，收到包之后就可以直接根据包的格式取出数据。 大小分配 结构体变量所占内存长度是其中最大字段大小的整数倍（参考：结构体大小的计算）。 共用体变量所占的内存长度等于最长的成员变量的长度。例如，教程中定义的共用体Data各占20个字节（因为char str[20]变量占20个字节）,而不是各占4+4+20=28个字节。 union的长度取决于其中的长度最大的那个成员变量的长度。即union中成员变量是重叠摆放的，其开始地址相同。 union mm{ char a;//元长度1 int b[5];//元长度4 double c;//元长度8 int d[3]; //元长度4 }; 本来mm的空间应该是sizeof(int)*5=20;但是如果只是20个单元的话,那可以存几个double型(8位)呢?两个半?当然不可以,所以mm的空间延伸为既要大于20,又要满足其他成员所需空间的整数倍,,因为含有double元长度8，故大小为24。 struct 定义 struct tag { member-list member-list member-list ... } variable-list ; 示例 在一般情况下，tag、member-list、variable-list 这 3 部分至少要出现 2 个。以下为实例： //此声明声明了拥有3个成员的结构体，分别为整型的a，字符型的b和双精度的c //同时又声明了结构体变量s1 //这个结构体并没有标明其标签 struct { int a; char b; double c; } s1; //此声明声明了拥有3个成员的结构体，分别为整型的a，字符型的b和双精度的c //结构体的标签被命名为SIMPLE,没有声明变量 struct SIMPLE { int a; char b; double c; }; //用SIMPLE标签的结构体，另外声明了变量t1、t2、t3 struct SIMPLE t1, t2[20], *t3; //也可以用typedef创建新类型 typedef struct { int a; char b; double c; } Simple2; //现在可以用Simple2作为类型声明新的结构体变量 Simple2 u1, u2[20], *u3; 在上面的声明中，第一个和第二声明被编译器当作两个完全不同的类型，即使他们的成员列表是一样的，如果令 t3=&amp;s1，则是非法的。 结构体的成员可以包含其他结构体，也可以包含指向自己结构体类型的指针，而通常这种指针的应用是为了实现一些更高级的数据结构如链表和树等。 //此结构体的声明包含了其他的结构体 struct COMPLEX { char string[100]; struct SIMPLE a; }; //此结构体的声明包含了指向自己类型的指针 struct NODE { char string[100]; struct NODE *next_node; }; 如果两个结构体互相包含，则需要对其中一个结构体进行不完整声明，如下所示： struct B; //对结构体B进行不完整声明 //结构体A中包含指向结构体B的指针 struct A { struct B *partner; //other members; }; //结构体B中包含指向结构体A的指针，在A声明完后，B也随之进行声明 struct B { struct A *partner; //other members; }; 实际使用示例 #include &lt;stdio.h&gt; #include &lt;string.h&gt; struct Books { char title[50]; char author[50]; char subject[100]; int book_id; }; /* 函数声明 */ void printBook( struct Books *book ); int main( ) { struct Books Book1; /* 声明 Book1，类型为 Books */ struct Books Book2; /* 声明 Book2，类型为 Books */ /* Book1 详述 */ strcpy( Book1.title, &quot;C Programming&quot;); strcpy( Book1.author, &quot;Nuha Ali&quot;); strcpy( Book1.subject, &quot;C Programming Tutorial&quot;); Book1.book_id = 6495407; /* Book2 详述 */ strcpy( Book2.title, &quot;Telecom Billing&quot;); strcpy( Book2.author, &quot;Zara Ali&quot;); strcpy( Book2.subject, &quot;Telecom Billing Tutorial&quot;); Book2.book_id = 6495700; /* 通过传 Book1 的地址来输出 Book1 信息 */ printBook( &amp;Book1 ); /* 通过传 Book2 的地址来输出 Book2 信息 */ printBook( &amp;Book2 ); return 0; } void printBook( struct Books *book ) { printf( &quot;Book title : %s\\n&quot;, book-&gt;title); printf( &quot;Book author : %s\\n&quot;, book-&gt;author); printf( &quot;Book subject : %s\\n&quot;, book-&gt;subject); printf( &quot;Book book_id : %d\\n&quot;, book-&gt;book_id); } Book title : C Programming Book author : Nuha Ali Book subject : C Programming Tutorial Book book_id : 6495407 Book title : Telecom Billing Book author : Zara Ali Book subject : Telecom Billing Tutorial Book book_id : 6495700 大小分配 博客园：结构体大小的计算 结构体中成员变量分配的空间是按照成员变量中占用空间最大的来作为分配单位,同样成员变量的存储空间也是不能跨分配单位的,如果当前的空间不足,则会存储到下一个分配单位中。 结构体变量的首地址能够被其最宽基本类型成员的大小所整除。 结构体每个成员相对于结构体首地址的偏移量(offset)都是成员大小的整数倍，如有需要编译器会在成员之间加上填充字节(internal adding)。即结构体成员的末地址减去结构体首地址(第一个结构体成员的首地址)得到的偏移量都要是对应成员大小的整数倍。 结构体的总大小为结构体最宽基本类型成员大小的整数倍，如有需要编译器会在成员末尾加上填充字节。 弹性数组 定义数组时，没有指明其长度，此为弹性数组。 弹性数组只能存在于结构体中，并且必须满足如下条件： 弹性数组必须为结构体的最后一个成员； 该结构体必须包含一个非弹性数组的成员； 编译器需要支持 C99 标准。 示例 #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;stdint.h&gt; ​ typedef struct { int32_t id; int32_t grade; int8_t name[]; }student_info_struct; ​ int main() { int32_t *tmp = 0; int8_t *name = &quot;sdc&quot;; student_info_struct *si = NULL; ​ printf(&quot;sizeof(struct) = %d\\n&quot;, sizeof(student_info_struct)); ​ si = (student_info_struct *)malloc(sizeof(student_info_struct) + strlen(name) + 1); // +1 是为了存储 '\\0' if(NULL == si) { printf(&quot;malloc failed\\n&quot;); return -1; } memset((void *)si, 0, sizeof(student_info_struct) + strlen(name) + 1); ​ si-&gt;id = 123; si-&gt;grade = 6; memcpy((void *)si-&gt;name, name, strlen(name)); ​ printf(&quot;addr:\\n&quot;); printf(&quot;si: 0x%p\\n&quot;, si); printf(&quot;si-&gt;grade: 0x%p\\n&quot;, &amp;si-&gt;grade); printf(&quot;si-&gt;name: 0x%p\\n&quot;, &amp;si-&gt;name); printf(&quot;si-&gt;name: 0x%p\\n&quot;, si-&gt;name); ​ return 0; } 输出结果： sizeof(struct) = 8 // 弹性数组不占空间 addr: &amp;si: 0x00000000003C21C0 &amp;si-&gt;grade: 0x00000000003C21C4 &amp;si-&gt;name: 0x00000000003C21C8 si-&gt;name: 0x00000000003C21C8 ","link":"https://blog.shunzi.tech/post/c-basic/"},{"title":"Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience","content":" FAST21: Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience Abstract RocksDB是一个针对大规模分布式系统的键值存储，并针对固态硬盘(ssd)进行了优化。本文描述了RocksDB在过去8年中开发重点的变化。这种演变是硬件发展趋势的结果，也是许多公司大规模生产RocksDB的丰富经验的结果。RocksDB在多个组织进行规模化生产。我们将描述RocksDB的资源优化目标是如何以及为什么从写放大、到空间放大、到CPU利用率迁移的。大规模应用的运行经验告诉我们，需要跨不同的RocksDB实例管理资源分配，数据格式需要保持向后和向前兼容，以允许增量软件上线，还需要对数据库复制和备份提供适当的支持。失败处理的教训告诉我们，需要更早地在系统的每一层检测到数据损坏错误。 Introduction RocksDB 起源于 LevelDB，针对 SSD 的一些特性进行了优化，以分布式应用程序为目标，同时也被设计成可以被嵌入到一些高级应用程序（如 Ceph）的 KV 库。每个RocksDB实例只管理单个节点上的数据，没有处理任何节点之间的操作（比如 replication、 loadbalancing），也不执行高级操作，比如 checkpoints，让上层应用来实现，底层只是提供一些支持。 RocksDB 可以根据负载以及性能需求进行定制和调优，主要包括对 WAL 的处理、压缩策略的选择。 RocksDB 应用广泛： Database: MySQL, Rocksandra, CockroachDB, MongoDB, TiDB Stream processing：Apache Flink, Kafka Stream, Samza, and Facebook’s Stylus. Logging/queuing services: Facebook’s LogDevice, Uber’s Cherami, Iron.io Index services: Facebook’s Dragon, Rockset Caching on SSD: Caching on SSD, Qihoo’s Pika, Redis 对不同应用特性进行了简单总结： FAST20 之前的一篇介绍 Facebook 实际的 RocksDB 负载特性也得出了一些数据信息： 使用如 RocksDB 这种公共的存储引擎有利有弊： 劣势在于每个系统需要基于 RocksDB 构建自己的子系统，那么就涉及到一系列复杂的崩溃一致性的保证操作（但比较好的公共存储引擎一般会 cover 这一点） 优势在于很多基础功能是可以复用的 其实就是因为 RocksDB 的应用广泛，以及一些硬件技术的发展，导致 RocksDB 的开发重点也在不断地变化。 Background flash的特性对RocksDB的设计产生了深刻的影响。读写性能的不对称和有限的寿命给数据结构和系统架构的设计带来了挑战和机遇。因此，RocksDB采用了flash友好的数据结构，并针对现代硬件进行了优化。 Embedded storage on flash based SSDs 低延迟高吞吐的高性能 SSD 的出现促进了相应设计的变化，在一些场景里，以往的 IO 瓶颈可能转移到了网络上，无论是吞吐量还是延迟。所以这时候数据存储到本地的 SSD 相比于存储到远端性能要好很多，这时候嵌入式的存储引擎的需求就增加了。 RocksDB 就基于该场景产生了，并基于 LSM 开始了相关设计。 RocksDB architecture LSM 树是 RocksDB 存储数据的主要数据结构 主要操作 写 步骤： 首先写入到名为 MemTable 的内存 Buffer 和磁盘上的 WAL 中 MemTable 基于跳表实现有序，插入和查询的复杂度均为 logN WAL 用于故障恢复，但其实不是强制的。 然后持续写入。一旦 MemTable 达到一定的大小（设定的阈值），当前 MemTable 和 WAL 就不可修改了 分配一个新的 MemTable 和 WAL 来接受后续的写入 不可修改的 Memtable 刷入到磁盘上的 SSTable 文件中 刷入后的 Memtable 和 WAL 相应地被丢弃 每个SSTable按排序顺序存储数据，并将其划分为大小相同的块。每个SSTable也有一个索引块，每个SSTable块有一个索引项用于二分查找。 Read 在读取路径中，在每个后续级别都进行键查找，直到找到键为止。它首先搜索所有memtable，然后搜索所有级别为0的sstable，然后依次搜索更高级别的sstable。在每一个级别上，都使用二分查找。Bloom过滤器用于消除SSTable文件中不必要的搜索。扫描要求搜索所有级别。 Compaction 最新的sstable由MemTable刷新创建，并放置在0级。高于0级的级别是由称为压缩的流程创建的。给定级别上sstable的大小受到配置参数的限制。当超过level-L的size目标时，选择level-L中的一些sstable与level-(L+1)中的重叠sstable合并。这样做，deleted 和 overwritten 的数据被删除，并优化表的读性能和空间效率。这个过程将写数据从0级逐渐迁移到最后一级。压缩I/O是高效的，因为它可以并行化，并且只涉及对整个文件的批量读写。 Level-0 sstable有重叠的键范围，因为SSTable覆盖了完整的 sorted run。后面的每个级别只包含一个sorted run，因此这些级别中的sstable包含其级别 sorted run 的一个分区。 RocksDB支持多种不同类型的压缩。 Leveled Compaction 是从LevelDB继承的，然后进行了改进，如上图所示，Level 的 target 大小呈指数级增长 Tiered Compaction （又称 Universal Compaction）是和 Cassandra 以及 HBase 使用的策略类似的。多个 sorted run 被延迟压缩在一起，或者当有太多的 sorted run，或者DB总大小与最大 sorted run 的大小之比超过了一个可配置的阈值。 FIFO Compaction 当 DB 大小达到某个阈值限制时直接丢弃以前的文件并只执行轻量压缩，主要用于内存缓存应用。 通过使用不同的压缩策略，RocksDB 可以被配置为 读友好/写友好/对某些特殊缓存负载非常写友好。然而，应用程序所有者将需要考虑他们特定用例的不同指标之间的权衡。 一个 lazy 的压缩策略可以提升写吞吐量和写放大，但读性能将下降 虽然更积极的压缩会牺牲写性能，但允许更快的读取。 日志和流处理服务需要进行大量的写操作，而数据库应用更喜欢进行权衡。表3通过微基准测试结果描述了这种灵活性。 RocksDB History Evolution of resource optimization targets 资源优化目标的转变过程：从写放大到空间放大再到 CPU 利用率 Write amplification 当初开发 RockDB 时主要关注节省 Flash 颗粒的擦除次数来减小 SSD 内部的写放大，这是当时社区的普遍看法，这个对于很多写密集型的应用确实是个极大的挑战，现在也仍然是个问题。 写放大实际出现在两个层次： SSD 本身的写放大，观察发现在 1.1 到 3 之间。 存储和数据库软件也会产生写放大，有时候甚至可能达到 100（比如当对小于100字节的更改写入整个4KB/8KB/16KB页时） Leveled Compaction 在 RocksDB 中通常显示出了 10-30 的写放大，在很多情况下，比使用 b 树要好好几倍。不过，10-30范围内的写放大对于写密集的应用程序来说还是太高了。 Tiered Compaction 将写放大降低到了 4-10，但也降低了读性能。如下图以及上表所示的对比结果： RocksDB应用程序所有者通常会选择一种压缩方法，在写速率高的时候减少写放大，在写速率低的时候更积极地压缩，以实现空间效率和读性能的目标。 Space amplification 经历了几年的开发之后发现，考虑到闪存写周期和写开销都没有限制，Space amplification 比 Write amplification 更重要，实际生产环境中表现出的 IOPS 是要比 SSD 本身可以提供的性能要低的，因此，我们将资源优化目标转移到磁盘空间。 幸运的是，由于lsm -tree的非碎片数据布局，它在优化磁盘空间时也可以很好地工作。然而，我们看到了通过减少 LSM 树中的老数据(即删除和覆盖的数据)的数量来改进 Leveled Compaction 的机会，因此开发了 Dynamic Leveled Compaction，树中每个 Level 的大小会根据最后一个 Level 的实际大小自动调整(而不是静态地设置每个 Level 的大小)。该策略相比于 Leveled Compaction 实现了更好的以及更稳定的空间效率。 CPU utilization 很多人认为对于高速 SSD，已经足够快，性能瓶颈已经从 SSD 转移到了 CPU。基于我们的经验，我们并不认为这是一个问题，我们也不希望它成为未来基于 SSD 的 NAND 闪存的问题，原因有二。 只有少部分应用被 SSD 提供的 IOPS 给限制，大多数应用还是被空间给限制。 其次，我们发现任何具有高端 CPU 的服务器都有足够的计算能力来饱和一个高端 SSD，在我们的环境中，RocksDB在充分利用SSD性能方面从来没有遇到过问题。 当然，可以配置一个系统，使 CPU 变成一个瓶颈（比如一个 CPU，多个 SSD）。然而，高效的系统通常是那些配置为良好平衡的系统，这是今天的技术所允许的 密集的写为主导的工作负载也可能导致CPU成为瓶颈。对于某些情况，可以通过配置RocksDB使用更轻量级的压缩选项来缓解这种问题。 对于其他情况，工作负载可能根本不适合SSD，因为它将超过允许SSD持续2-5年的典型闪存耐久预算。 为了证实我们的观点，我们调查了生产中 ZippyDB 和 MyRocks 的 42 种不同部署，它们分别服务于不同的应用程序。下图测试结果表明 大多数工作负载都受到空间限制。有些主机的确CPU利用率很高，但是主机通常没有得到充分的利用，没有为增长和处理数据中心或区域级故障留出空间(或由于错误配置)。 然而，减少 CPU 开销已经成为一个重要的优化目标，因为减少空间放大已经做的差不多了。减少CPU开销可以提高CPU确实受到限制的少数应用程序的性能，更重要的是，减少CPU开销的优化允许更经济有效的硬件配置，从成本的角度讲该优化也挺有必要的。 早期降低CPU开销的努力包括引入前缀bloom过滤器、在索引查找之前应用bloom过滤器，以及其他bloom过滤器改进。还有进一步改善的空间。 Adapting to newer technologies SSD 架构相关的改进可能很容易瓦解 RocksDB 的相关性，比如 OC-SSD，Multi-Stream SSD，ZNS 承诺改善查询延迟和节省flash擦除周期。然而，这些新技术只能使使用 RocksDB 的少数应用程序受益，因为大多数应用程序都受到空间限制，而没有消除周期或延迟限制。此外，如果让RocksDB直接使用这些技术，将会对RocksDB的经验构成挑战，一个可能的方法是将这些技术的适应能力委托给底层文件系统，也许RocksDB会给底层文件系统提供一定的额外的提示。 存储内计算可能会带来显著的收益，但目前还不清楚有多少RocksDB应用能从中受益。我们认为，RocksDB 要适应存储内计算将是一个挑战，可能需要对整个软件堆栈进行API更改，以充分利用。我们期待未来关于如何最好地做到这一点的研究。 计算型存储/存算一体如何实现？ 分类(远程)存储似乎是一个更有趣的优化目标，并且是当前的优先级，到目前为止，我们的优化假设flash是本地连接的，因为我们的系统基础设施主要是这样配置的。然而，目前更快的网络可以提供更多的远程 I/O，因此在越来越多的应用程序中，使用远程存储运行RocksDB的性能是可行的。使用远程存储，可以更容易地同时充分利用 CPU 和 SSD 资源，因为它们可以根据需要分别提供(使用本地附加的SSD很难实现这一点)，因此，优化 RocksDB 的远程闪存存储已成为当务之急。我们目前正在通过尝试合并和并行 I/O 来解决长 I/O 延迟的挑战。我们已经对RocksDB进行了改造，以处理瞬时故障，将QoS要求传递给底层系统，并报告分析信息。然而，还需要做更多的工作。 存储级内存(SCM)是一种很有前途的技术。我们正在研究如何最好地利用它。以下几种可能性值得考虑: (NVM) SCM介绍 使用 SCM 作为 DRAM 的延申，这就提出了这样的问题:如何用混合的 DRAM 和 SCM 来最好地实现关键数据结构(例如，块缓存或 memtable)，以及在尝试利用所提供的持久性时会引入哪些开销 使用 SCM 作为数据库的主存储，但是我们注意到RocksDB往往会受到空间或CPU的瓶颈，而不是 I/O 为 WAL 使用 SCM，但是，这就提出了这样一个问题:考虑到我们只需要在转移到 SSD 之前的一小块 staging 区域，这个用例是否单独证明了 SCM 的成本是合理的。 Main Data Structure Revisited 我们不断地重新讨论lsm -tree是否仍然合适的问题，但仍然得出它们确实合适的结论。SSD的价格还没有下降到足以改变大多数用例的空间和闪存续航瓶颈的程度，用CPU或DRAM交换SSD的使用只对少数应用程序有意义，虽然主要结论是一致的，但我们经常听到用户对写放大的要求低于RocksDB所能提供的。然而，我们注意到，当对象大小很大时，可以通过分离键和值来减少写放大，比如 WiscKey 和 ForrestDB，所以在 RocksDB 中也添加了这种支持 BlobDB Lessons on serving large-scale systems RocksDB是各种需求各异的大型分布式系统的基石。随着时间的推移，我们了解到需要在资源管理、WAL处理、批处理文件删除、数据格式兼容性和配置管理方面进行改进。 Resource management 分布式系统常常对数据进行分片，分片成 shards，shards 分布在多个服务器存储节点上，大小通常有限，因为要进行备份以及负载均衡，同时也会作为原子粒度进行一些一致性的保证。因此每个节点大概会有几十上百个 shards，一个 RocksDB 实例通常只服务于一个 shard，所以一个服务节点上可能通常会运行多个 RocksDB 实例。可能共享地址空间，也可能使用自己独立的地址空间。 一台主机可能会运行多个RocksDB实例，这对资源管理有一定影响。假设这些实例共享主机的资源，那么需要对资源进行全局(每个主机)和本地(每个实例)管理，以确保公平和有效地使用它们。当运行在单个进程模式下,拥有全局资源限制是很重要的,包括 (1) 内存写入缓冲器和块缓存, (2) Compaction 的 I/O 带宽、 (3) Compaction 线程数 (4) 总磁盘使用情况 (5) 文件删除率 还需要本地资源限制，例如，确保单个实例不能使用过多的任何资源。RocksDB允许应用程序为每种类型的资源创建一个或多个资源控制器(作为传递给不同DB对象的c++对象实现)，也可以在每个实例的基础上这样做。同时需要支持一些优先级策略，来保证一些资源可以优先被分配给一些最需要该资源的实例。 在一个进程中运行多个实例时得到的另一个教训是:大量地生成非池线程可能会有问题，特别是当线程是长期存在的时候。拥有太多的线程会增加CPU的概率，导致过多的上下文切换开销，并使调试变得极其困难，导致I/O strike。如果一个RocksDB实例需要使用一个可能会休眠或等待某个条件的线程来执行一些工作，那么最好使用一个线程池，这样可以方便地限制线程的大小和资源使用。 考虑到每个shard只有本地信息，当RocksDB实例运行在单独的进程中时，全局(每台主机)资源管理变得更加具有挑战性。可以采用两种策略。 首先，将每个实例配置为谨慎地使用资源，而不是贪婪地使用资源。例如，使用压缩，每个实例可以启动比“正常”更少的压缩，只有在压缩落后时才会增加。这种策略的缺点是，全局资源可能无法得到充分利用，导致资源使用不理想。 第二种，在操作上更具挑战性的策略是让实例之间共享资源使用信息，并相应地进行调整，试图在更大范围内优化资源使用。RocksDB还需要更多的工作来改善主机范围内的资源管理。 WAL treatment 传统的数据库倾向于在每一个写操作上强制执行write-ahead-log (WAL)，以确保持久性。相比之下，大型分布式存储系统通常为了性能和可用性复制数据，并且它们通过各种一致性保证来做到这一点。例如，当同一数据存在多个副本，其中一个副本损坏或无法访问时，存储系统会使用其他未受影响主机的有效副本重建故障主机的副本。对于这样的系统，RocksDB WAL 写的就不那么重要了。此外，分布式系统通常有自己的复制日志(例如Paxos日志)，在这种情况下，根本不需要RocksDB WAL。 我们了解到，提供调优WAL同步行为的选项，以满足不同应用程序的需要是很有帮助的。具体来说，我们介绍了不同的WAL操作模式:(i)同步WAL写，(i)缓冲WAL写，和(i)根本没有WAL写。对于buffered WAL处理，为了不影响RocksDB的流量延迟，在后台定期将WAL以低优先级写入磁盘。 Rate-limited file deletions RocksDB通常通过文件系统与底层存储设备交互。这些文件系统支持是能识别出 SSD 的;例如,XFS。每当一个文件被删除，使用实时丢弃，可以发出修剪命令 TRIM 到 SSD。修剪命令通常被认为可以改善 SSD 性能和 Flash 寿命，这是经过我们的生产经验验证的。然而也会产生性能问题，TRIM 破坏性很大：除了更新地址映射(通常在SSD的内部内存中)之外，SSD固件还需要将这些更改写入flash中的FTL的日志，这反过来可能触发SSD的内部垃圾收集，导致相当大的数据迁移，并对前台IO延迟造成负面影响。为了避免 TRIM 活动 spike 和I/O延迟的相关增加，我们引入了文件删除的速率限制，以防止多个文件同时被删除(在压缩后发生)。 Data format compatibility 型分布式应用程序在许多主机上运行它们的服务，它们期望零停机时间。结果，软件升级在主机之间逐步推出;当出现问题时，更新将被回滚。由于持续部署，这些软件升级频繁发生;RocksDB每个月都会发布一个新版本。因此，磁盘上的数据在不同的软件版本之间保持向后和向前兼容是很重要的。新升级(或回滚)的RocksDB实例必须能够理解前一个实例存储在磁盘上的数据。此外，为了构建副本或平衡负载，可能需要在分布式实例之间复制RocksDB数据文件，而这些实例可能运行不同的版本。由于缺少前向兼容性保障，在一些部署中，RocksDB的操作出现了困难，因此我们增加了这一保障。 RocksDB竭尽全力确保数据的前后兼容(除了新特性)。这在技术上和过程上都具有挑战性，但我们发现付出的努力是值得的。为了向后兼容，RocksDB必须能够理解之前写入磁盘的所有格式;这增加了软件和维护的复杂性。为了向前兼容，需要理解未来的数据格式，我们的目标是保持至少一年的向前兼容性。这可以通过使用通用技术部分实现，例如协议Buffer或Thrift所使用的技术。对于配置文件条目，RocksDB需要能够识别新的字段，并尽可能地猜测如何应用配置或何时放弃配置。我们不断地用不同版本的数据对不同版本的RocksDB进行测试。 Managing configurations RocksDB具有高度的可配置性，可以优化应用程序的工作负载。然而，我们发现配置管理是一个挑战。最初，RocksDB继承了LevelDB的参数配置方法，将参数选项直接嵌入到代码中。这造成了两个问题。 首先，参数选项通常与存储在磁盘上的数据绑定在一起，当使用一个选项创建的数据文件不能被新配置了另一个选项的RocksDB实例打开时，可能会产生兼容性问题。 其次，代码中没有明确指定的配置选项会自动设置为RocksDB的默认值。当RocksDB软件更新包括改变默认配置参数(例如，增加内存使用量或压缩并行度)时，应用程序有时会遇到意想不到的后果 为了解决这些问题，RocksDB首先引入了RocksDB实例使用包含配置选项的字符串参数打开数据库的能力。后来，RocksDB引入了对随数据库存储选项文件的可选支持。我们还引入了两个工具:(i)验证工具，验证打开数据库的选项是否与目标数据库兼容;(ii)迁移工具重写数据库以兼容所需的选项(尽管这个工具是有限的) RocksDB配置管理的一个更严重的问题是大量的配置选项。在RocksDB的早期，我们的设计选择是支持高度定制:我们引入了许多新的旋钮，并引入了可插拔组件的支持，所有这些都允许应用程序实现其性能潜力。这被证明是一种成功的策略，能够在早期获得最初的吸引力。然而，现在一个常见的抱怨是，选择太多了，很难理解它们的影响;也就是说，它变得非常困难 除了要调优许多配置参数之外，更令人生畏的是，最优配置不仅取决于嵌入了RocksDB的系统，还取决于上面应用程序产生的工作负载。以ZippyDB为例，ZippyDB是一个内部开发的大型分布式键值存储系统，在其节点上使用了RocksDB。ZippyDB服务于许多不同的应用程序，有时是单独的，有时是在多租户设置中。尽管在尽可能跨所有ZippyDB用例使用统一配置方面进行了大量努力，但不同用例的工作负载是如此不同，当性能很重要时，统一配置实际上是不可行的。表5显示了我们抽样的39个ZippyDB部署的25种不同配置 对于已交付给第三方的嵌入式RocksDB系统，调整配置参数也尤其具有挑战性。考虑第三方在其应用程序中使用MySQL或ZippyDB等数据库。第三方通常对RocksDB知之甚少，也不知道如何对其进行最佳调整。数据库所有者也不太愿意对客户的系统进行调优 这些真实的经验教训触发了我们配置支持策略的变化。我们在改进开箱即用性能和简化配置上花费了大量的精力。我们目前的重点是提供自动适应性，同时继续支持广泛的显式配置，考虑到RocksDB继续服务于专门的应用程序。我们注意到在追求自适应性的同时保持显式的可配置性会造成大量的代码维护开销，我们相信拥有一个统一的存储引擎的好处要大于代码的复杂性。 Replication and backup support RocksDB是一个单节点库。如果需要，使用RocksDB的应用程序将负责复制和备份。每个应用程序都以自己的方式实现这些函数(出于合理的原因)，因此RocksDB对这些函数提供适当的支持是很重要的。 通过复制现有副本的所有数据来引导一个新的副本可以通过两种方式完成。 首先，可以从源副本读取所有键，然后将其写入目标副本(逻辑复制)。在源端，RocksDB支持数据扫描操作，能够最大限度地减少对并发在线查询的影响;例如，通过提供选项来不缓存这些操作的结果，从而防止缓存垃圾化。在目标端，支持散装装载，并针对此场景进行了优化。 第二，可以通过直接复制sstable和其他文件(物理复制)来引导一个新的副本。RocksDB通过识别当前时间点的现有数据库文件，并防止它们被删除或更改，从而帮助物理复制。支持物理复制是RocksDB将数据存储在底层文件系统上的一个重要原因，因为它允许每个应用程序使用自己的工具。我们认为，RocksDB直接使用块设备接口或与FTL进行深度集成所带来的潜在性能提升，不会超过上述所述的好处。 备份是大多数数据库和其他应用程序的重要特性。对于备份，应用程序具有与复制相同的逻辑与物理选择。备份和复制之间的一个区别是，应用程序通常需要管理多个备份。虽然大多数应用程序都实现了自己的备份(以满足自己的需求)，但如果应用程序的备份需求很简单，RocksDB会提供一个备份引擎供其使用。 在这方面，我们看到有两个方面需要进一步改进，但都需要对key-value API进行更改; 第一个是在不同副本上以一致的顺序应用更新，这带来了性能挑战。 第二个问题涉及到每次发出一个写请求的性能问题，以及副本可能会落后，而应用程序可能希望这些副本能够更快地赶上 不同的应用程序实现了不同的解决方案来解决这些问题，但是它们都有局限性. 挑战在于，应用程序不能乱序写数据，也不能用它们自己的序列号读取快照，因为RocksDB目前不支持使用用户定义的时间戳进行多版本控制 Lessons on failure handling 通过生产经验，我们学到了三个主要的教训，关于故障处理。首先，需要尽早检测数据损坏，以最小化数据不可用或丢失的风险，并通过这样做来查明错误的起源。其次，完整性保护必须覆盖整个系统，以防止静默的破坏暴露给RocksDB客户或传播到其他副本(见图4)。第三，错误需要以不同的方式处理. Frequency of silent corruptions 由于性能原因，RocksDB用户通常不使用SSD(如DIF/DIX)的数据保护，存储介质损坏由RocksDB块校验和检测，这是所有成熟数据库的常规特性，因此我们在这里跳过分析。CPU/内存损坏很少发生，而且很难准确量化。使用RocksDB的应用程序通常会进行数据一致性检查，比较副本的完整性。这可以捕获错误，但这些错误可能是由RocksDB或客户端应用程序引入的(例如，在复制、备份或恢复数据时)。 我们发现，通过比较MyRocks数据库表中的主索引和二级索引，可以估计在RocksDB级别引入的损坏频率;任何不一致都可能在RocksDB级别上出现，包括CPU或内存损坏。根据我们的测量，大约每三个月，每100PB的数据就会出现一次RocksDB级别的故障。更糟糕的是，在这些案件中，有40%的损坏已经传播到其他副本。 在传输数据时也会发生数据损坏，通常是由于软件错误。例如，在处理网络故障时，底层存储系统中的错误导致我们在一段时间内看到，传输的每拍字节的物理数据大约有17个校验和不匹配。 Multi-layer protection 需要尽早检测到数据损坏，以减少停机时间和数据丢失。大多数RocksDB应用都将数据复制到多个主机上;当检测到校验和不匹配时，将丢弃损坏的副本，并用正确的副本替换。然而，只有在正确的副本仍然存在时，这才是可行的选择。 如今，RocksDB在多个层次上对文件数据进行校验和，以识别底层的损坏情况。这些以及计划的应用层校验和如图4所示。多层校验和是重要的，主要是因为它们有助于及早发现错误，也因为它们可以防止不同类型的威胁。从LevelDB继承的块校验和可以防止文件系统或文件系统以下的数据损坏暴露给客户端。文件校验和是在2020年增加的，用于防止底层存储系统传播到其他副本造成的损坏，以及防止在通过网络传输SSTable文件时造成的损坏。对于WAL文件，切换校验和能够在写时有效地早期检测损坏。 Block integrity 每个SSTable块或WAL片段都附加了一个校验和，在数据创建时生成。与仅在文件移动时才验证的文件校验和不同，这个校验和在每次读取数据时都要验证，因为它的作用域更小。这样做可以防止存储层损坏的数据暴露给RocksDB客户端。 File integrity 文件内容特别有可能在传输操作期间被破坏;例如，用于备份或分发SSTable文件。为了解决这个问题，sstable被它们自己的校验和保护，这些校验和是在创建表时生成的。SSTable的校验和记录在元数据的SSTable文件条目中，并在传输时使用SSTable文件进行验证。然而，我们注意到其他文件，如WAL文件，仍然没有采用这种方式保护。 Handoffintegrity 早期检测写损坏的一种已建立的技术是对将要写入底层文件系统的数据生成一个切换校验和，并将其与数据一起传递下去，由底层进行验证。我们希望使用这样的write API来保护WAL的写操作，因为与sstable不同的是，WAL可以从每次追加的增量验证中获益。不幸的是，本地文件系统很少支持这一点——但是，一些专门的堆栈，比如Oracle ASM就支持。 另一方面，在远程存储上运行时，write API可以更改为接受校验和，并与存储服务的内部ECC挂钩。RocksDB可以在现有的WAL片段校验和上使用校验和组合技术来高效地计算写切换校验和。由于我们的存储服务执行写时验证，我们希望将损坏检测延迟到读时的情况非常罕见。 End-to-end protection 虽然上面描述的保护层在许多情况下防止客户端暴露于损坏的数据，但它们并不全面。到目前为止提到的保护的一个不足是数据在文件I/O层以上是不受保护的;例如，MemTable中的数据和块缓存。在此级别损坏的数据将无法检测，因此最终将暴露给用户。此外，刷新或压缩操作可以持久化已损坏的数据，从而使损坏永久存在。 Key-value integrity 为了解决这个问题，我们目前正在实现每个键值校验和，以检测在文件I/O层之上发生的损坏。这个校验和将与键/值一起被复制，尽管我们将从已经存在替代完整性保护的文件数据中删除它。 Severity-based error handling RocksDB遇到的大多数错误是底层存储系统返回的错误。这些错误可能源于许多问题，从严重的问题(如只读文件系统)到短暂的问题(如全磁盘或访问远程存储的网络错误)。早期，RocksDB对此类问题的反应要么是简单地向客户端返回错误消息，要么是永久停止所有写操作。 今天，我们的目标是在错误不能在本地恢复的情况下中断RocksDB的操作;例如，临时网络错误不需要用户干预重启RocksDB实例。为了实现这一点，我们对RocksDB进行了改进，使其在遇到被归类为瞬态错误的错误后，能够周期性地重试恢复操作。因此，我们获得了运营上的优势，因为客户不需要为发生的大部分故障手动缓解RocksDB。 Lessons on the key-value interface 核心键值(KV)接口的用途惊人地多。几乎所有的存储工作负载都可以通过一个带有KV API的数据存储来提供;我们很少看到应用程序不能使用这个接口实现功能。这也许就是kv如此受欢迎的原因。KV接口是通用的。键和值都是可变长度的字节数组。应用程序在决定将哪些信息打包到每个键和值中方面具有很大的灵活性，而且它们可以从丰富的编码方案集中自由选择。因此，是应用程序负责解析和解释键和值。KV接口的另一个优点是可移植性。从一个键值系统迁移到另一个键值系统是相对容易的。然而，尽管许多用例通过这个简单的接口实现了最佳性能，但我们注意到它可能会限制一些应用程序的性能。 例如，在RocksDB之外构建并发控制是可能的，但很难提高效率，特别是在需要支持两阶段提交的情况下，在提交事务之前需要一些数据持久性。为此我们添加了事务支持，MyRocks (MySQL+RocksDB)使用了事务支持。我们会继续添加新功能;例如，gap/next 键锁定和大事务支持。 在其他情况下，这种限制是由key-value接口本身造成的。因此，我们已经开始研究基本键-值接口的可能扩展。其中一个扩展就是支持用户定义的时间戳 Versions and timestamps 在过去的几年里，我们已经认识到了数据版本控制的重要性。我们得出的结论是，版本信息应该成为RocksDB的头等公民，以适当地支持功能，如多版本并发控制(MVCC)和时间点读取。为了实现这一目标，RocksDB需要能够有效地访问不同的版本。 到目前为止，RocksDB内部一直在使用56位序列号来识别不同版本的kv -对。序列号由RocksDB生成，并在每次写入客户端时递增(因此，所有数据在逻辑上按排序顺序排列)。客户端应用程序不能影响序列号。然而，RocksDB允许应用程序对数据库进行快照，在此之后，RocksDB保证快照时存在的所有KV对将持续存在，直到应用程序显式地释放快照。因此，具有相同密钥的多个kv -对可能共存，并根据它们的序列号进行区分。 这种版本控制方法是不充分的，因为它并没有满足多种应用的要求。要从过去的状态读取数据，必须在前一个时间点已经拍摄了快照。RocksDB不支持对过去进行快照，因为没有API来指定时间点。此外，支持时间点读取是低效的。最后，每个RocksDB实例分配自己的序列号，并且只能根据每个实例获取快照。这使得具有多个(可能是复制的)分片(每个分片都是一个RocksDB实例)的应用程序的版本控制变得复杂。总之，创建提供跨分片一致读取的数据版本基本上是不可能的 应用程序可以通过在键或值中编码时间戳来绕开这些限制。但是，在这两种情况下，它们的性能都会下降。在键内进行编码会影响点查找的性能，而在值内进行编码会影响对相同键的无序写入的性能，并使读取旧版本的键变得复杂。我们相信应用程序指定的时间戳可以更好地解决这些限制，应用程序可以用可以全局理解的时间戳标记其数据，并且在键或值之外这样做 我们已经添加了对应用程序指定的时间戳的基本支持，并使用DB-Bench评估了这种方法。结果如表6所示。每个工作负载有两个步骤:第一步填充数据库，第二步测量性能。例如，在“fill_seq + read_random”中，我们通过按升序写一些键来填充初始数据库，在步骤2中执行随机读操作。相对于基线，应用程序将时间戳编码为密钥的一部分(对于RocksDB来说是透明的)，应用程序指定的时间戳API可以带来1.2倍或更好的吞吐量增益。这些改进来自于将时间戳作为元数据与用户键分开处理，因为这样就可以使用点查找而不是迭代器来获取键的最新值，并且Bloom过滤器可以识别不包含该键的sstable。此外，SSTable覆盖的时间戳范围可以存储在其属性中，这可以用来排除只能包含陈旧值的SSTable。 我们希望这个特性能够让用户更容易地在系统中实现单节点MVCC、分布式事务的多版本控制，或者解决多主复制中的冲突。然而，更复杂的API使用起来不那么简单，而且可能容易被误用。此外，与不存储时间戳相比，数据库将消耗更多的磁盘空间，而且对其他系统的可移植性也较差。 Related Work Storage Engine Libraries BerkeleyDB SQLite Hekaton Key-value stores for SSDs SILT: 在内存效率、CPU和性能之间进行平衡的键值存储 ForestDB: 使用 HB+ tree 在日志上建立索引 TokuDB：和其他数据库使用 FractalTree/Bε 树。 LOCS/NoFTL-KV/FlashKV 瞄准开放通道ssd以提高性能 LSM-tree improvements 一些系统也使用LSM树并改进了它们的性能。 写放大通常是最主要的优化目标： WiscKey PebblesDB IAM-tree TRIAD 这些系统在优化写放大方面比RocksDB做得更好，后者更关注不同指标之间的权衡。 SlimDB 优化 LSM 树空间效率 Monkey 试图在 DRAM 和 IOPs 之间实现平衡。 bLSM/VT-tree/cLSM 优化LSM树的一般性能 Large-scale storage systems Future Work and Open Questions 除了完成上面提到的改进，包括对分类聚合存储、键值分离、多层校验和和应用程序指定的时间戳的优化，我们计划统一 leveled 和 tiered 压缩并提高自适应能力。然而，一些开放的问题可以从进一步的研究中受益。 How can we use SSD/HDD hybrid storage to improve efficiency? How can we mitigate the performance impact on readers when there are many consecutive deletion markers? How should we improve our write throttling algorithms? Can we develop an efficient way of comparing two replicas to ensure they contain the same data? How can we best exploit SCM? Should we still use LSM tree and how to organize storage hierarchy? Can there be a generic integrity API to handle data handoff between RocksDB and the file system layer? Conclusion RocksDB已经从一个服务于小众应用的键值存储平台发展到目前广泛应用于众多工业大规模分布式应用的地位。作为主要数据结构的LSM树很好地服务于RocksDB，因为它表现出良好的写入和空间放大能力。然而，我们对业绩的看法是随着时间的推移而演变的。虽然写和空间放大仍然是主要关注的问题，但更多的焦点已经转移到CPU和DRAM效率，以及远程存储上。 运行大规模应用程序给我们带来了教训资源分配需要跨不同的RocksDB实例进行管理，数据格式需要保持向后和向前兼容，以允许增量软件部署，需要对数据库复制和备份提供适当的支持，配置管理需要简单且最好是自动化。失败处理的教训告诉我们，需要更早地在系统的每一层检测到数据损坏错误。key-value接口因其简单性和性能上的一些限制而广受欢迎。对界面进行一些简单的修改可能会产生更好的平衡。 ","link":"https://blog.shunzi.tech/post/RocksDB-Experience/"},{"title":"Series Three of Basic of Concurrency - Condition Variables","content":" 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第三篇（Condition Variables），条件变量。 Chapter Index Series One of Basic of Concurrency - Concurrency and Threads Series Two of Basic of Concurrency - Lock Series Three of Basic of Concurrency - Condition Variables Series Four of Basic of Concurrency - Semaphores Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency Condition Variables 到目前为止，我们已经形成了锁的概念，看到了如何通过硬件和操作系统支持的正确组合来实现锁。然而，锁并不是并发程序设计所需的唯一原语。 具体来说，在很多情况下，线程需要检查某一条件（condition）满足之后，才会继续运行。例如，父线程需要检查子线程是否执行完毕 [这常被称为 join()]。这种等待如何实现呢？我们来看如下所示的代码。 1 void *child(void *arg) { 2 printf(&quot;child\\n&quot;); 3 // XXX how to indicate we are done? 4 return NULL; 5 } 6 7 int main(int argc, char *argv[]) { 8 printf(&quot;parent: begin\\n&quot;); 9 pthread_t c; 10 Pthread_create(&amp;c, NULL, child, NULL); // create child 11 // XXX how to wait for child? 12 printf(&quot;parent: end\\n&quot;); 13 return 0; 14 } 我们期望能看到这样的输出： parent: begin child parent: end 我们可以尝试用一个共享变量，如下所示。这种解决方案一般能工作，但是效率低下，因为主线程会自旋检查，浪费 CPU 时间。我们希望有某种方式让父线程休眠，直到等待的条件满足（即子线程完成执行）。 1 volatile int done = 0; 2 3 void *child(void *arg) { 4 printf(&quot;child\\n&quot;); 5 done = 1; 6 return NULL; 7 } 8 9 int main(int argc, char *argv[]) { 10 printf(&quot;parent: begin\\n&quot;); 11 pthread_t c; 12 Pthread_create(&amp;c, NULL, child, NULL); // create child 13 while (done == 0) 14 ; // spin 15 printf(&quot;parent: end\\n&quot;); 16 return 0; 17 } CRUX: 多线程程序中，一个线程等待某些条件是很常见的。简单的方案是自旋直到条件满足，这是极其低效的，某些情况下甚至是错误的。那么，线程应该如何等待一个条件？ Definition and Routines 线程可以使用条件变量（condition variable），来等待一个条件变成真。条件变量是一个显式队列，当某些执行状态（即条件，condition）不满足时，线程可以把自己加入队列，等待（waiting）该条件。另外某个线程，当它改变了上述状态时，就可以唤醒一个或者多个等待线程（通过在该条件上发信号），让它们继续执行。Dijkstra 最早在“私有信号量”中提出这种思想。Hoare 后来在关于观察者的工作中，将类似的思想称为条件变量。 要声明这样的条件变量，只要像这样写：pthread_cond_t c;，这里声明 c 是一个条件变量（注意：还需要适当的初始化）。条件变量有两种相关操作：wait() 和 signal()。线程要睡眠的时候，调用 wait()。当线程想唤醒等待在某个条件变量上的睡眠线程时，调用 signal()。具体来说，POSIX 调用如下所示。 pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m); pthread_cond_signal(pthread_cond_t *c); 我们常简称为 wait()和 signal()。你可能注意到一点，wait()调用有一个参数，它是互斥量。它假定在 wait()调用时，这个互斥量是已上锁状态。wait()的职责是释放锁，并让调用线程休眠（原子地）。当线程被唤醒时（在另外某个线程发信号给它后），它必须重新获取锁，再返回调用者。这样复杂的步骤也是为了避免在线程陷入休眠时，产生一些竞态条件。我们观察一下如下所示代码中 join 问题的解决方法，以加深理解。 1 int done = 0; 2 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER; 3 pthread_cond_t c = PTHREAD_COND_INITIALIZER; 4 5 void thr_exit() { 6 Pthread_mutex_lock(&amp;m); 7 done = 1; 8 Pthread_cond_signal(&amp;c); 9 Pthread_mutex_unlock(&amp;m); 10 } 11 12 void *child(void *arg) { 13 printf(&quot;child\\n&quot;); 14 thr_exit(); 15 return NULL; 16 } 17 18 void thr_join() { 19 Pthread_mutex_lock(&amp;m); 20 while (done == 0) 21 Pthread_cond_wait(&amp;c, &amp;m); 22 Pthread_mutex_unlock(&amp;m); 23 } 24 25 int main(int argc, char *argv[]) { 26 printf(&quot;parent: begin\\n&quot;); 27 pthread_t p; 28 Pthread_create(&amp;p, NULL, child, NULL); 29 thr_join(); 30 printf(&quot;parent: end\\n&quot;); 31 return 0; 32 } 有两种情况需要考虑。 第一种情况是父线程创建出子线程，但自己继续运行（假设只有一个处理器），然后马上调用 thr_join()等待子线程。在这种情况下，它会先获取锁，检查子进程是否完成（还没有完成），然后调用 wait()，让自己休眠。子线程最终得以运行，打印出“child”，并调用 thr_exit()函数唤醒父进程，这段代码会在获得锁后设置状态变量 done，然后向父线程发信号唤醒它。最后，父线程会运行（从 wait()调用返回并持有锁），释放锁，打印出“parent:end”。 第二种情况是，子线程在创建后，立刻运行，设置变量 done 为 1，调用 signal 函数唤醒其他线程（这里没有其他线程），然后结束。父线程运行后，调用 thr_join()时，发现 done 已经是 1 了，就直接返回。 最后一点说明：你可能看到父线程使用了一个 while 循环，而不是 if 语句来判断是否需要等待。虽然从逻辑上来说没有必要使用循环语句，但这样做总是好的（后面我们会加以说明）。 为了确保理解 thr_exit()和 thr_join()中每个部分的重要性，我们来看一些其他的实现。首先，你可能会怀疑状态变量 done 是否需要。代码像下面这样如何？正确吗？ 1 void thr_exit() { 2 Pthread_mutex_lock(&amp;m); 3 Pthread_cond_signal(&amp;c); 4 Pthread_mutex_unlock(&amp;m); 5 } 6 7 void thr_join() { 8 Pthread_mutex_lock(&amp;m); 9 Pthread_cond_wait(&amp;c, &amp;m); 10 Pthread_mutex_unlock(&amp;m); 11 } 这段代码是有问题的。假设子线程立刻运行，并且调用 thr_exit()。在这种情况下，子线程发送信号，但此时却没有在条件变量上睡眠等待的线程。父线程运行时，就会调用 wait 并卡在那里，没有其他线程会唤醒它。通过这个例子，你应该认识到变量 done 的重要性，它记录了线程有兴趣知道的值。睡眠、唤醒和锁都离不开它。 下面是另一个糟糕的实现。在这个例子中，我们假设线程在发信号和等待时都不加锁。会发生什么问题？想想看！ 1 void thr_exit() { 2 done = 1; 3 Pthread_cond_signal(&amp;c); 4 } 5 6 void thr_join() { 7 if (done == 0) 8 Pthread_cond_wait(&amp;c); 9 } 这里的问题是一个微妙的竞态条件。具体来说，如果父进程调用 thr_join()，然后检查完 done 的值为 0，然后试图睡眠。但在调用 wait 进入睡眠之前，父进程被中断。子线程修改变量 done 为 1，发出信号，同样没有等待线程。父线程再次运行时，就会长眠不醒，这就惨了。 提示：发信号时总是持有锁 尽管并不是所有情况下都严格需要，但有效且简单的做法，还是在使用条件变量发送信号时持有锁。虽然上面的例子是必须加锁的情况，但也有一些情况可以不加锁，而这可能是你应该避免的。因此，为了简单，请在调用 signal 时持有锁（hold the lock when calling signal）。 这个提示的反面，即调用 wait 时持有锁，不只是建议，而是 wait 的语义强制要求的。因为 wait 调用总是假设你调用它时已经持有锁、调用者睡眠之前会释放锁以及返回前重新持有锁。因此，这个提示的一般化形式是正确的：调用 signal 和 wait 时要持有锁（hold the lock when calling signal or wait），你会保持身心健康的。 希望通过这个简单的 join 示例，你可以看到使用条件变量的一些基本要求。为了确保你能理解，我们现在来看一个更复杂的例子：生产者/消费者（producer/consumer）或有界缓冲区（bounded-buffer）问题。 The Producer/Consumer (Bounded Buffer) Problem 本章要面对的下一个问题，是生产者/消费者（producer/consumer）问题，也叫作有界缓冲区（bounded buffer）问题。这一问题最早由 Dijkstra 提出。实际上也正是通过研究这一问题，Dijkstra 和他的同事发明了通用的信号量（它可用作锁或条件变量）。 假设有一个或多个生产者线程和一个或多个消费者线程。生产者把生成的数据项放入缓冲区；消费者从缓冲区取走数据项，以某种方式消费。很多实际的系统中都会有这种场景。例如，在多线程的网络服务器中，一个生产者将 HTTP 请求放入工作队列（即有界缓冲区），消费线程从队列中取走请求并处理。 我们在使用管道连接不同程序的输出和输入时，也会使用有界缓冲区，例如 grep foo file.txt | wc -l。这个例子并发执行了两个进程，grep 进程从 file.txt 中查找包括“foo”的行，写到标准输出；UNIX shell 把输出重定向到管道（通过 pipe 系统调用创建）。管道的另一端是 wc 进程的标准输入，wc 统计完行数后打印出结果。因此，grep 进程是生产者，wc 是进程是消费者，它们之间是内核中的有界缓冲区，而你在这个例子里只是一个开心的用户。 因为有界缓冲区是共享资源，所以我们必须通过同步机制来访问它，以免产生竞态条件。为了更好地理解这个问题，我们来看一些实际的代码。- 首先需要一个共享缓冲区，让生产者放入数据，消费者取出数据。简单起见，我们就拿一个整数来做缓冲区（你当然可以想到用一个指向数据结构的指针来代替），两个内部函数将值放入缓冲区，从缓冲区取值。 1 int buffer; 2 int count = 0; // initially, empty 3 4 void put(int value) { 5 assert(count == 0); 6 count = 1; 7 buffer = value; 8 } 9 10 int get() { 11 assert(count == 1); 12 count = 0; 13 return buffer; 14 } 很简单，不是吗？put()函数会假设缓冲区是空的，把一个值存在缓冲区，然后把 count 设置为 1 表示缓冲区满了。get()函数刚好相反，把缓冲区清空后（即将 count 设置为 0），并返回该值。不用担心这个共享缓冲区只能存储一条数据，稍后我们会一般化，用队列保存更多数据项，这会比听起来更有趣。 现在我们需要编写一些函数，知道何时可以访问缓冲区，以便将数据放入缓冲区或从缓冲区取出数据。条件是显而易见的：仅在 count 为 0 时（即缓冲器为空时），才将数据放入缓冲器中。仅在计数为 1 时（即缓冲器已满时），才从缓冲器获得数据。如果我们编写同步代码，让生产者将数据放入已满的缓冲区，或消费者从空的数据获取数据，就做错了（在这段代码中，断言将触发）。 这项工作将由两种类型的线程完成，其中一类我们称之为生产者（producer）线程，另 一类我们称之为消费者（consumer）线程。下面展示了一个生产者的代码，它将一个整 数放入共享缓冲区 loops 次，以及一个消费者，它从该共享缓冲区中获取数据（永远不停），每次打印出从共享缓冲区中提取的数据项。 1 void *producer(void *arg) { 2 int i; 3 int loops = (int) arg; 4 for (i = 0; i &lt; loops; i++) { 5 put(i); 6 } 7 } 8 9 void *consumer(void *arg) { 10 while (1) { 11 int tmp = get(); 12 printf(&quot;%d\\n&quot;, tmp); 13 } 14 } A Broken Solution 假设只有一个生产者和一个消费者。显然，put()和 get()函数之中会有临界区，因为 put() 更新缓冲区，get()读取缓冲区。但是，给代码加锁没有用，我们还需别的东西。不奇怪，别的东西就是某些条件变量。在这个（有问题的）首次尝试中，我们用了条件变量 cond 和相关的锁 mutex。 1 int loops; // must initialize somewhere... 2 cond_t cond; 3 mutex_t mutex; 4 5 void *producer(void *arg) { 6 int i; 7 for (i = 0; i &lt; loops; i++) { 8 Pthread_mutex_lock(&amp;mutex); // p1 9 if (count == 1) // p2 10 Pthread_cond_wait(&amp;cond, &amp;mutex); // p3 11 put(i); // p4 12 Pthread_cond_signal(&amp;cond); // p5 13 Pthread_mutex_unlock(&amp;mutex); // p6 14 } 15 } 16 17 void *consumer(void *arg) { 18 int i; 19 for (i = 0; i &lt; loops; i++) { 20 Pthread_mutex_lock(&amp;mutex); // c1 21 if (count == 0) // c2 22 Pthread_cond_wait(&amp;cond, &amp;mutex); // c3 23 int tmp = get(); // c4 24 Pthread_cond_signal(&amp;cond); // c5 25 Pthread_mutex_unlock(&amp;mutex); // c6 26 printf(&quot;%d\\n&quot;, tmp); 27 } 28 } 来看看生产者和消费者之间的信号逻辑。当生产者想要填充缓冲区时，它等待缓冲区变空（p1～p3）。消费者具有完全相同的逻辑，但等待不同的条件——变满（c1～c3）。当只有一个生产者和一个消费者时，上图中的代码能够正常运行。但如果有超过一个线程（例如两个消费者），这个方案会有两个严重的问题。哪两个问题？ ……（暂停思考一下）…… 我们来理解第一个问题，它与等待之前的 if 语句有关。假设有两个消费者（Tc1 和 Tc2），一个生产者（Tp）。首先，一个消费者（Tc1）先开始执行，它获得锁（c1），检查缓冲区是否可以消费（c2），然后等待（c3）（这会释放锁）。 接着生产者（Tp）运行。它获取锁（p1），检查缓冲区是否满（p2），发现没满就给缓冲区加入一个数字（p4）。然后生产者发出信号，说缓冲区已满（p5）。关键的是，这让第一个消费者（Tc1）不再睡在条件变量上，进入就绪队列。Tc1 现在可以运行（但还未运行）。生产者继续执行，直到发现缓冲区满后睡眠（p6,p1-p3）。 这时问题发生了：另一个消费者（Tc2）抢先执行，消费了缓冲区中的值（c1,c2,c4,c5,c6，跳过了 c3 的等待，因为缓冲区是满的）。现在假设 Tc1 运行，在从 wait 返回之前，它获取了锁，然后返回。然后它调用了 get() (p4)，但缓冲区已无法消费！断言触发，代码不能像预期那样工作。显然，我们应该设法阻止 Tc1 去消费，因为 Tc2 插进来，消费了缓冲区中之前生产的一个值。下表展示了每个线程的动作，以及它的调度程序状态（就绪、运行、睡眠）随时间的变化。 问题产生的原因很简单：在 Tc1 被生产者唤醒后，但在它运行之前，缓冲区的状态改变了（由于 Tc2）。发信号给线程只是唤醒它们，暗示状态发生了变化（在这个例子中，就是值已被放入缓冲区），但并不会保证在它运行之前状态一直是期望的情况。信号的这种释义常称为Mesa 语义（Mesa semantic），为了纪念以这种方式建立条件变量的首次研究。另一种释义是 Hoare 语义（Hoare semantic），虽然实现难度大，但是会保证被唤醒线程立刻执行。实际上，几乎所有系统都采用了 Mesa 语义。 Better, But Still Broken: While, Not If 幸运的是，修复这个问题很简单：把 if 语句改为 while。当消费者 Tc1 被唤醒后，立刻再次检查共享变量（c2）。如果缓冲区此时为空，消费者就会回去继续睡眠（c3）。生产者中相应的 if 也改为 while（p2）。 1 int loops; // must initialize somewhere... 2 cond_t cond; 3 mutex_t mutex; 4 5 void *producer(void *arg) { 6 int i; 7 for (i = 0; i &lt; loops; i++) { 8 Pthread_mutex_lock(&amp;mutex); // p1 9 while (count == 1) // p2 10 Pthread_cond_wait(&amp;cond, &amp;mutex); // p3 11 put(i); // p4 12 Pthread_cond_signal(&amp;cond); // p5 13 Pthread_mutex_unlock(&amp;mutex); // p6 14 } 15 } 16 17 void *consumer(void *arg) { 18 int i; 19 for (i = 0; i &lt; loops; i++) { 20 Pthread_mutex_lock(&amp;mutex); // c1 21 while (count == 0) // c2 22 Pthread_cond_wait(&amp;cond, &amp;mutex); // c3 23 int tmp = get(); // c4 24 Pthread_cond_signal(&amp;cond); // c5 25 Pthread_mutex_unlock(&amp;mutex); // c6 26 printf(&quot;%d\\n&quot;, tmp); 27 } 28 } 由于 Mesa 语义，我们要记住一条关于条件变量的简单规则：总是使用 while 循环（always use while loop）。虽然有时候不需要重新检查条件，但这样做总是安全的，做了就开心了。 但是，这段代码仍然有一个问题，也是上文提到的两个问题之一。你能想到吗？它和我们只用了一个条件变量有关。尝试弄清楚这个问题是什么，再继续阅读。想一下！ ……（暂停想一想，或者闭一下眼）…… 我们来确认一下你想得对不对。假设两个消费者（Tc1 和 Tc2）先运行，都睡眠了（c3）。生产者开始运行，在缓冲区放入一个值，唤醒了一个消费者（假定是 Tc1），并开始睡眠。现在是一个消费者马上要运行（Tc1），两个线程（Tc2 和 Tp）都等待在同一个条件变量上。问题马上就要出现了 消费者 Tc1 醒过来并从 wait()调用返回（c3），重新检查条件（c2），发现缓冲区是满的，消费了这个值（c4）。这个消费者然后在该条件上发信号（c5），唤醒一个在睡眠的线程。但是，应该唤醒哪个线程呢？ 因为消费者已经清空了缓冲区，很显然，应该唤醒生产者。但是，如果它唤醒了 Tc2（这绝对是可能的，取决于等待队列是如何管理的），问题就出现了。具体来说，消费者 Tc2 会醒过来，发现队列为空（c2），又继续回去睡眠（c3）。生产者 Tp 刚才在缓冲区中放了一个值，现在在睡眠。另一个消费者线程 Tc1 也回去睡眠了。3 个线程都在睡眠，显然是一个缺陷。由表可以看到这个可怕灾难的步骤。 信号显然需要，但必须更有指向性。消费者不应该唤醒消费者，而应该只唤醒生产者，反之亦然。 The Single Buffer Producer/Consumer Solution 解决方案也很简单：使用两个条件变量，而不是一个，以便正确地发出信号，在系统状态改变时，哪类线程应该唤醒。 1 cond_t empty, fill; 2 mutex_t mutex; 3 4 void *producer(void *arg) { 5 int i; 6 for (i = 0; i &lt; loops; i++) { 7 Pthread_mutex_lock(&amp;mutex); 8 while (count == 1) 9 Pthread_cond_wait(&amp;empty, &amp;mutex); 10 put(i); 11 Pthread_cond_signal(&amp;fill); 12 Pthread_mutex_unlock(&amp;mutex); 13 } 14 } 15 16 void *consumer(void *arg) { 17 int i; 18 for (i = 0; i &lt; loops; i++) { 19 Pthread_mutex_lock(&amp;mutex); 20 while (count == 0) 21 Pthread_cond_wait(&amp;fill, &amp;mutex); 22 int tmp = get(); 23 Pthread_cond_signal(&amp;empty); 24 Pthread_mutex_unlock(&amp;mutex); 25 printf(&quot;%d\\n&quot;, tmp); 26 } 27 } 在上述代码中，生产者线程等待条件变量 empty，发信号给变量 fill。相应地，消费者线程等待 fill，发信号给 empty。这样做，从设计上避免了上述第二个问题：消费者再也不会唤醒消费者，生产者也不会唤醒生产者。 The Correct Producer/Consumer Solution 我们现在有了可用的生产者/消费者方案，但不太通用。我们最后的修改是提高并发和效率。具体来说，增加更多缓冲区槽位，这样在睡眠之前，可以生产多个值。同样，睡眠之前可以消费多个值。单个生产者和消费者时，这种方案因为上下文切换少，提高了效率。多个生产者和消费者时，它甚至支持并发生产和消费，从而提高了并发。幸运的是，和现有方案相比，改动也很小。 第一处修改是缓冲区结构本身，以及对应的 put() 和 get()方法。 1 int buffer[MAX]; 2 int fill_ptr = 0; 3 int use_ptr = 0; 4 int count = 0; 5 6 void put(int value) { 7 buffer[fill_ptr] = value; 8 fill_ptr = (fill_ptr + 1) % MAX; 9 count++; 10 } 11 12 int get() { 13 int tmp = buffer[use_ptr]; 14 use_ptr = (use_ptr + 1) % MAX; 15 count--; 16 return tmp; 17 } 我们还稍稍修改了生产者和消费者的检查条件，以便决定是否要睡眠。展示了最终的等待和信号逻辑。生产者只有在缓冲区满了的时候才会睡眠（p2），消费者也只有在队列为空的时候睡眠（c2）。至此，我们解决了生产者/消费者问题。 1 cond_t empty, fill; 2 mutex_t mutex; 3 4 void *producer(void *arg) { 5 int i; 6 for (i = 0; i &lt; loops; i++) { 7 Pthread_mutex_lock(&amp;mutex); // p1 8 while (count == MAX) // p2 9 Pthread_cond_wait(&amp;empty, &amp;mutex); // p3 10 put(i); // p4 11 Pthread_cond_signal(&amp;fill); // p5 12 Pthread_mutex_unlock(&amp;mutex); // p6 13 } 14 } 15 16 void *consumer(void *arg) { 17 int i; 18 for (i = 0; i &lt; loops; i++) { 19 Pthread_mutex_lock(&amp;mutex); // c1 20 while (count == 0) // c2 21 Pthread_cond_wait(&amp;fill, &amp;mutex); // c3 22 int tmp = get(); // c4 23 Pthread_cond_signal(&amp;empty); // c5 24 Pthread_mutex_unlock(&amp;mutex); // c6 25 printf(&quot;%d\\n&quot;, tmp); 26 } 27 } 提示：对条件变量使用 while（不是 if） 多线程程序在检查条件变量时，使用 while 循环总是对的。if 语句可能会对，这取决于发信号的语义。因此，总是使用 while，代码就会符合预期。 对条件变量使用 while 循环，这也解决了假唤醒（spurious wakeup）的情况。某些线程库中，由于实现的细节，有可能出现一个信号唤醒两个线程的情况。再次检查线程的等待条件，假唤醒是另一个原因。 Covering Conditions 现在再来看条件变量的一个例子。这段代码摘自 Lampson 和 Redell 关于飞行员的论文，同一个小组首次提出了上述的 Mesa 语义（Mesa semantic，他们使用的语言是 Mesa，因此而得名）。 他们遇到的问题通过一个简单的例子就能说明，在这个例子中，是一个简单的多线程内存分配库。 1 // how many bytes of the heap are free? 2 int bytesLeft = MAX_HEAP_SIZE; 3 4 // need lock and condition too 5 cond_t c; 6 mutex_t m; 7 8 void * 9 allocate(int size) { 10 Pthread_mutex_lock(&amp;m); 11 while (bytesLeft &lt; size) 12 Pthread_cond_wait(&amp;c, &amp;m); 13 void *ptr = ...; // get mem from heap 14 bytesLeft -= size; 15 Pthread_mutex_unlock(&amp;m); 16 return ptr; 17 } 18 19 void free(void *ptr, int size) { 20 Pthread_mutex_lock(&amp;m); 21 bytesLeft += size; 22 Pthread_cond_signal(&amp;c); // whom to signal?? 23 Pthread_mutex_unlock(&amp;m); 24 } 从代码中可以看出，当线程调用进入内存分配代码时，它可能会因为内存不足而等待。相应的，线程释放内存时，会发信号说有更多内存空闲。但是，代码中有一个问题：应该唤醒哪个等待线程（可能有多个线程）？ 考虑以下场景。假设目前没有空闲内存，线程 Ta 调用 allocate(100)，接着线程 Tb 请求较少的内存，调用 allocate(10)。Ta 和 Tb 都等待在条件上并睡眠，没有足够的空闲内存来满足它们的请求。这时，假定第三个线程 Tc调用了 free(50)。遗憾的是，当它发信号唤醒等待线程时，可能不会唤醒申请 10 字节的 Tb 线程。而 Ta 线程由于内存不够，仍然等待。因为不知道唤醒哪个（或哪些）线程，所以图中代码无法正常工作。 Lampson 和 Redell 的解决方案也很直接：用 pthread_cond_broadcast()代替上述代码中的 pthread_cond_signal()，唤醒所有的等待线程。这样做，确保了所有应该唤醒的线程都被唤醒。当然，不利的一面是可能会影响性能，因为不必要地唤醒了其他许多等待的线程，它们本来（还）不应该被唤醒。这些线程被唤醒后，重新检查条件，马上再次睡眠。 Lampson 和 Redell 把这种条件变量叫作覆盖条件（covering condition），因为它能覆盖所有需要唤醒线程的场景（保守策略）。成本如上所述，就是太多线程被唤醒。聪明的读者可能发现，在单个条件变量的生产者/消费者问题中，也可以使用这种方法。但是，在这个例子中，我们有更好的方法，因此用了它。一般来说，如果你发现程序只有改成广播信号时才能工作（但你认为不需要），可能是程序有缺陷，修复它！但在上述内存分配的例子中，广播可能是最直接有效的方案。 Summary 我们看到了引入锁之外的另一个重要同步原语：条件变量。当某些程序状态不符合要求时，通过允许线程进入休眠状态，条件变量使我们能够漂亮地解决许多重要的同步问题，包括著名的（仍然重要的）生产者/消费者问题，以及覆盖条件。 ","link":"https://blog.shunzi.tech/post/basic-of-concurrency-one/"},{"title":"Lock of Programming","content":" 编程中的锁。 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第二篇（Locks），锁。 Before 开始之前先简单阐述背景，最近项目测试发现在对于底层对象存储的并发访问过程中由于未进行并发控制而产生了一些问题，所以想趁此机会回顾“锁”的相关基础知识，并在此总结，看看能不能在总结的过程中顺便实现项目里的并发访问控制。 还是和往常一样，会贴很多链接，只是尝试着吸收网络上已经存在的优秀的教程，然后再进行一下转化（讲道理确实没啥新意，但主要是为了之后查阅复习起来比较方便） 参考链接 先贴参考链接主要是为了膜拜一下，并以表尊敬，因为绝大数情况下，参考链接里的文献就已经写的很好了，我的下文就其实没啥营养了大概，主要是为了节省大家的时间，看完早点去嗨皮~ 参考链接也会持续地更新补充，文章的思路将主要沿袭教科书 Three Easy Pieces（其实我是偷懒顺便把书看了~） [1] 知乎：通俗易懂 悲观锁、乐观锁、可重入锁、自旋锁、偏向锁、轻量/重量级锁、读写锁、各种锁及其Java实现！ [2] Operating Systems: Three Easy Pieces - Concurrency - Locks [3] 知乎：如何理解互斥锁、条件锁、读写锁以及自旋锁？ [4] 知乎：漫画|Linux 并发、竞态、互斥锁、自旋锁、信号量都是什么鬼？ [5] Java 并发编程的艺术 [6] 知乎：2w字 + 40张图带你参透并发编程！ [7] 多处理器编程的艺术 Github Repo [8] 知乎：volatile 关键字，你真的理解吗？ （这个参考可能有点例外，更多是针对缓存一致性的，但在并发场景也常使用） Locks 并发编程中的一个基本问题:我们希望原子地执行一系列指令，但由于单个处理器上出现中断(或多个线程同时在多个处理器上执行)，我们无法执行。我们通过引入所谓的锁来直接解决这个问题。程序员用锁注释源代码，把它们放在临界区周围，从而确保任何这样的临界区都像执行单个原子指令一样执行。 Locks: The Basic Idea 作为一个例子，假设我们的临界区是这样的，共享变量的规范更新: balance = balance + 1; 当然，也可能有其他关键部分，比如向链表添加元素，或者对共享结构进行其他更复杂的更新，但我们现在只讨论这个简单的示例。为了使用锁，我们在临界区周围添加如下代码: 1 lock_t mutex; // some globally-allocated lock ’mutex’ 2 ... 3 lock(&amp;mutex); 4 balance = balance + 1; 5 unlock(&amp;mutex); 锁只是一个变量，因此要使用一个锁，必须声明某种类型的锁变量(比如上面的互斥锁)。这个锁变量(或直接简称“lock”)在任何时刻保持锁的状态。它是可用的(或未锁定的或空闲的)，因此没有线程持有该锁，也没有线程获得该锁(或锁定或持有)，因此只有一个线程持有该锁，并且可能处于临界区。我们也可以存储一些额外的信息在数据类型里，例如哪个线程持有当前锁。或者一个预定锁获取的队列，但是这样的信息对锁的使用者是隐藏的。 lock() 和 unlock() 例程的语义很简单。调用例程lock()试图获取锁;如果没有其他线程持有这个锁(即，它是空闲的)，这个线程将获得这个锁并进入临界区;这个线程有时被称为锁的所有者。如果另一个线程在这个锁变量上调用lock()(在本例中是互斥)，当锁被另一个线程持有时，它不会返回;这样，当持有锁的第一个线程还在临界区中时，其他线程就无法进入临界区。 一旦锁的所有者调用unlock()，锁现在就可用了。如果没有其他线程在等待锁(也就是说，没有其他线程调用了lock()并被困在锁中)，锁的状态就会被更改为free。如果有正在等待的线程(卡在lock()中)，其中一个将(最终)通知(或被告知)锁状态的改变，获取锁，并进入临界区 锁为程序员提供了对调度的最低限度的控制。通常，我们将线程视为程序员创建但由操作系统调度的实体，以操作系统选择的任何方式。锁将部分控制权交还给程序员;通过在一段代码周围加一个锁，程序员可以保证在该代码中不会有超过一个线程处于活动状态。因此，锁有助于将传统操作系统调度的混乱转变为一种更可控的活动 Pthread Locks POSIX库用于锁的名称是互斥锁，因为它用于提供线程之间的互斥，例如，如果一个线程处于临界区，它将排除其他线程进入，直到它完成临界区。因此，当你看到下面的POSIX线程代码时，你应该明白它正在做和上面一样的事情(我们再次使用我们的包装器在锁定和解锁时检查错误): 1 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER; 2 3 Pthread_mutex_lock(&amp;lock); // wrapper; exits on failure 4 balance = balance + 1; 5 Pthread_mutex_unlock(&amp;lock); // Keeps code clean; only use if exit() OK upon failure void Pthread_mutex_lock(pthread_mutex_t *mutex) { int rc = pthread_mutex_lock(mutex); assert(rc == 0); } 您可能还注意到，POSIX版本将一个变量传递给lock和unlock，因为我们可能使用不同的锁来保护不同的变量。这样做可以增加并发性:与在访问任何关键部分时使用一个大锁(一种粗粒度的锁定策略)不同，通常使用不同的锁来保护不同的数据和数据结构，从而允许更多的线程同时处于锁定代码中(更细粒度的方法)。 Building A Lock 我们如何建立一个高效的锁?高效的锁以较低的成本提供互斥，还可能获得我们下面讨论的一些其他属性。需要什么硬件支持?操作系统支持什么? 为了构建一个有效的锁，我们需要我们的老朋友——硬件，以及我们的好朋友——操作系统的帮助。多年来，许多不同的硬件原语被添加到各种计算机体系结构的指令集中;虽然我们不会研究这些指令是如何实现的(毕竟，这是计算机架构类的主题)，但我们将研究如何使用它们来构建像锁一样的互斥原语。我们还将研究操作系统是如何参与进来的，从而使我们能够构建一个复杂的锁库。 Evaluating Locks 在构建任何锁之前，我们应该首先理解我们的目标是什么，因此我们应该询问如何评估特定锁实现的有效性。要评价锁是否有效(以及是否有效)，我们应该建立一些基本的标准。第一个问题是锁是否执行它的基本任务，即提供互斥。基本上，锁是否有效，防止多个线程进入一个临界区? 第二是公平。是否每个争用锁的线程都有机会在锁空闲时获取它?另一种考虑这个问题的方法是检查更极端的情况:是否有任何争用锁的线程在争用锁的时候饿死，从而永远得不到锁? 最后一个标准是性能，特别是使用锁所增加的时间开销。这里有几个不同的情况值得考虑。一种是没有争用的情况;当一个线程正在运行并获取和释放锁时，这样做的开销是什么?另一种情况是多个线程争夺单个CPU上的锁;在这种情况下，是否存在性能问题?最后，当涉及多个cpu，并且每个cpu上的线程都争用该锁时，该锁是如何执行的?通过比较这些不同的场景，我们可以更好地理解使用各种锁定技术对性能的影响，如下所述。 Controlling Interrupts 最早用于提供互斥的解决方案之一是禁用临界区中断;这种解决方案是为单处理器系统而发明的。代码如下所示: 1 void lock() { 2 DisableInterrupts(); 3 } 4 void unlock() { 5 EnableInterrupts(); 6 } 假设我们运行在这样一个单处理器系统上。通过在进入临界区之前关闭中断(使用某种特殊的硬件指令)，我们可以确保临界区内的代码不会被中断，从而可以像原子一样执行。当我们完成时，我们重新启用中断(同样是通过硬件指令)，这样程序就像往常一样继续运行。这种方法的主要优点是简单。你当然不需要绞尽脑汁来弄明白为什么这是可行的。在没有中断的情况下，线程可以确保它执行的代码将被执行，并且不会有其他线程干扰它。 不幸的是，负面的东西很多。 首先，这种方法要求我们允许任何调用线程执行特权操作(打开和关闭中断)，因此相信这种功能不会被滥用。正如你已经知道的，任何时候我们被要求信任一个任意的程序，我们可能会遇到麻烦。在这里，问题表现在许多方面:贪婪的程序可能会在执行开始时调用lock()，从而垄断处理器;更糟糕的是，一个错误的或恶意的程序可能会调用lock()并进入一个无穷循环。在后一种情况下，操作系统永远无法恢复对系统的控制，只有一种方法:重启系统。使用中断禁用作为通用的同步解决方案需要对应用程序有太多的信任。 第二，这种方法不能在多处理器上工作。如果多个线程运行在不同的cpu上，并且每个线程都试图进入相同的临界区，那么是否禁用中断是无关紧要的;线程可以在其他处理器上运行，因此可以进入临界区。由于多处理器现在很普遍，我们的一般解决方案必须做得比这更好。 第三，长时间关闭中断可能会导致中断丢失，从而导致严重的系统问题。例如，想象一下，如果CPU没有注意到磁盘设备已经完成了一个读请求。操作系统如何知道要唤醒等待读取的进程? 最后，也可能是最不重要的，这种方法可能效率低下。与普通的指令执行相比，屏蔽或取消中断的代码在现代cpu中执行得比较慢。 由于这些原因，关闭中断只在有限的上下文中用作互斥原语。例如，在某些情况下，操作系统本身将使用中断屏蔽来保证访问自己的数据结构时的原子性，或者至少防止出现某些混乱的中断处理情况。这种用法是有意义的，因为信任问题在操作系统内部消失了，操作系统无论如何总是信任自己执行特权操作。 A Failed Attempt: Just Using Loads/Stores 要超越基于中断的技术，我们将不得不依赖CPU硬件以及它提供给我们的构建正确锁的指令。让我们首先尝试通过使用单个标记变量来构建一个简单的锁。在这次失败的尝试中，我们将了解构建锁所需的一些基本思想，并(希望如此)了解为什么仅使用单个变量并通过正常的 load 和 store 访问它是不够的。 在如下第一次尝试中，想法非常简单:使用一个简单的变量(标志)来指示某个线程是否拥有锁。进入临界区的第一个线程将调用lock()，它将测试该标志是否等于1(在本例中，它不是)，然后将该标志设置为1，以表明该线程现在持有锁。当临界区结束时，线程调用unlock()并清除标志，从而表明锁不再被持有。 如果另一个线程碰巧在第一个线程处于临界区时调用了lock()，它将在while循环中简单地spin-wait，以便该线程调用unlock()并清除标志。一旦第一个线程这样做了，等待的线程将退出while循环，为自己设置标志为1，并继续进入临界区。 1 typedef struct __lock_t { int flag; } lock_t; 2 3 void init(lock_t *mutex) { 4 // 0 -&gt; lock is available, 1 -&gt; held 5 mutex-&gt;flag = 0; 6 } 7 8 void lock(lock_t *mutex) { 9 while (mutex-&gt;flag == 1) // TEST the flag 10 ; // spin-wait (do nothing) 11 mutex-&gt;flag = 1; // now SET it! 12 } 13 14 void unlock(lock_t *mutex) { 15 mutex-&gt;flag = 0; 16 } 不幸的是，代码有两个问题:一个是正确性，另一个是性能。一旦您习惯了并行编程，正确性问题就很容易看到。想象一下如果代码交错;假设flag=0开始。 正如您从这个交错中看到的，通过及时(不及时?)中断，我们可以很容易地产生这样一种情况，即两个线程都将标志设置为1，从而两个线程都能够进入临界区。这种行为被专业人士称为“bad”——我们显然没有提供最基本的要求:提供互斥。 性能问题(我们将在后面详细讨论)是这样一个事实:线程等待获取已经持有的锁的方式:它无休止地检查flag的值，这种技术称为旋转等待。旋转等待浪费了等待另一个线程释放锁的时间。在单处理器上，这种浪费是非常高的，服务端正在等待的线程甚至不能运行(至少在发生上下文切换之前)!因此，当我们向前推进并开发更复杂的解决方案时，我们也应该考虑避免这种浪费的方法。 ASIDE: DEKKER’S AND PETERSON’S ALGORITHMS 在20世纪60年代，Dijkstra向他的朋友们提出了并发问题，其中一位名叫Theodorus Jozef Dekker的数学家提出了一个解决方案。我们这里讨论的解决方案使用特殊的硬件指令甚至操作系统支持，而Dekker的算法只使用 load 和 store(假设它们彼此之间是原子的，这在早期的硬件上是正确的)。 Dekker的方法后来被Peterson改进了。同样，只使用load和store，这样做是为了确保两个线程不会同时进入临界区。下面是Peterson的算法(适用于两个线程);看看你能不能理解代码。flag 和 turn 的用途是什么? 如下代码其实同时使用了 flag 和 turn 来调度线程，对应线程的 flag 置为了 1 说明该线程即将持有锁，和前面的代码类似，但是还使用了一个 turn 来表明下一个需要调度的线程的 ID，此时两个线程都运行到了 while 处时，条件 1 必定为真，条件二必定其中一个线程在执行时为真，该线程相应地进行自旋 flag 表示哪个线程想要占用临界区状态，就是举手表示想要访问临界区 turn 用于标识当前允许谁进入，就是谁的轮次 int flag[2]; int turn; void init() { // indicate you intend to hold the lock w/ ’flag’ flag[0] = flag[1] = 0; // whose turn is it? (thread 0 or 1) turn = 0; } void lock() { // ’self’ is the thread ID of caller // 首先举手表示当前线程要访问临界区，eg 线程 1，即 flag[1]=1 flag[self] = 1; // make it other thread’s turn // 这里其实有一个“礼让”的逻辑 // 就是说虽然是当前线程在执行，但是会把轮次让给另外的线程 // 如果另外的那个线程确实也举手了， // 然后另外的线程还没来得及礼让，那么就由另外的线程执行 // 要是另外的线程也刚好礼让了，那么就由当前线程执行 turn = 1 - self; // 首先看另外的线程举手了没？ // case0. 没举手那就只有当前线程访问，直接运行就完事了 // case1. 另外的线程举手了，那这时候就要看该谁的轮次了 // case1.0 如果这时候的轮次确实该另外的线程 1-self，那当前线程就自旋等待 // case2.0 如果这时候的轮次该自个儿 self，那当前线程就执行 while ((flag[1-self] == 1) &amp;&amp; (turn == 1 - self)) ; // spin-wait while it’s not your turn } void unlock() { // simply undo your intent flag[self] = 0; } 由于某些原因，开发不需要特殊硬件支持就能工作的锁在一段时间内变得非常流行，这给理论类型带来了许多问题。当然，当人们意识到假定有一点硬件支持会容易得多(事实上，这种支持早在多处理的早期就存在了)时，这一行的工作就变得毫无用处了。此外，上面的算法不能在现代硬件上运行(由于内存一致性模型不严格)，因此它们比以前更没用了。然而，更多的研究被扔进了历史的垃圾箱 Peterson 算法结合了 LockOne 和 LockTwo 两种算法 LockOne 只满足互斥，无法避免死锁。 LockTwo 也只满足互斥，无法避免死锁 https://zh.wikipedia.org/wiki/Peterson%E7%AE%97%E6%B3%95#cite_note-3 并发编程的艺术02-过滤锁算法 class LockOne implements Lock { private boolean[] flag = new boolean[2]; // 因为 Lock 本身不互斥，所以两个线程可能同时都将对方的 flag 标识设置为 true // 导致两个线程都被阻塞在 while(true) {} 。 public void lock() { int i = ThreadId.get(); int j = 1 - i; flag[i] = true; while(flag[j]) {} } public void unlock() { int i = ThreadID.get(); flag[i] = false; } } class LockTwo implements Lock { private volatile int victim; // 当一个线程先执行，而另一个线程迟迟不执行 // 这时候第一个线程就被锁住无法进行向下执行 public void lock() { int i = ThreadId.get(); // 阻塞率先执行 victim = i 的线程（礼让执行） victim = i; while(victim == i) {} } public void unlock() { } } class PetersonLock implements Lock { private boolean[] flag = new boolean[2]; private volatile int victim; public void lock() { int i = ThreadId.get(); int j = 1 - i; flag[i] = true; victim = i; while(flag[j] &amp;&amp; victim == i) {} } public void unlock() { int i = ThreadID.get(); flag[i] = false; } } class FilterLock implements Lock { int[] level; int[] victim; int n; public FilterLock(int n) { level = new int[n]; victim = new int[n]; this.n = n; for (int i = 0;i &lt; n; i++) { level[i] = 0; } } public void lock() { int me = ThreadID.get(); for (int i = 0; i &lt; n; i++) { level[me] = i; victim[i] = me; for (int k = 0; k &lt; n;k++) { while ((k != me) &amp;&amp; (level[k] &gt;= i &amp;&amp; victim[i] == me))) { } } } } public void unlock() { int me = ThreadID.get(); level[me] = 0; } } Building Working Spin Locks with Test-And-Set 当使用TAS实现TASLock （Test And Set Lock）测试-设置锁，它的特点是自旋时，每次尝试获取锁时，底层还是使用CAS操作，不断的设置锁标志位的过程会一直修改共享变量的值（回写），会引发缓冲一致性流量发风暴。【因为每一次CAS操作都会发出广播通知其他处理器，从而影响程序的性能。】 Wikipedia: Test and Set 因为禁用中断不能在多个处理器上工作，而且使用load和store(如上所示)的简单方法也不能工作，所以系统设计人员开始发明对锁定的硬件支持。最早的多处理器系统，如20世纪60年代早期的Burroughs B5000，就有这样的支持;如今，所有系统都提供这种类型的支持，即使是单CPU系统。 需要理解的最简单的硬件支持是 test-and-set (或 atomic exchange) 指令。我们通过下面的C代码片段来定义 test-and-set 指令的作用: 1 int TestAndSet(int *old_ptr, int new) { 2 int old = *old_ptr; // fetch old value at old_ptr 3 *old_ptr = new; // store ’new’ into old_ptr 4 return old; // return the old value 5 } 1 typedef struct __lock_t { 2 int flag; 3 } lock_t; 4 5 void init(lock_t *lock) { 6 // 0: lock is available, 1: lock is held 7 lock-&gt;flag = 0; 8 } 9 10 void lock(lock_t *lock) { 11 while (TestAndSet(&amp;lock-&gt;flag, 1) == 1) 12 ; // spin-wait (do nothing) 13 } 14 15 void unlock(lock_t *lock) { 16 lock-&gt;flag = 0; 17 } test-and-set 指令的作用如下。它返回旧 ptr 所指向的旧值，并同时将该值更新为new。当然，关键是这个操作序列是以原子的方式执行的。 之所以称之为 “test-and-set”，是因为它使您能够同时“test”旧值(即返回的值)“set”内存位置为一个新值;事实证明，这个稍微强大一些的指令足以构建一个简单的旋转锁。 我们先弄清楚为什么这把锁可以用。首先想象一下这样一种情况: 一个线程调用了lock()，而当前没有其他线程持有该锁;因此，flag应该是0。当线程调用 TestAndSet(flag，1)，线程返回旧的 flag 值，即 0; 因此，正在 testing flag 值的调用线程将不会在 while 循环中被捕获，并将获得锁。线程也会自动地将该值设置为 1，从而表明现在已持有锁。当线程完成它的临界区时，它调用unlock()将标志设为0。 我们可以想象的第二种情况是，当一个线程已经持有锁(即，flag是1)。在这种情况下，这个线程将调用lock()，然后调用 TestAndSet(flag, 1)。这一次，teststandset()将返回旧的flag值，即1(因为锁被持有)，同时再次将其设置为1。只要锁被另一个线程持有，TestAndSet()就会反复返回1，因此这个线程就会不停地旋转，直到锁最终被释放。当标志最终被其他线程设置为0时，该线程将再次调用 TestAndSet()，它现在将返回0，同时原子地将值设置为1，从而获得锁并进入临界区。 通过将 Test(对旧锁的值)和 Set(对新值的值)都设置为单个原子操作，我们确保只有一个线程获得锁。这就是如何构建一个有效的互斥原语! 现在可能也理解了为什么这种类型的锁通常称为自旋锁。它是需要构建的最简单的锁类型，只是使用CPU周期旋转，直到锁可用为止。为了在单个处理器上正常工作，它需要一个抢占式调度程序(即，为了不时地运行另一个线程，它会通过计时器中断一个线程)。如果没有抢占，旋转锁在单个CPU上没有多大意义，因为在CPU上旋转的线程永远不会放弃它。 汇编代码实现： enter_region: ; A &quot;jump to&quot; tag; function entry point. tsl reg, flag ; Test and Set Lock; flag is the ; shared variable; it is copied ; into the register reg and flag ; then atomically set to 1. cmp reg, #0 ; Was flag zero on entry_region? jnz enter_region ; Jump to enter_region if ; reg is non-zero; i.e., ; flag was non-zero on entry. ret ; Exit; i.e., flag was zero on ; entry. If we get here, tsl ; will have set it non-zero; thus, ; we have claimed the resource ; associated with flag. leave_region: move flag, #0 ; store 0 in flag ret ; return to caller Evaluating Spin Locks 有了我们的基本自旋锁，我们现在可以评估它在我们前面描述的轴上的有效性。锁最重要的方面是正确性:它是否提供了互斥?答案是肯定的:旋转锁一次只允许一个线程进入临界区。这样，我们就有了一个正确的锁。 下一个轴是公平。旋转锁对等待的线程有多公平?你能保证等待的线程会进入临界区吗?不幸的是，这里的答案是坏消息:旋转锁不提供任何公平性保证。的确，在争用的情况下，在旋转等待的线程可能一直旋转。简单的旋转锁(到目前为止已经讨论过)是不公平的，可能会导致线程饿死。 最后一个轴是性能。使用旋转锁的成本是多少?为了更仔细地分析这个问题，我们建议考虑几个不同的案例。在第一种情况下，想象线程在单个处理器上竞争锁;在第二种情况下，考虑分布在多个cpu上的线程。 对于自旋锁，在单一CPU的情况下，性能开销是非常痛苦的;想象一下持有锁的线程在一个临界区中被抢占的情况。调度程序可能会运行其他每一个线程(假设有N−1个其他线程)，每个线程都试图获取锁。在这种情况下，每个线程都将在一个时间片的持续时间内旋转，然后放弃CPU，这是对CPU周期的浪费。 但是，在多个cpu上，旋转锁工作得相当好(如果线程的数量大致等于cpu的数量)。假设CPU 1上的线程A和CPU 2上的线程B都争用一个锁。如果线程(CPU 1)持有锁,线程B试图加锁,B将自旋(CPU 2)。然而,假定关键的部分较短,因此很快锁就变得可用,线程B将获得锁。旋转等待锁在另一个处理器不会浪费很多的周期在本例中,从而可以很有效。 更多避免旋转的理由:优先级反转 避免旋转锁的一个很好的理由是性能:正如正文中所描述的，如果一个线程在持有一个锁时被中断，其他使用旋转锁的线程将花费大量的CPU时间来等待锁可用。然而，在某些系统上避免自旋锁还有另一个有趣的原因:正确性。要警惕的问题是所谓的优先级反转。 现在,这个问题。假设T2由于某种原因被阻塞。所以T1运行，抓住旋转锁，进入一个临界区。T2现在变得畅通(可能是因为一个I/O完成了)，CPU调度器会立即调度它(从而重新调度T1)。T2现在尝试获取锁，因为它不能(T1持有锁)，所以它只是继续旋转。因为锁是自旋锁，T2永远自旋，系统就挂了。 不幸的是，仅仅避免使用旋转锁并不能避免反转的问题(唉)。假设有三个线程，T1, T2和T3。T3的优先级最高，T1的优先级最低。现在想象一下，T1抓住了一个锁。然后T3启动，由于它的优先级高于T1，所以它立即运行(抢占T1)。T3试图获取T1所持有的锁，但是由于T1仍然持有锁，所以被困在等待中。如果T2开始运行，它将拥有比T1更高的优先级，因此它将运行。T3的优先级高于T2，它被困在等待T1，由于T2正在运行，T1可能永远不会运行。强大的T3不能运行，而不起眼的T2控制着CPU ?高优先级已经不像以前那么重要了。 您可以通过多种方式解决优先级反转问题。在自旋锁导致问题的特定情况下，可以避免使用自旋锁(后面将详细描述)。一般来说，高优先级线程等待低优先级线程可以暂时提高低优先级线程的优先级，从而使其能够运行并克服反转，这种技术称为优先级继承。最后一个解决方案是最简单的:确保所有线程具有相同的优先级。 Compare-And-Swap 在计算机科学中，比较与交换(CAS)是多线程中用来实现同步的原子指令。它将内存位置的内容与给定值进行比较，只有当它们相同时，才将该内存位置的内容修改为新的给定值。这是作为单个原子操作完成的 Wikipedia: CAS 一些系统提供的另一个硬件原语是 Compare-And-Swap 指令(例如，SPARC上这样称呼它)或compare-and-exchange 指令(x86上这样称呼它)。也就是我们常说的 CAS 操作。这条指令的C伪代码如下： 1 int CompareAndSwap(int *ptr, int expected, int new) { 2 int original = *ptr; 3 if (original == expected) 4 *ptr = new; 5 return original; 6 } 其基本思想是通过 compare-and-swap 来测试ptr指定的地址上的值是否等于预期值;如果是，用新值更新ptr所指向的内存位置。如果不是，那就什么都不做。在这两种情况下，返回该内存位置的原始值，从而允许调用 compare-and-swap 的代码知道它是否成功。 CAS 操作其实就比 TAS 多了一个参数，expected 预期值，预期值 E 主要用于和当前值的比对，从而来决定是否更新新的值。 使用 compare-and-swap 指令，我们可以以类似于使用 test-and-set 的方式构建锁。例如，我们可以用以下代码替换上面的lock()例程: 1 void lock(lock_t *lock) { 2 while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1) 3 ; // spin 4 } 其余的代码与上面的测试集示例相同。这段代码的工作原理非常相似;它只是检查标志是否为0，如果是，就自动交换1，从而获得锁。当锁被持有时，试图获取锁的线程将被困在旋转中，直到锁最终被释放。 如果您想了解如何真正制作一个 c 可调用的x86版本的 compare-and-swap，看以下链接： https://github.com/remzi-arpacidusseau/ostep-code/tree/master/threads-locks #include &lt;stdio.h&gt; int global = 0; char compare_and_swap(int *ptr, int old, int new) { unsigned char ret; // Note that sete sets a ’byte’ not the word __asm__ __volatile__ ( &quot; lock\\n&quot; &quot; cmpxchgl %2,%1\\n&quot; &quot; sete %0\\n&quot; : &quot;=q&quot; (ret), &quot;=m&quot; (*ptr) : &quot;r&quot; (new), &quot;m&quot; (*ptr), &quot;a&quot; (old) : &quot;memory&quot;); return ret; } int main(int argc, char *argv[]) { printf(&quot;before successful cas: %d\\n&quot;, global); int success = compare_and_swap(&amp;global, 0, 100); printf(&quot;after successful cas: %d (success: %d)\\n&quot;, global, success); printf(&quot;before failing cas: %d\\n&quot;, global); success = compare_and_swap(&amp;global, 0, 200); printf(&quot;after failing cas: %d (old: %d)\\n&quot;, global, success); return 0; } 最后，您可能已经感觉到，compare_and_swap 指令比 test-and-set 指令更强大。我们将在以后简要地研究诸如无锁同步等主题时使用这种能力。然而，如果我们只是用它构建一个简单的自旋锁，它的行为与我们上面分析的自旋锁相同。 Load-Linked and Store-Conditional 一些平台提供了一对协同工作的指令来帮助构建关键部分。例如，在MIPS架构中，load-linked 和 store-conditional 指令可以串联使用来构建锁和其他并发结构。这些指令的 C 伪代码如下。Alpha、PowerPC和ARM提供了类似的说明 1 int LoadLinked(int *ptr) { 2 return *ptr; 3 } 4 5 int StoreConditional(int *ptr, int value) { 6 if (no update to *ptr since LoadLinked to this address) { 7 *ptr = value; 8 return 1; // success! 9 } else { 10 return 0; // failed to update 11 } 12 } load-linked的操作与典型的load指令非常相似，它只是从内存中获取一个值并将其放入寄存器中。关键的不同在于store-conditional，它只在没有对该地址进行存储的情况下才成功(并更新存储在刚刚进行load-linked的地址上的值)。在成功的情况下，store-conditional返回1并将ptr处的值更新为value;如果失败，则ptr处的值不会更新，返回0。 lock()代码是唯一有趣的部分。首先，线程旋转，等待标志被设置为0(从而表明锁没有被持有)。一旦这样，线程试图通过store条件获取锁;如果成功，线程就自动地将标志的值更改为1，因此可以进入临界区 请注意 store-conditional 是如何发生故障的。一个线程调用lock()并执行LoadLinked，当锁未被持有时返回0。在它尝试 store-conditional 之前，它被中断，另一个线程进入 lock代码，也执行 load-linked 指令，得到一个0，然后继续。此时，两个线程分别执行了load-linked，并且都准备尝试store-conditional。这些指令的关键特性是，只有其中一个线程会成功地将标志更新为1，从而获得锁;第二个尝试 store-conditional 的线程将失败(因为另一个线程更新了它的load-linked和store-conditional之间的标志值)，因此必须再次尝试获取锁。 1 void lock(lock_t *lock) { 2 while (1) { 3 while (LoadLinked(&amp;lock-&gt;flag) == 1) 4 ; // spin until it’s zero 5 if (StoreConditional(&amp;lock-&gt;flag, 1) == 1) 6 return; // if set-it-to-1 was a success: all done 7 // otherwise: try it all over again 8 } 9 } 10 11 void unlock(lock_t *lock) { 12 lock-&gt;flag = 0; 13 } 简化一下如上代码 1 void lock(lock_t *lock) { 2 while (LoadLinked(&amp;lock-&gt;flag) || 3 !StoreConditional(&amp;lock-&gt;flag, 1)) 4 ; // spin 5 } Fetch-And-Add 最后一个硬件原语是“Fetch-And-Add”指令，它在返回特定地址的旧值时自动增加一个值。Fetch-And-Add指令的C伪代码是这样的: 1 int FetchAndAdd(int *ptr) { 2 int old = *ptr; 3 *ptr = old + 1; 4 return old; 5 } 在本例中，我们将使用取加(fetch-and-add)来构建一个更有趣的 ticket lock，。锁定和解锁代码如下： 1 typedef struct __lock_t { 2 int ticket; 3 int turn; 4 } lock_t; 5 6 void lock_init(lock_t *lock) { 7 lock-&gt;ticket = 0; 8 lock-&gt;turn = 0; 9 } 10 11 void lock(lock_t *lock) { 12 int myturn = FetchAndAdd(&amp;lock-&gt;ticket); 13 while (lock-&gt;turn != myturn) 14 ; // spin 15 } 16 17 void unlock(lock_t *lock) { 18 lock-&gt;turn = lock-&gt;turn + 1; 19 } 这个解决方案不是使用单个值，而是组合使用 ticket 和 turn 变量来构建一个锁。基本操作非常简单:当线程希望获取锁时，它首先对票据值执行原子的取加操作;这个值现在被认为是这个线程的“回合”(myturn)。然后使用全局共享的 lock-&gt;turn 来确定线程的回合;当(myturn == turn)对于一个给定的线程，它是线程进入临界区的回合。解锁可以通过增加回合数来实现，这样下一个等待线程(如果有的话)就可以进入临界区。 注意这个解决方案与我们之前尝试的一个重要区别:它确保了所有线程的执行。一旦给一个线程分配了它的票证值，它将在未来的某个时间点被调度(一旦它前面的线程通过了临界区并释放了锁)。在我们以前的尝试中，并不存在这样的保证;在 test-and-set 上自旋的线程(例如)可能会永远自旋，即使其他线程获得和释放锁。 Too Much Spinning: What Now? 我们的简单的基于硬件的锁很简单(只有几行代码)，而且它们可以工作(如果您愿意，甚至可以通过编写一些代码来证明这一点)，这是任何系统或代码的两个优秀特性。然而，在某些情况下，这些解决方案可能非常低效。假设您在一个处理器上运行两个线程。现在，假设有一个线程(线程0)处于临界区，因此持有一个锁，不幸的是被中断。第二个线程(线程1)现在尝试获取锁，但发现锁被持有。因此，它开始自旋，接着自旋。 然后它继续自旋。最后，时钟中断产生，线程0再次运行，释放锁，最后(比如，下一次运行时)，线程1不必自选了，可以获得锁。因此，任何时候一个线程在这种情况下被捕获，它都浪费了整个时间片，自旋只检查一个不会改变的值! 当有N个线程争用一个锁时，这个问题会变得更糟;N−1个时间片也可能以类似的方式浪费，只是自旋并等待一个线程释放锁。因此，我们的下一个问题:我们怎样才能开发出一个不需要浪费时间在CPU上自旋的锁呢？ 仅靠硬件支持无法解决这个问题。我们也需要操作系统的支持!现在让我们来看看它是如何工作的。 A Simple Approach: Just Yield, Baby 硬件支持让我们走得很远:工作锁，甚至(就像票据锁的情况)获取锁的公平性。然而，我们仍然有一个问题:当临界区发生上下文切换，并且线程开始无休止地旋转，等待被中断时，该怎么办让(持有锁的)线程再次运行? 我们的第一个尝试是一个简单而友好的方法:当要自旋时，将CPU让给另一个线程。如下演示了这种方法： 1 void init() { 2 flag = 0; 3 } 4 5 void lock() { 6 while (TestAndSet(&amp;flag, 1) == 1) 7 yield(); // give up the CPU 8 } 9 10 void unlock() { 11 flag = 0; 12 } 在这种方法中，我们假设一个操作系统原语 yield()，当线程想要放弃CPU并让另一个线程运行时，可以调用它。线程可以处于以下三种状态中的一种(运行、准备或阻塞); yield只是一个系统调用，它将调用者从运行状态移动到就绪状态，从而使另一个线程运行。因此，yield 的线程本质上是对自己进行了重新调度。 考虑一个CPU上有两个线程的例子;在这种情况下，我们基于 yield 的方法非常有效。如果一个线程碰巧调用了lock()，并发现一个锁被持有，它就会释放CPU，这样另一个线程就会运行并完成它的临界区。在这个简单的例子中，yielding 方法很有效。 现在让我们考虑一下有许多线程(比如100个)反复争夺一个锁的情况。在这种情况下，如果一个线程获得了锁，并在释放它之前被抢占，其他99个线程将调用lock()，找到所持有的锁，并交出CPU。假设有某种轮询调度器，在持有锁的线程再次运行之前，这99个调度器中的每一个都将执行这个run-and-yield模式。虽然比我们的旋转方法更好(这将浪费99个时间切片旋转)，但这种方法仍然是昂贵的;上下文切换的成本可能很高，因此会产生大量的浪费。 更糟糕的是，我们根本没有解决饥饿问题。当其他线程反复进入和退出临界区时，一个线程可能会陷入一个无穷 yield 循环。我们显然需要一种直接解决这一问题的方法。 Using Queues: Sleeping Instead Of Spinning 我们之前方法的真正问题是，它们留给了太多的机会。调度程序决定下一个线程运行;如果调度程序做出了错误的选择，那么运行的线程要么必须自旋，等待锁(我们的第一种方法)，要么立即 yield CPU (我们的第二种方法)。无论哪种方式，都有浪费的可能，而且无法防止饥饿。 因此，在当前持有者释放锁之后，必须显式地对下一个获得锁的线程施加一些控制。为此，我们需要更多的操作系统支持，以及一个队列来跟踪哪些线程正在等待获取锁。 为简单起见，我们将使用 Solaris 提供的支持，包括两种调用： park() 将调用线程置于睡眠状态，unpark(threadID) 唤醒由threadID指定的特定线程。 这两个例程可以一起使用来构建一个锁，如果调用者试图获取一个被占用的锁，那么这个锁将使调用者处于休眠状态，当锁空闲时将其唤醒。如下代码所示 1 typedef struct __lock_t { 2 int flag; 3 int guard; 4 queue_t *q; 5 } lock_t; 6 7 void lock_init(lock_t *m) { 8 m-&gt;flag = 0; 9 m-&gt;guard = 0; // 初始化锁的等待队列 10 queue_init(m-&gt;q); 11 } 12 13 void lock(lock_t *m) { // 尝试获取锁，将 guard 设置为 1，如果已经为 1 自旋等待 14 while (TestAndSet(&amp;m-&gt;guard, 1) == 1) 15 ; //acquire guard lock by spinning 16 if (m-&gt;flag == 0) { // 获取到了锁，将 guard 置为 0 17 m-&gt;flag = 1; // lock is acquired 18 m-&gt;guard = 0; 19 } else { // 未获取到锁，加入该锁的等待队列中 20 queue_add(m-&gt;q, gettid()); // 将 guard 置为 0 21 m-&gt;guard = 0; // 将该线程休眠，等待唤醒 22 park(); 23 } 24 } 25 26 void unlock(lock_t *m) { // 将 guard 设置为 1，如果已经为 1 自选等待 27 while (TestAndSet(&amp;m-&gt;guard, 1) == 1) 28 ; //acquire guard lock by spinning 29 if (queue_empty(m-&gt;q)) // 如果该锁的等待队列为空，那就标记锁为空闲态 30 m-&gt;flag = 0; // let go of lock; no one wants it 31 else // 如果该锁的等待队列不为空，相应的出队一个线程，唤醒该线程去获取锁 32 unpark(queue_remove(m-&gt;q)); // hold lock 33 // (for next thread!) // 将 guard 设置为 0 34 m-&gt;guard = 0; 35 } 在这个例子中，我们做了一些有趣的事情。首先，我们将旧的 test-and-set 思想与一个显式的锁等待队列相结合，以获得更高效的锁。其次，我们使用队列来帮助控制谁下一个获得锁，从而避免饥饿。 您可能会注意到 Guard 是如何使用的，它基本上是围绕着锁所使用的标志和队列操作的自旋锁。因此，这种方法不能完全避免旋转等待;一个线程可能在获取或释放锁时被中断，从而导致其他线程旋转——等待这个线程再次运行。然而，旋转所花费的时间非常有限(锁定和解锁代码中只有几条指令，而不是用户定义的关键部分)，因此这种方法可能是合理的。 您可能还会注意到，在lock()中，当线程无法获得锁(它已经被持有)时，我们会小心地将自己添加到队列中(通过调用gettid()函数来获取当前线程的线程ID)，将guard设置为0，并 yield CPU。给读者一个问题:如果在park()之后，而不是之前，释放守卫锁会发生什么?提示: something bad。（park之后设置guard的话，就会出现guard一直为1的情况，然后一直陷入自旋等待） 您还可以进一步检测到，当另一个线程被唤醒时，该标志没有被设回0。这是为什么呢?好吧，这不是错误，而是必须的!当一个线程被唤醒时，它将像从park()返回一样;然而，它在代码的那一点上并没有保持保护，因此甚至不能尝试将标志设置为1。因此，我们只需将释放锁的线程直接传递给获取锁的下一个线程;flag在两者之间没有设置为0 最后，您可能会注意到解决方案中在调用park()之前出现了可感知的竞争条件。在错误的时间（比如 park 调用之前），一个线程将会被暂停，假设它应该休眠，直到锁不再被持有。此时切换到另一个线程(例如，持有锁的线程)可能会导致问题，例如，如果该线程随后释放了锁。第一个线程之后的park 将永远休眠(潜在的)，这个问题有时被称为wakeup/waiting race。为了解决这个问题，需要做一些额外的工作。（其实就是在无限等待 park） Solaris通过添加第三个系统调用 setpark() 解决了这个问题。通过调用这个例程，线程可以指示它将要 park。如果它恰好被中断了，刚好另一个线程被调度，并且另一个线程在实际调用 park 之前调用unpark，那么随后的 park 将立即返回，而不是休眠。lock() 内部的代码修改非常小 1 queue_add(m-&gt;q, gettid()); 2 setpark(); // new code 3 m-&gt;guard = 0; 另一种解决方案可以将 guard 传递到内核中。在这种情况下，内核可以采取预防措施，原子地释放锁并使正在运行的线程退出队列。 Different OS, Different Support 到目前为止，我们已经看到了操作系统为了在线程库中构建更有效的锁而提供的一种支持。其他操作系统也提供了类似的支持;细节有所不同。 例如，Linux提供了一个futex，它类似于Solaris接口，但提供了更多的内核功能。具体来说，每个futex都有一个特定的物理内存位置，以及一个每个futex的内核队列。调用者可以根据需要使用futex调用(如下所述)来睡眠和唤醒。 具体来说，有两个调用可用。对 futex_wait(address, expected) 的调用将调用线程置于睡眠状态，假设 address 上的值等于 expected。如果不相等，则调用立即返回。对例程futex_wake(address) 的调用唤醒一个正在队列中等待的线程。这些调用在 Linux 互斥中的用法如下所示： 1 void mutex_lock (int *mutex) { 2 int v; 3 /* Bit 31 was clear, we got the mutex (the fastpath) */ // 自旋锁 4 if (atomic_bit_test_set (mutex, 31) == 0) 5 return; // 请求锁，相应的 mutex+1 6 atomic_increment (mutex); 7 while (1) { // 被unlock唤醒了！！获取锁然后维护等待队列长度 8 if (atomic_bit_test_set (mutex, 31) == 0) { 9 atomic_decrement (mutex); 10 return; 11 } 12 /* We have to waitFirst make sure the futex value 13 we are monitoring is truly negative (locked). */ 14 v = *mutex; // 判断整数的正负，正则为被持有 15 if (v &gt;= 0) 16 continue; // 为负，即该锁将被持有，会有竞争发生，休眠线程 // 原子性的检查 mutex 中计数器的值是否为val,如果是则让进程休眠， // 直到FUTEX_WAKE或者超时(time-out)。也就是把进程挂到 mutex 相对应的等待队列上去。 17 futex_wait (mutex, v); 18 } 19 } 20 21 void mutex_unlock (int *mutex) { 22 /* Adding 0x80000000 to counter results in 0 if and 23 only if there are not other interested threads */ // 解锁，如果等待队列长度是0就不用唤醒！ // 不把这个逻辑放futex_wake是为了减少sys call的开销。 24 if (atomic_add_zero (mutex, 0x80000000)) 25 return; 26 27 /* There are other threads waiting for this mutex, 28 wake one of them up. */ // 唤醒等待 mutex 的进程 29 futex_wake (mutex); 30 } 这段来自nptl库(gnu libc库的一部分)中的 lowlevellock.h 的代码片段很有趣，原因有几个。首先，它使用单个整数来跟踪锁是否被持有(整数的高位)和锁上的等待者数量(所有其他位)。因此，如果锁是负的，它将被持有(因为设置了高位，该位决定了整数的符号)。 其次，代码片段展示了如何针对常见情况进行优化，特别是当没有争用锁的时候;只有一个线程获取和释放锁，完成的工作很少(获取锁时 TestAndSet 原子位运算，释放锁原子位加法)。 看看您能否解开这个“现实世界”锁的其余部分，以理解它是如何工作的。做到这一点，成为Linux锁的大师，或者至少是当一本书告诉你要做什么的时候倾听的人。 Two-Phase Locks 最后一点:Linux方法有一种老方法，这种方法已经断断续续地使用了多年，至少可以追溯到 Dahm 锁在1960年代早期，现在被称为两阶段锁。两阶段锁意识到自旋是有用的，特别是当锁即将被释放的时候。所以在第一阶段，锁会自旋一段时间，希望它能获得锁。 但是，如果在第一个旋转阶段没有获得锁，那么就会进入第二个阶段，在这个阶段中调用者被置于睡眠状态，直到后来锁被释放时才会被唤醒。上面的Linux锁就是这种锁的一种形式，但它只旋转一次;更常见的方式是在循环中自旋固定的次数，然后使用 futex 睡眠。 两阶段锁是混合方法的另一个实例，在这种方法中，结合两个好的想法可能会产生一个更好的想法。当然，是否这样做很大程度上取决于许多因素，包括硬件环境、线程数量和其他工作负载细节。像往常一样，制作一个适用于所有可能用例的通用锁是一个很大的挑战。 Summary 上面的方法展示了目前如何构建真正的锁:一些硬件支持(以更强大的指令的形式)加上一些操作系统支持(例如，在Solaris上以park()和unpark()原语的形式，或在Linux上以futex的形式)。当然，细节是不同的，执行这种锁定的确切代码通常是经过高度调整的。如果您想了解更多细节，请查看Solaris或Linux代码库;它们是一本引人入胜的读物。也可以参阅David等人关于现代多处理器上的锁策略比较的优秀工作。 Lock-based Concurrent Data Structures 对于特定数据结构，如何加锁才能让该结构功能正确？进一步，如何对该数据结构加锁，能够保证高性能，让许多线程同时访问该结构，即并发访问（concurrently）？ 这里会简单介绍一些并发的数据结构，后期补充一个 并发跳表。 Concurrent Counters 如下先定义一个非并发的计数器，操作很简单，主要三种操作。+ / - / r 1 typedef struct counter_t { 2 int value; 3 } counter_t; 4 5 void init(counter_t *c) { 6 c-&gt;value = 0; 7 } 8 9 void increment(counter_t *c) { 10 c-&gt;value++; 11 } 12 13 void decrement(counter_t *c) { 14 c-&gt;value--; 15 } 16 17 int get(counter_t *c) { 18 return c-&gt;value; 19 } 为了实现并发的效果，简单粗暴地加锁就完事了。其实很好理解，就是在相应的非线程安全函数执行之前对应的加锁和释放锁就 OK。显然简单粗暴多半就意味着性能是有一定问题的。 1 typedef struct counter_t { 2 int value; 3 pthread_mutex_t lock; 4 } counter_t; 5 6 void init(counter_t *c) { 7 c-&gt;value = 0; 8 Pthread_mutex_init(&amp;c-&gt;lock, NULL); 9 } 10 11 void increment(counter_t *c) { 12 Pthread_mutex_lock(&amp;c-&gt;lock) 13 c-&gt;value++; 14 Pthread_mutex_unlock(&amp;c-&gt;lock); 15 } 16 17 void decrement(counter_t *c) { 18 Pthread_mutex_lock(&amp;c-&gt;lock); 19 c-&gt;value--; 20 Pthread_mutex_unlock(&amp;c-&gt;lock); 21 } 22 23 int get(counter_t *c) { 24 Pthread_mutex_lock(&amp;c-&gt;lock); 25 int rc = c-&gt;value; 26 Pthread_mutex_unlock(&amp;c-&gt;lock); 27 return rc; 28 } 为了理解简单方法的性能代价，我们运行了一个基准测试，其中每个线程更新一个共享计数器固定次数;然后我们改变线程的数量。下图显示了在一到四个线程处于活动状态时所花费的总时间;每个线程更新计数器一百万次。这个实验是在装有四个Intel 2.7 GHz i5 cpu的iMac上进行的;随着更多的cpu处于活动状态，我们希望在单位时间内完成更多的工作 从图上方的曲线（标为“Precise”）可以看出，同步的计数器扩展性不好。单线程完成 100 万次更新只需要很短的时间（大约 0.03s），而两个线程并发执行，每个更新 100 万次，性能下降很多（超过 5s！）。线程更多时，性能更差。 理想情况下，你会看到多处理上运行的多线程就像单线程一样快。达到这种状态称为完美扩展（perfect scaling）。虽然总工作量增多，但是并行执行后，完成任务的时间并没有增加。 Scalable Counting 令人吃惊的是，关于如何实现可扩展的计数器，研究人员已经研究了多年。更令人吃惊的是，最近的操作系统性能分析研究表明，可扩展的计数器很重要。没有可扩展的计数，一些运行在 Linux 上的工作在多核机器上将遇到严重的扩展性问题。 尽管人们已经开发了多种技术来解决这一问题，我们将介绍一种特定的方法。这个方法是最近的研究提出的，称为懒惰计数器（sloppy counter） 懒惰计数器通过多个局部计数器和一个全局计数器来实现一个逻辑计数器，其中每个CPU 核心有一个局部计数器。具体来说，在 4 个 CPU 的机器上，有 4 个局部计数器和 1 个全局计数器。除了这些计数器，还有锁：每个局部计数器有一个锁，全局计数器有一个。 懒惰计数器的基本思想是这样的。如果一个核心上的线程想增加计数器，那就增加它的局部计数器，访问这个局部计数器是通过对应的局部锁同步的。因为每个 CPU 有自己的局部计数器，不同 CPU 上的线程不会竞争，所以计数器的更新操作可扩展性好。但是，为了保持全局计数器更新（以防某个线程要读取该值），局部值会定期转移给全局计数器，方法是获取全局锁，让全局计数器加上局部计数器的值，然后将局部计数器置零。 这种局部转全局的频度，取决于一个阈值，这里称为 S（表示 sloppiness）。S 越小，懒惰计数器则越趋近于非扩展的计数器。S 越大，扩展性越强，但是全局计数器与实际计数的偏差越大。我们可以抢占所有的局部锁和全局锁（以特定的顺序，避免死锁），以获得精确值，但这种方法没有扩展性。 为了弄清楚这一点，来看一个例子（见表）。在这个例子中，阈值 S 设置为 5，4 个 CPU 上分别有一个线程更新局部计数器 L1,…, L4。随着时间增加，全局计数器 G 的值也会记录下来。每一段时间，局部计数器可能会增加。如果局部计数值增加到阈值 S，就把局部值转移到全局计数器，局部计数器清零。 上面的曲线图中接近 X 轴的是阈值 S 为 1024 时懒惰计数器的性能。性能很高，4 个处理器更新 400 万次的时间和一个处理器更新 100 万次的几乎一样。 下图展示了了阈值 S 的重要性，在 4 个 CPU 上的 4 个线程，分别增加计数器 100 万次。如果 S 小，性能很差（但是全局计数器精确度高）。如果 S 大，性能很好，但是全局计数器会有延时。懒惰计数器就是在准确性和性能之间折中。 下面代码就是懒惰计数器的基本实现。 1 typedef struct counter_t { 2 int global; // global count 3 pthread_mutex_t glock; // global lock 4 int local[NUMCPUS]; // local count (per cpu) 5 pthread_mutex_t llock[NUMCPUS]; // ... and locks 6 int threshold; // update frequency 7 } counter_t; 8 9 // init: record threshold, init locks, init values 10 // of all local counts and global count 11 void init(counter_t *c, int threshold) { 12 c-&gt;threshold = threshold; 13 14 c-&gt;global = 0; 15 pthread_mutex_init(&amp;c-&gt;glock, NULL); 16 17 int i; 18 for (i = 0; i &lt; NUMCPUS; i++) { 19 c-&gt;local[i] = 0; 20 pthread_mutex_init(&amp;c-&gt;llock[i], NULL); 21 } 22 } 23 24 // update: usually, just grab local lock and update local amount 25 // once local count has risen by 'threshold', grab global 26 // lock and transfer local values to it 27 void update(counter_t *c, int threadID, int amt) { 28 pthread_mutex_lock(&amp;c-&gt;llock[threadID]); 29 c-&gt;local[threadID] += amt; // assumes amt &gt; 0 30 if (c-&gt;local[threadID] &gt;= c-&gt;threshold) { // transfer to global 31 pthread_mutex_lock(&amp;c-&gt;glock); 32 c-&gt;global += c-&gt;local[threadID]; 33 pthread_mutex_unlock(&amp;c-&gt;glock); 34 c-&gt;local[threadID] = 0; 35 } 36 pthread_mutex_unlock(&amp;c-&gt;llock[threadID]); 37 } 38 39 // get: just return global amount (which may not be perfect) 40 int get(counter_t *c) { 41 pthread_mutex_lock(&amp;c-&gt;glock); 42 int val = c-&gt;global; 43 pthread_mutex_unlock(&amp;c-&gt;glock); 44 return val; // only approximate! 45 } Concurrent Linked Lists 接下来看一个更复杂的数据结构，链表。同样，我们从一个基础实现开始。简单起见， 我们只关注链表的插入操作，其他操作比如查找、删除等就交给读者了。 1 // basic node structure 2 typedef struct node_t { 3 int key; 4 struct node_t *next; 5 } node_t; 6 7 // basic list structure (one used per list) 8 typedef struct list_t { 9 node_t *head; 10 pthread_mutex_t lock; 11 } list_t; 12 13 void List_Init(list_t *L) { 14 L-&gt;head = NULL; 15 pthread_mutex_init(&amp;L-&gt;lock, NULL); 16 } 17 // 从链表头插入 18 int List_Insert(list_t *L, int key) { 19 pthread_mutex_lock(&amp;L-&gt;lock); 20 node_t *new = malloc(sizeof(node_t)); 21 if (new == NULL) { 22 perror(&quot;malloc&quot;); 23 pthread_mutex_unlock(&amp;L-&gt;lock); 24 return -1; // fail 25 } 26 new-&gt;key = key; 27 new-&gt;next = L-&gt;head; 28 L-&gt;head = new; 29 pthread_mutex_unlock(&amp;L-&gt;lock); 30 return 0; // success 31 } 32 33 int List_Lookup(list_t *L, int key) { 34 pthread_mutex_lock(&amp;L-&gt;lock); 35 node_t *curr = L-&gt;head; 36 while (curr) { 37 if (curr-&gt;key == key) { 38 pthread_mutex_unlock(&amp;L-&gt;lock); 39 return 0; // success 40 } 41 curr = curr-&gt;next; 42 } 43 pthread_mutex_unlock(&amp;L-&gt;lock); 44 return -1; // failure 45 } 从代码中可以看出，代码插入函数入口处获取锁，结束时释放锁。如果 malloc 失败（在极少的时候），会有一点小问题，在这种情况下，代码在插入失败之前，必须释放锁。事实表明，这种异常控制流容易产生错误。最近一个 Linux 内核补丁的研究表明，有40%都是这种很少发生的代码路径（实际上，这个发现启发了我们自己的一些研究，我们从 Linux 文件系统中移除了所有内存失败的路径，得到了更健壮的系统）。 因此，挑战来了：我们能够重写插入和查找函数，保持并发插入正确，但避免在失败情况下也需要调用释放锁吗？ 在这个例子中，答案是可以。具体来说，我们调整代码，让获取锁和释放锁只环绕插入代码的真正临界区。 前面的方法有效是因为部分工作实际上不需要锁，假定 malloc()是线程安全的，每个线程都可以调用它，不需要担心竞争条件和其他并发缺陷。只有在更新共享列表时需要持有锁。 1 void List_Init(list_t *L) { 2 L-&gt;head = NULL; 3 pthread_mutex_init(&amp;L-&gt;lock, NULL); 4 } 5 6 void List_Insert(list_t *L, int key) { 7 // synchronization not needed 8 node_t *new = malloc(sizeof(node_t)); 9 if (new == NULL) { 10 perror(&quot;malloc&quot;); 11 return; 12 } 13 new-&gt;key = key; 14 15 // just lock critical section 16 pthread_mutex_lock(&amp;L-&gt;lock); 17 new-&gt;next = L-&gt;head; 18 L-&gt;head = new; 19 pthread_mutex_unlock(&amp;L-&gt;lock); 20 } 21 22 int List_Lookup(list_t *L, int key) { 23 int rv = -1; 24 pthread_mutex_lock(&amp;L-&gt;lock); 25 node_t *curr = L-&gt;head; 26 while (curr) { 27 if (curr-&gt;key == key) { 28 rv = 0; 29 break; 30 } 31 curr = curr-&gt;next; 32 } 33 pthread_mutex_unlock(&amp;L-&gt;lock); 34 return rv; // now both success and failure 35 } Scaling Linked Lists 尽管我们有了基本的并发链表，但又遇到了这个链表扩展性不好的问题。研究人员发现的增加链表并发的技术中，有一种叫作过手锁（hand-over-hand locking，也叫作锁耦合，lock coupling）。 原理也很简单。每个节点都有一个锁，替代之前整个链表一个锁。遍历链表的时候，首先抢占下一个节点的锁，然后释放当前节点的锁。 从概念上说，过手锁链表有点道理，它增加了链表操作的并发程度。但是实际上，在遍历的时候，每个节点获取锁、释放锁的开销巨大，很难比单锁的方法快。即使有大量的线程和很大的链表，这种并发的方案也不一定会比单锁的方案快。也许某种杂合的方案（一定数量的节点用一个锁）值得去研究。 如果方案带来了大量的开销（例如，频繁地获取锁、释放锁），那么高并发就没有什么意义。如果简单的方案很少用到高开销的调用，通常会很有效。增加更多的锁和复杂性可能会适得其反。话虽如此，有一种办法可以获得真知：实现两种方案（简单但少一点并发，复杂但多一点并发），测试它们的表现。毕竟，你不能在性能上作弊。结果要么更快，要么不快。 有一个通用建议，对并发代码和其他代码都有用，即注意控制流的变化导致函数返回和退出，或其他错误情况导致函数停止执行。因为很多函数开始就会获得锁，分配内存，或者进行其他一些改变状态的操作，如果错误发生，代码需要在返回前恢复各种状态，这容易出错。因此，最好组织好代码，减少这种模式。 Concurrent Queues 你现在知道了，总有一个标准的方法来创建一个并发数据结构：添加一把大锁。对于一个队列，我们将跳过这种方法。我们来看看 Michael 和 Scott 设计的、更并发的队列。 仔细研究这段代码，你会发现有两个锁，一个负责队列头，另一个负责队列尾。这两个锁使得入队列操作和出队列操作可以并发执行，因为入队列只访问 tail 锁，而出队列只访问 head 锁。 Michael 和 Scott 使用了一个技巧，添加了一个假节点（在队列初始化的代码里分配的）。该假节点分开了头和尾操作。研究这段代码，或者输入、运行、测试它，以便更深入地理解它。 队列在多线程程序里广泛使用。然而，这里的队列（只是加了锁）通常不能完全满足这种程序的需求。更完善的有界队列，在队列空或者满时，能让线程等待。这是下一章探讨条件变量时集中研究的主题。读者需要看仔细了！ 1 typedef struct __node_t { 2 int value; 3 struct __node_t *next; 4 } node_t; 5 6 typedef struct __queue_t { 7 node_t *head; 8 node_t *tail; 9 pthread_mutex_t head_lock, tail_lock; 10 } queue_t; 11 12 void Queue_Init(queue_t *q) { 13 node_t *tmp = malloc(sizeof(node_t)); 14 tmp-&gt;next = NULL; 15 q-&gt;head = q-&gt;tail = tmp; 16 pthread_mutex_init(&amp;q-&gt;head_lock, NULL); 17 pthread_mutex_init(&amp;q-&gt;tail_lock, NULL); 18 } 19 20 void Queue_Enqueue(queue_t *q, int value) { 21 node_t *tmp = malloc(sizeof(node_t)); 22 assert(tmp != NULL); 23 tmp-&gt;value = value; 24 tmp-&gt;next = NULL; 25 26 pthread_mutex_lock(&amp;q-&gt;tail_lock); 27 q-&gt;tail-&gt;next = tmp; 28 q-&gt;tail = tmp; 29 pthread_mutex_unlock(&amp;q-&gt;tail_lock); 30 } 31 32 int Queue_Dequeue(queue_t *q, int *value) { 33 pthread_mutex_lock(&amp;q-&gt;head_lock); 34 node_t *tmp = q-&gt;head; 35 node_t *new_head = tmp-&gt;next; 36 if (new_head == NULL) { 37 pthread_mutex_unlock(&amp;q-&gt;head_lock); 38 return -1; // queue was empty 39 } 40 *value = new_head-&gt;value; 41 q-&gt;head = new_head; 42 pthread_mutex_unlock(&amp;q-&gt;head_lock); 43 free(tmp); 44 return 0; 45 } Concurrent Hash Table 我们讨论最后一个应用广泛的并发数据结构，散列表。我们只关注不需要调整大小的简单散列表。支持调整大小还需要一些工作，留给读者作为练习。 本例的散列表使用我们之前实现的并发链表，性能特别好。每个散列桶（每个桶都是一个链表）都有一个锁，而不是整个散列表只有一个锁，从而支持许多并发操作。 1 #define BUCKETS (101) 2 3 typedef struct __hash_t { 4 list_t lists[BUCKETS]; 5 } hash_t; 6 7 void Hash_Init(hash_t *H) { 8 int i; 9 for (i = 0; i &lt; BUCKETS; i++) 10 List_Init(&amp;H-&gt;lists[i]); 11 } 12 13 int Hash_Insert(hash_t *H, int key) { 14 return List_Insert(&amp;H-&gt;lists[key % BUCKETS], key); 15 } 16 17 int Hash_Lookup(hash_t *H, int key) { 18 return List_Lookup(&amp;H-&gt;lists[key % BUCKETS], key); 19 } 下图展示了并发更新下的散列表的性能（同样在 4 CPU 的 iMac，4 个线程，每个线程分别执行 1 万～5 万次并发更新）。同时，作为比较，我们也展示了单锁链表的性能。可以看出，这个简单的并发散列表扩展性极好，而链表则相反。 建议：避免不成熟的优化（Knuth 定律） 实现并发数据结构时，先从最简单的方案开始，也就是加一把大锁来同步。这样做，你很可能构建了正确的锁。如果发现性能问题，那么就改进方法，只要优化到满足需要即可。正如 Knuth 的著名说法“不成熟的优化是所有坏事的根源。” 许多操作系统，在最初过渡到多处理器时都是用一把大锁，包括 Sun 和 Linux。在 Linux 中，这个锁甚至有个名字，叫作 BKL（大内核锁，big kernel lock）。这个方案在很多年里都很有效，直到多 CPU 系统普及，内核只允许一个线程活动成为性能瓶颈。终于到了为这些系统优化并发性能的时候了。Linux 采用了简单的方案，把一个锁换成多个。Sun 则更为激进，实现了一个最开始就能并发的新系统，Solaris。读者可以通过 Linux 和 Solaris 的内核资料了解更多信息。 Summary 我们已经介绍了一些并发数据结构，从计数器到链表队列，最后到大量使用的散列表。同时，我们也学习到：控制流变化时注意获取锁和释放锁；增加并发不一定能提高性能；有性能问题的时候再做优化。关于最后一点，避免不成熟的优化（premature optimization），对于所有关心性能的开发者都有用。我们让整个应用的某一小部分变快，却没有提高整体性能，其实没有价值。 当然，我们只触及了高性能数据结构的皮毛。Moir 和 Shavit 的调查提供了更多信息，包括指向其他来源的链接。特别是，你可能会对其他结构感兴趣（比如 B 树），那么数据库课程会是一个不错的选择。你也可能对根本不用传统锁的技术感兴趣。这种非阻塞数据结构是有意义的，在常见并发问题的章节中，我们会稍稍涉及。但老实说这是一个广泛领域的知识，远非本书所能覆盖。感兴趣的读者可以自行研究。 ","link":"https://blog.shunzi.tech/post/lock/"},{"title":"Series One of Basic of Concurrency - Concurrency and Threads","content":" 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第一篇（Concurrency and Threads），并发和线程。 Chapter Index Series One of Basic of Concurrency - Concurrency and Threads Series Two of Basic of Concurrency - Lock Series Three of Basic of Concurrency - Condition Variables Series Four of Basic of Concurrency - Semaphores Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency ","link":"https://blog.shunzi.tech/post/basic-of-concurrency-three/"},{"title":"SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage","content":" FAST 2021 的文章《SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage》 SpanDB: A Fast, Cost-Effective LSM-tree Based KV Store on Hybrid Storage Elvis Zhang's Blog Abstract 键值(KV)存储支持许多关键的应用程序和服务。它们在内存中执行的速度很快，但仍然经常受到I/O性能的限制。最近出现的高速商业NVMe ssd推动了利用其超低延迟和高带宽优势的新kv系统设计。与此同时，转换到全新的数据布局并将整个数据库扩展到高端 SSD 需要大量的投资。 作为一种折衷方案，我们提出了SpanDB，一种基于 LSM 树的 KV 存储，适应流行的RocksDB 系统，利用高速 SSD 选择性部署。SpanDB 允许用户将大量数据存储在更便宜、更大的SSD上，同时将预写日志(WAL)和 LSM 树的顶层重新定位到更小、更快的NVMe SSD上。为了更好地利用这个快速磁盘，SpanDB通过SPDK提供了高速、并行的WAL写入，并支持异步请求处理，以减轻线程间同步开销，并有效地使用基于轮询的I/O工作。我们的测试结果显示，SpanDB 同时将 RocksDB 的吞吐量提高了8.8x，并将延迟降低了9.5% - 58.3%。与专为高端 SSD 设计的 KVell 系统相比，SpanDB的吞吐量达到 96-140%，延迟 2.3-21.6x 更低，存储配置更便宜。 Github Repo Introduction LSM 树应用广泛，它仍然具有吸引力，因为生产KV环境经常被发现是写密集型的，特别是由于积极的内存缓存。正如最近的系统所证明的那样，最近快速的、商业NVMe SSD可以显著提高KV性能，如 KVell、KVSSD，通过丢弃为硬盘设计的LSM-tree数据结构或将KV数据管理卸载到专门的硬件，这些系统提供了高吞吐量和可伸缩性，整个数据集托管在高端设备上。 相反，这项工作的目标是将主流基于lsm树的KV设计适应于快速NVMe ssd和I/O接口，并特别关注在生产环境中具有成本效益的部署，因为我们发现当前基于 LSM 树的 KV 存储未能充分挖掘 NVMe SSD 的潜力（对比 NVMe SSD 和 SATA SSD），特别是，常见的KV存储设计的I/O路径严重地未充分利用超低延迟NVMe ssd，特别是对于小的写操作（对比 ext4 和 SPDK） 这对写前日志记录(WAL)尤为不利，WAL 它对数据持久性和事务原子性至关重要，位于写的关键路径上，是主要瓶颈。其次，现有的KV请求处理采用慢速设备，如果切换到快速的基于轮询的I/O，工作流设计将引入高软件开销或浪费CPU周期 此外，新的NVMe接口附带了访问约束(例如要求绑定整个设备以进行SPDK访问，或者建议将线程固定在core上)。这使得使用高端 SSD 来处理不同类型的KV I/O的KV设计变得复杂，并降低了当前的常见实践(如同步请求处理)的效率。 像Optane这样的顶级ssd对于大规模部署来说是昂贵的。由于大型的、写密集型的KV存储不可避免地拥有大量的冷数据，在这些相对较小和昂贵的设备上托管所有数据可能超出用户或云数据库服务提供商的预算。 针对这些挑战，我们提出了SpanDB，一个基于LSM tree的KV系统，采用高端NVMe ssd进行部分部署，对使用 SPDK I/O 的 KV 系统的瓶颈和挑战进行了全面分析。 它通过合并一个相对较小但速度较快的磁盘(SD)来扩大对最新数据的所有读写处理，同时将数据存储扩展到一个或多个更大、更便宜的磁盘(CD)上。 它可以通过SPDK实现快速并行访问，从而更好地利用SD，绕过Linux I/O堆栈，特别是允许高速WAL写入。(据作者描述，这是研究KV存储的SPDK支持的第一个工作。) 它设计了一个适合基于轮询的I/O的异步请求处理管道，它消除了不必要的同步，积极地将I/O等待与内存中处理重叠，并自适应地协调前台/后台I/O。 它根据实际的KV工作负载有策略地自适应地划分数据，积极地将CD的I/O资源，特别是带宽纳入其中，以帮助共享当代KV系统中常见的写放大 SpanDB重新设计了RocksDB的KV请求处理、存储管理和组WAL写，利用快速SPDK接口，保留了RocksDB的lsm树组织、后台I/O机制、事务支持等数据结构和算法。 因此，它的设计与RocksDB的许多其他优化方案是互补的。添加SD后，可以将现有的RocksDB数据库迁移到SpanDB 我们使用YCSB和LinkBench进行的评估显示，SpanDB在所有测试用例中(吞吐量、平均延迟和尾部延迟)的性能都显著优于RocksDB，特别是在写密集型测试用例中。相对于最近设计用来利用高端ssd的KVell系统，SpanDB在大多数情况下提供了更高的吞吐量(只占KVell延迟的一小部分)，而不牺牲事务支持。 Background &amp; Motivation LSM-tree based KV Stores 整体架构不再赘述。 几个观点： WAL 仍然是面向客户的数据库的一个组成部分，并且位于处理写请求的关键路径上 读操作可能会在 LSM 的很多层上产生随机数据访问，尽管有很多系统采用内存缓存来优化读性能，但是对于数据量较大的情况或者局部性较差的情况还是会产生 I/O 操作，严重增加了尾延迟，影响整体性能。 flush 和 compaction 操作都会产生较大的顺序 I/O，它对前台请求处理的影响体现在 I/O 争用和写停滞(当用户写需要等待刷新以清空MemTable空间时)。 RocksDB和LevelDB通过用户可配置的flush/compaction线程数量来控制后台I/O速率，当有后台I/O任务时，它们被激活，否则休眠。研究人员已经注意到后台线程设置对性能的影响，并提出了相关的优化 （SILK）。然而，现有的解决方案仍然保留了后台线程设计，假定I/O速度较慢和基于中断的同步，这在新的基于轮询的I/O接口(将在下面讨论)中不能很好地工作。 Group WAL Writes 当前写 WAL 的常见做法是分组日志记录(group logging)，即对一次日志数据写入批量处理多个写请求。在很多常见的数据库系统中都使用了该技术，除了容错之外，组日志还通过促进顺序写操作，在速度较慢的存储设备(其中随机访问往往更慢)上提供了更好的写性能。 图 1 展示了批量写 WAL 的流程。WAL 的写入过程是顺序的，任何时候最多只有一个 group 在写 log。 当有一个正在进行中的写操作时，处理写请求的 workers 线程通过加入一个共享队列组成一个新的组，第一个进入队列的线程被指定为组的 leader。图示 1 - 3 leader 从 peers 中收集日志项，直到前一个组的 leader (刚刚完成写入)通知它继续执行。这将关闭当前组的大门，随后到达的线程将启动一个新的组。 leader 以单个同步 I/O 步骤(使用fsync/fdatasync, 4)将日志条目写入持久存储。然后，leader 唤醒组成员，启动 MemTables 中的更新，使这样的写入对后续的请求可见 5 - 6 通过将最后一个可见序列推进到其条目中的最新序列号来完成组提交 7 解散组 8，然后唤醒下一个 leader 使用高端 NVMe SSD 的话，上图中的步骤 4 的时间可以大幅减少。与此同时批处理写操作合并了小 IO。因此，同步组日志导致的软件开销增加，导致大多数线程将时间浪费在不同类型的等待上 （步骤 1-3 和 7） 例如，我们测量了RocksDB在通过ext4访问的SATA SSD上的这4个步骤上平均花费了68.1%的写请求处理时间，而在Optane上通过 SPDK 则增长到了81.0%。（因为总花费的时间减少了，等待的时间所占据的比重就增加了） 如果上面的 Group Logging 流程不够清晰，可以看下面这个图 图源：知乎 bluesky - RocksDB 写入流程 High-Performance SSDs Interfaces 高性能 SSD Intel Optane、Toshiba XL-Flash、Samsung Z-SSD 提供了更低的延迟和更高的吞吐量。考虑到Linux内核I/O堆栈开销在总I/O延迟中不再是可以忽略的，Intel 开发了 SPDK，一组用户态的库/工具来访问高速的 NVMe 设备。SPDK 把驱动移动到了用户空间，避免了系统调用并支持了零拷贝访问。它轮询硬件完成，而不是使用中断，并避免I/O路径中的锁。在这里我们总结了在本研究中发现的与KV存储相关的SPDK性能行为和策略限制 SPDK 整体性能 Intel Optane P4800X and P4610 使用不同类型和不同大小的 IO 组合来模拟 I/O，下图展示了 P4800X 的结果，P4160 展示的测试结果趋势类似。ext4 使用了 write 调用，每一个都进行了 fdatasync，SPDK 使用了内建的 perf tool (spdk_nvme_perf) 结果表明：（此处无随机写测试） 顺序读下 ext4 和 SPDK 接近 通过ext4进行的4KB顺序写操作(WAL-style)只实现了硬件潜力的一小部分，延迟时间比SPDK高4.05x(IOPS相应降低)。 4KB的随机读和64KB的顺序写测试可以看到这两个极端之间的ext4-SPDK差距 这些结果突出表明，SPDK可以显著改善KV I/O，特别是日志和写密集型工作负载 SPDK concurrency 为了评估SPDK服务并发顺序写的能力，我们分析了各个SPDK请求，并发现7-8µs的单线程延迟中的大部分确实被busy-wait占用，由于争用下的I/O变慢，busy-wait随着并发写的线程增多而增加。 然后我们设计了一个 pipeline 方案，其中每个线程管理多个并发的SPDK请求。它允许“窃取”I/O等待时间来发出新请求，并检查未完成请求的完成状态(每个请求的完成时间低于1µs)。 图3 给出了Intel上的延迟和吞吐量结果P4610 (N)和Intel Optane (O) ssd硬盘。我们改变线程的数量(“3-N”有3个线程写入SSD N)和每个线程并发请求的上限(“CR=2”，每个线程最多发出2个请求)。NVMe ssd提供了目前RocksDB/LevelDB single-WAL-writer设计所没有的并行性。特别是，Optane (O)显示出比P4610 (N)更高的并发性，在更多的写入的情况下，具有更低的延迟和更快的吞吐量增长。然而，即使使用O，超过3个并发写入也不能提供更高的SPDK IOPS:使用3个loggers，每个CR=3似乎可以提供峰值的WAL速度，我们表示为3L3R。另一方面，N在2L4R处饱和。 SPDK access restrictions 快速使用spdk访问高端NVMe ssd带来的性能好处体现在： 一旦某个SSD被一个进程绑定到SPDK上，其他进程(无论是通过SPDK还是通过Linux I/O堆栈)都无法访问它。这简化了与用户级访问相关的工作负载间隔离，但也禁止将文件系统部分部署到通过SPDK访问的SSD上。 此外，建议用户将spdk access线程绑定到特定的内核。我们验证了不这样做会带来显著的I/O性能损失。这一点，再加上基于轮询的I/O模式，使得使用后台刷新/压缩线程的常见做法不适合SPDK访问:未绑定线程的I/O很慢，而绑定线程在空闲时不能轻松释放核心资源 SpanDB Overview Design rationale 我们提出了SpanDB，一个高性能，经济有效的基于lsm树的KV存储使用异构存储设备。SpanDB提倡使用小型、快速且通常更昂贵的NVMe SSD作为快速磁盘(SD)，同时部署更大、更慢、更便宜的SSD(或此类设备的阵列)作为容量磁盘(CD)。SpanDB使用SD有两个目的:(1)对WAL进行写操作(2)存储RocksDB的LSM-tree的顶层。 由于WAL的处理成本是用户可见的，并且直接影响延迟，我们预留了足够的资源(内核和并发的SPDK请求，以及足够的SPDK队列)，以最大化其性能。同时，WAL数据只需要维护到相应的flush操作，通常需要GBs的空间，而今天的“小型”高端ssd，如Optane，提供超过300GB的空间，这促使SpanDB将RocksDB的LSM-tree的顶层移至SD。这还会从CD上卸载大量的刷新/压缩流量，CD上驻留着大量较冷的数据 SpanDB architecture 下图展示了 SpanDB 的架构。在 DRAM 中保留了 RocksDB Memtable 的相关设计，使用一个可变和多个不可变memtable。SpanDB 几乎不会对 RocksDB 的 KV 数据结构、算法或者操作语义进行修改。这里的主要区别在于它的异步处理模型，以减少同步开销和自适应调度任务。 磁盘上的数据分布在 CD 和 SD 上，两个物理存储分区。 SD 进一步被分区，一个小部分为 WAL 区域，另一个部分为数据区域。SpanDB 通过 SPDK 将 SD 当作裸设备管理并重新设计了 RocksDB 的 WAL 批量写入，以实现快速、并行的日志记录，将日志记录带宽提高 10x。为了最小化 RocksDB 的修改，SpanDB 实现了一个轻量级的自带缓存的文件系统 TopFS，允许简单和动态的 LSM levels 重定位。 CD 分区同时存储了 tree stump，经常包含较冷的大部分数据。SpanDB 的管理还是与RocksDB一样，通过文件系统访问，并借助操作系统页面缓存进行辅助。 下图还对 SpanDB 不同类型的流量进行了区分。WAL 区域只进行 log 的操作，SD 的数据区域接受所有 flush 操作。此外，SD 数据区和 CD 都可以容纳用户读和压缩读/写，SpanDB 执行额外的优化，以支持在两个分区上同时进行压缩，并自动协调前台/后台任务。最后，SpanDB 能够基于对两个分区的实时带宽监控实现动态 tree level 布局。 Sources of performance benefits 通过采用一个通过SPDK访问的小而快的SD，它加快了WAL的速度，并行WAL写入。 通过将SD也用于数据存储，它优化了这种快速ssd的带宽利用率 通过启用工作负载自适应的SD-CD数据分发，它积极地聚合设备间可用的I/O资源(而不是仅仅将CD用作“溢出层”)。 虽然主要是通过将I/O卸载到SD来优化写，但它减少了读密集型工作负载的尾部延迟 通过减少同步和主动平衡前台/后台I/O需求，它利用了快速轮询I/O，同时节省CPU资源 Limitations 由于前面提到的SPDK访问约束，SD需要绑定到一个进程，这使得共享资源 SD 变得困难 对于全读的工作负载，SpanDB在异步处理中产生的速度提升很小，并带来轻微的开销。 Design and Implementation Asynchronous Request Processing 很多 KV 系统都使用了嵌入式的 DB 处理，所有前端任务都被假设成客户端，每次同步处理一个KV请求。这样的处理通常是 I/O 限制(特别是WAL写)，用户通常通过线程过度配置获得更高的总体吞吐量(每秒请求数)，拥有比 cores 更多的客户端线程。具有快速NVMe ssd和SPDK等接口，线程同步(比如睡眠和唤醒)很容易花费比 I/O 请求更长的时间。在这种情况下，线程过度配置不仅增加了延迟，还会降低总体资源利用率，从而降低吞吐量。 此外，使用基于轮询的SPDK I/O，让线程在相同的 cores 上共存就失去了在 I/O 等待期间提高 CPU 利用率的吸引力。这也适用于使用后台线程管理LSM-tree刷新/压缩任务的常见实践。特别是，由于SPDK I/O的“fsync”涉及到忙等待，现有的 RocksDB 设计可能会释放许多后台线程，这会对其他线程造成巨大的破坏，并浪费 CPU 周期。 认识到这些，SpanDB采用异步请求处理。在 n 核机器上，用户将客户端线程的数量配置为N client，每个线程占用一个核。剩下的 n−Nclientn - N_{client}n−Nclient​ 个核心处理 SpanDB 内部服务线程，相应地被分为两个角色：loggers and workers。这些线程都被绑定到他们分配的核心上。 Loggers 主要执行 WAL 写入，Workers 处理后台任务处理（flush/compaction）以及一些非 I/O 任务，例如 Memtables 的写入和更新，WAL 日志项的准备，相关事务的加锁/同步。根据观察到的写强度，head-server 线程自动自适应地决定logger 的数量，这些 logger 被绑定到分配了 SPDK 队列的核上，从而保证 WAL 的写带宽 Asynchronous APIs SpanDB 提供了简单、直观的异步 api。对于现有的 RocksDB 同步 get 和 put 操作，它增加了异步对等对象A_get和A_put，再加上A_check来检查请求状态。类似的API扩展适用于扫描和删除。相应地，SpanDB扩展了RocksDB的 status 枚举。 如下代码示例大致描述了异步请求的流程。 SpanDB request processing SpanDB管理前台请求处理的各个阶段，以及多个队列中的后台刷新/压缩任务。这些队列在线程之间传递子任务，还提供关于某个系统组件的压力级别的反馈。根据这些反馈，SpanDB可以调节客户端请求发出率(通过上述IsBusy接口)，也可以动态调整内部 workers 的任务分配 图 5 描绘了 SpanDB 中的相关队列。flush 和 compaction 队列本身就是来自于 RocksDB 的设计，即便 SpanDB 修改了实际的操作来使用 SPDK I/O。SPDK 还额外加了四个队列，读队列、以及写相关的三个队列（ProLog、Log、EpiLog） 对于异步读，当请求不需要进行 I/O 时，SpanDB 保持了原有 RocksDB 的同步模型，在KV应用程序中，由于具有典型的局部性，许多读操作都是从MemTable中进行的，特别是在今天由 DRAM 支持的更大的 MemTable 的场景下。给定一个 Key，能够快速在检查 Memtables 是否命中，如果命中直接完成读请求。命中的情况下大概只花费 4-6 微秒，相反即使在争用的情况下从 Optane 上读取也只花费 30 微妙，相应地会把读请求插入到读请求等待队列中然后返回。之后会有 worker 从队列中取出该请求，完成剩下的和 RocksDB 相同的读任务并设置他的完成状态。 对于异步写，SpanDB 将处理过程划分成了三个部分。 客户端先将请求 dump 到 ProLog 队列中，由 worker 来处理。 worker 相应地生成 WAL 日志项，按顺序传递到 Log 队列中。这两个队列都旨在促进批处理日志：一个 worker/logger 在这些队列中收集所有的日志项。除了批处理之外，logger 还将写日志流水线化，最大限度地提高了SPDK写并发性 当将一个组写入到了 SD 之后，logger 把相应的请求添加到 EpiLog 队列中，以便 worker 能够完成最终的处理。主要包括实际 MemTable 的更新。像读操作一样，这里的任务需要单独的注意力，不可能通过批量执行来加速。 如上图所示，ProLog 和 Log 队列都是 flat 的无锁队列，也就很容易地抓取所有请求进行出队。 Read、Epilog 都是环形队列，只在出队的时候要求加锁。 Task scheduling 上述SpanDB队列为调整内部资源分配提供了自然反馈。我们的SPDK基准测试结果表明 NVMe SSD 提供了并行性但是可能会被每个核心发出好几个并发请求使其饱和。因此 SpanDB 从一个 logger 开始，根据当前的写入强度在 1 到 3 之间增加和减少这个分配。 对于 Worker 因为需要处理所有其他的队列，所以比较灵活。在三个前端任务队列之间，SpanDB 根据队列的实际长度和每个任务的处理时间的加权来执行负载均衡。在前端队列和后端队列之间，SpanDB 以处理前台任务优先，使用一个合适的阈值来监控后台队列长度，从而主动执行清理，特别是在写密集型工作负载下。 Transaction support SpanDB 完整支持了事务并提供了一个异步提交接口 A_commit，对 RocksDB 做了一些小改动。注意在 RocksDB 的事务模式，写操作将在一个内部 Buffer 中生成 WAL 项，只会在提交的时候才会被写入。而 SpanDB 异步提交操作则是将相应的写任务插入到了 ProLog 队列。 High-speed Logging via SPDK Enabling parallelism and pipelining SpanDB 保留了 group logging 机制，但也允许多个 WAL 流并发写入。它不是让一个客户端作为 leader (并迫使 follower 等待)，而是使用专用的 logger，并发地发出批处理写操作。每个 logger 抓取它在 QLog 中看到的所有请求，并将这些 WAL 条目聚合为尽可能少的 4KB 块。它通过为一个请求窃取 SPDK 繁忙等待时间准备/检查其他请求来执行 pipeline。例如，对于 2L4R，有多达 8 个未完成的 WAL 写组 Log data management 并行的 WAL 写入让日志数据的管理变得复杂，特别是针对没有文件系统的裸设备。幸运的是，使用原子的 4KB SPDK 写操作，协调并发的 WAL 流只增加了很少的开销。 SpanDB 首先在 SD 上分配可配置数量的 Pages 给相应的 WAL 区域（测试中分配了 10GB），每个都会有一个逻辑页号 LPN。这些页中的一个会被设置为元数据页，在任何时候只会有一个可变的 Memtable，其对应的日志文件会不断增大。我们会分配固定数量的逻辑页 groups，每个组会包含连续的页，而且足够大可以装下一个 Memtable 的日志数据。被使用了的日志页和对应的 Memtable 给组织在一起：SpanDB 重用了 RocksDB Memtable 的日志文件号 LFN 作为日志标签号 LTN，嵌入在所有日志页的开头以便进行故障恢复。 下图给了一个包含四个 Memtable 的例子，一个 mutable 三个 immutable，对应元数据页中的 Active 和 InActive 状态。 在一个 Memtable 被 flush 之后，日志页的整个分片将被回收，保证 Memtable 日志的连续性。对于每个 Immutable Memtable，元数据页都记录了对应日志页的起始和终止 LPN。假设典型的 KV 存储使用少量 memtable，一个页面就足够容纳这样的元数据了。 Logger 发出并发请求，每个 Logger 提供一个 WAL 数据缓冲区和大小，唯一的同步点是日志页面的分配。我们使用比较和交换(CAS)操作实现了轻量级的原子页面分配。下图中的淡蓝色部分显示了三个请求的日志页分配过程，分别分配了 1，3，2 个页面，可以并行地进行分配。这些 WAL 写入不会修改元数据页，在元数据页中，只有当MemTable变得不可变时，per-MemTable end LPN 才会被追加。 在日志 Page 中，logger 首先记录当前 LTN，然后是一组日志条目，每个条目都有其大小的注释。图7中的放大部分描述了这种布局，包括每个条目的校验和(已经在RocksDB中计算过了)。 Correctness SpanDB的并行WAL写设计保留了RocksDB一致性语义。它不会改变用于协调和排序客户机请求的并发控制机制。因此，带有happened-before限制的事务在日志页面中不会出现顺序错误，如下面的简要解释。RocksDB 默认的隔离保证是 READ COMMITTED，也会检查写与写操作之间的冲突并序列化两个并发事务（假定两个更新相同 KV 项）。对于任何两个更新事务T1 和 T2，使用这两个隔离保证，READ COMMITTED 表明如果 T1 发生在 T2 之前，即 T2 看到了 T1 的效果，那么 T1 在 T2 开始执行之前必须提交。通过RocksDB group WAL写协议的设计，可以看出T1和T2的日志记录应该分两批出现，其中T1的批提交早于T2的批提交，在T2的批提交之前完成。当批量日志并行写入到 SpanDB 时，传递一个序列化的point 以便进行原子的页分配。因此 T1 的批任务仍然保证了比 T2 获得的序列号更小，后者就能看见前者的更新。相似的，从 WAL 中恢复的时候，SpanDB 总是按序列号升序执行 redo。 Log recovery 恢复操作首先读元数据页，检索出 log page groups 的数量以及相应的页地址范围。实际的从 log page group 恢复和从 RocksDB 的日志文件中恢复是一致的。同样，每个页面中的LTN号帮助标识 Active 日志 log group 的“结束”。 一个难点是 SpanDB 回收日志页 groups 时，有的 group 包含一些老的日志页，在恢复过程中 SpanDB 需要知道当前组中哪些页已经被重写了。RocksDB 依赖文件系统进行故障恢复：读取数据是否包含在活跃的 log file 中。没有文件系统，SpanDB 在回收之前可以持久化一个单独的元数据更新或者将老的日志页 wipe out(写 0)。会影响 SD 的 WAL 写带宽，降低到一半。而我们重用了每个 Memtable 的 LTN 作为日志页的 color。因为 SSD 可以保证 4K 原子写，LTN 通常又都是在页的开始写的，这些页已经暗含了最后一次成功写的位置信息了。元数据页维护了当前活跃的 LTN，在这个组中，具有过时LTN的页面还没有从当前MemTable中被覆盖写。 Offloading LSM-tree Levels to SD 为了 KV 服务器的持续均衡的运行，SpanDB将RocksDB的lsm树的顶层迁移到SD，为用户提供更多的硬件投资回报。下面我们讨论所涉及的主要挑战和解决办法 Data area storage organization 在NVMe SSD上使用SPDK的一个限制是，整个设备必须与本机内核驱动程序解除绑定，并且不能通过传统的I/O堆栈访问。因此不能对SD进行分区，只能使用SPDK将WAL写入一个区域，在另一个区域安装文件系统。 SpanDB 实现的 TopFS，这是一个精简的文件系统，在SPDK I/O之上提供熟悉的文件接口包装器。SST 文件的仅追加和此后不可变的特性，加上它们的单 writer 访问模式，简化了TopFS设计。比如文件在创建的时候就知道了大小或者有有一个已知的大小限制。而且每个 SST 只会被单个线程写一次。大多数场景下，输入的数据不会被删除直到 SST 文件写入成功完成。此外，TopFS 保证了文件 close 时的数据的一致性。从而支持了每个文件连续的 LPN 范围分配，和前面提到的日志 groups 相似。元数据管理也比较简单：一个哈希表，按文件名进行索引，存储文件的起始和结束 LPN。TopFS 使用一个 LPN 空闲列表管理空闲空间，相应的连续的 LPN 范围被合并。 Ensuring WAL write priority flush/Compaction 操作是会影响前台负载的，但他们的延迟很多时候都对用户是透明的，因此 SD 可以完全更充分利用其带宽，比如在 WAL 写上，因为 WAL 的写入延迟用户时能感知到的。SPDK 提供了足够的 NVMe Queue Pairs，这允许对不同的请求类型进行单独管理。不幸的是，没有一个现有的通用 SSD 实现对这些队列的优先级管理。此外，这些队列提供的操作非常有限:用户只能发出请求和检查完成状态。 因此除了前后端任务在队列上的协调以外，SpanDB 还对 WAL 写请求赋予了优先级。我们发现他们的优先级可以有效地保护通过以下步骤： (1) 分配到每个日志请求 slot 专用队列(例如,8 个 L2R4 队列) (2) 减少 flush/compaction 的 I/O 请求的大小从 1MB 到 64KB 来减小有 WAL 时的 I/O 争用 (3) 限制执行 flush/compaction 的 worker 线程数量 SpanDB SPDK cache SPDK bypass 了 OS page cache，如果无人看管，这会带来出色的原始 I/O，但会带来灾难性的应用程序 I/O 性能。为了克服这个问题，我们在 TopFS 上实现了 SpanDB 自己的缓存。注意，对于SPDK I/O，传递的所有数据缓冲区必须通过 spdk_dma_malloc() 在固定内存中分配。SpanDB重用这样的缓冲区作为缓存，从而节省额外的内存复制。 在SpanDB初始化时，它在hugepage中分配一个大内存缓存(大小可配置)。在创建SST文件时，SpanDB在缓存中保留适当数量的连续64KB缓冲区(文件大小或大小限制是已知的)。SpanDB使用另一个哈希表来管理这个缓存，同样以RocksDB SST文件名作为键。value字段是一个数组，存储每个文件块的缓存项，如果块被缓存，则存储相应的内存地址，否则为空。块大小配置显然涉及到缓存数据粒度和元数据开销之间的权衡。我们的评估使用SpanDB默认块大小64KB，对100GB的数据库产生&lt;500KB的元数据开销。 Dynamic level placement 通过上述所有机制，我们可以动态地调整SD上驻留的树级别的数量。最初，我们使用一个分析模型来直接计算最佳SD-CD级别分区，从而最大限度地提高整个系统的吞吐量。然而，我们无法找到与我们的测量结果一致的精确的 LSM-tree 写放大模型。特别是，最先进的工作在这前端的似乎没有考虑写速度和它的变化。我们的测试表明，这些因素会严重影响暂态的“树状结构”(最高层在不同程度上超出其大小限制)，进而影响写/读放大级别。 因此，SpanDB 使用了临时的动态分区，通过观察 SD 和 CD 之间持续的资源利用率的不平衡，它的 head-server 线程监视 SD 带宽使用情况，并在低于 BWL{BW}_LBWL​ 时触发SST文件重定位，直到达到 BWH{BW}_HBWH​ 或 SD 满，其中 BWL{BW}_LBWL​ 和 BWH{BW}_HBWH​ 是两个可配置的阈值。 由于SST文件不断地进行合并，SpanDB并没有在SD和CD之间迁移数据，而是通过将文件创建重定向到不同的目标，逐步“提升”或“降级”整个级别。它有一个指针，指示当前哪些级别应该转到快速NVMe设备。例如，指针 3 覆盖了所有的前3个级别。然而，该指针仅确定新SST文件的目的地。因此，在SD上有一个新的L3文件，在CD上有一个旧的L2文件是可能的，尽管这种“倒置”是罕见的，因为顶层更小，他们的文件更新更频繁。 Evaluation Related Work Tiered storage NVMFS, Strata, Ziggurat: File system based on heterogeneous devices HiLSM, MatrixKV: Hybrid storage devices for KV Mutant: ranks SST files by popularity and places them on different cloud storage devices PrismDB: makes LSM-trees “read-aware” by pinning hot objects to fast devices. KV stores optimizations for fast, homogeneous storage homogeneous storage UniKV LSM-trie SlimDB FloDB PebblesDB KVell SplinterDB FPGA X-Engine KVSSD ATC20 PinK PM HiKV NoveLSM NVMRocks Bullet SLM-DB FlatStore MyNVM MyRocks Logging optimizations NVLogging NVWAL Other related work SILK Monkey ElasticBF TRIAD WiscKey HashKV ","link":"https://blog.shunzi.tech/post/SpanDB/"},{"title":"PebblesDB: Building Key-Value Stores using Fragmented Log-Structured Merge Trees","content":" SOSP19 PebblesDB: Building Key-Value Stores using Fragmented Log-Structured Merge Trees https://github.com/utsaslab/pebblesdb Abstract LSM 写性能较好，但是遭受严重的写放大，写放大原因其实就是数据结构的问题。 本文受跳表的启发，提出了分段LSM树，也就是 Fragmented Log-Structured Merge Trees (FLSM)。引入了 guard 的概念来组织日志，同时避免在相同层重写数据。 本文通过修改 HyperLevelDB 来使用 FLSM 数据结构。测试结果表现较好，写放大改善效果显著。 Introduction 主要介绍 KV Store 应用广泛，然后写放大是个很严重的问题，对 SSD 的寿命/磨损以及读写吞吐量都会产生影响。下图描述出写放大有多严重，然后指出以前的解决写放大的方案都有一定的限制。【ATC15 NVMKV】、【SOCC12 Using Vector Interfaces To Deliver Millions Of IOPS From A Networked Key-Value Storage Server】都需要一些特定的硬件，而且可能牺牲例如查询性能的其他方面。但在如今的场景中，常常是需要读写兼顾的。 简单分析了造成写放大的原因：因为 LSM 中数据是有序存放的，为了利用顺序写的性能，数据的查询也会相对比较顺序、比较高效。但是当有新的数据写入的时候，为了维持原有的有序性，就会导致一下现有的数据被重写，从而导致大量的写 IO。 本文结合了 LSM 和 SkipList 提出了 FLSM，同时提出了一个新的 compaction 算法。FLSM通过极大地减少(在许多情况下，消除)数据重写，而不是将数据分割成更小的块，并使用存储上的 guards 来组织这些块，从而消除了写放大的根源。Guards 提升了 LSM Tree 的检索效率。LSM 的写操作常常会因为 compaction 操作而 block 或者 stall，那么 FLSM 的思想因为极大减少了写 IO，让 compaction 相应地也就更快完成，提升了写吞吐。 除此之外，PebblesDB 还使用了一组其他技术，如 Parallel Seeks、aggressive seek-based compaction 以及 sstable-level bloom filters 等技术来减小 FLSM 引入的开销。 FLSM 数据结构并不适合在初次写操作爆发后涉及大量范围查询的工作负载。但是如果范围查询和写操作是穿插的话，是不会引入开销的。 Contributions 设计了新的 FLSM，结合了 Skiplist 和 LSM 设计并实现了 PebblesDB，使用 FLSM 和几个重要的优化 Parallel Seeks aggressive seek-based compaction sstable-level bloom filters 实验表明 PebblesDB 比其他主流 LSM 存储在读写表现上都要更好 Background KV and LSM KV 常见操作和 LSM 树此处不再赘述。只是简单列举几个例子和观点 B+ Trees 不适用于写密集负载，因为更新树要求随机写，所以写密集都使用了 LSM KyotoCabinet BerkeleyDB LSM 中查询操作定位一个 Key 通常需要两个二分查找： 第一次找到对应的 SSTable，根据 SSTable 的起始 Key （单独维护的元数据） 第二次在 SSTable 内部找到相应的 Key，如果查询失败，说明不在当前 level 每个 Key 都会包含它的版本，找到每一层的那个 Key 需要读取和搜索一个 SStable seek() 和 next() 操作都要求在整个键值存储区上定位迭代器，所以使用了多个迭代器来实现，每一层一个迭代器。每个迭代器首先定位到每一层相应的 sstable，然后将迭代器的结果合并。seek() 操作首先找到每一层相应的 SSTables，然后定位 SSTable 迭代器，SSTable 迭代器的结果将合并然后定位到对应的 KV 存储迭代器。next() 操作只是向前推进正确的 sstable 迭代器，再次合并迭代器，并重新定位键值存储迭代器。(这一块可能要等看了源码中的迭代器的部分之后才能理解) put() 操作将键-值对和一个单调递增的序号（序列号/版本号）一起写入内存中称为memtable的跳过列表。 更新和删除操作都不是就地更新的，因为所有 IO 都是顺序的。相反，一个 Key 插入到数据库中使用了一个更高的序列号，删除一个键相应地就是插入一个特殊的 flag tombstone。因为有更高的序列号，所以最近版本的 flag 会返回给用户。 WA 写放大 下图描述了 compaction 的过程，刚开始就只有两层 l0, l1。假定 l0 配置为一次只包含一个 SSTable，到达限制后将触发 compaction，t1 时刻加入了一个 SSTable，达到了 L0 的限制，触发 compaction，t3，t5 也是如此。 当压缩一个sstable时，下一层的所有sstable的密钥范围与被压缩的sstable相交的sstable都被重写。在这个例子中，由于所有0级sstable的键范围都与 Level 1 所有的键范围相交，每次Level 0 sstable被压缩后，Level 1 sstable就会被重写。在下面这个最糟糕的例子中，Level1 在 compaction 时可能会被重写 3 次（t2, t4, t6），因此比较高的写放大可以归根于 compaction 操作的多次 SSTable 重写。 Challenge: 在LSM中减少写放大的一种简单方法是在压缩期间不合并sstable，而是向每个级别添加新的sstable （FIFO compaction， Universal(tiered) Compaction）。然而，读取和范围查询性能将显著下降，原因有两个。首先，如果没有merge，键值存储将最终包含大量sstable。其次，由于多个 SSTable 现在可以包含相同的键，并且在同一级别上有重叠的键范围，读取操作将不得不检查多个 SSTable (因为无法二分查找sstable)，这会导致很大的开销。 FLSM - FRAGMENTED LOG-STRUCTURED MERGE TREES 三个目标：低写放大、高写吞吐量和良好的读性能。 日志结构的合并树的基本问题是，当新数据被压缩到sstable中时，sstable 通常会被重写多次，FLSM 通过将sstable分割成更小的单元来解决这个问题。FLSM的压缩只是将一个新的sstable片段添加到下一个级别，而不是重写整个sstable。这样做可以确保数据在大多数级别中只写入一次。最后几个最高级别使用了不同的压缩算法。FLSM使用一种新颖的存储布局和使用 Guard 组织数据来实现这一点 Guards 传统 LSM 中，每一层包含的 SSTable 都不包含相交的 key ranges（每个 Key 在一个 Level 里只会出现在一个 SSTable 中）。我们的主要观点是，保持这个不变是写放大的根本原因，因为它迫使数据在同一级别上重写。FLSM 丢掉了这一不变性：每一层包含多个 SSTables，且键范围可以重叠，所以一个 Key 可以出现在多个 SSTables 中，但是牺牲了查询性能，所以 FLSM 将 SSTables 组织在 guards 中。 每个 level 包含多个 guards，每个 guard 把 key 空间分成很多个不相交的子空间，每个 Guard 都有个关联的 Key，是从 Keys 中选出来的然后插入到 FLSM 中的，每个 Level 比在他之上的 level 都包含更多的 Guard，随着数据越来越深入地进入FLSM，Guard 的粒度(间隔)也越来越大，在跳转列表中，如果一个键是给定 level i 的守卫，那么它将是所有 level &gt; i 的守卫。 每个守卫有一组关联的 SSTables，每个 SSTable 是有序的。如果 GiG_iGi​ 和 Key KiK_iKi​ 关联，Gi+1G_{i+1}Gi+1​ 和 Key Ki+1K_{i+1}Ki+1​ 关联，那么范围为 [KiK_iKi​, Ki+1K_{i+1}Ki+1​) 的 SSTable 将被关联到 GiG_iGi​，小于第一个守卫的对应范围的 SSTable 包含在每层的一个特殊的 sentinel guard 里，最后一个守卫则存储了大于对应 Key 的所有范围的 SSTables，在一个 Level 的 Guards 不会有重叠的范围，因此为了在一个给定的 level 里找到 key，只需要访问对应的一个 Guard。 在 FLSM compaction 的时候，一个给定的 Guard 对应的 SSTables 被归并排序然后分段，所以每个子守卫接收到一个新的 SSTable，它进入下一个 level 的子守卫的键范围。 下图所示例子，进行 put() 操作： 首先被添加到内存中 Memtable，图中未表示，当 Memtable 满了之后被写入到 L0 的 SSTable，L0 没有守卫，只是收集最近写入的 SSTables 随着 level 数量的增加，守卫数量也相应增加，但每个 level 的守卫数量不一定呈指数增长 每个 Level 有一个哨兵守卫，主要负责对应 Key 小于当前守卫的 SSTables，如下所示，Key &lt; 5 的就被关联到 sentinel guard FLSM Level 里的数据部分有序，守卫没有重叠的键范围，但是每个守卫关联的 SSTables 可能会有重叠的键范围。 Selecting Guards FLSM 的性能起始会受到 guards 如何选择的显著影响，最差的情况就是一个 guard 包含所有的 SSTable，读和查询一个如此大的 Guard 将导致一个不可接受的延迟的增加。因此，守卫不是静态选择的;而是从插入的键中概率地选择守卫，防止倾斜。 Guard Probability 当一个 Key 插入到 FLSM 时，Guard Probability 决定了是否该 Key 成为一个 Guard，gp(key, i) 就是一个 Key 在 level i 成为 guard 的概率。例如，如果 gp 是 1/10，那么每十个插入的 keys 将有一个被随机选择成为一个 guard。gp 被设计成在 level 1 上是最小的，因为该层有最少的守卫，然后随着 level 数的增加概率变大，因为更高的 level 有更多的守卫。以这种方式来选择守卫，将守卫以一种平滑的方式分布在插入的键上，从而可能防止倾斜。 更像跳表的是，如果一个 Key K 被选择作为了 level i 的守卫，那么它将成为后续所有 level 的守卫。这样选择守卫将允许在每个守卫之间的间隔在每个更深的 level 中依次被修改。比如上图中，键 5 被选择为 level 1 的守卫，也被选择为 level 2 和 level 3 的守卫。 FLSM 为了简便是直接选择了插入的键作为守卫，但其实是没有要求一定要在 KV 存储中有相应的 KV 对和守卫相对应。 Other schemes for selecting guards 当前选择守卫的方式的优点在于足够简单，很容易计算，而且分布也比较公平，但是呢其实是没有把在压缩期间分区 SSTable 导致的 IO 数量考虑进去的。FLSM 可能会在压缩时为每个级别选择新的守卫，这样就使得 SSTable 分区最小化;然而，这可能会带来倾斜。我们把其他可选的守卫选择方式留到未来的工作中。 Inserting and Deleting Guards Insert 当守卫被选择之后不会被同步地插入到 FLSM 中，因为插入守卫可能导致 SSTables 的分裂或移动，如果一个守卫插入到很多 level，那么就会在这些所有层上都产生 SSTables 的相关变化。因此守卫是异步插入 FLSM 中的。 守卫被选出来之后，首先添加到内存中一个较 未提交的守卫列表中，这时候的存储设备上的 SSTable 还不会进行分区，这时候的 FLSM 的读操作就还是按照未提交的守卫不存在的情况下进行的。在下一个 compaction 周期的时候，SSTables 基于老的守卫和未提交的守卫来进行分区和压缩。因为未提交的守卫而需要分裂的 SSTable 被 compacted 到下一层。在 compaction 结束的时候，未提交的守卫再被持久化到存储设备上，然后被添加到整个守卫的集合中。未来的读操作将基于整个守卫集合来进行服务了。 Delete 我们发现在我们测试过的很多负载中，其实守卫的删除是不需要的，如果一个守卫对应的 keys 为空那么就可以删除这个守卫，但是空的守卫其实不会产生显著的性能降级，因为 get 和 range query 操作都是为跳过空的守卫的。但是，删除守卫在以下两个场景是有用的： 守卫是空的或者当前 level 的数据在守卫之间分布不均的时候 更少的守卫整合部分数据可以提高性能的时候 守卫的删除和插入是类似的，都是异步的，删除的守卫也被添加到一个内存的集合中，在下一个 compaction 周期的时候，sstable 被重新组织来适应删除守卫后的分布。删除 level i 的守卫 G 是在 compaction 时候 lazy 进行的。在 compaction 的时候，守卫 G 被删除然后对应的 SSTable 将被分去并追加到对应的同一层的相邻的守卫里或者追加到下一层的子守卫里。压缩过程照常进行，因为删除的是 level i 的守卫 G，但是在 level i+1 守卫 G 还是存在的。在压缩结束时，FLSM 持久化包含了删除 level i 层的守卫 G 的元数据信息。如果有需求的话，其他层的守卫也是会按照相似的逻辑去删除。但是需要注意的是，如果 level i 的守卫 G 删除了，那么 levels &lt; i 的守卫 G 都得被删除；FLSM 可以自己选择决定是否删除 levels &gt; i 的守卫。 FLSM Op Get 还是先检查内存里的数据结构，没找到再从磁盘上从 level 0 开始找，找到了肯定就立马返回，因为最靠近 DRAM 的 level 的数据肯定就是最新的。为了确定一个 Key 是否存在于当前 level，首先用一个二分查找找到对应的守卫，即该守卫可能包含该键，守卫找到了，那么对应的 SSTables 也就找到了，再去查对应的 SSTables。 所以最坏的情况是，每一层都去读了一个守卫，然后还读取了每个守卫里的所有 SSTables Range Query 范围查询要求收集给定范围的所有 Keys，首先识别出每一层和给定范围有交集的守卫，然后在每一个守卫里，可能很多个 SSTables 都会和给定范围有交集，那么 首先对每一个 SSTable 执行二分查找，以确定整个范围内最小的键，识别范围中下一个最小的键类似于归并排序中的归并过程;但是，不需要执行完整的排序。 当达到范围查询间隔的结束时，操作结束，将查询结果返回给用户。 对于 seek 和 next 操作，LSM 的通过合并多个 level 的迭代器来实现，而 FLSM 中的 level 迭代器本身是通过合并相应的的守卫内部sstable 上的迭代器来实现的 Put put 操作把数据添加到内存中的 memtable 中，满了之后执行 flush，然后再是 compaction，FLSM 其实就是通过分区 SSTable 并关联到下一个 Level 的守卫来实现 Compaction 从而避免了 SSTable 的重写。 Key Updates and Deletions 和 LSM 类似，更新和删除一个 Key 引入了把一个带有序列号或者删除标志的 key 插入存储的开销。查询操作将会忽略带有删除标识的 keys，如果一个 key 的插入导致了守卫的形成，key 的删除是不会导致守卫的删除的。删除一个守卫将引入大量的 compaction work，因此空的 guard 是可能的。 Compaction 当一个 guard 积累了一定阈值数量的 SSTables 时，将被压缩到下一层。在这个守卫里的 SSTables 首先被排序然后基于下一层的守卫进行分区，得到新的 SSTables，然后新的 SSTables 被关联到正确的守卫。例如，假设在 Level 1 的一个守卫包含 {1, 20, 45, 101, 245}，如果下一层有守卫 1、40、200，SSTable 将被分成三个 SSTable {1, 20}, {45, 101}, and {245} 然后被分别关联到三个守卫， 大多数场景下，FLSM 压缩不会重写 sstables，这是 FLSM 减少写放大的核心思想，新的 SSTable 只是很简单地被添加下一层的正确守卫里，但是有两个例外的情况： 在最高的 Level，比如 Level 5，该层的 SSTable 在 compaction 中必须被重写，因为没有下一层了。 对于第二高的 Level，比如 Level 4，如果在最高级别合并到一个大的 sstable，那么 FLSM 将重写一个sstable到相同的级别(因为如果守卫已满，我们无法在最后一级别附加新的sstable)。如果合并导致 25× 更多的IO，精确的启发式被重写在第二高级别 FLSM 的 Compaction 可以很简单地就实现并行，因为每次 Compaction 只会涉及到当前守卫和他的子守卫。守卫的选择方式保证了压缩一个守卫不会和同级其他守卫的压缩冲突。例如图 3 中 Level2 的守卫 375 被拆分成 Level3 的守卫 375 和 1023，只有这些守卫在压缩过程中会受到影响。压缩守卫 5，如果有数据的话，是不会影响到正在进行守卫 375 压缩。因此这些守卫压缩就可以同时进行，可以充分由 SSD 的一些内部并行性来充分处理并行 IO，从而减少 Compaction 的整体时间。压缩后的 KV 存储就会有更低的读延迟，因为并行的压缩可以让存储更快地进入到该状态，从而也提升了读吞吐。 Tuning FLSM 其实影响读写性能关键参数是：每个守卫中的 SSTables 数量。太多了，查询操作的延迟就很高，因此 FLSM 提供了一个 knob 从而可以调整行为，max_sstables_per_guard，它限制了在FLSM中每个守卫中存在的sstable的最大数量。当守卫中积累了这么多的 SSTables，守卫将被压缩到下一层。 调整该参数允许用户在更多写 IO 和低的读延迟之间做 trade-off，有意思的是，如果这个值被设置成了 1，FLSM 表现得就像 LSM 然后有差不多的读写性能，因此 FLSM 可以被看成 LSM 结构的一般化。 Limitations FLSM 显著减小了写放大而且压缩也更快（因为 FLSM 的压缩需要更少的读写 IO），由于更快的压缩，写吞吐量也会增加。然而，FLSM数据结构并非没有局限性 因为 get 和 range query 需要检索一个守卫里的所有 SSTables，这些操作的延迟相比于 LSM 就增加了。下面将会描述这些延迟如何克服，使用一些比较知名的技术可以减小甚至消除 FLSM 数据结构引入的开销，导致键值存储实现了低写放大、高写吞吐量和高读吞吐量的三连胜。 Asymptotic Analysis 使用一个理论模型来对 FLSM 操作进行分析。 Model 使用标准的磁盘访问模型 DAM，假设每一个读写操作都可以以一个单位的开销访问大小为 B 的块。为了分析的简便，我们假设 n 个数据被存储在了其中。 Asymptotic Analysis 假设 FLSM 中 guard 选择的概率是 1/B，所以在 level i+1 的守卫的数量是 level i 的 B 倍，因为 FLSM 对应的 fanout 是 B，那么很大概率 n 个数据将有 H=logBnH = log_BnH=logB​n 层，每个数据项在每层只有一个，因为数据只会被追加一次而且从来不会在同一层进行重写。导致写开销 O(H)=O(logBn)O(H) = O(log_Bn)O(H)=O(logB​n)。因为 DAM model，FLSM 写了一个包含 B 个项的数据块作为单位开销，那么任何 put 操作的开销均摊之后为 O(H/B)=O(logBn)/BO(H/B) = O(log_Bn)/BO(H/B)=O(logB​n)/B，超过了整个 compaction 的生命周期，然而，最后一层的 compaction 必须重写数据。所以最后一层的重写会比较大概率地为 O(B)O(B)O(B) 倍，然后最终的总的分摊开销为 O((B+logBn)/B)O((B + log_Bn)/B)O((B+logB​n)/B) FLSM 的守卫引入了一个深度为 B 的跳表，关于 B 跳表的具体理论分析表明每个守卫将会有 O(B)O(B)O(B) 个孩子，每个守卫将有最多 O(B)O(B)O(B) 个 SSTables，每个 SSTable 将最多会有 O(B)O(B)O(B) 个数据项。查询对应的项首先要求找到正确的守卫，然后在守卫内部找到对应的 SSTable，因为最后一层有最多的守卫 BHB^HBH，二分查找开销将被最后一层的开销给统治:O(log2BH)=O(Hlog2B)=O(logBn∗log2B)=O(log2n)O(log_2B^H) = O(Hlog_2B) = O(log_Bn*log_2B) = O(log_2n)O(log2​BH)=O(Hlog2​B)=O(logB​n∗log2​B)=O(log2​n)，因为有 O(H)=O(logBn)O(H) = O(log_Bn)O(H)=O(logB​n) 层需要搜索，这就产生了总的开销为 O(log2nlogBn)O(log_2n log_Bn)O(log2​nlogB​n) 的内存操作来找到每一层的相应的守卫。 然而，在 FLSM 中，守卫和布隆过滤器都是存储在内存中的，FLSM 在二分查找找到每一层对应的守卫时执行 O(log2nlogBn)O(log_2n log_Bn)O(log2​nlogB​n) 内存操作。然后对于每个守卫找到之后，执行一次布隆过滤器查询，在每个守卫对应的 O(B)O(B)O(B) 个 SSTable 上执行，每次查询消耗 O(log(1/ϵ))O(log(1/ϵ))O(log(1/ϵ)) 在内存操作中。在 DAM 模型中所有的内存操作没有开销。 最后，平均下来，布隆过滤器将指示 1 + O(1) 个 SSTables 要读（很大概率），读取这些 SSTables 在 DAM model 中花费 1 + O(1)。因此，一个 get 操作总的读开销只有 O(1)（假设内存足够装下守卫和布隆过滤器）。 FLSM 不能利用布隆过滤器来优化范围查询，每一层的二分查找仍然是在内存中完成的。对于每一层，二分查找输出一个守卫然后 FLSM 需要读取 O(B) 个关联的 SSTables，所以对应一个范围查询（查询 K 个元素）的总开销是 O(BlogBn+k/B)O(Blog_Bn + k/B)O(BlogB​n+k/B) BUILDING PEBBLESDB OVER FLSM Improving Read Performance Overhead Cause FLSM 中的 get() 操作将检查每个级别中一个守卫的所有 sstable。相反，在日志结构的合并树中，每个级别只需要检查一个sstable。因此，读操作会在基于 FLSM 的键值存储中产生额外的开销。 Sstable Bloom Filters Bloom flter 是一种节省空间的概率数据结构，用于测试一个元素是否在常量时间内出现在给定的集合中。bloom flter可以产生假阳性，但不会产生假阴性。PebblesDb 在每个sstable上附加一个bloom flter，以便有效地检测给定的键是否存在于sstable中。SSTable 的布隆过滤器允许 PebblesDB 避免从存储中读取不必要的sstable，极大地减少了由于FLSM数据结构而产生的读取开销。 RocksDB 也采用了sstable级别的bloom flters。许多key-value store(包括RocksDB和LevelDB)为sstable的每个块使用bloom flters。如果使用sstable级别的bloom filters，则不需要块级别过滤器。 Improving Range Query Performance Overhead Cause 类似于 get()操作，范围查询(使用seek()和next()调用实现)还需要检查FLSM守卫的所有sstable。因为LSM存储在每个级别只检查一个sstable，所以FLSM存储在范围查询上有很大的开销 Seek-Based Compaction 与LevelDB类似，PebblesDb实现由连续seek()操作的阈值次数触发的压缩(默认值:10)。一个守卫中的多个sstable被合并并写入到下一级守卫中。目标是减少一个守卫内sstable的平均数量。PebblesDb也积极地压缩 levels:如果 level i 的大小在 level i+1 的某个阈值比例(默认为25%)内，那么 level i 就被压缩为 level i+1。这种积极的压缩减少了需要搜索seek() 的活动 level 的数量。尽管这样的压缩增加了写 IO，但是 PebblesDb 总体上仍然显著降低了 IO 量 Parallel Seeks PebblesDb采用的一种独特的优化方法是使用多个线程并行地搜索sstable以获取seek()。每个线程从存储中读取一个sstable，并对其进行二分搜索。然后将二元搜索的结果合并，并为seek()操作正确定位迭代器。由于这种优化，即使一个守卫包含多个sstable，与LSM seek()延迟相比，FLSM seek()延迟只会带来很小的开销。 不能轻率地使用并行查找:如果正在检查的sstable被缓存了，那么使用多个线程的开销要高于并行查找的好处。由于无法知道给定的sstable是否被缓存(因为操作系统可能会在内存压力下删除缓存的sstable)，Pebbesdb使用了一个简单的启发式方法:并行查找只在键值存储的最后一层使用。选择这一启发式的原因是，最后一层包含的数据量最大;此外，最后一个级别的数据不是最近的，因此不太可能被缓存。这个简单的启发式方法在实践中似乎很有效。 PebblesDb Operations 本节简要描述在PebblesDb中如何实现各种操作，以及它们与在FLSM数据结构上执行相同操作的区别。PebblesDb中的put()操作的处理方式与FLSM中的put操作类似。 Get 使用二分查找定位每一层合适的守卫，并在守卫内检索 SSTables。PebblesDB 和 FLSM 在 get 操作不同的地方是利用了 SSTable 级的 BloomFilters 来避免从存储上读取不必要的 SSTables。 Range Query PebblesDb处理范围查询的方法是在每个级别中定位适当的守卫，并通过对sstable执行二分查找将迭代器放置在守卫中每个 sstable 的正确位置。PebblesDb 通过并行读取和搜索sstable 来优化这一点，并在接收到连续的 seek() 请求达到阈值时主动压缩 level Deleting Keys PebblesDb 通过将键插入到存储中并标记为已删除的标记来删除键。插入的键的序列号将其标识为键的最新版本，指示PebblesDb丢弃键的以前版本，以便进行读和范围查询操作。注意，bloom flter是在sstable上创建的;由于sstable永远不会就地更新，现有的bloom flter在删除键时不需要修改。标记为删除的键在压缩期间被垃圾收集。 Crash Recovery 通过仅附加数据而不覆盖任何数据，peblesdb构建在与LSM相同的基础上，以提供强大的崩溃一致性保证。PebblesDb构建于LevelDB代码库之上，LevelDB已经为这两个数据提供了经过良好测试的崩溃恢复机制(数据，sstable；元数据 MANIFEST file)。PebblesDb只是添加更多的元数据(守卫信息)，以便在MANIFEST file中持久化。PebblesDb sstable和LevelDB sstable使用相同的格式。崩溃恢复测试(测试在随机选取的点崩溃后恢复的数据)确保在崩溃后PebblesDb正确恢复插入的数据和相关的guard相关元数据。 Implementation PebblesDb 是作为键值存储的LevelDB家族的一个变体实现的。PebblesDb是通过修改HyperLevelDB 来构建的，这是LevelDB的一个变种，在压缩期间改进了并行性和更好的写吞吐量。我们简要地检查了RocksDB代码库，但发现HyperLevelDB代码库更小，文档更完善(源自LevelDB)，更容易理解。因此，HyperLevelDB被选择为Pebblesdb的基础 我们添加和修改了大约 9100 行代码，大部分的工作都是引入守卫并修改 compaction 操作，因为守卫是建立在sstable之上的，所以PebblesDb能够利用成熟的、经过良好测试的处理sstable的代码。PebblesDb与HyperLevelDB的api兼容，因为所有的更改都在键值存储内部。 Selecting Guards 与跳过列表类似，Pebblesdb从插入的键中随机挑选守卫。当一个键被插入时，一个随机数被选择来决定这个键是否是一个守卫。然而，为每次键插入获取一个随机数在计算上是昂贵的;相反，PebblesDb对每个传入的键进行散列，散列的最后几位确定键是否为守卫(以及在哪个级别)。 计算上开销较小的 MurmurHash 算法用于散列每个插入的键。可配置参数 top_level_bits 决定了最少连续多少个应该设置散列 Key 的位表示中的有效位(LSBs)，以便选择该密钥作为第一级 Level 1 的守卫密钥。另一个参数 bit_decemre 决定每增加一级约束(要设置的 lsb 数量)被放宽的比特数。 举个例子，top_level_bits = 17，bit_decrement = 2，level 1 守卫的 key 应该有 17 个连续的在其哈希值中设置的 LSBs，第 2 级的守卫键应该在其哈希值中设置 15 个连续的 LSBs，以此类推。top_level_bits 和 bit_increment 参数需要根据经验确定;根据我们的经验，bit_increment 值为 2 似乎是合理的，但是如果用户希望在 Pebblesdb 中插入超过 1 亿个键，则 top_level_bits 可能需要从默认值 27 增加。过高估计存储中的键数量是无害的(导致许多空的守卫);低估的话可能会导致守卫倾斜 Implementing Guards 每个守卫存储关于它所拥有的sstable数量的元数据，sstable中存在的最大和最小的键，以及sstable列表。每个sstable由一个唯一的64位整数表示。守卫与关于键值存储中的sstable的元数据一起被持久化到存储中。在崩溃后从 MANIFEST 和异步 WAL 日志中恢复守卫。守卫数据的恢复被编织到 Keys 和 SSTable 信息的键值存储恢复中。我们还没有在 PebblesDB 中实现删除守卫，因为在我们的实验中，额外的守卫并没有导致读取的显著性能下降，而且保持空守卫的成本相对来说是微不足道的。我们计划在不久的将来实现守卫删除 Multi-threaded Compaction 与RocksDB类似，PebblesDb使用多个线程进行后台压缩。每个线程选择一个级别并将其压缩到下一个级别。根据每个级别的数据量选择要压缩的级别。当一个级别被压缩时，只有包含超过阈值数量sstable的守卫被压缩。我们还没有在 PebblesDB 中实现基于守卫的并行压缩;即使没有并行压缩，PebblesDB 中的压缩也比基于 LSM 的存储(如RocksDB)中的压缩快得多。 Limitations 这一节描述了传统的基于 LSM 的存储可能比 PebblesDB 更好的选择。 首先，如果工作负载数据完全可以全部放在内存中，PebblesDb 具有更高的读取和范围查询延迟。在这样的场景中，读或范围查询请求将不涉及存储 IO，并且在一个守卫中定位正确的守卫和处理sstable 的计算开销将导致更高的延迟。考虑到每天生成和处理的数据量不断增加，大多数数据集不会都在内存中。在数据量很小的少数情况下，将 max_sstables_per_guard 设置为 1，可使PebblesDb 的行为类似于 HyperLevelDB，从而减少读取和范围查询的延迟开销。 其次，对于将具有顺序键的数据插入键值存储的工作负载，PebblesDB 的写 IO 要高于基于 LSM 的键值存储。如果数据是按顺序插入的，sstable 不会相互重叠。基于 LSM 的存储可以有效地处理这种情况，只需将 SSTable 从一个级别移动到下一个级别，只修改元数据(而不执行写IO);在 PebblesDb 的情况下，SSTable 在移动到下一个级别时可能会被分区，导致写IO。我们相信现实世界中按顺序插入数据的工作负载很少，因为大多数工作负载是多线程的;在这种罕见的情况下，我们提倡使用基于LSM 的存储，如RocksDB。 第三，如果工作负载涉及到初始的写操作突发，然后是大量的小范围查询，那么 PebblesDB 可能不是最好的。对于这种在紧凑的键值存储上的范围查询，与基于 LSM 的存储相比，PebblesDB 的开销很大(30%)。但是，当范围查询变得更大时，开销就会降低，如果范围查询中穿插了插入或更新，开销就会完全消失（如 YCSB-E）。 Evaluation ","link":"https://blog.shunzi.tech/post/PebblesDB/"},{"title":"Linux Software Management","content":" Linux 软件包管理安装。 参考书目/链接 Linux 就该这么学：Linux软件包管理基本操作入门 segmentfault: Linux系统中软件的“四”种安装原理详解 博客园：Linux下rpm、yum和源码三种安装方式简介 IBM Developer - 构建和分发包 软件包 Windows 开始介绍 Linux 软件包之前，先大致回忆一下 Windows 场景下的我们常常使用的软件包形态。 Win 环境下： 大量的软件应用程序都是使用诸如 .exe/.msi 等类型的文件进行安装，直接运行该安装程序，按照步骤勾选相应的可选配置就能安装对应的软件。 MSI 就是 microsoft installer 的简写，msi 文件就是 window installer 的数据包，把所有和安装文件相关的内容封装在一个包里。 exe 是一个安装引导程序。主要是用于检查安装的环境，当检查成功后，会自动再安装 msi 文件。 除了安装程序以外，常常还有一些压缩包的形式来安装软件（也就是所谓的绿色版），本质就是把程序的全部数据以及依赖的环境给压缩到一个文件夹中，而下载该文件夹就可以直接执行其中保存的可执行程序。 Linux 大多数现代类 Unix 操作系统都提供了一个集中的软件包管理机制，以帮助用户搜索、安装和管理软件。而软件通常以「包」的形式存储在仓库「repository」中，对软件包的使用和管理被称为包管理。 Linux 包的基本组成部分通常有：共享库、应用程序、服务和文档。 Linux 软件包分类: 源码包 二进制包（RPM/DEB包） yum/apt 源在线安装 脚本安装包（本质还是源码包和二进制包） Linux 软件包管理 软件包管理系统 大多数包管理系统是建立在包文件上的集合，包文件通常包含编译好的二进制文件和其它资源组成的：软件、安装脚本、元数据及其所需的依赖列表。 因为 Linux 有很多发行版，各自的管理系统也有一定的差异 系统 格式 工具 Debian .deb apt, apt-cache、apt-get、dpkg Ubuntu .deb apt、apt-cache、apt-get、dpkg CentOS .rpm yum Fedora .rpm dnf Debian 及其衍生产品如：Ubuntu、Linux Mint 和 Raspbian 的包格式为.deb文件，APT 是最常见包操作命令，可：搜索库、安装包及其依赖和管理升级。而要直接安装现成.deb包时需要使用dpkg命令。 CentOS、Fedora 及 Red Hat 系列 Linux 使用RPM包文件，并使用yum命令管理包文件及与软件库交互。在最新的 Fedora 版本中，yum命令已被dnf取代进行包管理。 常见操作 更新本地包数据库列表： apt-get update yum check-update dnf check-update 升级已安装的包： apt-get upgrade apt-get dist-upgrade yum update /dnf upgrade 查找/搜索软件包： apt-cache search xxx yum search xxx yum search all xxx dnf search xxx dnf search all xxx 查看某个软件包信息: apt-cache show [pkg_name] dpkg -s [pkg_name] (显示包的安装状态) yum info [pkg_name] yum deplist [pkg_name] （列出包的依赖） dnf info [pkg_name] dnf repoquery –requires [pkg_name] 安装包： apt-get install xxx yum instal xxx dnf install xxx 移除包： apt-get remove xxx apt-get autoremove (自动移除已知不需要的包) yum remove xxx dnf erase xxx 本地文件系统安装 首先简要介绍本地安装软件包的场景和主要命令，然后以 RPM 为例详细说明。 从本地文件系统直接安装包 很多时候，我们在进行测试或从某个地方直接拿到软件包之后需要从本地文件系统直接安装包。（特别是针对一些无公网的环境，无法直接使用包管理工具直接安装相关依赖） Debian 及衍生系统可以使用 dpkg 进行安装，CentOS 和 Fedora 系统使用 yum 和 dnf 命令进行安装。 本地安装命令 dpkg -i [pkg_name].deb apt-get install -y gdebi&amp;&amp; sudo gdebi [pkg_name].deb （使用gdebi检索缺少的依赖关系） yum install [pkg_name].rpm dnf install [pkg_name].rpm RPM RPM命名“RedHat Package Manager”，简称则为RPM。这个机制最早由Red Hat这家公司开发出来的，后来实在很好用，因此很多distributons就使用这个机制来作为软件安装的管理方式，包括Fedora，CentOS，SuSE等知名的开发商都是用它。 RPM最大的特点就是需要安装的软件已经编译过，并已经打包成RPM机制的安装包，通过里头默认的数据库记录这个软件安装时需要的依赖软件。当安装在你的Linux主机时，RPM会先依照软件里头的数据查询Linux主机的依赖属性软件是否满足，若满足则予以安装，若不满足则不予安装。 构建流程 要构建 RPM，必须： 依照 rpmbuild 规范设定一个目录结构。 将源代码和附带文件放在目录中合适的位置。 创建 spec 文件。 编译 RPM。可以选择编译源 RPM，以与其他人共享您的源代码。 目录结构 BUILD。BUILD 用作实际编译软件的暂存空间。 RPMS。RPMS 包含 rpmbuild 所编译的二进制 RPM。 SOURCES。SOURCES 存储源代码。 SPECS。SPECS 包含您的 spec 文件，您想要构建的一个 RPM 对应一个 spec 文件。 SRPMS。SRPMS 包含在这个过程中构建的源 RPM。 准备文件 step1. 将源代码（理想情况下应捆绑为一个 tarball 压缩文件）复制到 SOURCES 目录 step2. 创建 spec 文件。spec 文件只是一个具有特殊语法的文本文件。 此处举一个简单例子 wget 打包 # This is a sample spec file for wget # 定义了五个变量 %define _topdir /home/strike/mywget %define name wget %define release 1 %define version 1.12 %define buildroot %{_topdir}/%{name}-%{version}-root # 设置若干关键参数 BuildRoot: %{buildroot} Summary: GNU wget License: GPL Name: %{name} Version: %{version} Release: %{release} Source: %{name}-%{version}.tar.gz Prefix: /usr Group: Development/Tools # 简单明了地描述软件。这一行将在用户运行 rpm -qi 来查询 RPM 数据库时显示。您可以说明包的用途，描述任何警告或额外的配置说明等。 %description The GNU wget program downloads files from the Internet using the command-line. # 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行 # 准备源代码 # %setup -q 是一个 %prep 宏，用于自动解压 Source 中的特定 tarball 压缩文件 %prep %setup -q # 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行 # 手动配置和启动构建过程的步骤 %build ./configure make # 生成一个 shell 脚本，该脚本嵌入到 RPM 中，随后作为安装的一部分运行 %install make install prefix=$RPM_BUILD_ROOT/usr # 列出应该捆绑到 RPM 中的文件，还可以设置权限和其他信息。 %files %defattr(-,root,root) /usr/local/bin/wget # %doc 告诉 RPM 该文件为一个文档文件，所以如果用户使用 --excludedocs 安装包，将不会安装该文件。 %doc %attr(0444,root,root) /usr/local/share/man/man1/wget.1 再来看 tcmu-runner.spec 的例子 %global _hardened_build 1 # 运行 rpmbuild 时读取的一些条件参数 # without rbd dependency # if you wish to exclude rbd handlers in RPM, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without rbd %bcond_without rbd # without glusterfs dependency # if you wish to exclude glfs handlers in RPM, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without glfs %bcond_without glfs # without qcow dependency # if you wish to exclude qcow handlers in RPM, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without qcow %bcond_without qcow # without zbc dependency # if you wish to exclude zbc handlers in RPM, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without zbc %bcond_without zbc # without file backed optical dependency # if you wish to exclude fbo handlers in RPM, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without fbo %bcond_without fbo # without tcmalloc dependency # if you wish to exclude tcmalloc, use below command # rpmbuild -ta @PACKAGE_NAME@-@PACKAGE_VERSION@.tar.gz --without tcmalloc %bcond_without tcmalloc # 设置若干关键参数 Name: tcmu-runner Summary: A daemon that handles the userspace side of the LIO TCM-User backstore Group: System Environment/Daemons License: ASL 2.0 or LGPLv2+ Version: 1.0 URL: https://github.com/open-iscsi/tcmu-runner Release: 0%{dist} BuildRoot: %(mktemp -udp %{_tmppath}/%{name}-%{version}) Source: %{name}-%{version}.tar.gz ExclusiveOS: Linux BuildRequires: cmake make gcc BuildRequires: libnl3-devel glib2-devel zlib-devel kmod-devel # 针对不同条件参数下的依赖处理 %if %{with rbd} BuildRequires: librbd1-devel librados2-devel Requires(pre): librados2, librbd1 %endif %if %{with glfs} BuildRequires: glusterfs-api-devel Requires(pre): glusterfs-api %endif %if %{with tcmalloc} BuildRequires: gperftools-devel Requires: gperftools-libs %endif # 基本依赖 Requires(pre): kmod, zlib, libnl3, glib2, logrotate, rsyslog Requires: libtcmu = %{version}-%{release} # 软件描述 %description A daemon that handles the userspace side of the LIO TCM-User backstore. LIO is the SCSI target in the Linux kernel. It is entirely kernel code, and allows exported SCSI logical units (LUNs) to be backed by regular files or block devices. But, if we want to get fancier with the capabilities of the device we're emulating, the kernel is not necessarily the right place. While there are userspace libraries for compression, encryption, and clustered storage solutions like Ceph or Gluster, these are not accessible from the kernel. The TCMU userspace-passthrough backstore allows a userspace process to handle requests to a LUN. But since the kernel-user interface that TCMU provides must be fast and flexible, it is complex enough that we'd like to avoid each userspace handler having to write boilerplate code. tcmu-runner handles the messy details of the TCMU interface -- UIO, netlink, pthreads, and DBus -- and exports a more friendly C plugin module API. Modules using this API are called &quot;TCMU handlers&quot;. Handler authors can write code just to handle the SCSI commands as desired, and can also link with whatever userspace libraries they like. # 生成的 RPM 包 libtcmu %package -n libtcmu Summary: A library supporting LIO TCM-User backstores processing Group: Development/Libraries %description -n libtcmu libtcmu provides a library for processing SCSI commands exposed by the LIO kernel target's TCM-User backend. # 生成的 RPM 包 libtcmu-devel %package -n libtcmu-devel Summary: Development headers for libtcmu Group: Development/Libraries Requires: %{name} = %{version}-%{release} Requires: libtcmu = %{version}-%{release} %description -n libtcmu-devel Development header(s) for developing against libtcmu. %global debug_package %{nil} # 准备源代码 %prep %setup -n %{name}-%{version} # 手动配置和启动构建 %build %{__cmake} \\ -DSUPPORT_SYSTEMD=ON -DCMAKE_INSTALL_PREFIX=%{_usr} \\ %{?_without_rbd:-Dwith-rbd=false} \\ %{?_without_zbc:-Dwith-zbc=false} \\ %{?_without_qcow:-Dwith-qcow=false} \\ %{?_without_glfs:-Dwith-glfs=false} \\ %{?_without_fbo:-Dwith-fbo=false} \\ %{?_without_tcmalloc:-Dwith-tcmalloc=false} \\ . %{__make} %install %{__make} DESTDIR=%{buildroot} install %{__rm} -f %{buildroot}/etc/tcmu/tcmu.conf.old %{__rm} -f %{buildroot}/etc/logrotate.d/tcmu-runner.bak/tcmu-runner # 列出应该捆绑到 RPM 中的文件 %files %{_bindir}/tcmu-runner %dir %{_sysconfdir}/dbus-1/ %dir %{_sysconfdir}/dbus-1/system.d %config %{_sysconfdir}/dbus-1/system.d/tcmu-runner.conf %dir %{_datadir}/dbus-1/ %dir %{_datadir}/dbus-1/system-services/ %{_datadir}/dbus-1/system-services/org.kernel.TCMUService1.service %{_unitdir}/tcmu-runner.service %dir %{_libdir}/tcmu-runner/ %{_libdir}/tcmu-runner/*.so %{_mandir}/man8/* %doc README.md LICENSE.LGPLv2.1 LICENSE.Apache2 %dir %{_sysconfdir}/tcmu/ %config %{_sysconfdir}/tcmu/tcmu.conf %config(noreplace) %{_sysconfdir}/logrotate.d/tcmu-runner %ghost %attr(0644,-,-) %{_sysconfdir}/tcmu/tcmu.conf.old %ghost %attr(0644,-,-) %{_sysconfdir}/logrotate.d/tcmu-runner.bak/tcmu-runner %files -n libtcmu %{_libdir}/libtcmu*.so.* %files -n libtcmu-devel %{_libdir}/libtcmu*.so 构建 RPM 例如构建 wget: rpmbuild -v -bb --clean SPECS/wget.spec 此命令使用指定的 spec 文件构建一个二进制包（-bb 表示 “构建二进制包”），还会生成详细的输出（-v）。构建实用程序在生成包之后删除构建树（--clean）。如果还希望构建源 RPM，指定 -ba（“构建所有包”）来代替 -bb。（查看 rpmbuild 清单页面，了解完整的选项列表。） rpmbuild 执行以下步骤： 读取并解析 wget.spec 文件。 运行 %prep 节，将源代码解压到临时目录。在这里，临时目录为 BUILD。 运行 %build 节，编译代码。 运行 %install 节，将代码安装到构建机器上的目录中。 从 %files 节读取文件列表，将它们收集到一起，然后创建一个二进制 RPM（和源 RPM 文件，如果已选择）。 安装流程 rpm -ivh package-name -i：install的意思，安装 -v：查看更详细的安装信息画面（provide more detailed output） -h：以安装信息栏显示安装进度 错误处理 安装rpm包时提示错误：依赖检测失败: --nodeps --force [root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm 错误：依赖检测失败： libhcs_obj_util.so()(64bit) 被 tcmu-runner-2.0.1-0.el7.x86_64 需要 [root@localhost ~]# rpm -ivh tcmu-runner-2.0.1-0.el7.x86_64.rpm --nodeps --force 准备中... ################################# [100%] 正在升级/安装... 1:tcmu-runner-2.0.1-0.el7 ################################# [100%] [root@localhost ~]# systemctl status tcmu-runner ● tcmu-runner.service - LIO Userspace-passthrough daemon Loaded: loaded (/usr/lib/systemd/system/tcmu-runner.service; disabled; vendor preset: disabled) Active: inactive (dead) Docs: man:tcmu-runner(8) ","link":"https://blog.shunzi.tech/post/linux-software-mangement/"},{"title":"Reading Group Notes","content":" 参加 Systems Reading Group 记录的论文笔记。 System Reading Group Group 介绍以及 Presentation 安排： https://learn-sys.github.io/cn/reading/ THU AOS 2020: http://os.cs.tsinghua.edu.cn/oscourse/AOS2020 Week 1: Operating System Course Notes Video: THU AOS P7 - P11 OS Architecture &amp; Structure OS Structure: Simple kernel Monolithic kernel Micro kernel Exokernel VMM(Virtual Machine Monitor), etc... Monolithic kernel UNIX Arch Linux Arch Micro kernel 微内核：功能相对较少的内核，只提供某些核心功能。从而相比于单体内核，把很多单体内核中的事情放到用户空间去做，解耦了内核的各个 features，让整个系统的稳定性和灵活性得到了提升。但也就因为 IPC 的开销导致性能表现不尽如人意。 Kernel with minimal features Moves as much from the kernel into user space Address spaces Interprocess communication (IPC) Scheduling Benefits Flexibility Safety Modularity Detriments (Poor Performance) Address spaces Interprocess communication (IPC) Scheduling Mach L4 - Microkernel– L4Second generation microkernel synchronous IPCs –&gt; async IPCs (like epoll in Linux) smaller, Mach 3(330 KB) –&gt; L4 (12KB) IPC security checks moved to user process IPC is hardware dependent Exokernel Exokernel 要做的事情其实是把内核也近乎给 PASS 掉，尽可能减少抽象层次，允许应用程序直接访问硬件，而ExoKernel只负责保护和分配系统资源。说白了就是把硬件资源都直接交给应用程序自己来组织了，因为有大量的应用程序想要自己独立控制可管理硬件，而不需要你操作系统层面的过多干涉。 Paper 1: The Multikernel: A new OS architecture for scalable multicore systems SOSP09 SIGOPS Hall of Fame Award 2020 Abstract 普通计算机系统包含越来越多的处理器核心，并呈现出越来越多的架构权衡，包括内存层次结构、互连、指令集和变体，以及IO配置。 以前的高性能计算系统在特定情况下进行了扩展，但是现代客户机和服务器工作负载的动态特性，加上不可能针对所有工作负载和硬件变体静态地优化操作系统，对操作系统结构构成了严重的挑战。 我们认为，迎接未来多核硬件挑战的最好方法是拥抱机器的网络化本质，重新思考使用来自分布式系统的思想的操作系统架构。我们研究了一种新的操作系统结构，即 Multikernel，它将机器视为一个由独立核心组成的网络，假定在最低层次上没有核间共享，并将传统的操作系统功能转移到一个通过消息传递进行通信的分布式进程系统。 我们已经实现了一个多内核操作系统来证明这种方法是有前途的，并且我们描述了操作系统的传统的可伸缩性问题(如内存管理)是如何通过消息有效地重新解决的，以及如何利用分布式系统和网络的洞察力。在多核系统上对我们的原型的评估表明，即使在现在的机器上，多内核的性能也可以与传统的相媲美，并且可以更好地扩展以支持未来的硬件。 Problems 随着不断变化的技术对摩尔定律的限制，处理器架构变得越来越多样化，且逐渐转向异构化，并向可扩展的架构发展，以适应高性能的应用。传统的单体操作系统在解决可伸缩性问题和针对不同硬件结构进行优化方面面临着巨大的挑战。 Systems are increasingly diverse Cores are increasingly diverse The interconnect matters Messages cost less than shared memory Cache coherence is not a panacea Messages are getting easier 本文作者尝试通过在内核之间使用显式消息传递和在内核之间复制内核状态来解决这个问题，而不是使用共享内存模型。他们的另一个主要目标是使这个操作系统与硬件无关，不针对任何机器架构。 Contributions 多内核操作系统的主要贡献嵌入在它们的三个设计原则中： 通过消息传递显式地实现内核间通信 使操作系统结构与硬件无关 在内核之间复制内核状态 该系统侧重于非共享内存模型，通过消息的显式通信来维护缓存的一致性。基于以上原则，设计实现了 MultiKernel 原型 Barrelfish Week 2: Virtualization Course Notes Video THU AOS P12 - P21 IPADS MOS P70 - P87 ","link":"https://blog.shunzi.tech/post/ReadingGroup/"},{"title":"MapReduce: Simplified Data Processing on Large Clusters","content":" 该篇文章来自于 OSDI2004，Google 当年率先提出的 MapReduce 框架，开启了分布式和大数据的纪元。 Before Beginning 为什么要读这篇论文呢？其实这篇文章之前也已经简单看过了，只是最近开始刷 MIT6.824，本来是想直接做相关 Lab 的，但是发现还是有整理不清楚的思路，觉得还是有必要回顾一下，那就多花点时间继续研读吧~ 网上关于 6.824 以及 MapReduce 的资料很多了，我在这里只是做一些简单的记录，如果有发现其他大佬做的比较好的笔记，也会贴在这里，以供膜拜学习。我的 6.824 系列的的博客介绍大抵都是如此。 话不多说，学习开始~ Abstract 大致流程: 用户指定一个 map 函数来处理键/值对以生成一组中间键/值对，以及一个 reduce 函数来合并与同一中间键相关的所有中间值 为什么这么做？ 是想充分利用不同机器的并行性来处理大量的数据，分别执行 map 和 reduce 任务来完成大数据任务，提高每个 host 的利用率。（也就是分布式系统的原型） 需要解决的问题： 数据输入的切分 不同机器上执行的任务的调度 机器故障处理 机器间通信的管理 Introduction 根本矛盾：少数据量时单机能够直接运行简单的任务来完成相关计算，但面对大数据量的情况下，引入多计算实例组成的系统的复杂性和本身计算任务的简单性之间的矛盾。 思想起源：来自于 Lisp 语言的函数式编程思想中的 map/reduce 函数。 在 lisp 语言中，map 作为一个输入函数接受一个序列，然后处理每个序列中 value 值，然后 reduce 将最终的 map 计算出来的结果整理成最终程序输出。 Programming Model MapReduce 本质是一种编程模型 Map: 由用户编写，接受一个输入对并生成一组中间键/值对 // map (k1,v1) → list(k2,v2) map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;); Reduce: 也由用户编写，接受一个中间键和该键的一组值。它将这些值合并在一起，形成一个可能更小的值集 // reduce (k2,list(v2)) → list(v2) reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); 应用实例： Distributed Grep Count of URL Access Frequency：map &lt;URL, 1&gt;, reduce &lt;URL, total count&gt; Reverse Web-Link Graph: map &lt;target, source&gt;, reduce &lt;target, list(source)&gt; Term-Vector per Host Inverted Index: map &lt;word, document ID&gt;, reduce &lt;word, list(document ID)&gt; Distributed Sort Implementation Map 函数分布在多个机器上，相应地自动将输入数据划分为 M 份，然后可以由分布了 Map 函数的机器并行处理这些数据。而对于 Reduce 则是将中间数据划分为 R 份，通常需要使用一个分割函数，常见的就是 hash(key) mod R 来将中间 Key 进行区分。 下图演示了整个 MapReduce 的流程，当用户程序调用 MapReduce 函数时将按照以下顺序执行： MapReduce Library 首先将输入文件划分为 M 个分片，每个分片大小通常为 16MB or 64MB，可以由用户控制，然后开始将程序拷贝到各个机器上，也就是图中的 fork 过程 fork 的过程中会有一个特殊的情况，即 master 节点上运行的程序。剩下的 worker 对应执行的任务都是由 master 分配的，有 M 个 map task 和 R 个 reduce task 需要分配，master 选择空闲的 worker 来执行 map 或者 reduce task。 被分配到 map task 的 worker 首先读取分片的数据内容，它从输入数据中解析键/值对，并将每对键/值传递给用户定义的 Map 函数，然后由 Map 产生的中间键值对将被缓冲在内存中； 缓冲在内存中的中间数据将定期执行刷回操作写到磁盘，然后再由用户定义的分割函数执行将中间数据分割为 R 个区域，这些原本缓冲在内存中的数据持久化到磁盘之后的地址将传递给 master，然后 master 负责告诉 reduce task worker 这些数据在哪里。 执行 reduce task 的 worker 在接收到来自 master 的数据地址的通知之后，使用 RPC 来从 map worker 的本地磁盘中读取数据，当一个 reducer 读取到了所有的中间数据之后，就可以根据中间键对它进行排序，以便将所有出现的相同键组合在一起。之所以需要排序，是因为通常有许多不同的键映射到同一个reduce任务。如果中间数据量太大，无法装入内存，则使用外部排序。 reduce worker 迭代已排序的中间数据，对于遇到的每个惟一的中间键，它将键和相应的中间值集传递给用户的 reduce 函数。Reduce 函数的输出被追加到这个 Reduce 分区的最终输出文件中； 所有的 map 任务和 reduce 任务都完成后，master 唤醒用户程序。此时，用户程序中的MapReduce 调用返回到用户程序。 Master Data Structures master 节点保存了几个数据结构： 对于每个 map 和 reduce task，它存储了 task 对应的状态（idle, in-progress, completed） worker machine 的标识（非空闲任务运行所在的机器） master 是一个管道，通过它将中间文件区域的位置从 map task 传播到 reduce task。因此，对于每个完成的 map task，master 存储了 map task 产生的 R 个中间文件区域的位置和大小，当 map task 完成时，master 将接收对该位置和大小信息的更新。信息被递增地推送给正在进行 reduce task 的 worker。 Fault Tolerance 由于 MapReduce 库被设计用来帮助处理使用成百上千台机器的大量数据，所以这个库必须能够优雅地容忍机器故障。 Worker Failure master 周期地 ping 每个 worker，如果在确定时间内未收到对应的响应，则认为该 worker 宕机，标记该 worker 为 failed，由 worker 已经完成的任何 map task 都将被重置回初始的空闲 idle 状态，因此有资格对其他 worker 进行重新调度，类似地，在一个失败的 worker 上正在进行的任何 map task 或reduce task 也会被重置为空闲，并可以重新调度。 在发生故障时，完成的 map task 将被重新执行，因为它们的输出存储在故障机器的本地磁盘上，因此无法访问。已完成的 reduce 任务不需要重新执行，因为它们的输出存储在全局文件系统中。 假设一个 map task A 一开始由 A 执行，之后由 B 执行（因为 A failed），所有正在执行 reduce task 的 workers 将被通知重新执行，任何尚未从 worker A 读取数据的 reduce task 都将从 worker B 读取数据。 Master Failure 让 master 定期对上面描述的 master 节点存取的数据结构做 checkpoint 很容易。如果 master task 失效，可以从最后一个检查点状态启动一个新的副本。然而，考虑到只有一个主机，它的失败是不太多见，因此，如果 master 失败，我们当前的实现将终止 MapReduce 计算。客户端可以检查这种情况，如果他们愿意，可以重试 MapReduce 操作。 Semantics in the Presence of Failures 当用户提供的 map 和 reduce 操作符是其输入值的确定性函数时，我们的分布式实现产生的输出与整个程序的非故障顺序执行产生的输出相同。 我们依赖 map 和 reduce task 输出的原子提交来实现该属性。每个正在进行的任务都将其输出写入私有临时文件。一个 reduce task 生成一个这样的文件，map task 生成 R 个这样的文件(每个 reduce task 一个)。当 map task 完成时，worker 向 master 发送一条消息，并在消息中包含 R 临时文件的名称，如果 master 接收到一个已经完成的 map task 的完成消息，它将忽略该消息。否则，它将在主数据结构中记录 R 文件的名称。 当一个 reduce task 完成之后，reduce worker 自动地将其临时输出文件重命名为最终输出文件。如果在多台机器上执行相同的 reduce 任务，那么将对相同的最终输出文件执行多个 rename 调用。我们依赖于底层文件系统提供的原子重命名操作，以确保最终文件系统状态只包含一次执行 reduce 任务所产生的数据。 我们的 map 和 reduce 操作符绝大多数都是确定性的，在这种情况下，我们的语义等价于顺序执行，这使得程序员可以很容易地推断他们的程序行为。当 map 或 reduce 操作符是不确定的时，我们提供较弱但仍然合理的语义。在存在非确定性操作符的情况下，特定 reduce 任务 R1 的输出等价于由非确定性程序的顺序执行产生的 R1 的输出。然而，不同 reduce 任务 R2 的输出可能对应于非确定性程序的不同顺序执行产生的 R2 输出。 考虑 map 任务 M 和 reduce 任务 R1 和 R2。设 e(Ri) 是所承诺的 Ri 的执行(只有一个这样的执行)。由于 e(R1) 可能读取了 M 的一次执行产生的输出，而 e(R2) 可能读取了 M 的另一次执行产生的输出，所以语义较弱。 Locality 在我们的计算环境中，网络带宽是一个相对稀缺的资源。通过利用输入数据(由 GFS 管理)存储在组成集群的机器的本地磁盘这一事实，我们节约了网络带宽。GFS 将每个文件划分为64 MB的块，并在不同的机器上存储每个块的多个副本(通常是3个副本)，MapReduce master 将输入文件的位置信息考虑在内，并尝试在包含相应输入数据副本的机器上调度map任务。如果失败，它将尝试调度靠近该任务输入数据副本的 map 任务(例如，在与包含数据的机器在同一网络交换机上的工作机器上)。当在集群中相当一部分 worker 上运行大型MapReduce 操作时，大部分输入数据都是在本地读取的，不会消耗网络带宽 Task Granularity 如上所述，我们将 map 阶段细分为 M 个部分，将 reduce 阶段细分为 R 个部分。理想情况下，M 和 R 应该远远大于工作机器的数量。让每个 worker 执行许多不同的任务可以改善动态负载平衡，并在 worker 失败时加快恢复速度:它完成的许多 map 任务可以分散到所有其他 worker 机器上。 在我们的实现中，M 和 R 的大小最多有多大是有实际限制的，因为如上所述，master 必须做出 O(M + R) 调度决策，并在内存中保持 O(M*R) 状态。(内存使用的常量是很小的:状态的 O(M∗R) 部分由每个 map任务/reduce任务对大约一个字节的数据组成。) 此外，R 常常受到用户的限制，因为每个 reduce 任务的输出都以单独的输出文件结束。在实践中，我们倾向于选择 M，以便每个单独的任务大约有 16MB 到 64MB 的输入数据(以便上面描述的局部性优化最有效)，并且我们将 R 设为预期使用的工作机器数量的小倍数。我们经常使用 2000 台 worker 机器进行 M = 200000 和 R = 5000 的 MapReduce 计算。 Backup Tasks 导致 MapReduce 操作总时间延长的一个常见原因是“掉线”(straggler)。一种需要异常长时间才能完成计算过程中最后几个 map 或 reduce 任务之一的机器。掉队者出现的原因有很多。例如，磁盘有问题的机器可能会经常出现可纠正错误，导致读性能从 30MB/s 降至 1MB/s。集群调度系统可能已经调度了机器上的其他任务，由于 CPU、内存、本地磁盘或网络带宽的竞争，导致它执行 MapReduce 代码的速度变慢。我们最近遇到的一个问题是，机器初始化代码中的一个bug导致了处理器缓存被禁用:受影响机器的计算速度降低了 100 倍以上。 我们有一个一般性的机制来缓解掉队者的问题。当 MapReduce 操作接近完成时，master 会对剩余的正在执行的任务进行备份。只要主执行或备份执行完成，任务就被标记为完成。我们已经调优了这种机制，因此它通常不会增加操作使用的计算资源超过几个百分点。我们发现，这大大减少了完成大型 MapReduce 操作的时间。以5.3中所述的排序程序为例，关闭备份机制后，排序程序完成的时间会增加 44%。 Refinements Partitioning Function MapReduce 的用户指定他们想要的 reduce 任务/输出文件的数量(R)，使用中间键上的分区函数在这些任务之间对数据进行分区。提供了一个默认的分区函数，使用哈希(例如&quot; hash(key) mod R &quot;)。这往往会导致相当平衡的分区。然而，在某些情况下，通过键的其他函数来分区数据是有用的。例如，有时输出键是url，我们希望单个主机的所有条目都在同一个输出文件中结束。为了支持这种情况，MapReduce 库的用户可以提供一个特殊的分区函数。例如，使用&quot; hash(Hostname(urlkey)) mod R &quot;作为分区函数会导致来自同一主机的所有 url 最终出现在同一个输出文件中。 Ordering Guarantees 我们保证在给定的分区中，中间键/值对按键的递增顺序进行处理。这种排序保证使得为每个分区生成有序的输出文件变得很容易，当输出文件格式需要支持按键进行有效的随机访问查找，或者输出的用户发现对数据进行排序很方便时，这很有用。 Combiner Function 在某些情况下，每个 map 任务产生的中间键有显著的重复，并且用户指定的 Reduce 函数是可交换的和关联的。单词计数示例就是一个很好的例子。由于单词频率倾向于遵循 Zipf 分布，每个 map 任务将产生数百或数千个 &lt;the, 1&gt; 形式的记录。所有这些计数将通过网络发送到一个 reduce 任务，然后由 reduce 函数相加产生一个数字。我们允许用户指定一个可选的Combiner函数，该函数在通过网络发送数据之前对数据进行部分合并。 Combiner 函数在每一个执行 map task 上的机器执行，通常使用相同的代码来实现 combiner 和 reduce 函数，reduce 函数和 combiner 函数之间的唯一区别是 MapReduce 库如何处理函数的输出。reduce 函数的输出被写入最终的输出文件。combiner 函数的输出被写入一个中间文件，该文件将被发送到reduce 任务。 部分 Combine 大大加快了 MapReduce 操作的某些类。 Input and Output Types MapReduce 库支持以几种不同的格式读取输入数据。text 模式的输入将每一行视为键/值对：键是文件中的偏移量，值是行内容。另一种常见的支持格式存储按键排序的键/值对序列。每个输入类型实现都知道如何将自己分割成有意义的范围，以便作为单独的 map 任务进行处理(例如，文本模式的范围分割确保范围分割只发生在行边界)。用户可以通过提供一个简单的 reader 接口的实现来添加对新输入类型的支持，尽管大多数用户只使用少数预定义输入类型中的一种。 reader 并不一定需要提供从文件中读取的数据。例如，很容易定义从数据库或映射在内存中的数据结构中读取记录的 reader。 以类似的方式，我们支持一组输出类型来生成不同格式的数据，并且用户代码很容易添加对新输出类型的支持。 Side-effects 在某些情况下，MapReduce 的用户发现从他们的 map 或 reduce 操作生成辅助文件作为额外的输出是很方便的。我们依靠应用程序 writer 使这些副作用具有原子性和幂等性。通常，应用程序会写入一个临时文件，并在完全生成该文件后自动重命名该文件。 我们不支持单个任务生成的多个输出文件的原子两阶段提交。因此，产生具有跨文件一致性要求的多个输出文件的任务应该是确定的。这种限制在实践中从来就不是问题。 Skipping Bad Records 有时，用户代码中的错误会导致 Map 或 Reduce 函数在特定记录上崩溃。此类 bug 会导致 MapReduce 操作无法完成。通常的做法是修复 bug，但有时这是不可行的;这个 bug 可能存在于源代码不可用的第三方库中。此外，有时忽略一些记录是可以接受的，例如在对一个大数据集进行统计分析时。我们提供了一个可选的执行模式，MapReduce 库会检测哪些记录导致确定性崩溃，并跳过这些记录以便继续前进。 每个工作进程都安装一个信号处理程序来捕获分割违规和总线错误。在调用 Map 或 Reduce 操作之前，MapReduce 库会将参数的序号存储在全局变量中。如果用户代码产生信号，信号处理器发送一个包含序列号的“最后一口气” UDP 包给 MapReduce master。当主服务器在一个特定的记录上看到多个失败时，它指示在下一次重新执行对应的 Map 或 Reduce 任务时应该跳过该记录。 Local Execution 在 Map 或 Reduce 函数中调试问题可能会很棘手，因为实际的计算发生在分布式系统中，通常在几千台机器上，由 master 动态地做出工作分配决策。为了方便调试、分析和小规模测试，我们开发了 MapReduce 库的替代实现，在本地机器上顺序执行 MapReduce 操作的所有工作。控件提供给用户，以便计算可以限制到特定的映射任务。用户可以用一个特殊的标志来调用他们的程序，然后可以很容易地使用任何他们认为有用的调试或测试工具(例如gdb)。 Status Information 主服务器运行一个内部HTTP服务器，并导出一组状态页面供人们使用。状态页面显示了计算的进度，例如有多少任务已经完成，有多少任务正在进行，输入字节数，中间数据字节数，输出字节数，处理速率等。这些页面还包含指向每个任务生成的标准错误和标准输出文件的链接。用户可以使用这些数据来预测计算将花费多长时间，以及是否应该向计算中添加更多的资源。这些页面还可以用于计算何时会比预期的慢得多。 此外，顶级状态页面显示哪些 worker 失败了，以及当他们失败时正在处理哪些 map 和 reduce 任务。当试图诊断用户代码中的错误时，此信息非常有用。 Counters MapReduce 库提供了一个计数器来计算各种事件的发生次数。例如，用户代码可能需要计算已处理的字的总数或索引的德文文档的数量，等等。 要使用这个功能，用户代码创建一个命名的计数器对象，然后在 Map 或 Reduce 函数中适当地增加计数器。例如: Counter* uppercase; uppercase = GetCounter(&quot;uppercase&quot;); map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;); 来自各个 worker 机器的计数器值定期传播到 master (在 ping 响应中附带)。master 聚合成功的 map 和 reduce 任务的计数器值，并在 MapReduce 操作完成时将它们返回给用户代码。当前计数器值也显示在 master 状态页面上，以便人们可以观看实时计算的进度。在聚合计数器值时，master 消除了重复执行同一个 map 或 reduce 任务的影响，以避免重复计算。(重复执行可能是由于我们使用了备份任务以及由于失败而重新执行任务引起的。) 一些计数器值由 MapReduce 库自动维护，例如处理的输入键/值对的数量和产生的输出键/值对的数量。 用户已经发现 counter 工具对于检查 MapReduce 操作的行为是非常有用的。例如，在一些 MapReduce 操作中，用户代码可能希望确保生成的输出对的数量恰好等于处理的输入对的数量，或者确保处理的德文文档的比例在处理的文档总数的某个可容忍的比例内。 Conclusion MapReduce编程模型已经在谷歌上成功地用于许多不同的目的。我们认为这种成功有几个原因。首先，该模型易于使用，即使对于没有并行和分布式系统经验的程序员也是如此，因为它隐藏了并行化、容错、局部性优化和负载平衡的细节。其次，大量的问题可以通过MapReduce计算很容易地表达出来。例如，MapReduce被用于谷歌生产web搜索服务的数据生成、排序、数据挖掘、机器学习以及许多其他系统。第三，我们开发了一个MapReduce的实现，它可以扩展到由数千台机器组成的大型机器集群。该实现有效地利用了这些机器资源，因此适合用于在谷歌中遇到的许多大型计算问题。 我们从这项工作中学到了一些东西。首先，对编程模型的限制使得并行化和分布式计算变得容易，并使这些计算具有容错性。其次，网络带宽是一种稀缺资源。因此，我们系统中的许多优化都旨在减少通过网络发送的数据量:局部性优化允许我们从本地磁盘读取数据，将中间数据的单个副本写入本地磁盘可以节省网络带宽。第三，冗余执行可以用来减少慢速机器的影响，并处理机器故障和数据丢失。 #include &quot;mapreduce/mapreduce.h&quot; // 用户实现map函数 class WordCounter : public Mapper { public: virtual void Map(const MapInput&amp; input) { const string&amp; text = input.value(); const int n = text.size(); for (int i = 0; i &lt; n; ) { // 跳过前导空格 while ((i &lt; n) &amp;&amp; isspace(text[i])) i++; // 查找单词的结束位置 int start = i; while ((i &lt; n) &amp;&amp; !isspace(text[i])) i++; if (start &lt; i) Emit(text.substr(start,i-start),&quot;1&quot;); } } }; REGISTER_MAPPER(WordCounter); // 用户实现reduce函数 class Adder : public Reducer { virtual void Reduce(ReduceInput* input) { // 迭代具有相同key的所有条目,并且累加它们的value int64 value = 0; while (!input-&gt;done()) { value += StringToInt(input-&gt;value()); input-&gt;NextValue(); } // 提交这个输入key的综合 Emit(IntToString(value)); } }; REGISTER_REDUCER(Adder); int main(int argc, char** argv) { ParseCommandLineFlags(argc, argv); MapReduceSpecification spec; // 把输入文件列表存入&quot;spec&quot; for (int i = 1; i &lt; argc; i++) { MapReduceInput* input = spec.add_input(); input-&gt;set_format(&quot;text&quot;); input-&gt;set_filepattern(argv[i]); input-&gt;set_mapper_class(&quot;WordCounter&quot;); } //指定输出文件: // /gfs/test/freq-00000-of-00100 // /gfs/test/freq-00001-of-00100 // ... MapReduceOutput* out = spec.output(); out-&gt;set_filebase(&quot;/gfs/test/freq&quot;); out-&gt;set_num_tasks(100); out-&gt;set_format(&quot;text&quot;); out-&gt;set_reducer_class(&quot;Adder&quot;); // 可选操作:在map任务中做部分累加工作,以便节省带宽 out-&gt;set_combiner_class(&quot;Adder&quot;); // 调整参数: 使用2000台机器,每个任务100MB内存 spec.set_machines(2000); spec.set_map_megabytes(100); spec.set_reduce_megabytes(100); // 运行它 MapReduceResult result; if (!MapReduce(spec, &amp;result)) abort(); // 完成: 'result'结构包含计数,花费时间,和使用机器的信息 return 0; 论文的部分到此结束，后面展开讲一下 MapReduce 的其他东西。 Other MapReduce 最重要的贡献：MR takes care of, and hides, all aspects of distribution! Problems What if the master gives two workers the same Map() task? Perhaps the master incorrectly thinks one worker died. it will tell Reduce workers about only one of them. What if the master gives two workers the same Reduce() task? they will both try to write the same output file on GFS! atomic GFS rename prevents mixing; one complete file will be visible. What if a single worker is very slow -- a &quot;straggler&quot;? perhaps due to flakey hardware. master starts a second copy of last few tasks. What if a worker computes incorrect output, due to broken h/w or s/w? too bad! MR assumes &quot;fail-stop&quot; CPUs and software. Current status Hugely influential (Hadoop, Spark, &amp;c). Probably no longer in use at Google. Replaced by Flume / FlumeJava (see paper by Chambers et al). GFS replaced by Colossus (no good description), and BigTable. Conclusion MapReduce single-handedly made big cluster computation popular. -Not the most efficient or flexible. +Scales well. +Easy to program -- failures and data movement are hidden. ","link":"https://blog.shunzi.tech/post/MapReduce/"},{"title":"What is license for source code?","content":" 什么是源代码许可？以及如何选择源代码许可？困扰了很久的问题，查了下资料决定把这个坑埋了。 开源许可长啥样？ 我们常常在 Github 上看到关于 License 的信息，仿佛 NB 点的项目都挂了个 License（啊没有不挂就不 NB 的意思），一般都长成下面这样，只是可能协议啥的会有区别。如 RocksDB 的 repo，使用了 GPLv2 和 Apache 2.0 License 协议，然后 repo 内也有相应的协议文件与之对应，COPYING 和 LICENSE.Apache 到底啥是开源许可？ License 可能大家听说的相对于 Copyright 少一点。那么不妨先说啥是 Copyright。 Copyright(C) Copyright：中文译作版权，大家在一些数字媒体软件上可能感受的真切一点，比如某首歌只有部分音乐公司拥有其版权，那为什么有的公司没有版权就不能提供相关音乐的播放和下载服务呢，不妨拆词解义 copy + right，即复制的权利，说的简单点就是没有某个产品 Copyright 的公司就无法对该产品进行复制，就更别说进行修改发布了。 百度百科：版权是对计算机程序、文学著作、音乐作品、照片、游戏，电影等的复制权利的合法所有权。除非转让给另一方，版权通常被认为是属于作者的。大多数计算机程序不仅受到版权的保护，还受软件许可证的保护。版权只保护思想的表达形式，而不保护思想本身。算法、数学方法、技术或机器的设计均不在版权的保护之列。 如果有去公司实习或者工作过的同学应该就知道，往往在公司的项目里写相关代码的时候往往会有一条编程规范的限制，即 Copyright 的声明，许多 IDE 也有相关 Copyright 模板和自动生成插件的提供。如下为 RocksDB 源代码中关于 CopyRight 的声明。Copyright 约定了版权归属谁，并归定了这个软件的使用许可证方式。 // Copyright (c) 2011-present, Facebook, Inc. All rights reserved. // This source code is licensed under both the GPLv2 (found in the // COPYING file in the root directory) and Apache 2.0 License // (found in the LICENSE.Apache file in the root directory). package org.rocksdb; public abstract class Cache extends RocksObject { protected Cache(final long nativeHandle) { super(nativeHandle); } } Copyright 是作者或者创建者因为其原创性的工作，所拥有的复制，分发，出售以及其他一系列的排他性权利，如上所示代码声明了“这段代码的版权归属于 Facebook”，拥有一切版权保护的相关权利。 Copyleft(Ɔ) 其实还有个东西叫 Copyleft(Ɔ)， “Copyleft”最初是为反对商业软件而生，但它并不是放弃版权。反对软件一切权利归作者私有，保护知识共享、权利共享。 软件的版权归原作者所有，其它一切权利归任何人所有。用户和软件的作者享有除版权外的完全同等的权利，包括复制软件和重新发布修改过的软件的权利。 自由软件在承认著作权的基础上，可以通过许可协议，与公众共享作品的其它权利 百度百科：著佐权（Copyleft）是一个由自由软件运动所发展的概念，是一种利用现有著作权体制来保护所有用户和二次开发者的自由的授权方式。在自由软件授权方式中增加著佐权条款之后，该自由软件除了允许使用者自由使用、散布、修改之外，著佐权许可证更要求使用者修改后的衍生作品必须要以同等的授权方式（除非许可证或者版权声明里面例外条款所规定的外）释出以回馈社会。 所以正是因为 Copyleft(Ɔ) 的思想，才逐渐衍生出后来的 License。可以简单理解为 Copyleft = Copyright+GPL License 版权法默认禁止共享，也就是说，没有许可证的软件，就等同于保留版权，虽然开源了，用户只能看看源码，不能用，一用就会侵犯版权。所以软件开源的话，必须明确地授予用户开源许可证。 License 是 Copyright 拥有者授予其他人处置其原创性成果的权利，如上代码版权声明所示，“Facebook 授予了这段代码 GPLv2 和 Apache2.0 的许可”。 开放源码许可证是符合开放源码定义的许可证——简而言之，它们允许“自由”地使用、修改和共享软件。这里的自由其实是相对的，相应地需要遵守对应 License 下的规定。 软件许可是告诉其他人，他们能够对您的代码做什么，不能做什么。 大多数人将其许可文件放在仓库根目录的文件 LICENSE.txt（或 LICENSE.md）中，如 RocksDB 中的 COPYING 和 LICENSE.Apache。 GPLv2 COPYING GNU GENERAL PUBLIC LICENSE Version 2, June 1991 Copyright (C) 1989, 1991 Free Software Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. ... Apache2.0 LICENSE.Apache Apache License Version 2.0, January 2004 http://www.apache.org/licenses/ TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION 1. Definitions. ... 上面其实介绍的都是开源 license，但其实在软件市场中还是有大量的商业 license 的，毕竟也是要恰饭的嘛。主要就是一些商业软件的使用，常常需要大家购买对应软件的 license 才能使用，而且很多 license 大多都是有时间期限的，例如 IDEA 可能还有一些 license server 的机制，但总体思想都是通过销售 license 来获取盈利。 BTW，很多盗版软件其实就是尝试着去碰撞出一个可能有效的 license，甚至有人共享对应的 license 来进行多用户使用。 有哪些开源许可 https://www.gnu.org/licenses/license-list.zh-cn.html 几种常见的开源许可： GPL（GNU General Public License）：GNU 通用公共许可协议，免费使用、引用、修改代码，但不能用在闭源软件中发布及销售。“传染性” 表示如果一个软件使用了 GPL 协议的开源代码，那么这个软件也必须开源，仍然免费使用。不能用于商业产品。 LGPL（GNU Lesser General Public License）：宽松GPL，规定：如果A项目采用LGPL许可证，那么基于A开发出来的B项目也必须采用LGPL，即必须也开源，但是，如果B项目不是基于A开发出来的，而仅仅调用了A的接口，那么B项目可不必开源，倘若换做GPL的话，那么B项目也是要开源的（所以叫做宽松的GPL）。 BSD License（original BSD license、FreeBSD license、Original BSD license）：伯克利软件套装，规定：如果A项目采用BSD许可证，那么基于A开发出来的B项目可以选择闭源，即私有化、商业化，但是必须注明B项目采用了A这个开源项目。主要限制在于不能用开源代码的作者或机构进行商品推广。 MIT(The MIT License)：麻省理工学院许可证，规定：这是一个自由度很高的开源许可证，几乎同意了可以随意使用一个开源项目（使用、复制、修改、合并、出版发行、散布、再授权、贩售软件及软件的副本），只要在你的项目中包含或提及原开源项目的MIT许可证。至于你会不会通过它进行商品推广，作者并不关心，只想保留版权。 Apache Licence：Apache软件基金会，规定：大致上和BSD许可证类似，只是有一点细微差别，它除了需要注明B项目源于开源项目A，也要在每个修改过的A项目的文件注明此文件已被修改，并且原文件是A开源项目中的哪个文件。相对于 MIT，如果修改了源代码，需要进行说明。 不推荐用于商业产品的协议：GPL (eg. Linux), LGPL, MPL 适用于商业产品的协议：BSD, MIT, Apache (eg. RocksDB) Dual-Licensed: 但是我们在如上的 RocksDB 中的例子观察到，RocksDB 中包含了两个开源许可，一个是不推荐用于商业产品的 GPLv2，一个是推荐用于商业产品的 Apache2.0，而在 RocksDB 关于 License 的介绍中我们发现本身该项目就是基于两个开源协议的，而其他软件开发者可以根据自己的实际需求来决定使用哪一个 License。 Other 可能还会有小伙伴看过这样的版权例子，但大多都是一些知识产品，比如博客、slides、文档以及网页等等。 CC License: Creative Commons license，简称CC许可，是一种公共版权许可协议，知识共享许可协议，其允许分发受版权保护的作品。一个创作共用许可，用于一个作者想给他人分享、使用、甚至创作派生作品的权利。创作共用提供给作者灵活性（例如，他们可以选择允许非商业用途使用他们的作品），保护使用或重新分配他人作品的人，所以他们只要遵守由作者指定的条件，不必担心侵犯版权。 百度百科：知识共享（Creative Commons，简称CC，台湾译名创用CC）是一个非营利组织，也用是一种创作的授权方式。此组织的主要宗旨是增加创意作品的流通可及性，作为其它人据以创作及共享的基础，并寻找适当的法律以确保上述理念。 CC-BY-NC-SA 本质是几种权利的组合： CC：创作共用 BY：署名：您（用户）可以复制、发行、展览、表演、放映、广播或通过信息网络传播本作品；您必须按照作者或者许可人指定的方式对作品进行署名。 NC：非商业性使用（英语：Noncommercial，nc）您可以自由复制、散布、展示及演出本作品；您不得为商业目的而使用本作品。 SA：相同方式共享（英语：ShareAlike，sa）您可以自由复制、散布、展示及演出本作品；若您改变、转变或更改本作品，仅在遵守与本作品相同的许可条款下，您才能散布由本作品产生的派生作品。（参见copyleft。） 除此以外还包含一种权利： ND：禁止演绎（英语：No Derivative Works，nd)，您可以自由复制、散布、展示及演出本作品；您不得改变、转变或更改本作品。 怎么选开源许可 如何选 License: https://www.gnu.org/licenses/license-recommendations.html 千言万语一大堆，不如一张图。图源阮一峰博客 http://www.ruanyifeng.com/blog/2011/05/how_to_choose_free_software_licenses.html https://choosealicense.com/ ","link":"https://blog.shunzi.tech/post/license/"},{"title":"Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging","content":" SIGMMOD18 Dostoevsky: Better Space-Time Trade-Offs for LSM-Tree Based Key-Value Stores via Adaptive Removal of Superfluous Merging Abstract 无论是学术界还是工业界，所有主流的基于 LSM 树的键值存储都在更新的 I/O 成本和查询和存储空间的 I/O 成本之间进行了权衡。因为在所有的 LSM Tree 的级别上都需要执行 Compaction 操作来限制查询遍历的 runs，并删除 obsolete 的数据项来腾出存储空间。即便是最先进的 LSM Tree 设计，来自 LSM Tree 所有层此的合并操作（除了最大的层此）减少的点查询成本、大范围查询成本和存储空间，减少的效果可以忽略不计；与此同时还增加了更新操作的平摊开销。 为了解决这个问题，我们提出了 Lazy Leveling，一种新的设计，从除开最大层以外的所有 level 中删除合并操作。同时提出了 Fluid LSM-tree，一种可以涵盖整个 LSM-tree 设计领域的通用设计，可以参数化以假设任何现有的设计。相对于 Lazy level, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。 Dostoevsky，一种键值存储，通过基于应用程序工作负载和硬件来动态调整的弹性的 LSM-tree 设计，自适应地消除多余的合并。基于 RocksDB 实现，测试表明无论是性能还是存储空间方面都优于目前最先进的设计。 Introduction LSM-tree 将要插入/更新的条目缓冲在内存中，并在缓冲区填满时将缓冲区作为 sorted run 刷新到次要存储。LSM-tree 稍后对这些 runs 进行排序合并，以限制查找必须扫描的run 数量，并删除过时的条目。LSM-tree 将运行组织成指数级增长的容量，更大的级别包含更老的运行。当条目被替换更新时，点查找通过从最小到最大的级别查找条目的最新版本，并在查找目标键时终止查找。另一方面，范围查找必须在所有级别的所有 run 中访问相关的键范围，并从结果集中删除过时的条目。为了提高单个 run 的查询速度，设计中常常包含了两个额外的内存中的数据结构。首先，对于每个 run，都有一组包含每个 run 块的第一个键的 fence 指针，这允许查找在一个 run 中只使用一个 I/O 就可以访问特定的键；第二，每个 run 会有一个 BoolmFilter，这允许点查询跳过不包含目标键的 runs。这个设计被应用到了大量的现代 KV 存储中，如 LevelDB、BigTable 等。 问题：LSM-tree 中的合并操作的频率控制了在 更新的 I/O 成本 和 查询和存储空间放大的 I/O 成本之间的 trade-off，另外的问题就是现有的设计在这些指标之间的 trade-off 并不理想。下图表示了指标之间的权衡关系，虽然这些 y 轴指标具有不同的单位，但它们相对于 x 轴的权衡曲线具有相同的形状。两个极端分别是 log 和 sorted array。LSM-tree在完全不合并或尽可能多地合并时，分别退化为这些边缘点。我们将主流系统放置在这些边缘点之间的顶部曲线上，基于它们的默认合并频率，我们为 Monkey 绘制了一个优越的权衡曲线，我们证明了存在一个甚至比Monkey更好的权衡曲线。现有的设计放弃了大量的性能和/或存储空间，因为没有沿着这条底部曲线设计。 问题来源：通过分析最先进的 LSM 树的设计空间，我们指出了问题的根源，即最坏情况下的更新代价、点查询代价、范围查询代价和空间放大在不同的层次上产生不同的结果。 Update: 更新的 I/O 成本稍后通过更新条目参与的合并操作来分担。虽然较大级别的合并操作需要成倍地增加工作，但它们发生的频率却成倍地减少。因此，更新从所有级别的合并操作中同等地获得它们的 I/O 成本。 Point lookups：虽然 图1 中沿顶部曲线的主流设计将跨 LSM-tree 所有级别的 Bloom flters 假阳性率设置为相同，但目前最先进的 Monkey 为更小的层此设置更低的假阳性率。他被证明可以最小化所有筛选器的误报率之和，从而最小化点查找的 I/O。与此同时，这意味着进入较小层此的可能性呈指数级下降，因此大多数点查询 I/O 将直接命中最大的层次。 Long range lookup：因为 LSM Tree 的容量呈指数级增长，最大曾通常包含绝大部分数据，所以该层更可能包含给定键范围内的数据，因此大多数由大范围查询引起的 I/O 都将对最大层进行操作。 Short range lookup： 使用极小键范围的范围查找在每次 run 中只能访问大约一个块，而不管 run 的大小，因为每一层 run 的最大个数是固定的，因此小范围查询在所有曾中是相当的。 Space-Amplifcation：空间放大最差的情况就是较低层次的数据被更新到最大层此，因此在最大层中老旧的数据项比例最大。 因为最坏情况下的点查询开销、大范围查询开销、空间放大都主要来源于最大层，LSM-tree 中所有级别的合并操作，除开最大层(即大多数合并操作)在这些指标上几乎没有改进，同时显著增加了更新的平摊成本。这导致了次优的权衡。我们用三个步骤从头开始解决这个问题： Solution 1: Lazy Leveling to Remove Superﬂuous Merging： 我们使用 Lazy level 拓展了 LSM-tree 设计思路，这种新设计除去了 LSM-tree 最大级别之外的所有合并。Lazy Leveling 改进了最坏情况下更新的成本复杂性，同时在点查找成本、大范围查找成本和空间放大上保持相同的限制，同时在小范围查找成本上提供具有竞争力的限制。我们证明改进的更新开销可以用来降低点查找开销和空间放大。这生成了 图1 中的底部曲线，它提供了更丰富的时空权衡，这是迄今为止最先进的设计无法实现的。 Solution 2: Fluid LSM-Tree for Design Space Fluidity. 我们引入了 Fluid LSM-tree 作为新一代的 LSM Tree 支持在整个 LSM-tree 设计思路中流畅地切换。Fluid LSM-tree 通过分别控制最大级别和所有其他级别合并操作的频率，相对于 Lazy leveling, Fluid LSM-tree 可以通过在最大级别上合并更少的内容来优化更新，或者通过在所有其他级别上合并更多内容来优化小范围查询。 Solution 3: Dostoevsky to Navigate the Design Space Dostoevsky: Space-Time Optimized Evolvable Scalable Key-Value Store。Dostoevsky 分析地找到了Fluid LSM-tree 的调优方法，以最大限度地提高特定应用程序工作负载和硬件在空间放大方面的用户约束，通过精简搜索空间来快速找到最佳调优，并在运行时对其进行物理调整。因为 Dostoevsky 跨越了所有现有的设计，并能够针对给定的应用程序 navigate 到最佳的设计，因此它在性能和空间放大方面严格控制了现有的键值存储。 BACKGROUND 如下图所示，为了优化写操作，LSM-Tree 初始时缓冲了所有的更新、插入和删除操作到内存中，当 Buffer 满了之后，LSM Tree 将 Buffer 以 Sorted Run 刷回到了第二层存储，LSM Tree 归并排序 runs 是为了：限制查询操作必须访问的 runs 的数量，以及删除老旧的数据项以回收空间。runs 被组织成 L 个呈指数级增长的层此，Level 0 是主存中的 Buffer，其他层次都位于二级存储。 在合并的 I/O 开销和查询 I/O 开销以及空间放大之前的权衡可以由两个参数来控制，第一个是相邻两个层次之间的比例 T，T 控制了层级的个数因此决定了一个数据能够在层级之间合并多少次。第二个参数是合并策略，决定了数据项在一个 level 内的合并次数。所有的现有设计都使用了 tiering 或 leveling 两种策略。 tiering：当一个 level 到达容量时合并该 level 内的 runs leveling: 当一个新的 run 出现，就会在 level 中执行合并 如下图所示，size ratio 为 4，buffer 大小为一个 entry 的大小。在两种策略中，当 buffer flushing 造成 Level 1 到达容量时触发合并操作。对于 tiering，Level 1 的所有 runs 都被合并成同一个新的 run 放置在 Level 2。而对于 Leveling，合并操作还会包含 Level2 原有的 run。 Number of Levels：Level 0 拥有的数据项个数 B * P。$$L = [log_T(\\frac{N}{BP}\\frac{T-1}{T})]$$。层级之间的大小比例 T 被限制到 2≤T≤Tlim2 ≤ T ≤ T_{lim}2≤T≤Tlim​，其中 TlimT_{lim}Tlim​ 被定义成 NB∗P\\frac{N}{B*P}B∗PN​，当 size ratio 达到上界时，levels 的数量减少到接近 1，超过上界之后将无结构性的变化。大于下界就表明 第 i 级合并操作的结果运行永远不会大到超过第 i + 1 级。换句话说就是确保了 runs 不会跨多个 levels。 Finding Entries：因为数据项是异地更新，相同 key 的数据的多个版本可能出现在多个 level 中，甚至对于 tiering 策略可能存在于一个 level 的多个 runs 中，为了确保查询操作总是能够找到最新版本的数据，LSM Tree 采用了如下措施：1. 当数据项被插入到 buffer 中且 buffer 中包含相同 key 对应的数据时，新的数据项将代替老的数据；2. 当两个包含相同 key 的数据项的 runs 被合并的时候，只有最新版本的数据将被保留；3. 为了能够获取到来自不同 runs 的相同 key 的不同数据项的插入顺序，一个单独的 run 只能够和相邻时间的 run 进行合并。从而保证当有两个 runs 包含不同版本的相同 key 对应的数据时，younger run 包含的是最新版本的数据。 Point Lookups：点查询通过从最小到最大层次进行遍历来查询最新版本的数据，对于 tiering 则是在一个 level 中从最新到最老的 runs 中遍历进行查询。当找到一个匹配当前 key 的数据时则终止。 Range Lookups：范围查询需要查找指定范围的键对应的所有最新的数据，通过对所有 levels 所有 runs 的相关键范围进行排序合并。当 sort-merging 时，识别出来自不同 runs 具有相同 key 的数据，然后丢弃掉老版本的数据。 Deletes：通过给每个数据项添加一位 flag 来实现。如果查询操作找到了该数据想的最新版本，且该数据项上有该 flag 那么将不会返回对应的 value 给应用。当一个删除的数据项和 最老的 run 合并的时候，该数据将被删除，因为该数据项已经代替了之前所有插入的具有当前 key 的数据。 Fragmented Merging：为了换接较大级别上由于长时间合并操作而导致的性能下降，主流设计把 runs 分区成了文件，也叫 Sorted String Tables，然后一次合并一个 SSTable 和下一个 older run 中具有重叠键范围的多个 SSTables，该技术不会影响最坏情况下的合并 I/O 开销，而只会影响这种开销如何调度。在整篇文章中，为了便于阅读，我们将合并操作讨论为具有 runs 的粒度，尽管它们也可以具有 sstables 的粒度。 Space-Amplifcation：过时条目的存在使存储空间增大的因素称为空间放大。由于磁盘的可承受性，空间放大传统上并不是数据结构设计的主要关注点。然而，SSD 的出现使空间放大成为一个重要的成本问题。我们将空间放大作为成本指标，以提供我们所引入和评估的设计的完整描述。 Fence Pointers：所有主要的基于 LSM 树的键值存储都在主存中对每次运行的每个块的第一个键建立索引，也就是图 2 所示的 fence pointer，通常这些指针占据内存空间大小为 O(N/B)，但是让查询操作中找到每个 runs 的 key 范围变成了只需要一次 I/O。 Bloom Filters：为了加速点查找，只需要在主存中为每个 run 维护一个 BloomFilter，点查找在访问存储中相应的 runs 之前首先检查 Bloom flter。如果 filter 返回 true positive，那么查询操作配合 fence pointer 只需要一次 I/O 就能访问对应的 run，从而找到对应的数据项并终止。如果返回 negative，那么将跳过该 run 并节省一次 I/O 操作。但还有 false positive 的情况，浪费一次 I/O 然后再去下一个 run 继续查找该 key。 Bloom flter 有一个有用的特性，如果它被分割成较小的等大小的 Bloom flter，其中的条目也被等分，每一个新的分区布隆滤片的 FPR 渐近与原滤片的 FPR 相同(虽然实际略高)。为了便于讨论，我们将Bloom flters称为非分区的，尽管它们也可以按照工业中的某些设计进行分区（比如每个 run 的每个 block），从而为空间管理提供更大的灵活性。(例如，对于那些不经常被点查询读取的块，可以将其 offload 到存储器中以节省内存) Applicability Beyond Key-Value Stores：根据工业上的设计，我们的讨论假设一个键在运行过程中与它的值相邻存储。为了便于阅读，本文中的所有图形都将条目描述为键，但它们表示键-值对。我们的工作也适用于没有 value 的应用程序(例如，LSM-tree 被用来回答关于键的集合成员查询)，其中的值是指向存储在 LSM-tree 之外的数据对象的指针，或者 LSM-tree 被用作解决更复杂算法问题的构建块(例如，图分析)， FTL 设计等)。我们将分析的范围限制在基本操作和 LSM-tree 的大小上，以便它可以很容易地应用于这些其他情况。 DESIGN SPACE AND PROBLEM ANALYSIS 现在，我们分析更新和查找的最差情况下的空间放大和 I/O 成本是如何从不同的级别派生出与合并策略和大小比例相关的。为了分析更新和查找，我们使用磁盘访问模型来计算每个操作的 I/O 数量，其中 I/O 是从二级存储读取或写入一个块。 分析结果如下所示： Updates：更新成本通常都是由更新条目参与的后续合并操作产生的，分析假设最坏情况的工作负载，其中所有更新的目标条目都在最大级别。这意味着一个过时的条目不会被删除，直到它相应的更新的条目达到最大级别。因此，每个条目都会在所有级别上合并(即，而不是在某个更小的级别上被最近的条目丢弃，从而减少以后合并操作的开销)。 tiering：每层合并 O(1) 次，每个合并过程中的 I/O 操作从原始的 run 中拷贝 B 个数据项到新的 run，因此每个数据项平均的更新操作成本开销如图所示。填满 level i，需要 B⋅P⋅TiB · P · T^iB⋅P⋅Ti 次更新，导致合并操作拷贝 B⋅P⋅TiB · P · T^iB⋅P⋅Ti 个数据项。 leveling：到达 level i 的第 j 个 run 触发了一个合并操作，合并操作包括 level i 现有的 runs，这些 runs 是自上次 level i 为空以来到达的前 T−j 个 runs 的合并操作产生的。因此平均每个数据项在该层数据到达容量之前合并了 T/2 次，可以表示为 O(T)，同样需要除以一个块对应的 B 个数据项。每 B⋅P⋅Ti−1B · P · T^{i-1}B⋅P⋅Ti−1 次更新（每次有一个新的 run come in）之后执行一次合并操作，然后拷贝平均 B⋅P⋅Ti2\\frac{B · P · T^i}{2}2B⋅P⋅Ti​ 项数据，通过将复制的条目数除以级别 i 的合并操作的频率，我们观察到，在长期运行中，每个级别上的合并操作所做的工作量是相同的，直觉是，虽然合并操作在更大的级别上以指数方式完成更多的工作，但它们的频率也以指数方式降低 Analyzing Point Lookups：为了分析最坏情况下的点查找代价，我们将重点放在 zero-result 点查找（例如查询不存在的 Key）上，因为它们最大化了浪费的 I/O 的平均值。这种分析对于插入前判断是否存在的操作就很有用。开销最大的情况即为所有的 BloomFilter 返回 false positive，此时点查询操作会对每一个 run 发起一次 I/O，对于 leveling 浪费的 I/O 为 O(L)，对于 tiering 浪费的 I/O 为 O(T · L) 。但实际上，Bloom flters 对于不存在的 key 能节省很大一部分 I/O，在工业中，键值存储对每一个Bloom flters使用 10 位，这会导致误报率(FPR)为每个过滤器约为 1%，出于这个原因，我们将重点放在预期的最坏情况点查找成本上，它将点查找发出的 I/O 数量作为关于 Bloom flters FPRs 的长期平均值进行估计。我们估计这个成本为所有Bloom flters的FPRs之和。原因是，查询单个 run 的 I/O 成本是一个独立的随机变量，其期望值等于相应的 Bloom flter 的 FPR，多个独立随机变量的期望值之和等于它们各自的期望值之和。在工业界的键值存储中，所有级别的 BloomFilter 的每个条目的比特数是相同的。因此，最大级别的 Bloom flter(s) 比所有较小级别的 filter 的总和要大，因为它们以指数形式表示更多的条目。根据公式 FPR=e−(bits/entries)⋅ln(2)2FPR = e^{−(bits/entries)·ln(2)^2}FPR=e−(bits/entries)⋅ln(2)2，最大层的 FPRpLFPR_{pL}FPRpL​ 上界被限制在 O(e−M/N)O(e^{−M/N})O(e−M/N)，所以对于 leveling，即为 O(e−M/N⋅L)O(e^{−M/N} · L)O(e−M/N⋅L)，对于 tiering，即为 O(e−M/N⋅L⋅T)O(e^{−M/N} · L · T )O(e−M/N⋅L⋅T). 关于这个问题的最新论文 Monkey 表明，为所有级别的过滤器设置相同的每个条目的比特数并不能最小化浪费的I/O 的预期数量。相反，Monkey 在最大级别上对 filter 中的每个条目重新分配≈1比特，它使用这些比特来设置较小级别上每个条目的比特数，作为不断增加的等差数列：即 Level i 的每个数据项为 a+b⋅(L−i)a + b · (L - i)a+b⋅(L−i)，a 和 b 都是比较小的常数，这导致 FPR 在最大水平上有一个小的、渐近恒定的增加，在较小的水平上有一个指数下降，因为它们包含较少的条目。由于 FPRs 在较小的级别是指数递减的，所以 FPRs 的总和收敛于一个与级别数无关的乘法常数。Monkey 从点查找的复杂性中去掉了一个 L 的因素，这种复杂性导致了 O(e−M/N⋅L)O(e^{−M/N} · L)O(e−M/N⋅L) I/O (level)和 O(e−M/N⋅L)O(e^{−M/N} · L)O(e−M/N⋅L) I/O (tiering)，如图3 (B)所示。对于 zero and non-zero result 的结果点查找以及任何类型的偏差，使用 Monkey 总是有益的。 总的来说，我们观察到使用 Monkey 的点查找成本主要来自于最大的 level，因为较小的 level 的 FPRs 呈指数级下降，所以访问它们的可能性也呈指数级下降。 Analyzing Range Lookups：我们将范围查找的 selectivity 表示为在目标键范围内的所有 run 的唯一条目的数量。范围查找在所有 runs 中扫描和排序合并目标键范围，并从结果集中删除过时的条目。范围查询扫描并排序合并所有 runs 的目标键范围，从结果集中消除老数据。为了分析，如果访问的块数至少是可能的最大级别数的两倍，那么就认为范围查询的范围很大，sB&gt;2⋅Lmax\\frac{s}{B} &gt; 2 · L_{max}Bs​&gt;2⋅Lmax​，在均匀随机分布的更新下，这个条件意味着在目标键范围内的大多数条目都有很高的概率处于最大级别。 小范围查询对每个 run 发起近一个 I/O，叠加起来就是 leveling o(L)，tiering 就是 O(L·T)。对于长范围查询，在消除过时条目之前的结果集的大小平均是其 selectivity 和空间放大的乘积。我们用这个乘积除以块大小来得到 I/O 成本，tiering 即为 O(T⋅sB)O(\\frac{T·s}{B})O(BT⋅s​)，leveling 为 O(sB)O(\\frac{s}{B})O(Bs​) 一个关键的区别是，短范围查找从所有级别获得的开销大致相同，而长范围查找的大部分开销来自访问最大级别 Analyzing Space-Amplifcation：我们将空间放大定义为条目总数 N 除以唯一条目数 unq，amp=Nunq−1amp = \\frac{N}{unq} − 1amp=unqN​−1。为了分析最坏情况的空间放大，我们观察到 LSM-tree 的 1 到 L−1 级包含其容量 1T\\frac{1}{T}T1​ 的一部分，而 L 级包含其容量 T−1T\\frac{T−1}{T}TT−1​ 的剩余部分。使用 leveing，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，从而导致 Level L 的最多有 1T\\frac{1}{T}T1​ 是过时的。空间放大因此是 O(1/T)O(1/T)O(1/T)。对于 tiering，空间放大最坏的情况是当 Level 1 到 L-1 的条目都是对 Level L 的不同条目的更新时，且 Level L 的每个 run 包含相同的数据项集时，Level L 完全由过时的条目组成，所以空间放大是 O(T)，因为 Level L 比所有其他 Level 加起来要大 T−1 倍。总的来说，在最坏的情况下，带有 leveling 和 tiering 的空间放大主要是由于在最大级别上存在过时的条目。 Mapping the Design Space to the Trade-Oﬀ Space：更新成本与查找和空间放大成本之间存在一种内在的权衡。如下图实线绘制了在y轴上查找和空间放大的不同成本，以及在x轴上更新的成本(当我们改变大小比例时)。当大小比例设置为其限制值TlimT_{lim}Tlim​(意味着存储中只有一个级别)时，tiered 的 LSM-tree 退化为日志，而 leveled 的 LSM-tree 退化为排序的数组。当尺寸比设置为其下限2时，随着 level 和 tiering 的行为趋于一致，性能特征逐渐收敛：级别的数量是相同的，当第二个 run come in 时，每个级别都会触发合并操作。一般来说，随着 leveling/tiering 大小比例的增加，查找成本和空间放大相应 减少/增加，更新成本相应 增加/减少。因此，对权衡空间进行了分区:与分层相比，level 相比于 tiering 具有更好的查找成本和空间放大，更糟糕的更新成本。 The Holy Grail：图5中的实线反映了Monkey的属性，即当前的最先进的设计。图5 还显示了标记为“难以捉摸的最佳”的虚线。指导我们研究的问题是，其他设计是否可能通过时空权衡更接近甚至达到难以捉摸的最佳设计。 The Opportunity: Removing Superﬂuous Merging：我们已经确定了不对称性:点查找成本、长范围查找成本和空间放大主要来自最大的级别，而更新成本来自所有级别。这意味着在更小的级别上合并操作显著地放大了更新成本，同时为空间放大、点查找和远程查找带来的好处相对较小。因此，有一个合并策略的启发，在较小的层次上合并较少次数。 LAZY LEVELING, FLUID LSM-TREE, AND DOSTOEVSKY Lazy Leveling Lazy Leveling 一种合并策略，除了LSM-tree的最大级别之外，它完全消除了合并。其动机是，在这些更小的级别上合并会显著增加更新成本，同时对点查找、远程查找和空间放大产生的改进相对较小。相比于 Leveling，Lazy Leveling： improves the cost complexity of updates maintains the same complexity for point lookups, long range lookups, and space-amplifcation provides competitive performance for short range lookups. Basic Structure：Lazy Leveling 结构如下所示，其核心类似于缓和 tiering 和 leveling 两种结构，它在最大 level 上应用 leveling，在所有其他 level 上应用 tiering。结果，最大 level 的 runs 数量为 1，其他 level 的 runs 数量最多为 T−1 (即，合并操作在第 T 个 run 到达时发生)。 Bloom Filters Allocation：如何保持点查找的成本复杂性不变，尽管有更多的 rims 在较小的级别上被检索。我们通过优化不同级别之间的 BloomFilter 内存预算来做到这一点。我们开始建模点查找成本和 filter 的总体内存占用与 FPRs 有关。最坏情况下，每次查找的预期浪费I/O 数由零结果点查询造成，等于每次运行的 Bloom flters 的误阳性率之和。 Fluid LSM-Tree References [1] 知乎 - 叶提：SIGMOD'18|Dostoevsky ","link":"https://blog.shunzi.tech/post/Dostoevsky/"},{"title":"CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost","content":" FAST2020 主要是利用纠删码基于 Raft 进行优化，降低一致性开销 Abstract 一致性协议主要是在分布式系统中用于保证可靠性和可用性的，现有的一致性协议大多都是要将日志项给备份到所有的服务器中，这种全量的副本的策略在存储和网络上的开销都很大，严重影响性能，所以后来出现了纠删码，即在保证相同的容错能力的条件下减少存储和网络的开销。 RS-Paxos 是第一个支持 EC 数据的一致性协议，但是比起通用的一致性协议，如 Paxos/Raft，可用性都相对更差。我们指出了RSPaxos的活性问题，并试图解决，基于 Raft 提出了 CRaft，既能使用 EC 码像 RS-Paxos 一样降低存储和网络开销，也能保证如 Raft 一样的 liveness。 基于 CRaft 实现了一个 KVs，实验表明相对于 Raft 节省了 66% 的存储空间，写吞吐量提升了 250%，写延迟减少了 60.8% Introduction 共识算法介绍：共识协议协议通常保证安全性和活动性，这意味着它们总是返回正确的结果，并且在大多数服务器都没有发生故障的情况下可以完全正常工作。 Google’s Chubby 会使用 Paxos 对 metadata 做副本 Gaios(NSDI2011) 表明一致性协议可以被用于所有数据的 replicated 现如今大量应用如 etcd, TinyKV, FSS 等大规模系统都使用了 Raft/Paxos 来 replicated TB 数量级的数据，并提供更好的可用性 多副本介绍：数据操作通常在分布式系统中被转换为一系列的日志指令，然后使用一致性协议在所有的服务器之间进行备份，所以数据需要经过网络传输到所有的服务器，然后还要刷会到磁盘持久化保存。一致性问题中，容错率如果为 F，那么则至少需要 N = (2F + 1) 的服务器，否则就可能因为分组的原因出现不一致的情况。因此传统的副本策略往往就意味值原始数据量的 N 倍的网络和存储开销，而且随着这些协议在大规模存储系统中得到了越来越多的应用，N 倍的网络和存储开销带来的则是延迟的增加和吞吐量的下降。所以出现了 Erasure Coding 纠删码介绍：纠删码相比于全量拷贝的副本策略，极大地减小了存储和网络的开销。通过将数据进行分片，编码分片后的数据并生成一些校验的分片，原始的数据就能从足够数量的分片子集中恢复出来，这时候每个服务器只存储一个分片，而不是数据的全量拷贝，开销极大减小。FSS 中就使用了纠删码来减少存储开销，但是 FSS 在编码之前使用了一个 5 way 流水线 Paxos 来备份完整的用户数据和元数据，因此额外的网络开销还是有 4 倍数据量大小。 RS-Paxos 是第一个结合了 Paxos 和 EC 的共识协议，虽然减少了存储和网络的开销，但是在可用性上比 Paxos 还是更差，RA-Paxos 牺牲了 liveness 来使用 EC 提升性能，换句话说就是 RS-Paoxs 如果有 N = (2F + 1) 的服务器不再能容忍 F 个错误，即容错率下降了，主要是因为 RS-Paxos 中的提交要求越来越严格。 作者提出了 erasure-coding-supported version of Raft CRaft (Coded Raft)。该方案中，一个 leader 有两种方法备份日志项到 followers，如果 leader 能够和足够数量的 followers 通信，那么 leader 将使用分片后的日志项进行备份，即传统纠删码的方式，否则将备份完整的数据以保证可用性。相比于 RS-Paxos，CRaft 最大的不同是拥有和 Paxos/Raft 相同级别的 liveness，而 RS-Paxos 没有，但是两个方案都节省了网络和存储的成本。 Background Raft https://raft.github.io/ Raft 原始论文：https://raft.github.io/raft.pdf Raft 中主要有三个角色/三种状态。Candidate 收到了来自大多数 servers 的选票后成为 Leader，一个 Server 只会给 和该 Server 日志同步的 Candidate 投票。每个 Server 每一轮最多投一次，所以 Raft 保证每一轮最多就一个 leader。 Leader: 处理所有客户端交互，日志复制等，一般一次只有一个Leader。 Follower: 类似选民，完全被动 Candidate候选人: 类似Proposer，可以被选为一个新的 Leader leader 从客户端接收日志条目，并试图将它们复制到其他服务器，迫使其他服务器的日志与自己的日志一致。当 leader 发现这一轮中有日志被被分到了大多数 servers，该日志项和之前的日志将被安全地应用到状态机中。Leader 将提交并应用这些日志项，然后告诉 followers 也 apply 他们。 用于实际系统的共识协议通常具有以下特性： Safety：它们不会在所有非拜占庭条件下返回错误的结果 Liveness：只要大多数服务器都处于活动状态，并且能够相互通信和与客户端通信，它们就能完全发挥作用。我们称这组服务器是健康的 Raft 中的 Safety 是由 Leader Completeness Property 来保证的， 如果在给定 term 提交了日志条目，那么该条目将出现在所有编号较高的 term 的 leader 日志中。 Liveness 由 Raft 规则保证，通常使用了一致性协议的系统的服务器的数量常常为奇数，假设 N = 2F + 1，Raft 可以容忍 F 个错误，我们定义一个一致性协议可以容忍的失败数量作为 liveness level，所以此时的 liveness level 为 F，更高的 liveness level 意味着更好的 liveness，没有一个协议的 liveness level 可以达到 F+1，因为如果存在这样的协议，则可能存在两个分裂的 F 个健康服务器组，这两个组可以分别就不同的内容达成一致，这是违反安全特性的。 Erasure Coding 擦除编码是存储系统和网络传输中容忍错误的常用技术。们已经提出了大量的编码，其中最常用的是Reed-Solomon (RS)编码。RS 码中有两个可配置的正整数参数 k 和 m，数据被分成了相同大小的 k 个分片，然后使用这 k 个原始的数据分片计算出 m 个类似的校验分片，也就是编码过程，此时总共将有 k+m 个分片，(k,m)-RS 码就意味着所有分片中的任意 k 个分片就能恢复出原始数据，这就是 RS 码的容错原理。（类似于解方程的过程） 当引入一致性协议，k + m = N，N 为服务器的总数量，存储和网络开销将被见效的全拷贝的 1/k，然而如何保证 safety 和 liveness 不容忽视 RS-Paxos RS-Paxos 是将纠删编码与 Paxos 相结合的一种 Paxos 的改革版本，可以节省存储和网络成本。在 Paxos 中，命令被完全传输。然而，在 RS-Paxos 中，命令是通过代码片段传输的。根据这一变化，服务器在 RS-Paxos 中只能存储和传输片段，从而降低了存储和网络成本。 为了保证安全性和活动性，Paxos和Raft基于以下包容-排斥原则。 ∣A∪B∣=∣A∣+∣B∣−∣A∩B∣|A∪B| = |A|+|B| −|A∩B| ∣A∪B∣=∣A∣+∣B∣−∣A∩B∣ 包含排除原则保证在两个不同的服务器组合中至少有一个服务器的数量差距，这样安全性就可以得到保证。 RS-Paxos 的想法是增加交集集的大小。具体来说，在选择了一个 (k,m)-RS 代码后，读quorum QR、写 quorum QW 和服务器数量 N 应该符合以下公式。 QR+QW−N≥kQ_R +Q_W −N ≥ k QR​+QW​−N≥k ","link":"https://blog.shunzi.tech/post/CRaft/"},{"title":"Alluxio","content":" Alluxio 简单介绍，测试报告，然后会结合一些实际体验。 Alluxio Alluxio（之前名为 Tachyon），是一个开源的具有内存级速度的虚拟分布式存储系统， 使得应用程序可以以内存级速度与任何存储系统中的数据进行交互。 源码：https://github.com/Alluxio/alluxio 论文：https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-29.pdf 架构 文档：https://docs.alluxio.io/os/user/stable/cn/Overview.html 初衷：建立底层存储和大数据计算框架之间的存储系统，为大数据应用提供一个数量级的加速，同时它还提供了通用的数据访问接口。 主要分为两层：UFS 和 Alluxio UFS：底层文件存储，该存储空间代表不受Alluxio管理的空间。 UFS存储可能来自外部文件系统，包括如HDFS或S3。 Alluxio可能连接到一个或多个UFS并在一个命名空间中统一呈现这类底层存储。 通常，UFS存储旨在相当长一段时间持久存储大量数据。 Alluxio 存储： Alluxio 做为一个分布式缓存来管理 Alluxio workers 本地存储，包括内存。这个在用户应用程序与各种底层存储之间的快速数据层带来的是显著提高的I/O性能。 Alluxio存储主要用于存储热的、暂时的数据，而不关注长期的持久性。 要管理的每个Alluxio工作节点的存储数量和类型由用户配置决定。 即使数据当前不在Alluxio存储中，通过Alluxio连接的UFS​​中的文件仍然 对Alluxio客户可见。当客户端尝试读取仅可从UFS获得的文件时数据将被复制到Alluxio存储中。 和其他常见的分布式文件系统对比： 角色 Alluxio的设计使用了单Master和多Worker的架构。从高层的概念理解，Alluxio可以被分为三个部分，Master，Worker和Client。 Master和Worker一起组成了Alluxio的服务端，它们是系统管理员维护和管理的组件。 Client通常是应用程序，如Spark或MapReduce作业，或者Alluxio的命令行用户。 以前的版本需要借助 ZooKeeper 进行高可用选主，后续的 Alluxio 自己实现了高可用机制。（注：Tachyon 为 Alluxio 旧称） Master 主从模式：主Master主要负责处理全局的系统元数据，从Master不断的读取并处理主Master写的日志。同时从Master会周期性的把所有的状态写入日志。从Master不处理任何请求。 主从之间心跳检测 主Master不会主动发起与其他组件的通信，它只是以回复请求的方式与其他组件进行通信。一个Alluxio集群只有一个主Master。 简单模式：最多只会有一个从Master，而且这个从Master不会被转换为主Maste。 高可用模式：可以有零个或者多个从Master。 当主Master异常的时候，系统会选一个从Master担任新的主Master。 Worker 类似于 OSD Alluxio的Worker负责管理分配给Alluxio的本地资源。这些资源可以是本地内存，SSD 或者硬盘，其可以由用户配置。 Alluxio的Worker以块的形式存储数据，并通过读或创建数据块的方式处理来自Client读写数据的请求。但Worker只负责这些数据块上的数据；文件到块的实际映射只会存储在Master上。 Features 全局命名空间 Alluxio通过使用透明的命名机制和挂载API来实现有效的跨不同底层存储系统的数据管理。 https://www.alluxio.io/resources/whitepapers/unified-namespace-allowing-applications-to-access-data-anywhere/ 智能多层级缓存 Alluxio支持分层存储，以便管理内存之外的其它存储类型。目前Alluxio支持这些存储类型(存储层)：MEM (内存)，SSD (固态硬盘)，HDD (硬盘驱动器) 单层/多层 区别？ 单层存储 启动时默认分配一个 ramdisk，Alluxio将在每个worker节点上默认发放一个ramdisk并占用一定比例的系统的总内存。 此ramdisk将用作分配给每个Alluxio worker的唯一存储介质。 可以显示地设置每个 Worker 的 ramdisk 大小 alluxio.worker.ramdisk.size=16GB 可以指定多个存储介质共同组成一个 level，也可以自定义添加存储介质类型 alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk,/mnt/ssd1,/mnt/ssd2 alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM,SSD,SSD 所提供的路径应该指向安装适当存储介质的本地文件系统中的路径。要启用短路操作，这些路径的权限应该允许客户端用户对该路径进行读、写和执行。例如，启动Alluxio服务的同一用户组中的客户端用户需要770权限。 在更新存储媒体之后，我们需要指出为每个存储目录分配了多少存储空间。例如，如果我们想在ramdisk上使用 16GB，在每个 SSD 上使用 100GB: alluxio.worker.tieredstore.level0.dirs.quota=16GB,100GB,100GB 多层存储 通常建议使用具有异构存储介质的单一存储层。在某些环境中，工作负载将受益于基于I/O速度的存储介质显式排序。Alluxio假设层是根据I/O性能从上到下排序的。例如，用户经常指定以下层: MEM SSD HDD 写策略：用户写新的数据块时，默认情况下会将其写入顶层存储。如果顶层没有足够的可用空间， 则会尝试下一层促成。如果在所有层上均未找到存储空间，因Alluxio的设计是易失性存储，Alluxio会释放空间来存储新写入的数据块。会基于 block annotation policies 尝试从 worker 中驱逐数据，如果不能释放出新的空间，那么该写入将会失败。 eviction model 是同步的且是代表客户端来执行空间的释放的，主要是为要写入的客户端的数据腾出一块空闲空间，这种同步模式预计不会导致性能下降，因为在 block annotation policies 下有序的一组数据块通常都是可用的。 读策略：如果数据已经存在于Alluxio中，则客户端将简单地从已存储的数据块读取数据。 如果将Alluxio配置为多层，则不一定是从顶层读取数据块， 因为数据可能已经透明地挪到更低的存储层。有两种数据读取策略：ReadType.CACHE and ReadType.CACHE_PROMOTE。 用 ReadType.CACHE_PROMOTE 读取数据将在从worker读取数据前尝试首先将数据块挪到 顶层存储。也可以将其用作为一种数据管理策略 显式地将热数据移动到更高层存储读取。 ReadType.CACHE Alluxio将块缓存到有可用空间的最高层。因此，如果该块当前位于磁盘(SSD/HDD)上，您将以磁盘速度读取该缓存块。 # configure 2 tiers in Alluxio alluxio.worker.tieredstore.levels=2 # the first (top) tier to be a memory tier alluxio.worker.tieredstore.level0.alias=MEM # defined `/mnt/ramdisk` to be the file path to the first tier alluxio.worker.tieredstore.level0.dirs.path=/mnt/ramdisk # defined MEM to be the medium type of the ramdisk directory alluxio.worker.tieredstore.level0.dirs.mediumtype=MEM # set the quota for the ramdisk to be `100GB` alluxio.worker.tieredstore.level0.dirs.quota=100GB # configure the second tier to be a hard disk tier alluxio.worker.tieredstore.level1.alias=HDD # configured 3 separate file paths for the second tier alluxio.worker.tieredstore.level1.dirs.path=/mnt/hdd1,/mnt/hdd2,/mnt/hdd3 # defined HDD to be the medium type of the second tier alluxio.worker.tieredstore.level1.dirs.mediumtype=HDD,HDD,HDD # define the quota for each of the 3 file paths of the second tier alluxio.worker.tieredstore.level1.dirs.quota=2TB,5TB,500GB Block Allocation Policies Alluxio使用块分配策略来定义如何跨多个存储目录(在同一层或不同层中)分配新块。分配策略定义将新块分配到哪个存储目录中。这是通过 worker 属性 alluxio.worker.allocate.class 配置的。 MaxFreeAllocator：从 0 层开始尝试到最低层，尝试将块分配到当前最具有可用性的存储目录。这是默认行为。 RoundRobinAllocator：从 0 层到最低层开始尝试。在每一层上，维护存储目录的循环顺序。尝试按照轮询顺序将新块分配到一个目录中，如果这不起作用，就转到下一层。 GreedyAllocator：这是 Allocator 接口的一个示例实现。它从顶层循环到最低层，尝试将新块放入可以包含该块的第一个目录中。 [Experimental] Block Allocation Review Policies 这是在Alluxio 2.4.1中增加的一个实验特性。在未来的版本中，接口可能会发生变化。 Alluxio 使用块分配审查策略来补充分配策略。与定义分配应该是什么样子的分配策略相比，分配审查过程验证分配决策，并防止那些不够好的分配决策。评审者与分配器一起工作 这是由worker属性 alluxio.worker.review.class 配置的。 ProbabilisticBufferReviewer：基于每个存储目录对应的可用的空间，概率性低拒绝把新的数据块写入对应目录的请求。这个概率由 alluxio.worker.reviewer.probabilistic.hardlimit.bytes 和 alluxio.worker.reviewer.probabilistic.softlimit.bytes 来决定。 当可用空间低于 hardlimit，默认是 64MB，新的块将被拒绝 当可用空间大于 softlimit，默认 256MB，新的数据块将不会被拒绝 当可用空间介于上下限之间时，接受新的块的写入的概率将会随着可用容量的下降而线性低下降，我们选择在目录被填满之前尽早拒绝新的块，因为当我们读取块中的新数据时，目录中的现有块会扩大大小。在每个目录中留下缓冲区可以减少 eviction 的机会。 AcceptingReviewer：此审阅者接受每个块分配。和 v2.4.1 之前的行为完全一样 Block Annotation Policies Alluxio使用块注释策略(从v2.3开始)来保持存储中块的严格顺序。Annotation策略定义了跨层块的顺序，并在以下过程中被参考: Eviction Dynamic Block Placement. 与写操作一起发生的 Eviction 操作将尝试根据块注释策略执行的顺序删除块。按注释顺序排列的最后一个块是驱逐的第一个候选者，无论它位于哪一层。 可配置对应的 Anotator 类型，alluxio.worker.block.annotator.class。有如下 annotation 实现： LRUAnnotator：根据最近最少使用的顺序注释块。这是Alluxio的默认注释器。 LRFUAnnotator：使用可配置的权重，根据最近最不常用和最不常用的顺序注释块。 如果权重完全偏向最近最少使用的，行为将与LRUAnnotator相同。 使用 alluxio.worker.block.annotator.lrfu.step.factor 和 alluxio.worker.block.annotator.lrfu.attenuation.factor 来配置。 Managing Data Replication in Alluxio Passive Replication 与许多分布式文件系统一样，Alluxio中的每个文件都包含一个或多个分布在集群中存储的存储块。默认情况下，Alluxio可以根据工作负载和存储容量自动调整不同块的复制级别。例如，当更多的客户以类型CACHE或CACHE_PROMOTE请求来读取此块时Alluxio可能会创建此特定块更多副本。当较少使用现有副本时，Alluxio可能会删除一些不常用现有副本 来为经常访问的数据征回空间(块注释策略)。 在同一文件中不同的块可能根据访问频率不同而具有不同数量副本。 默认情况下，此复制或征回决定以及相应的数据传输 对访问存储在Alluxio中数据的用户和应用程序完全透明。 Active Replication 除了动态复制调整之外，Alluxio还提供API和命令行 界面供用户明确设置文件的复制级别目标范围。 尤其是，用户可以在Alluxio中为文件配置以下两个属性: alluxio.user.file.replication.min 是此文件的最小副本数。 默认值为0，即在默认情况下，Alluxio可能会在文件变冷后从Alluxio管理空间完全删除该文件。 通过将此属性设置为正整数，Alluxio 将定期检查此文件中所有块的复制级别。当某些块 的复制数不足时，Alluxio不会删除这些块中的任何一个，而是主动创建更多 副本以恢复其复制级别。 alluxio.user.file.replication.max 是最大副本数。一旦文件该属性 设置为正整数，Alluxio将检查复制级别并删除多余的 副本。将此属性设置为-1为不设上限(默认情况)，设置为0以防止 在Alluxio中存储此文件的任何数据。注意，alluxio.user.file.replication.max 的值 必须不少于 alluxio.user.file.replication.min。 Evaluation Testing Alluxio for Memory Speed Computation on Ceph Objects https://blog.zhaw.ch/icclab/testing-alluxio-for-memory-speed-computation-on-ceph-objects/#more-12747 4th SEPTEMBER 2020 环境介绍 底层存储：Ceph mimic 6 OpenStack VMs one Ceph monitor three storage devices running Object Storage Devices (OSDs) one Ceph RADOS Gateway (RGW) node one administration node total storage size of 420GiB was spread over 7 OSD volumes attached to the three OSD nodes Alluxio 2.3， Java8 (换成 Java11 即升级 Alluxio 后会有后续提升) Spark 3.0.0 两种模式： 单 VM 运行 Alluxio 和 Spark （16vCPU，40GB of memory） 集群模式：two additional Spark and Alluxio worker nodes are configured (with 16vCPUs and 40GB of memory). 对比测试： 直接访问 Ceph RGW 和 通过 Alluxio 访问 通过 Alluxio 访问时，第一次访问文件的话，Alluxio 会将文件上传到内存中，后续的文件访问将直接命中内存，从而带来显著的性能提升。 不同文件大小： 1GB, 5GB and 10GB，记录第一层和第二次访问文件需要的时间。 平均会运行超过 10 次 然后再次启动相同的应用程序，以再次测量相同的文件访问时间。这样做的目的是展示内存中的 Alluxio 缓存如何为以后访问相同数据的应用程序带来好处。 测试结果： 如下为单节点测试测试结果，Ceph 上第二次访问该文件相比于 Alluxio 在 1GB,5GB,10GB 时的执行时间分别为 75x，111x，107x 如下为集群模式下的测试结果，所有情况的整体时间比单机的时候少了很多，Ceph 相比于 Alluxio 的第二次访问时间为 35x, 57x, 65x Accelerate And Scale Big Data AnAlytics with Alluxio And intel®optane™ persistent Memory https://www.alluxio.io/app/uploads/2020/05/Intel-Alluxio-DCPMM-Whitepaper-200507.pdf Reliability Testing TODO Install &amp; Deploy Single Server Download Download Binary: https://www.alluxio.io/download/ Choose Version. (eg. Alluxio 2.4.1 Release. 1.4GB) Tar file: tar -xzf alluxio-2.4.1-bin.tar.gz Initial Config cd alluxio-2.4.1/conf &amp;&amp; cp alluxio-site.properties.template alluxio-site.properties echo &quot;alluxio.master.hostname=localhost&quot; &gt;&gt; conf/alluxio-site.properties [Optional] If use local file system, you can specific configuration in conf files like this: echo &quot;alluxio.master.mount.table.root.ufs=/root/shunzi/Alluxio/tmp&quot; &gt;&gt; conf/alluxio-site.properties Validate env: ./bin/alluxio validateEnv local 2 Errors: ValidateRamDiskMountPrivilege ValidateHdfsVersion Start Alluxio Format journal and storage directory: ./bin/alluxio format It may throw exceptions java.nio.file.NoSuchFileException: /mnt/ramdisk/alluxioworker in log/task.log. So you need to mkdir -p /mnt/ramdisk/alluxioworker Start alluxio (with a master and a worker): ./bin/alluxio-start.sh local SudoMount Stop local server: ./bin/alluxio-stop.sh local ./bin/alluxio-stop.sh all Verify Access website http://localhost:19999 to check the master server status. Access website http://localhost:30000 to check the worker server status. For internal network, you can use reverse proxy like this: (And you can access website master http://114.116.234.136:19999 and worker http://114.116.234.136:30000) autossh -M 1999 -fNR 19999:localhost:19999 root@114.116.234.136 autossh -M 3000 -fNR 30000:localhost:30000 root@114.116.234.136 Run tests Verify run status and run test cases: ./bin/alluxio runTests The runTests command runs end-to-end tests on an Alluxio cluster to provide a comprehensive sanity check. It will generate directory /default_tests_files and use different cache policy to upload files. BASIC_CACHE_ASYNC_THROUGH BASIC_CACHE_CACHE_THROUGH BASIC_CACHE_MUST_CACHE BASIC_CACHE_PROMOTE_ASYNC_THROUGH BASIC_CACHE_PROMOTE_CACHE_THROUGH BASIC_CACHE_PROMOTE_MUST_CACHE BASIC_CACHE_PROMOTE_THROUGH BASIC_CACHE_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_ASYNC_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_CACHE_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_MUST_CACHE BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_ASYNC_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_CACHE_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_MUST_CACHE BASIC_NON_BYTE_BUFFER_CACHE_PROMOTE_THROUGH BASIC_NON_BYTE_BUFFER_CACHE_THROUGH BASIC_NON_BYTE_BUFFER_NO_CACHE_ASYNC_THROUGH BASIC_NON_BYTE_BUFFER_NO_CACHE_CACHE_THROUGH BASIC_NON_BYTE_BUFFER_NO_CACHE_MUST_CACHE BASIC_NON_BYTE_BUFFER_NO_CACHE_THROUGH BASIC_NO_CACHE_ASYNC_THROUGH BASIC_NO_CACHE_CACHE_THROUGH BASIC_NO_CACHE_MUST_CACHE BASIC_NO_CACHE_THROUGH Simple Example Upload files from local server Show fs command help: ./bin/alluxio fs List the files in Alluxio: ./bin/alluxio fs ls / Copy files from local server: ./bin/alluxio fs copyFromLocal LICENSE /LICENSE List again: ./bin/alluxio fs ls / Cat the file: ./bin/alluxio fs cat /LICENSE References [1] Alluxio 快速上手指南 [2] CSDN - Alluxio学习 [3] 知乎 - 路云飞：Alluxio 技术分析 [4] 知乎 - Alluxio 专栏 [5] 简书 - Alluxio：架构及数据流 [6] InfoQ - Alluxio在多级分布式缓存系统中的应用 [7] Alluxio 开源 AI 和大数据存储编排平台-顾荣 ","link":"https://blog.shunzi.tech/post/Alluxio/"},{"title":"From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees","content":" 该篇文章来自于 OSDI2020 From WiscKey to Bourbon: A Learned Index for Log-Structured Merge Trees 很久没看 LSM tree 的文章了，恰好这几天 OSDI2020 开了，果不其然还是有 LSM-tree 相关的，而且是和学习索引结合的，之前对 Learned Index 也是一知半解，所以拜读了一下。 这篇文章从写作和行文的角度讲写的特别的清楚，实验做的可能是我见过的最充分的，从一开始发现问题就开始用了大量实验来证明，后续的负载测试也极其丰富，回答了读者可能会有的各种各样的问题。值得一读。 Abstract Bourbon，一个利用了机器学习来提供快速查询的 LSM-tree Bourbon 使用贪婪的分段线性回归来学习 key 分布，以最小的计算代价实现快速查找，并根据成本和收益来决定何时学习是值得的。实验表明，查询性能相比于最先进的 LSMs 提升了 1.23x-1.78x Introduction 学习索引是个啥？ 作者上来先简要介绍了一波机器学习，当然是为了引出重点 学习索引，简单地讲，学习索引就是指当你查询一个 key 的时候，系统使用该索引（或者该函数）预测出你要查询的 key 所对应的位置，相比于传统的数据结构中的查找性能有比较大的提升，某些场景下可能提升可能更为明显，同时一定程度上因为不直接构建具体的数据结构节省了空间开销。基于这项工作，很多人提出了更好的模型、更好的树结构来减少对基于树的索引结构的访问和开销。 学习索引和 LSMs 能擦出啥火花？ 两者理论上的矛盾 现有的学习索引大多是基于数据库场景中的 B 树来做的，很少有人提出说将学习索引应用到 LSM-tree 上，所以作者就尝试着把学习索引的想法应用到 LSM-tree 上（LSM-tree 的应用就不再具体介绍）。那么问题来了，为什么其他人没想到说把学习索引用到 LSM-Tree 上呢？主要是因为学习索引主要针对只读设置而量身定做的，而 LSMs 则主要是针对写进行了优化。 听着很抽象？那先简单解释一下。LSM tree 是对写比较友好的，但是写操作会影响学习索引，因为学习索引通常是基于原有的数据学习出来的，现在数据都变了，那你索引肯定得需要做出相应的改变才能保证你索引的准确性，最直接的办法当然是直接再学习。 然而，作者发现 LSMs 非常适合用于学习索引，虽然写操作修改了 LSM，但树的大多数部分是不可变的；因此，学习一个预测键/值位置的函数只需要完成一次，并且只要不可变数据存在就可以使用它。然而也有别的问题，可变的键或值大小使学习预测位置的函数变得更加困难，过早地执行模型构建可能导致大量的资源浪费。 Bourbon 做了啥 作者研究了 WiscKey，得出了几条 guidelines 虽然学习 LSM 中稳定的低级别是有用的，但是学习更高级别也会带来好处，因为查找必须始终先搜索更高级别 并不是所有的文件都是相同的：一些文件即使在较低的级别也是非常短暂的：系统必须避免学习这些文件，否则会浪费资源 工作负载和数据感知非常重要：根据工作负载和数据加载方式，了解树的某些部分可能比了解其他部分更有益 Bourbon 基于 WiscKey 实现，WiscKey 原本大约 20K 行代码，Bourbon 增加了大约 5K 行，使用分段线性回归，这是一种简单但有效的模型，能够在很小的空间开销下实现快速训练(即学习)和推理(即查找)，使用文件学习:模型建立在文件之上，假设一个LSM文件一旦创建，就不会被修改。实现了一个成本效益分析器，它动态地决定是否学习一个文件，在最大化收益的同时减少了不必要的学习。 Background LSM &amp; LevelDB 如下图所示是 LevelDB 和 WiscKey 的原理示意图。具体的介绍请参考其他资料，此处不再详细展开。本文中提到的 higher level 是指存放了更新的数据的 level，lower level 是指存放了更老的数据的 level。 简要介绍查询步骤，如图所示，便于后文引入学习索引： step1. FindFiles：如果 key 在内存中的 tables 中没有找到，LevelDB 将会查找到一组候选的来自磁盘的可能包含键 k 的 sstables。最坏的情况是 k 可能出现在所有 L0 文件中(因为重叠的范围)，并在每个连续级别的一个文件中 step2. LoadIB+FB：对于每一个候选的 SSTable，其索引块和布隆过滤器块首先被加载 step3. SearchIB：对索引块进行二分查找，从而找到可能包含 k 的数据块 step4. SearchFB：查询过滤器判断 k 是否存在对应的 datablock 中 step5. ReadValue：如果 k 在对应的 datablock 中，相应地读取出对应的 value，然后查询结束。如果上一步的 filter 显示该 key 不存在或者对 datablock 查询时没有找到相应的 key，搜索操作将继续在下一个候选文件中执行 NOTICE：blocks 不一定总是从磁盘中加载出来的，index block 和 filter block，以及经常访问的 data blocks 很有可能就在内存中可以被直接访问（文件系统缓存）。 作者对于索引的步骤和数据访问的步骤做了简单区分。 本文目标即为减少索引过程中的开销。 FindFiles, SearchIB, SearchFB, and SearchDB 都是通过文件和 blocks 找到对应的键，也就是所谓的 indexing steps LoadIB+FB, LoadDB, and ReadValue 从存储中读取数据就是所谓的 data-access steps WiscKey WiscKey 是为了解决 LSM-Tree 中比较严峻的写放大问题而提出的，架构如上图所示，主要是讲 Key Value 分离，只是对 Key 使用 LSM 存储，Value 直接使用 Value Log 进行存储，因为数据量小了，写放大也就得到了缓解，同时因为比较小就可以缓存在内存中，因此一个查询操作可能最终只涉及到一次 IO 操作来读取 Value Log 上指定位置的 Value Optimizing Lookups in LSMs 因为 LSM Tree 本身结构的原因，对于 LSM Tree 的查询可能需要对多个 level 进行查询，而且 LSM Tree 本能就是以写性能见长的，在读性能方面表现较差，所以对 LSM Tree 的读操作进行优化就很有必要。 受学习索引的启发，现在有很多工作考虑使用机器学习模型来替代传统的索引结构，核心思想是针对输入训练一个模型（使用如线性回归或者神经网络的方法）从而预测出输入对应的记录子啊排序好了的数据集中的具体地址。模型可能有误差，因此预测有一个相关的误差界。在查找过程中，如果模型预测的键的位置是正确的，则返回记录;如果错误，则在错误范围内执行本地搜索。例如，如果预测的位置是 pos，而最小和最大的误差范围是 αminα_minαm​in 和 αmaxα_maxαm​ax，那么根据错误的预测，在 pos - αmminα_mminαm​min 和 pos + αmaxα_maxαm​ax 之间进行局部搜索。 学习的索引可以大大加快查找速度。直观地说，一个学习过的索引将 b 树的 O(log-n) 查找变成 O(1) 操作。根据实际的经验表明，学习索引提升 B 树的查询性能约 1.5x-3x 传统的学习索引不支持更新，因为在现有数据上学习的模型会随着修改而改变。但是，LSMs 在写密集型工作负载中的高性能很有吸引力，因为它们只按顺序执行写操作。因此提出了关键问题给：如何实现学习索引同时保证 LSM 对写性能带来的提升？ Learned Indexes: a Good Match for LSMs? Learned Indexes: Beneficial Regimes LSM-Trees 中的查询操作包含索引和数据访问两个方面的操作，如前面章节所述。优化后的索引如学习索引可以减少索引的一些步骤的开销，但是对于数据访问的开销没什么影响。在 WiscKey 中，学习索引可以减少如 FindFiles, SearchIB, and SearchDB 的开销。因此如果索引在总查找延迟中占相当大的比例，学习索引就可以显著提升查询的性能。 首先，当数据集或它的一部分缓存在内存中时，数据访问成本很低，因此索引成本就变得很重要。下图展示了在 WiscKey 中的延迟分解情况。柱状图的第一条显示了全部缓存在内存中的情况，第二条显示了数据存储在 SATA SSD 上的情况。其实第一条就相当于有缓存的情况，数据访问和索引成本对延迟的贡献几乎是相等的，优化索引部分可以将查找延迟降低约 2 倍，当不缓存数据集时，数据访问成本占主导地位，因此优化索引可能产生较小的好处，大约只有 20%。 然而，学习索引并不局限于数据缓存在内存中的场景。它们为当前流行的快速存储设备提供了优势，并且可以在正在出现的更快的设备上发挥更大的作用，如图所示随着设备的升级，即便延迟大大降低，但是索引结构的操作所占的延迟比重越来越大，如在 Optane SSD 中，索引结构的操作占据了大约 44% 的比例，因此优化索引结构的相关操作可以将性能提升约 1.8x。随着存储器件的发展，学习索引能够发挥的效果也越来越显著。 Learned Indexes with Writes 与传统索引相比，学习索引为只读分析工作负载提供了更高的查找性能。：然而，学习索引的一个主要缺点是它们不支持诸如插入和更新之类的修改，因为修改操作改变了数据的分布，所以模型就必须重新学习，对于写密集型的负载就常常需要重建模型，频繁的重建就会导致比较高的开销。 乍一看，学习索引似乎并不适合那些 LSMs 优化的写密集的负载，然而，我们观察到 LSMs 的设计很适合学习索引。我们的关键认识是，尽管更新可以改变 LSM 树的一部分，但大部分仍然是不变的。具体来说，新修改的项缓冲在内存结构中，或者存在于树的较高级别中，而稳定的数据驻留在较低级别。考虑到数据集的很大一部分位于稳定的、较低的级别，对这一部分的查找可以更快，而无需或只需进行少量的重新学习。相比之下，在更高层次的学习可能没有那么有效果，它们变化的速度更快，因此必须经常重新学习。 我们还认识到，SStable 文件的不可变特性使它们成为理想的学习单元。一旦学习之后，这些文件就再也不会被更新，因此一个模型可以一直被使用，除非该文件被替换。除此以外，SSTable 内的数据还是有序的，有序的数据就可以采用更简单的模型学习，一个级别是许多不可变文件的集合，也可以使用简单的模型作为一个整体来学习。一个级别中的数据也进行了排序：对各个sstable进行了排序，并且在sstable之间不存在重叠的键范围。 进行了一些实验来证明上述结论，实验的目标是确定一个模型在多长时间内是有用的，以及模型使用的频率。只要SSTable 文件存在，为该文件建立的模型就有用，因此，我们首先测量和分析 SSTable 的寿命。一个模型被使用的额频率将由内部查询的次数决定，因此只需要测试每个文件的内部查询次数即可，因为模型也可以基于整个 level 构建，所以作者也测试了 level 的 lifetimes，实验是基于 WiscKey 做的，但作者认为对应的实验结论也应该适用于大多数 LSM 的实现。 SSTable Lifetimes 下图 a 显示了不同层级的 SSTables 文件的平均寿命（寿命通过使用文件的创建时间和删除时间来衡量）。 较低级别的 SSTable 文件的平均寿命大于较高级别的。 在较低的写比例的负载下，即使是较高级别的文件也有相当长的生存期，但较低级别的寿命此时更长 即便随着更高的写比例导致文件的寿命下降，但是对于低级别的文件而言，寿命还是很长 图 b 展示了 5% 的写比例的情况下 L1 和 L4 的寿命的分布情况， 可以发现有的文件寿命非常短，有的文件寿命非常长。如 L1 的大约 50% 的寿命只有 2.5s，如果过了该临界值，寿命就会变得特别长，超过五分钟。而对于 L4，只有很少的文件寿命很短，大约有 2% 的寿命不超过 1s，造成该现象的原因可能是： 压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件会被立即选择用于压缩到下一个级别 压缩一个 Li 层的文件会在 Li+1 中创建一个新文件，该文件与从 Li 压缩的下一个文件有重叠的键范围 图 c 展示了不同写请求比例下 L1 和 L4 的寿命分布，规律和 5% 时大体相同。 于是乎总结出了两条 guidelines： Favor learning files at lower levels：学习索引最好用于较低层次的文件，因为这些文件的寿命通常比较长 Wait before learning a file：学习索引最好在某个文件存活的时间达到一定阈值之后才开始学习，因为有的文件寿命可能很短，即便是在一些较低层次的文件，因为存活持续了一段时间之后该文件才可能存活的比较长时间。 Number of Internal Lookups Per File 为了测试模型的使用频率，就分析了 SSTables 对应的内部查询次数。如下图所示，图 a 显示了数据集以一个随机顺序加载的情况，较高层次对应的总的内部查询次数更多，即便是很大一部分数据驻留在较低的级别上（图 (a)(ii) 则显示了查询不命中的情况），而如图(a)(iii)所示的查询命中的情况时候，低级别的文件的查询次数更多。结果表明更高级别的文件通常服务于一些不命中也就是 negative 的查询操作，虽然采用了 BloomFilter 来尽可能加速 negative lookup 的过程，但是 index block 在查询 filter block 之前还是会被查询。 同时还针对 zipfian 的负载（大多数请求都是针对一小组键的）下进行了同样的测试，结果表明大多数情况下都和随机加载的负载是相似的，除了 positive lookup，如图 (a)(iv)，在 zipfian 负载下，更高层级的文件处理更多的 positive lookups，因为负载经常访问一小组常被更新的键，因此这组键被常常存储在更高的 level 上。 图 b 显示了数据集被顺序加载（keys 按照升序或者降序的方式被插入）的情况，相比于随机加载的情况，就没有了 negative lookups，因为不同 SSTable 的键不会重叠，即便是在不同的 level 上也不会重叠，FindFiles 步骤可以直接找到可能包含该 key 的唯一的文件。因此，较低的级别提供更多的查找，并可以从学习索引中获得更多好处。 从上述的实验观察中也得到了两个 guidelines： Do not neglect files at higher levels：即便更底层次的文件寿命更长，处理的查询也更多，但是更高层次的文件在某些负载下也是可能处理很多 negative 甚至 positive 的查询请求的，因此学习索引在高层次的文件中也能让内部查询更快 Be workload- and data-aware：尽管大多数数据位于较低的级别，但如果工作负载不查找这些数据，那么学习这些级别带来的收益就很悠闲；因此，学习索引必须能够感知工作负载的情况。除此以外，数据被加载的顺序性也会影响那些层处理更多的内部查询请求，即会影响请求处理的层级分布情况，因此学习索引还需要感知数据的情况。内部查询请求次数可以同时代表工作负载和数据加载顺序，所以基于请求次数就可以动态地决定是否要学习某个文件。 Lifetime of Levels 前面章节有描述过一整个层级也是可以被学习的，所以作者测试分析了整个层级的寿命。 因为 L0 层无序，L0 层的文件可能有重叠的键范围，所以不能应用层级学习的策略。一旦一个层级被学习了之后，任何对于该层级的更新都可能导致重学习，而层级更新则是指新的 SSTable 文件在该层次被创建，或者一个已经存在的被删除，因此，一个层级的的寿命与单个 SSTable 相同或更短。在层级的粒度上进行学习的好处是不需要在单独的步骤中找到候选 SSTables，而是在查询时候模型直接输出对应的 SSTable 和文件内的偏移。 下图 a 展示了在 5% 的写比例下不同层级的文件变化情况，纵轴上的 0 表示当前时间该层级没有发生变化，此时则可以进行学习。如果大于 0 则表示该层级发生了变化，因此就需要进行重新学习。更高层次对应的文件变化频率更高。随着级别的下降，更改的文件的比例会减少，因为较低的级别在许多文件中包含大量数据 对层级文件的更改通常是突发性的。该突发通常是由压缩引起的该层级中的很多文件同时被修改，因此这些突发的时机在不同层次表现的时间基本相同。这背后的原因是，对于我们使用的数据集，级别 L0 到 L3 已经满了，因此任何在一层上的压缩都会导致级联压缩，最终在未满的 L4 级别上存放。在这些突发情况之间层级的文件基本会保持平稳不会发生变化。 但是随着写请求比例的上升，突发间隔会逐渐减小，如图 b 所示，层级的平稳周期将大幅减小。如图所示 5% 时，大约有 5 分钟的周期，在 50% 的时候，就只有大约 25s 了。 基于上述实验，又总结了一条 guidelines Do not learn levels for write-heavy workloads：当写请求比例较低的时候，学习一整个层级还算比较合适，但是写密集型的负载，因为层级寿命变得很短就可能导致很频繁的重新学习。 Summary 通过对 WiscKey 的实验分析，总结了五条 guidelines: Favor learning files at lower levels Wait before learning a file Do not neglect files at higher levels Be workload- and data-aware Do not learn levels for write-heavy workloads Bourbon Design Learning the Data 回顾学习索引的目标：预测 key 在一个有序的数据集中的位置。 本文设计了两种学习索引，对应学习的粒度不同。 File Learning：预测 key 对应的在文件内的偏移 Level Learning：预测出对应的 SSTable 文件和文件内的偏移 对于学习索引的要求：无论是学习过程还是查询过程，开销都需要很低才能真正优化整个系统。除此以外，因为优化的是磁盘上的数据结构对应的存储系统，空间的开销也需要尽可能的小。作者发现分段线性回归（PLR）能够同时满足上面的要求。PLR 的本质是用一些线段来表示有序的数据集，PLR 构造了一个有误差限制的模型，每个数据点 d 必须在范围 [dposd_{pos}dpos​ − δ, dposd_{pos}dpos​ + δ] 内，其中 dposd_{pos}dpos​ 数据集中的 d 预测的位置，δ 是提前定义好的误差限制。 为了训练 PLR 模型，Bourbon 方案使用了 Greedy-PLR 算法，一次处理一个数据点，如果数据点不能在不超出定义的误差限制的情况下被添加到当前的线段中，那么将创建一个新的线段，并将数据点添加到其中，最终 Greedy-PLR 生成了一组代表数据的线段。Greedy-PLR 的运行时间与数据点的数量呈线性关系。 一旦模型学习完成，推理就会很快。首先，找到包含键的正确线段(使用二分查找)。在该线段内，目标键的位置是通过将键与直线的斜率相乘并加上截距得到的。如果键不在预测的位置，在误差范围内进行局部查询。因此查询操作除了常数时间做局部搜索，只需要花费 O(logs)O(logs)O(logs) 的时间，其中 s 是线段的数量。PLR 的空间开销很小：每个线段只有几十个字节。 其他模型诸如 RMI，PGMIndex，splines 等可能更适合 LSMs 且提供比 PLR 更好的表现，未来可以采用这些模型来进行实现。 Supporting Variable-size Values 如果 KV 对大小相同的话，学习索引预测 KV 对的偏移量将会很容易，模型可以将 key 的预测位置乘以 KV 对的大小，从而产生最终的偏移量。但是对于许多系统而言，往往允许任意大小的 KV 对。 Bourbon 要求 Key 是固定大小的，但是 Value 可以是不固定的。作者认为这是一个合理的设计，因为大多数数据集有确定大小的 key，比如 user-id 通常有 16bytes，但是 value 的大小就不固定了。即使 keys 大小不同，可以填充使所有 keys 的大小相同。Bourbon 通过借鉴WiscKey的键值分离思想来支持可变长度的 value。 Key Value 分离的方式，Bourbon 中的 SSTables 就只会包含 key 和对应的指向 value 的指针，value 被单独维护在 value log 中，在这样的模式下，Bourbon 通过从模型中得到预测的位置，然后乘以对应的记录大小（通常是 keySize + pointerSize），从而获取要访问的 KV 对的偏移量，Value 指针用作 Value Log 的偏移量，最终从该日志读取值。 Level vs. File Learning 前面的分析表明文件的寿命比层级的寿命通常更长，特别是在写密集的负载下，也就意味着以文件的粒度进行学习可能是更好的选择。作者为了在 file 和 level 之间进行权衡测试了不同负载下对应的性能，初始化的时候都加载一个数据集并构建模型，对于只读负载，模型不需要重新学习，在混合负载中，因为数据的改变模型需要重新学习。 如下表所示，混合负载下，Level 明显不如 File，因为有稳定的写入流，系统无法对 Level 进行学习。只有 1.5% 的内部查找采用模型路径；这些查找是在加载数据之后以及初始的 Level 模型可用时执行的。作者观察到所有尝试的 66 次 level 学习都失败了因为在学习完成之前 level 已经发生了改变。由于重新学习的额外成本，level 学习的性能甚至比 50% 写操作的基线还要差.而使用 file model，大比例的查询操作都能从学习索引中获益，因此 file model 相比于基线性能有所提升。 对于读敏感的负载（ 5% 的写），尽管 level model 相比于基线有一定的提升，但还是比 file model 性能表现要差，原因还是因为重新学习的额外成本和仅作用了有限的查询操作，带来的提升有限。 只有在只读负载下，Level 学习才能带来比较大的提升，甚至比起 file learning 都提升了 10%，因此，只有只读工作负载的部署可以从层级学习中获益。 鉴于 Bourbon 的目标是在支持写操作的同时提供更快的查找，对于学习粒度来说，level 并不是一个合适的选择，所以 Bourbon 默认使用文件学习，但同时也会支持 level 学习以便适应只读负载。 Cost vs. Benefit Analyzer 因为还是有部分文件的寿命较短，对这类文件的学习可能是对资源的浪费，所以需要开销和收益的分析机制来决定是否要对某一个文件进行学习。 Wait Before Learning 从前文的 Guidelines 中了解到，需要设定一个等待时间的阈值 TwaitT_{wait}Twait​，即在学习一个文件之前，需要等待相应的时间，该系数的具体值体现了开销和收益的权衡。值太小，导致一些寿命较短的文件也被学习，引入了较大的开销，值太大导致执行大量的查询的时候，因为模型还未学习构建，导致大量的查询不能通过学习索引来进行优化，导致性能下降。 BOURBON 将 TwaitT_{wait}Twait​ 设置为学习一个文件大概所需要的时间。测试发现学习一个文件（最大 4MB）的最长时间大约为 40ms，作者保守地将 TwaitT_{wait}Twait​ 设置为 50ms To Learn a File or Not 虽然现在有了 TwaitT_{wait}Twait​，但是一个文件即便是存活了很长时间但是可能也不是特别有益的。作者实验发现更低级别的文件通常寿命更长，对于有的工作负载和数据集，他们服务的查询操作比更高级别的文件要少得多，更高级别的文件尽管寿命较短，但是在有的场景下服务了大量的 negative lookups。因此除了考虑模型对应的开销以外，还需要考虑模型可能带来的收益。如果一个模型的收益（BmodelB_{model}Bmodel​）大于构建该模型的开销（CmodelC_{model}Cmodel​）那么该模型就是有利的。 Estimating CmodelC_{model}Cmodel​ 评估开销的一种方式是假设学习过程完全是在后台完成的且不会影响系统其他部分，那么开销就为 0，如果有很多空闲的 core，学习线程可以利用它们，这样就不会干扰前台任务（工作负载的处理或者压缩过程等）。但是 Bourbon 采用了一种比较保守的办法并且假设学习线程会干扰和减慢系统的其他部分，所以，Bourbon 假设开销等于为单个文件构建 PLR 模型的时间 TbuildT_{build}Tbuild​，我们发现，这个时间与文件中的数据点数量成线性比例，因此可以通过将训练一个数据点的平均时间和该文件中包含的点的数量相乘从而得到该时间。 Estimating BmodelB_{model}Bmodel​ 评估模型带来的收益相对比较复杂，直观地说，模型为内部查找提供的好处由 Tb−TmT_b−T_mTb​−Tm​ 给出，其中 TbT_bTb​ 和 TmT_mTm​ 分别是基线和模型路径中查找的平均时间。如果一个文件在生命周期中服务了 N 个查询请求，那么该模型的净收益即为 (Tb−Tm)∗N(T_b−T_m) * N(Tb​−Tm​)∗N。作者将查询操作又划分成了 negative 和 positive，因为大多数 negative 的查询操作在 filter 处就终止了，所以最终的收益模型为：Bmodel=((Tn.b−Tn.m)∗Nn)+((Tp.b−Tp.m)∗Np)B_{model} = ((T_{n.b} −T_{n.m}) ∗N_n)+((T_{p.b} −T_{p.m}) ∗N_p)Bmodel​=((Tn.b​−Tn.m​)∗Nn​)+((Tp.b​−Tp.m​)∗Np​)。其中 NnN_nNn​ 和 NpN_pNp​ 则是 negative lookup 和 positive lookup 的数量，T 为对应分类下的时间。 如果不知道文件将执行的查找次数或查找将花费的时间，则无法计算文件的 BmodelB_{model}Bmodel​。因此分析器为了预估这些指标，维护了这些文件在他们生命周期内的统计信息，为了估计文件 F 的这些指标，分析器使用与 F 处于同一级别的其他文件的统计数据，我们只在同一层次上考虑统计数据，因为这些统计数据在不同层次上差异很大。 Bourbon 在学习一个文件之前的等待时间里，查询操作可能在 baseline 的路劲中被服务处理，Bourbon 将把该过程的处理时间作为基线的查询处理时间 Tn.bT_{n.b}Tn.b​, Tp.bT_{p.b}Tp.b​，Tn.mT_{n.m}Tn.m​ 和 Tp.mT_{p.m}Tp.m​ 将使用同一层级的其他文件的平均模型查询时间来进行估计。对于 NnN_nNn​ 和 NpN_pNp​，分析器首先获取该级别中其他文件的平均 negative 查找和 positive 查找，然后，将其按 f=s/slf = s/s_lf=s/sl​的倍数进行缩放，其中 s 是文件的大小，sls_lsl​ 是该层级的文件的平均大小。在估计上述数量的时候，Bourbon 将会过滤掉寿命较短的文件。 当模型开始引导时，分析器可能还没有足够的统计信息，所以，初始化的时候，Bourbon 以 always-learn 的模式来运行，一旦足够的统计信息收集到了之后，分析器就可以开始执行开销和收益的权衡，来判断 BmodelB_{model}Bmodel​ 和 CmodelC_{model}Cmodel​ 的大小关系来决定是否学习某一个文件。如果同时选中了多个文件进行学习，Bourbon 则把多个文件放在一个最大优先级队列中，按照 Bmodel−CmodelB_{model} - C_{model}Bmodel​−Cmodel​ 的顺序进行排序，因此优先级最高的文件对应的收益也就最大。 Bourbon: Putting it All Together 总体的流程如下图所示，主要分为两条路径：model exist 和 no model 对应的 baseline。baseline 和前文描述的 LevelDB 的检索方式基本相同，只是此处采用了 WiscKey 的方式来布局。 对于学习索引的查询方式，步骤如下： step1. FindFiles：因为使用了文件学习，所以该步骤需要执行，即找到候选的 SSTables 文件 step2. LoadIB+FB：BOURBON 加载了 filter 和 index block，这些块可能已经被缓存在内存中了 step3. Model Lookup：FB： BOURBON 在候选的 SSTables 文件中对要查询的 Key 进行检索，模型相应地输出键 k 对应的文件内偏移 pos 和误差边界 δ。然后 BOURBON 计算包含记录 pos−δ 到 pos+δ 的数据块 step4. SearchFB：首先检查该数据块对应的 filter block 来判断 k 是否存在，如果存在，则 BOURBON 计算要加载的块对应的字节范围（因为 keys 和 pointers 大小固定，计算相对简单） step5. LoadChunk：加载对应的字节范围 step6. LocateKey：键位于加载的块中，那么该 key 将位于预测的位置上（加载的 chunk 的中点）；如果不在，BOURBON 将会对该 chunk 执行二分查找 step7. ReadValue：使用相应的 Value Pointer 从 ValueLog 中读取对应的 Value Possible improvements BOURBON 少了一些 features，现有的实现中，不支持字符串 keys 以及 keys 的压缩 对于字符串键，我们计划探索的一种方法是将字符串视为base-64整数，并将它们转换为64位整数，然后可以采用本文描述的相同学习方法。虽然这种方法可以很好地用于小 keys，但是大 keys 可能需要更大的整数(大于64位)，因此高效的大整数数学可能是必不可少的。 BOURBON 暂时不支持 level 和 file model 的切换，目前只是一个静态配置。 Evaluation 测试之前抛出了几个关键问题： BOURBON 究竟优化了查询的哪些部分？ BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？ BOURBON 范围查询表现如何？ 在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？ BOURBON 在真实负载下的性能也能表现得和预期一致吗？ 数据存储在存储设备上时（不再在内存） BOURBON 是否有用？ 有限的内存的情况下，BOURBON 是否有用？ BOURBON 在误差范围和空间开销上的权衡是怎么样的？ 测试环境 硬件环境 20-core Intel Xeon CPU E5-2660 160-GB memory a 480GB SATA SSD 系统参数 16B integer keys and 64B values error bound - 8 Unless specified, our workloads perform 10M operations. 负载 构造了四个合成数据集，64M key-value pairs linear, 键都是连续的 segmented-1%, 在连续的100个键之后有一个间隙 segmented-10% , 在10个连续的键之后会有一个间隙 normal，从标准正态分布 N(0,1) 中抽样 64M 个唯一值，并按比例缩放到整数 真实负载：Amazon reviews (AR) &amp; New York OpenStreetMaps (OSM) 测试结果 BOURBON 究竟优化了查询的哪些部分？ 减少了索引花费的时间。图中标记为 Search 的部分对应基线中的 SearchIB 和 SearchDB 还降低了数据访问成本，因为 BOURBON 加载的字节范围比基线加载的整个块要小。 BOURBON 在有模型但没有写的情况下表现如何？性能会随着数据集、加载顺序以及请求分布的变化发生什么样的变化？ 无论哪种情况 BOURBON 都可以提供显著的加速 对于所有数据集， BOURBON 比 WiscKey 更快；根据数据集的不同，提升也不同(1.23倍到1.78倍)。 BOURBON 对线性数据集提升最大，因为它有最小的片段数(每个模型一个)；使用更少的段，找到目标需要检索的段也就更少。 延迟随着段数的增加而增加 level learning 适用于只读负载，BOURBON-level 比基线快1.33 - 1.92倍，比 BOURBON 更好，因为 level 学习查找对应的 SSTables 更快。 由于 level 模型只对只读工作负载提供好处，并且与文件模型相比最多提高10%，所以后续测试主要针对 file learning。 负载加载顺序的差异，使用顺序加载，sstable 甚至在不同的级别上都不会有重叠的键范围;然而，在随机加载的情况下，某个级别的 sstable 可能会与其他级别的 sstable 重叠。 无论负载顺序如何，BOURBON 都比基线有显著的优势（1.47× – 1.61×）。 与顺序加载情况相比，随机加载情况下的平均查找延迟有所增加。这是因为，虽然在顺序情况下没有 negative 的内部查找（10M），但在随机情况下有很多 negative 的查找（23M） 在随机情况下，对基线的加速比顺序情况下要小。虽然 BOURBON 同时优化了 positive 查找和 negative 查找，但 negative 查找的收益较小，因为 negative 的查询路径比较短，在 filter 处可能就终止了，没有加载或搜索数据块，而且 negative 的查询比 positive 的查询多。 请求的分布情况，测试了六种请求分布下的延迟，sequential, zipfian, hotspot, exponential, uniform, and latest。 BOURBON 使查找速度比基线快1.54倍- 1.76倍。总的来说，无论请求分布如何，BOURBON 都减少了延迟。 BOURBON 范围查询表现如何？ 如图展示了 BOURBON 的吞吐量标准化到了 WiscKey 之后的结果。对于较短的范围，索引开销(即查找范围的第一个键的开销)占主导地位，BOURBON 优化效果比较明显，但随着范围的增大，其效果就不再那么明显，这是因为BOURBON 可以加速索引部分，但它遵循与 WiscKey 类似的路径来扫描后续键。因此，在大范围查询时，索引查询占较少的总性能，性能提升就不明显了。 在有写的情况下，BOURBON 的开销-收益分析器和其他从不重新学习的方法相比表现如何？ 首先了解三种策略： BOURBON-offline 在写发生时不执行学习，模型仅针对最初加载的数据而存在 BOURBON-always 只要写发生了就会重新学习，不考虑成本 BOURBON-cba 会使用开销-收益分析器来决定是否学习 图 a 显示了在前端查询和插入花费的时间，图 b 显示了学习过程花费的时间，图 c 显示了总的时间开销，图 d 显示了采用基线路径的内部查找的比例 结果表明： 所有的策略都相比于 WiscKey 减少了前台任务的时间开销，随着写比例的增加，查询比例的减小，优化的效果就减弱。offline 的策略表现最差，即便是在只有 1% 的写的情况下，这时候大多都是通过基线的索引查询方式（如图 d 所示的比例）来进行，所以对于数据改变之后的重新学习十分关键 BOURBON-always 在前台任务的时间开销上表现最好，几乎不会退化成基线的查询方式，但是对应的学习时间就特别长，在 50% 时候就大概花费 134s 进行学习，所以总的时间开销当写请求的比例较大的时候，可能比基线的时间开销还大 写请求比例较低的时候，BOURBON-cba 几乎会学习所有的文件，所以此时和 always 的表现比较相近，当写请求比例增大时，BOURBON-cba 不再学习特别多的文件，学习时间开销只有 13.9s，相比于 always 大大减小，因此这时候的很多查询都是按照基线对应的路径，因为此时数据变化迅速，查找次数较少，学习的收益较小。 总结：积极学习策略提供快速查找，但代价高昂；没有 re-learning 的话几乎不能加快速度。也不理想。相比之下，BOURBON 提供了与积极学习类似的高收益，同时显著降低了总成本 BOURBON 在真实负载下的性能也能表现得和预期一致吗？ YCSB：BOURBON 提高了读操作的性能;同时，波旁威士忌并不影响写的表现。 SOSD 数据存储在存储设备上时（不再在内存） BOURBON 是否有用？ 即使数据存在存储设备上，BOURBON 也有一定程度的提升(查询速度比WiscKey快1.25倍到1.28倍)。 有限的内存的情况下，BOURBON 是否有用？ 如下表所示，使用了 SATA SSD 和大小只有数据量的 25% 的内存，BOURBON 速度只有 WiscKey 的 1.04 倍，因为大部分时间都花在了将数据加载到内存上。 相比之下，在 zipfian 工作负载中，索引时间（而不是数据访问时间）占主导地位，因为大量请求访问已经缓存在内存中的一小部分数据，所以能够提供 1.25x 的加速和低得多的延迟。 BOURBON 在误差范围和空间开销上的权衡是怎么样的？ 随着误差范围增大，更少的线段被创建，从而需要检索的线段就更少，延迟也就相应减小，然而当 δ 超过 8，尽管需要检索的分段更少，但是延迟增加了，对于其他数据集也是 δ = 8 是转折点。还显示了空间开销，因为创建的线段更少了，空间开销也就更小了。 对于各种数据集，开销与数据集的总大小相比是很小的(0% - 2%)。 Related Work Learned Index: XIndex FITingTree AIDEL Alex SageDB LSM optimizations: Monkey Dostoevsky HyperLevelDB bLSM cLSM RocksDB Model choices: Greedy-PLR Neural networks one-pass learning algorithm based on splines ","link":"https://blog.shunzi.tech/post/osdi-Bourbon/"},{"title":"intelligent cache research","content":" 本篇为智能缓存相关的研究调研，可能涉及 AI for System 以及相关缓存策略的设计 Smart SSD Cache 智能缓存顾名思义为不同的负载下提供相应的缓存策略来保证缓存的高效，主要包含多种缓存策略执行和 IO 负载的捕捉两个方面，同时可能结合不同的缓存层级需要进行动态调整。 缓存策略本身很难有普适的，现有做法和方案常常需要根据 IO 负载来进行决策，而对于 IO 负载的建模往往采用统计或者机器学习的方法 OceanStor V5 系列 V500R007 SmartCache 参考 https://support.huawei.com/enterprise/zh/doc/EDOC1000181455/1064ba78 定义 华为技术有限公司开发的 SmartCache 特性又叫智能数据缓存特性。 缓存池化：利用 SSD 盘对随机小I/O读取速度快的特点，将 SSD 盘组成智能缓存池，将访问频率高的随机小I/O读热点数据从传统的机械硬盘移动到由 SSD 盘组成的高速智能缓存池中。由于 SSD 盘的数据读取速度远远高于机械硬盘，所以 SmartCache 特性可以缩短热点数据的响应时间，从而提升系统的性能。 多粒度：SmartCache 将智能缓存池划分成多个分区，为业务提供细粒度的SSD缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。从而向关键应用提供更多的缓存资源，保障关键应用的性能。 场景 SmartCache特性对LUN（块业务）和文件系统（文件业务）均有效。 SmartCache特性可以提高业务的读性能。尤其是存在热点数据，且读操作多于写操作的随机小I/O业务场景。例如：OLTP（Online Transaction Processing ）应用、数据库、Web服务、文件服务应用等。 原理 SmartCache特性在对SSD盘资源进行管理上，分为智能缓存池和SmartCache分区两部分。 智能缓存池 智能缓存池管理本控制器的所有SSD盘，用以保证每个智能缓存分区的资源来自不同SSD盘，从而避免不同SSD盘负载不均衡。 存储系统默认在每个控制器上生成一个智能缓存池。 读流程 SSD Cache 缓存命中 SmartCache读未命中 当从 HDD 读取到 RAM Cache 后，RAM Cache 将数据返回给应用服务器，同时 RAM Cache 将该数据同步到智能缓存池中。当智能缓存池容量不够时，则智能缓存池根据时间顺序释放旧数据，释放数据内存，完成旧数据的淘汰。 SmartCache分区 SmartCache分区负责为业务提供细粒度（4KB、8KB、16KB、32KB、64KB、128KB，与前端I/O自适应，即根据前端下发的I/O大小申请不同粒度的SSD缓存资源）的SSD缓存资源。 每两个控制器创建一个默认的SmartCache分区。除了默认分区外，每两个控制器最多支持创建8个用户自定义分区。 通过SmartCache分区调控，各业务独立使用所分配的SmartCache分区，避免不同类型应用之间的相互影响，保障存储系统整体的服务质量。可以通过设置SmartCache大小，实现不同业务与性能的最佳匹配。通过限制非关键应用的缓存资源，向关键应用提供更多的缓存资源，保障关键应用的性能。 SmartCache分区负责为业务提供细粒度的SSD盘缓存资源，不同的业务可以共享同一个分区，也可以分别使用不同的分区，各个分区之间互不影响。 Machine Learning DATE20 - A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage 发现：作者在腾讯云的云存储环境中（主要是块存储系统），测试统计发现大约有 47.9% 的写操作是 write-only 的，即在某一个确定的时间窗口内所写的这些块不会再次被访问。那么这些 write-only 的写操作对应的数据放置在缓存中其实并不会带来性能的提升，反而会占据缓存容量，影响其他真正需要缓存的数据来进行缓存。 问题：现有的写策略大致分为两种： 将所有要写的数据加载到缓存中（write-back 和 write through） write-back：在数据更新时只写入缓存Cache。只在数据被替换出缓存时，被修改的缓存数据才会被写到后端存储。此模式的优点是数据写入速度快，因为不需要写存储；缺点是一旦更新后的数据未被写入存储时出现系统掉电的情况，数据将无法找回。 Write-through（直写模式）在数据更新时，同时写入缓存Cache和后端存储。此模式的优点是操作简单；缺点是因为数据修改需要同时写入存储，数据写入速度较慢。 不加载数据到缓存，直接写存储设备 （write-around） 方案：提出了一种基于机器学习的方法来识别 write-only 数据和 normal 数据，针对不同类型的数据动态应用不同的写策略。最大的挑战是怎么样才能够实时地区分数据类型 使用了五种监督学习方法：Naive Bayes, Logistic Regression, Decision Tree, AdaBoost, and Random Forest。随机森林准确率最高，但是耗时最长，不满足实时性，朴素贝叶斯在准确率、召回率和预测时间上基本做到了 trade-off，所以最终选择了朴素贝叶斯。 数据指标的选择：不同于在文件级别或者片上系统的基于机器学习的优化，在块级系统中部署机器学习算法来分类不同的数据面临很大的障碍，因为块级能提供的信息是有限的，常用的信息包括时间特征和空间特征（如最近访问时间和最近访问的地址）。本文中我们扩展了数据指标到 IO 请求，收集了如 average write size, write request ratio 等，这是由于具有类似请求级别的数据往往表现出类似的访问特征 Temporal features：这些特性包括数据块的访问近因和时间间隔。 Last Access Timestamp 在本文中被定义成了当前时间和最近访问该块的时间间隔。 Average Re-access Time Difference：对一个数据块进行两个相邻访问的时间间隔 Spatial features：这些特性包含地址信息，例如卷 ID、偏移量等 因为地址会随着时间不断变化，对算法的效果会有显著影响，在作者实现的算法中其实没有使用空间特征。 Request-level features Average Request Size 平均请求大小，单位 KB，上界为 100KB Big Request Ratio 大请求比例，请求大小大于 64KB 即为大请求 Small Request Ratio 小请求比例，请求小于 8KB 即为小请求 Write Request Ratio 写请求的比例 统计粒度： 块设备的最小粒度为一个块，假设一个块为 8KB，如果我们为每一个块都统计对应的数据特征，因为块设备容量很大，那么统计这些特征就会造成巨大的开销，同时太大可能影响算法的准确率，所以实际应用过程中，作者采用了 1MB 为统计粒度（称之为 tablet），1MB 连续的数据块中所包含的最小物理块对应的数据特征相同，从而减少统计的开销。 整个模型的工作流程如下： 写请求的所有数据被首先被写入到内存中并顺序地记录在日志文件中，然后当脏数据的比例达到阈值则刷回到后端存储服务器，当在内存写缓冲中要执行对应的刷回操作时，进行下一步 分类器获取最近刷回的数据块的 tablet 特征并预测其写请求类型，如果是 write-only 跳转到下一步，如果不是（即为 normal data）跳转到第四步 如果该数据位于 SSD Cache 中且为脏数据，那么首先将该脏数据刷回到 HDD。对应的数据块将直接在 HDD 上进行写 数据块首先写入到 SSD 缓存然后使用 write-back 的策略异步刷回 HDD 分类器每天训练一次用于第二天的预测，系统运行过程中收集样本数据，当 SSD cache 发生 eviction 的时候将增加一个样本，如果 evicted-data 有一次或者多次读命中，该样本将被设置为 0，也就是 normal data，否则设置为 1，write-only data。在作者的想法中，真实的实现里，write-only 的数据块是指一个在从缓存中 evict 之前不会被读的块 效果：实验结果表明，与业界广泛部署的回写策略相比，ML-WP 减少了对 SSD 缓存的写流量 41.52%，同时提高了 2.61% 的命中率，降低了 37.52% 的平均读延迟 Related Work 写策略优化 ATC15 - Request-oriented durable write caching for application performance 分析IO工作负载（判断是否会发生争用），然后部署最合适的写策略，使用了固定的写策略 Cache Optimization Based on Machine Learning: MICRO16 Perceptron learning for reuse prediction 感知器学习用于判断 last-level cache 的重用预测 （神经网络做预测） 通过使用多个能够展示程序和存储器行为的特征（七个参数化特征）来从多个视角对LLC中的缓存块的未来重用进行预测。利用预测的结果，设计三种cache的管理策略：block placement，replacement，bypass PC, address, reference count, etc ICPP18 Efficient ssd caching by avoiding unnecessary writes using machine learning 使用机器学习（决策树）来进行一次访问排除，能够准确地识别和过滤一次访问的照片，并阻止它们写入cache，提高社交网络照片缓存服务的效率。更多的使用照片的相关数据以及用户的登录请求数据来作为数据集 使用 reaccess distance 来定义是否为只访问一次的图像，被定义为从该图像进入cache 开始到下一次被访问之间的总的图像访问量 命中率提高了17%，缓存写操作降低了79%，平均访问延迟降低了 7.5% NSDI19 Flashield: a hybrid key-value cache that controls flash write amplification 由于键值更新会对 SSD 造成严重的有害的小随机I/O写操作，因此使用机器学习（支持向量机 SVM）的方法来选择期望读取频率更高的对象 Flashield这里引入了Flashiness的概念，作为一个对象被Cache价值的一个评价指标，一个高Cache价值的对象要满足两点条件 一个对象在访问之后会在不远的将来访问n次(及以上)，这里的n作为参数定义， 在将来的一段时间内不会被修改 New Cache Policy TPDS20 - Efficient SSD Cache for Cloud Block Storage via Leveraging Block Reuse Distances 发现：我们在一个典型云块存储的服务器端发现了这一点，有很大比例的块具有很大的重用距离，这意味着很多块只有在遥远的将来才会被重新引用。在这样的场景中，如果在每次 miss 时进行简单的替换，新访问的块会污染缓存，而不会给命中率带来任何好处，从而降低缓存效率。现有的缓存算法在存在较大的重用距离时，缓存效率往往不理想 如下图所示，A 点意味着 CBS 访问中，有大约 50% 的块的重用距离大于 32GB，而对于内存和主机上的 IO 则要小得多 因此，如果我们使用像LRU这样的缓存算法，当缓存大小等于或小于 32gb 时，50% 的重用块不会被命中 现象分析： 多数具有小重用距离的数据块已经被客户端缓存设备缓存和过滤。 单个云磁盘的容量可以达到几十TB，这比传统磁盘高出几个数量级。因此，数据规模较大的云应用程序可能导致更大的重用距离 在多租户共享云环境中，来自不同租户的访问会混合在一起，导致重用距离增加 贡献：我们提出了一种新的缓存算法，专为 CBS 和其他类似场景设计的 LEA，LEA在缓存未命中时默认不进行替换，除非满足某些(惰性)条件。条件包含两个方面：新访问块的频率 和 缓存中的候选逐出块的值（该值是指该块对缓存的重要性或有用性）。这样，块的缓存持续时间可以大大延长。更重要的是，可以大大减少对 SSD 的写操作，延长 SSD 的生命周期 Reuse Distance：数据块的重用距离定义为对同一块的两个连续引用之间的唯一数据量。例如 1-2-4-5-4-3-3-2 的块访问序列中，Block 2 的 reuse distance 就是三个块的大小，在有的研究中，Block 2 的 reuse distance 为 5 个块的大小。本文采用了第二种定义。 传统的 LRU 当缓存空间满时，对每次miss进行替换，主要遵循时间局部性原则，这些缓存替换算法的主要目标是保留重用距离较小的块，驱逐重用距离较大的块。当大部分数据重用距离小于缓存大小时，这些算法可以有效地工作。 LEA Algorithm 维护两个队列，Lazy Eviction List (LEL) 和 Block Identity List (BIL)。每个队列有两个端点 Insertion Point (IP) 和 Eviction Point (EP) 标识只包括块的卷号和地址信息(偏移量)。一个条目包含额外的块信息，如 last_access, reuse_distance, age, flag，等等。条目中的信息用于计算条目块的值。 age 表示块的当前时间和最后一次访问时间(last_access)之间的时间差，如果 age 小于 reuse_distance，它表明该块对缓存具有较高的价值。这里的时间是逻辑时间，当一个新的块请求到来时，逻辑时间增加1。这里，重用距离是最后两次访问之间的时间(而不是访问之间的平均时间)。 flag 也用来判断块的值，当块在 LEL 列表中命中时增加 1，当块被视为块驱逐候选，但由于懒惰替换还没有被驱逐时减少一半，如果 flag 大于 0，它表明该块对于缓存是有价值的。 当发生缓存 miss 时，LEA 不进行替换，只在满足延迟条件时将丢失的块标识插入 BIL 列表，否则，它将执行替换并将丢失的块条目插入到 LEL 列表中。如果在 BIL 列表中命中了一个块，那么它进入LEL列表的概率更高。通过这样做，间接地考虑了访问频率 ATC20 - OSCA: An Online-Model Based Cache Allocation Scheme in Cloud Block Storage Systems OSCA 可以在非常低的复杂度下找到接近最佳的配置方案，从而提高缓存服务器的总体效率 部署了一个新的缓存模型来获得云基础设施块存储系统中每个存储节点的 miss ratio curve (MRC)。使用一种低开销的方法来获得一个时间窗口内从重新访问流量与总流量之比的数据重用距离。然后将得到的重用距离分布转化为 miss ratio curve (MRC)。 通过了解存储节点的缓存需求，将总命中流量指标定义为优化目标 使用动态规划方法搜索接近最优的配置，并基于此解进行缓存重新分配 在实际工作负载下的实验结果表明，模型达到了一个平均值绝对误差(MAE)，可与现有的最先进的技术相媲美，但同时可以做到没有跟踪收集和处理的开销。由于命中率的提高，相对于在相同缓存内存的情况下对所有实例的等分配策略，OSCA 减少了到后端存储服务器的 IO 流量 13.2% ","link":"https://blog.shunzi.tech/post/intelligent-cache-research/"},{"title":"Ceph FS 介绍和使用","content":" 一个项目测试使用到了 CephFS，故简要整理 CephFS 资料和相关文档 CephFS Ceph 常用命令 Deploy a Ceph Cluster Manually Overview CephFS 应用相比于 RBD/RGW 不够广泛主要是因为文件系统采用树状结构管理数据（文件和目录）、基于查表寻址的设计理念与 Ceph 扁平化的数据管理方式、基于计算进行寻址的设计理念有些违背；其次文件系统的支持常常需要集中的元数据管理服务器来作为树状结构的统一入口，这又与 Ceph 去中心化、追求近乎无限的横向扩展能力的设计思想冲突。 由于分布式文件系统的需求仍旧很大，应用场景尤为广泛，在 Ceph 不断的版本迭代中，CephFS 也取得了越来越好的支持。 背景 要想实现分布式文件系统，那么就必须实现分布式文件系统的特点，即具有良好的横向扩展性，性能能够随着存储规模呈线性增长，为了实现这样的目标则需要对文件系统命名空间分而治之，即实现相应的负荷分担和负载均衡，采用相应的数据路由算法。 文件系统数据负载均衡分区 静态子树分区 手工分区，数据直接分配到某个固定的服务节点，负载不均衡时再手动调整。 HASH 计算分区 HASH 计算数据的存储位置，保证了数据分布的均衡，但如果环境变化（集群规模变化）此时需要固定原有的数据分区而减少数据的迁移，或者根据元数据的访问频率，要想保证 MDS 负载均衡，需要重新决定元数据的分布，此时则不适合使用 HASH 动态子树分区 通过实时监控集群节点的负载，动态调整子树分布于不同的节点。这种方式适合各种异常场景，能根据负载的情况，动态的调整数据分布，不过如果大量数据的迁移肯定会导致业务抖动，影响性能。在元数据存储、流量控制和灵活的资源利用策略方面，动态分区比其他技术有许多优势。 https://ceph.com/wp-content/uploads/2016/08/weil-mds-sc04.pdf 动态子树分区方法的核心是将文件系统作为层次结构处理。通过将层次结构的子树的权限委托给不同的元数据服务器，对文件系统进行分区。委托可以嵌套:例如，/usr可以分配给一个MDS，而/usr/local可以分配给另一个MDS。但是，在没有显式分配子树的情况下，嵌套在某个点下的整个目录树被假定驻留在同一台服务器上。 这个结构中隐含着层次结构遍历的过程，以便找到并打开嵌套的索引节点，以便随后下降到文件层次结构中。这样的路径遍历对于验证POSIX语义所要求的嵌套项的用户访问权限也是必要的，对于在目录层次结构深处定位一个文件来说，这个过程可能代价很高。 为了允许有效地处理客户机请求(以及正确响应它们所需的路径遍历)，每个MDS都缓存缓存中所有项的前缀索引节点，以便在任何时候缓存的层次结构子集仍然是树结构。也就是说，只有叶子项可以从缓存中过期;在目录中包含的项首先过期之前，不能删除目录。这允许对所有已知项进行权限验证，而不需要任何额外的I/O成本，并保持层次一致性。 为了适应文件系统发展和工作负载变化的要求，MDS集群必须调整目录分区，以保持工作负载的最佳分布。动态分布是必要的，因为层次结构部分的大小和流行度都以一种不均匀和不可预测的方式随时间变化。通过允许MDS节点传输目录层次结构的子树的权限，元数据分区会随着时间的推移进行修改。MDS节点定期交换心跳消息，其中包括对其当前负载级别的描述。此时，忙碌的节点可以识别层次结构中适当流行的部分，并发起一个双重提交事务，将权限传递给非繁忙节点。在此交换过程中，所有活动状态和缓存的元数据都被转移到新的权威节点，这既是为了保持一致性，也是为了避免磁盘I/O，否则，新权威节点将需要磁盘I/O来重新读取它，而磁盘I/O会慢上几个数量级。 CephFS MDS 特点 采用多实例消除性能瓶颈并提升可靠性 采用大型日志文件和延迟删除日志机制提升元数据读写性能 讲 Inode 内嵌至 Dentry 中来提升文件索引率 采用目录分片重新定义命名空间层次结构，并且目录分片可以在 MDS 实例之间动态迁移，从而实现细粒度的流控和负载均衡机制 架构 虽然 Ceph 文件系统中的 inode 数据存储在 RADOS 中并由客户端直接访问，但是 inode 元数据和目录信息由Ceph metadata server (MDS)管理。MDS 充当所有与元数据相关的活动的中介，将结果信息存储在与文件数据不同的RADOS 池中。 CephFS中的所有文件数据都存储为RADOS对象。CephFS客户端可以直接访问RADOS对文件数据进行操作。MDS只处理元数据操作。 要读/写CephFS文件，客户端需要有相应inode的“文件读/写”功能。如果客户端没有需要的功能 caps，它发送一个“cap消息”给MDS，告诉MDS它想要什么。MDS将在可能的情况下向客户发布功能 caps。一旦客户端有了“文件读/写”功能，它就可以直接访问 RADOS 来读/写文件数据。文件数据以 , 的形式存储为RADOS对象。如果文件只由一个客户端打开，MDS还会向唯一的客户端提供“文件缓存/缓冲区”功能。“文件缓存”功能意味着客户端缓存可以满足文件读取要求。“文件缓冲区”功能意味着可以在客户端缓存中缓冲文件写。 CepgFS Client 访问示例 Client 发送 open file 请求给 MDS MDS 返回 file node, file size, capability 和 stripe 信息 Client 直接 READ/WRITE 数据到 OSDs（如果无 caps 信息需要先向 MDS 请求 caps） MDS 管理 Client 对该 file 的 capabilities Client 发送 close file 请求给 MDS，释放 file 的 capabilities，更新 file 的详细信息 MDS 文件锁 当客户机希望在 inode 上操作时，它将以各种方式查询 MDS，然后授予客户机一组功能。它们授予客户端以各种方式操作 inode 的权限。与其他网络文件系统(例如 NFS 或 SMB)的主要区别之一是，所授予的功能非常细粒度，多个客户机可能在同一个 inode 上拥有不同的功能。 CephFS 客户机可以请求MDS代表它获取或更改 inode 元数据，但是MDS还可以为每个 inode 授予客户机功能 (caps) /* generic cap bits */ #define CEPH_CAP_GSHARED 1 /* client can reads (s) */ #define CEPH_CAP_GEXCL 2 /* client can read and update (x) */ #define CEPH_CAP_GCACHE 4 /* (file) client can cache reads (c) */ #define CEPH_CAP_GRD 8 /* (file) client can read (r) */ #define CEPH_CAP_GWR 16 /* (file) client can write (w) */ #define CEPH_CAP_GBUFFER 32 /* (file) client can buffer writes (b) */ #define CEPH_CAP_GWREXTEND 64 /* (file) client can extend EOF (a) */ #define CEPH_CAP_GLAZYIO 128 /* (file) client can perform lazy io (l) */ 然后通过特定数量的位进行移位。这些表示 inode 的数据或元数据的一部分，在这些数据或元数据上被授予能力: /* per-lock shift */ #define CEPH_CAP_SAUTH 2 /* A */ #define CEPH_CAP_SLINK 4 /* L */ #define CEPH_CAP_SXATTR 6 /* X */ #define CEPH_CAP_SFILE 8 /* F */ 一个 Cap 授予客户端 缓存和操作与 inode 关联的部分数据或元数据的能力。当另一个客户机需要访问相同的信息时，MDS 将撤销该 cap，而客户机最终将返回该功能，以及 inode 元数据的更新版本(如果它在保留功能时对其进行了更改)。 客户机可以请求 cap，并且通常会获得这些 cap，但是当 MDS 面临竞争访问或内存压力时，这些 cap 可能会被 revoke。当一个 cap 被 revoke 时，客户端负责尽快返回它。未能及时这样做的客户端可能最终被阻塞并无法与集群通信。 由于缓存是分布式的，所以 MDS 必须非常小心，以确保没有客户机拥有可能与其他客户机的 cap 或它自己执行的操作发生冲突的 cap。这使得 cephfs 客户机比 NFS 这样的文件系统依赖于更大的缓存一致性，在 NFS 中，客户机可以缓存数据和元数据，而这些数据和元数据在服务器上已经更改了。 基于 caps，构建了 ceph 的分布式文件锁，分布式文件锁保证了多个客户端并发且细粒度访问同一文件、目录、文件系统，同时保证一致性、可靠性。ceph 实现的分布式文件系统锁，客户端可见部分是 caps，服务端可见部分包括 caps和各种 lock，每个类型的 lock 又有多种状态，根据客户端的请求、持有、释放情况，lock 转换自身状态，并和客户端同步 caps 信息。最终实现分布式锁的访问。 参考链接 [1] CephFS 介绍及使用经验分享 ","link":"https://blog.shunzi.tech/post/CephFS/"},{"title":"FAST20 Some Interesting Papers Overview","content":" FAST20 Overview 最近进入了一个怠惰期，一是苦于没有明确的研究方向，二是沉湎于琐碎小事，所以科研上很多事情都被搁置 遇到了问题就总得寻找解决的办法，所以决定还是好好看几篇论文，看看能不能找到感兴趣的点，再深入挖掘 最近也没太多新的会议，故还是先好好看看 FAST20 上的文章，对一些可能感兴趣的文章做些简单记录 FAST20 此次会议主要分为了 Cloud Storage、File Systems、HPC Storage、SSD and Reliability、Performance、Key Value Storage、Caching 和 Consistency and Reliability 几个 Topic。本篇文章不会对所有的 Topic 下的所有论文都进行总结记录，譬如 HPC Storage 由于缺乏一定的了解就没有在此处记录，针对每个 Topic 下的文章也只是选择了自己有一定的了解或者感兴趣的几篇来简单地记录。 有的文章会详细展开，有的因为受限于我自己的水平无法深入，故只能泛泛而谈，但会尽力尝试去理解所要解决的问题以及解决问题的方式。 本篇博文会持续记录更新，因为阅读量还挺大的，遇到比较好的感兴趣的论文会单独开一篇进行详细的理解记录。 Cloud Storage 云存储主题下 FAST20 有三篇： MAPX: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems Li Wang, Didi Chuxing; Yiming Zhang, NiceX Lab, NUDT; Jiawei Xu and Guangtao Xue, SJTU Lock-Free Collaboration Support for Cloud Storage Services with Operation Inference and Transformation Jian Chen, Minghao Zhao, and Zhenhua Li, Tsinghua University; Ennan Zhai, Alibaba Group Inc.; Feng Qian, University of Minnesota - Twin Cities; Hongyi Chen, Tsinghua University; Yunhao Liu, Michigan State University &amp; Tsinghua University; Tianyin Xu, University of Illinois Urbana-Champaign POLARDB Meets Computational Storage: Efficiently Support Analytical Workloads in Cloud-Native Relational Database Wei Cao, Alibaba; Yang Liu, ScaleFlux; Zhushi Cheng, Alibaba; Ning Zheng, ScaleFlux; Wei Li and Wenjie Wu, Alibaba; Linqiang Ouyang, ScaleFlux; Peng Wang and Yijing Wang, Alibaba; Ray Kuan, ScaleFlux; Zhenjun Liu and Feng Zhu, Alibaba; Tong Zhang, ScaleFlux 云存储主题下的文章被国内的企业和高校包揽，其实不难发现云存储还是更偏向于工业界的实际应用的，而国内的企业在相应的领域都有着各自丰富的积累，特别是在数据库领域更是百花齐放（POLARDB Meets Computational Storage）；也有自己在运维相关存储系统时的经验总结以及对应的优化方案（MAPX）；还有一个比较有意思的点就是近年来比较多的团队协作式的云存储（Lock-Free Collaboration Support for Cloud Storage），又恰逢疫情更是推动了云存储的普及。 MAPX: Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems 这篇因为之前有针对全文的翻译和理解，此处不做过多的介绍，更详细的可以参考 知乎：信息存储论文选读 - MAPX shunzi blog - MAPX 简要介绍 问题：该文解决的问题其实是 CURSH 算法最大的问题——集群扩容或者添加中间逻辑结构（如PGs）时会造成不受控制的数据迁移，虽然迁移可以在扩展之后立即重新平衡整个系统的负载，但是在扩展规模较大时会导致显著的性能下降。（如以机架的规模进行扩容） 为什么会产生这样的问题？ 设计 CRUSH 算法目的其实主要是为了去中心化，即不需要像 HDFS 等分布式存储系统依赖中心化的目录或元数据管理来寻址对应的数据，而是通过直接计算的方式来实现从而降低对中心化的元数据管理的耦合。但是 CRUSH 相比于中心化管理，中心化目录的存储系统可以保证原有的数据在扩容过程中不受影响，只把新的数据存储在新的存储节点上，CRUSH 由于缺少了关于集群扩容过程中带来的物理存储节点（OSD）的差异信息，在进行计算时都统一处理，由于相应的逻辑节点的权重发生了显著变化，CRUSH 计算结果发生了变化，就会导致大量的数据迁移。 解决方案 受中心化数据布局策略的启发，致力于实现扩容过程中数据迁移的可控，所以基于 CRUSH 设计实现了 MAPX，核心思想就是引入时间维度的映射机制来区分 新老 对象/OSD ，同时保留 CRUSH 算法随机和均匀的优点。 为了尽可能少地修改原有的 CRUSH 算法，在原本的 CRUSH 根节点下插入一个虚拟层，如图所示，每一个虚拟节点代表一次扩容。虚拟层对应的通过使用 MAPX 实现可控数据迁移，通过在执行原本的 CRUSH 算法之前将新的对象映射到新的 layer，因为新的 layer 不会影响原有 layer 的权重，原有对象的放置还是和以前一样，不会发生改变。 MAPX 能保证每一层的负载均衡，主要是因为通用 CRUSH 算法的随机性和均匀性，随着时间的迁移，新的 layer 中的数据增多到和前一个 layer 时则实现了 layers 层面的负载均衡。但是一个 layer 的负载可能会因为 对象的删除、OSD 的宕机发生一些不可预测的负载变化。如图所示，当 layer1 中的负载跟原始集群 layer0 的负载一样高时，则可能会执行一次扩容产生 layer2，假设 layer1 中执行了大量的对象删除操作，则会造成不同 layers 之间的负载不均衡。 为了解决这个问题，MAPX 设计了三种灵活的策略来动态管理 MAPX 中的负载： PG 重映射：可以控制 PGs 到 Layer 的映射，来保证 layers 负载均衡 集群缩容：缩容时需要进行 PG 重映射调整负载，但也要保留部分元数据来保证映射关系不变 layers 合并：使用时间戳来保证物理层的变化在逻辑层 layer 上保持相同以实现负载均衡 Lock-Free Collaboration Support for Cloud Storage Services with Operation Inference and Transformation 这篇文章呢其实主要是对现如今比较时髦的一个场景进行了综合性的分析，也提供了自己的方案。即针对共享文档的编辑时的版本管理和冲突解决的方式进行了深入探讨，结合了市面上现有的多种云存储的团队协作方案，分析之后他们提出了一种无锁的共享文档的编辑的方案，设计了一种比较智能的办法来减少冲突，且开源。 提出了问题同时也利用并设计了算法上的优化，难点主要在于对整个过程进行建模的过程。 这篇文章还有的一个比较有意思的点在于，本文涉及到的别的云存储共享编辑的产品其本身是不开源的，所以在分析这些产品可能的实现方式的时候其实是个很复杂的过程，作者们通过抓包、解密网络流量、分析数据驱动的方法、阅读部分代码和文档的方式才渐渐分析出大致的实现原理 对共享文档的编辑或者云存储协作感兴趣的同学可以深入阅读，由于不涉及到自己更了解的存储系统领域，此处就不班门弄斧了。 POLARDB Meets Computational Storage: Efficiently Support Analytical Workloads in Cloud-Native Relational Database 开始之前先贴几篇讲解和总结（主要是不想重复造轮子hhhh） 知乎：阿里云数据库技术 - 深度思考 | 读POLARDB论文有感 : 异构计算和数据库软硬一体化设计 知乎：CobbLiu - FAST20论文赏析（一） CSDN：黄小米吖 - 论文阅读——POLARDB 简要介绍 PolarDB 整体架构 存算分离架构的背景下，云原生的关系型数据库需要把数据敏感型的任务（例如 table scan）从前端数据库给下发到后端存储节点，以便充分支持分析工作负载，减小存储节点和计算节点之间的链路传输,，但将该类任务转发到存储节点之后对于存储节点的效率则提出了挑战，新出现的 computational storage drives （也称存内计算或者计算型存储）使得上述想法成为可能。 要想使存储节点有足够的能力支持”计算下推”，通常有两个方案，一是scale-up存储节点的CPU能力；二是在存储节点上配备特殊硬件（比如GPU 或者FPGA）来用这些特殊硬件执行存储节点上的”计算下推“任务。第一个方案会较大增加存储节点的成本；第二个方案会引起存储节点内大量的数据迁移， 同时在拥有多块NVMe SSD的存储节点上很容易使特殊硬件成为热点，制约单个存储节点的对外能力。 论文提出了一个新的思路，直接将”计算下推“的工作offload到物理介质 然而，异构计算实际可行的实现和实际部署仍然完全缺失，至少在开放文献中是这样。这主要是由于很难解决两个挑战: (1) 如何实际支持跨整个软件层次的表扫描 pushdown 由用户空间POLARDB存储引擎发起，该存储引擎通过指定文件中的偏移量来访问数据，而表扫描在物理上由计算存储驱动器提供，它作为原始块设备运行，并使用LBA(逻辑块地址)管理数据。整个存储I/O堆栈位于POLARDB存储引擎和计算存储驱动器之间。因此，我们必须内聚地增强/修改整个软件/驱动程序堆栈，以便创建一个支持表扫描叠加的路径 组成： 前端分析处理引擎 PolarDB MPP：该分析处理引擎与 MySQL 协议兼容，可以解析、优化和重写使用 AST(抽象语法树)的 SQL 和许多嵌入式优化规则，它将每个 SQL 查询转换为一个 DAG(有向无环图)执行计划，由操作符和数据流拓扑组成。该引擎原生就支持 pushdown 存储引擎 PolarDB Storage Engine：遵循了 LSM-tree 实现，数据被组织成了多个文件，每个文件包含很多块。 原有的实现中，存储引擎可以使用存储节点上的 CPU 来处理 table scan 请求，因此 table scan pushdown 将与底层的 IO 堆栈无关。 为了利用可计算型存储的特点，需要修改该引擎以便将 table scan 请求 pushdown 到 PolarFS 上。存储引擎根据文件中的偏移量访问数据块。每一个 table scan 请求包括： 要被扫描的数据的定位信息（文件内的偏移量）； 应用表扫描的表的 schema； table scan condition。 POLARDB 存储引擎分配一个内存缓冲区来存储从计算存储驱动器返回的数据，每个 table scan 请求都包含这个内存缓冲区的位置 POLARDB部署在分布式文件系统PolarFS上，该文件系统管理跨所有存储节点的数据存储。computational storage drives 只能以 LBA 的形式定位数据，PolarFS 在收到来自 POLARDB存储引擎的每个表扫描请求后需要进行数据转换 计算存储驱动器完全由内核空间中的主机端驱动程序管理，该驱动程序将每个计算存储驱动器公开为块设备。当收到每个表扫描请求时，驱动程序执行以下操作 分析 scan conditions，可能对 conditions 重新进行组织以获得更好的性能 驱动程序进一步转换将被扫描数据的位置信息从LBA域到物理块地址(PBA)域，其中每个PBA与NAND闪存中的一个固定位置相关联。 (2) 如何实现低成本的计算存储驱动器具有足够的表扫描处理能力 虽然基于FPGA的设计方法可以显著降低开发成本，但FPGA往往比较昂贵。此外,由于 FPGA通常仅工作在200 ~ 300MHz(与之相比，CPU时钟频率为2 ~ 4GHz)，为了实现足够高的性能，我们必须使用大量的电路级实现并行性(因此需要更多的硅资源)。因此，我们必须在我们的实现中开发出能够使用低成本FPGA芯片的解决方案 为了解决计算存储驱动器实现成本的挑战，关键是最大化FPGA硬件资源的利用效率。为了实现这一目标，我们跨软件和硬件层进一步开发了以下技术。 Hardware-Friendly Data Block Format FPGA 并行化 个人见解 因为自己不是做数据库系统的，所以对于背景可能了解的不够，但这不妨碍对 PolarDB 整体结构的理解，个人感觉 PolarDB 对外提供的虽然是简单的关系型数据库服务，但是内部实现了大量的存储系统的工作。虽然本篇是基于 data-intensive 的负载 offload 到计算型存储的角度来组织的，但还是大体能看出 PolarDB 内部复杂的工作细节，以及各个组件都进行了各种软件栈上的优化。PolarDB 本身是一个很大的分布式存储系统，其中的很多小的创新点也基本都是能够组成一个 Paper Idea 的水准，且有工业界的实际实现，经得住实际负载的考验。 抛开本篇论文的主要内容不谈，PolarDB 本身就是一个十分出众的数据库，此处给一些 PolarDB 的整体介绍链接： 阿里云 - PolarDB 帮助文档 知乎 - 如何评价阿里云新一代关系型数据库 PolarDB？ 阿里云 - 读懂POLARDB不能错过的18篇深度文章！ Github PolarDB 官方文档库 File Systems Read as Needed: Building WiSER, a Flash-Optimized Search Engine Jun He and Kan Wu, University of Wisconsin—Madison; Sudarsun Kannan, Rutgers University; Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau, University of Wisconsin—Madison How to Copy Files Yang Zhan, The University of North Carolina at Chapel Hill and Huawei; Alexander Conway, Rutgers University; Yizheng Jiao and Nirjhar Mukherjee, The University of North Carolina at Chapel Hill; Ian Groombridge, Pace University; Michael A. Bender, Stony Brook University; Martin Farach-Colton, Rutgers University; William Jannen, Williams College; Rob Johnson, VMWare Research; Donald E. Porter, The University of North Carolina at Chapel Hill; Jun Yuan, Pace University Read as Needed: Building WiSER, a Flash-Optimized Search Engine 这篇文章是针对现有的搜索引擎进行设计和优化的（以前的研究中很少有针对搜索引擎这类上层应用进行优化的，但随着数据规模的增加，搜索引擎的应用也越来越广泛，尤其是在一些文本检索的领域），搜索引擎本身是一种计算密集型的应用，会有一些读取存储设备的操作，作为一种 &quot;read as needed&quot; 类型的负载，对于存储系统的要求和其他类型的应用负载稍有不同。本文则是实现了一个以相对较少的主存（main memory）提供高吞吐量和低延迟的搜索引擎。 简要介绍 背景 由于本身研究的点不同于以往，所以就按照惯例先介绍了一下这个问题有多关键。此处不表。 对于存储系统而言，不管你上层的应用是什么，在存储设备上本质都是数据结构的存取，而搜索引擎和数据库以及图等负载稍有不同，主要使用了倒排索引等结构，又随着 SSD 的普及，所以基于 SSD 的优化方案就比较有搞头。 需要注意的是搜索引擎对于存储系统的要求： low data latency：因为搜索引擎的交互性很强，常常需要较低的延迟来保证 high data throughput：因为要处理和检索的数据很多，对于吞吐量的要求也较高 high scalability：因为搜索引擎主要面型文本存储领域，数据规模会随着时间不断地变大，相应地就需要较好的扩展性来提供相应的支持。 以前的搜索引擎也会面对上述问题，但之前的解决方案都是利用内存作为一种存储介质，由于数据规模较大，为了保证低时延和吞吐量等特性，就常常需要把大量的数据放在内存中（虽然可能会丢，但重建肯定也是一个很大的开销） 作者觉得既然现在存储设备已经很快了现在，考虑到对大型数据集使用RAM的成本过高，那么是否可以重新构建一个搜索引擎，以更好地利用 SSD 来实现必要的性能目标，而使用比较少的内存。更明智的做法是重新组织传统的搜索数据结构，以创建和改进读流，从而利用现代 SSD 提供的带宽 针对 read as needed 的负载，作者提出了设计理念 use small memory read data from SSDs as needed do not attempt to cache data in memory attempt to read data from SSDs efficiently 分析了现有的搜索引擎，其中 state-of-the-art 的 ElasticSearch。ES 不能实现高性能原因主要是因为读放大，因为 ES 将不同阶段的数据分组到多个位置，并将数据进行排列，使早期的数据项更小。其目的是缓存早期阶段的数据，因为早期阶段的数据要比后期阶段的数据访问得更频繁。然而，按阶段分组数据也可能导致比较大的读放大。 因为 SSD 本身的一些限制（有限的带宽、高延迟、大IO友好），以及在 ES 中发现的问题，为了实现上述的搜索引擎对于存储系统的要求，就需要 reduce read amplification hide i/o latency use large request to improve device efficiency Design 设计了四个关键技术 Grouping data by term 即不再采用分阶段存取在不同的文件中的方式进行存储，而是使用连续的压缩的数据块来进行放置，从而将小的读请求转换成了大的读请求 Two-way Cost-aware Bloom Filters 针对于 phrase query 的场景，作者使用了两路布隆过滤器来帮助快速检索短语。 设置前后两路匹配的原因，是因为 before 和 after 的查询开销可能不一样，会尝试选择开销更小的那一路的布隆过滤器来进行匹配。但可能存在两路开销都很大的情况，这时候则直接使用默认的原有的短语匹配方式。这也就是所谓的 Cost-aware Adaptive Prefetching 为了获得最佳性能，预取应该适应查询和持久数据的结构。在倒排索引的所有数据中，最常被访问的数据包括元数据、跳跃表、文档id和词频，这些数据经常被一起顺序访问; Trade Disk Space for I/O 压缩的粒度和 ES 不同，从而减小读放大，但其实做了 trade-off，压缩后的大小比 ES 压缩后的要大 个人见解 这篇其实也不是个人所熟悉的领域，只是因为很早以前自己接触过 ElasticSearch 的应用（ELK 体系那一套），所以对 ES 还是比较感兴趣，这篇文章其实都是在和 ES 对比，可以理解为是对现有的 ES 上的优化，虽然作者指出可以延伸到按需读取场景下的其他负载，但本质还是在搜索引擎的这个领域下的优化。奈何才疏学浅，也就只能总结到这了。 How to Copy Files 先放几个链接： XSKY解读FAST'20 论文 《关于如何高效率的对文件目录树进行快速克隆操作》 知乎 - BetrFS: 一种写优化的文件系统 Github - BetrFS betrfs 文章从标题开始其实就很吸引眼球，毕竟是一个看着很简单的问题，但是作为论文标题就不禁想让我这种门外汉也要尝试着去看看作者在里面写了些啥我不知道的东西哈哈。走马观花看了看发现嗯自己确实不懂。。 本文其实是做的文件系统领域中关于复制或者克隆操作的优化，现有的很多克隆其实是基于 Copy-On-Write 实现的，但原生的 Copy-On-Write 启发算法有一定的问题，所以作者在这上边也下了很多功夫，提出了 copy-on-abundant-write，基于 BetrFS 实现，除此以外还做了一些别的针对性的优化，复制操作性能提升也比较明显。 简要介绍 背景 当然首先还是按惯例介绍这个问题多重要，所以就先介绍拷贝操作应用广泛，其实主要体现在备份、快照这些机制中，然后云计算领域中的容器、虚拟机等技术使用拷贝比较频繁。但拷贝本身其实可以分为逻辑拷贝和物理拷贝，从字面上也很好理解，物理拷贝肯定就是物理存储空间上进行完整的拷贝了，时间和空间的消耗可能会因为大文件而特别大，所以很多时候都用逻辑拷贝，也是 volume snapshots 中用的比较多的 Copy-On-Write 写时复制。CoW 可以对块设备进行，也可以在文件系统中对文件或者目录进行，取决于具体的产品实现。譬如 Linux 本身支持的 cp -reflink 引出主题 CoW 之后相应地指出现有的 CoW 的问题：标准的 CoW 本身其实是在写放大和局部性之间做了 trade-off。即 CoW 的粒度大小的设定将影响这两方面的表现。 如果粒度为文件的大小，那么对文件的小写操作带来的写放大就很严重，相应的写延迟也就增加了，空间也被浪费了（因为共享的数据被写操作给破坏了） 如果粒度很小，更新操作很快，但是容易产生碎片，顺序读副本的开销就变大了，因为局部性被破坏了。 想要实现的效果，也就是文中作者描述为 Nimble clones 的复制操作应该是这样的：（那肯定现有的文件系统里的 CoW 实现就不是 nimble 的，文中有测试） be fast to create have excellent read locality have fast writes conserve space 总结下来问题其实出在对克隆的写操作粒度与共享数据副本的粒度这两点，其实就是对于文件小写，不能直接就将源文件拷贝重写了，要设定一个相对大的 copy size 来保证局部性，即得 buffer 小写。所以就得把对克隆的写操作和副本的相关操作解耦。所以作者选了 BetrFS 来实现自己的方案。毕竟 BetrFS 以写见长，也有一些自己的 buffer 机制，当然主要还是测试出来的，文中的一个关于性能降级的测试。 Design 需要首先理解一下 btrfs 的原理和其中的数据结构 Bε-tree。 btrfs 基于 KV 来管理文件系统中的数据，元数据 KV 存储的是 文件全路径 Full Path 到文件系统元数据的映射，数据 KV 存储的是 {fullpath + block number} 到 block的映射。 Bε-tree 可以简单理解为一个带 buffer 缓冲的 B-tree，主要吸收随机小写。 在 Bε-tree 基础上实现逻辑拷贝，即 Bε-DAG。在树中增加新的边 edge，以使得在访问克隆后的文件夹时，能通过某种方法访问克隆前的数据（克隆后的数据在不修改内容前，都是完全共享的），通过使用 DAG （有向无环图）的思想来支持共享访问，对应地需要实现一层如图所示的转换 red-&gt;green 对于此时的写操作，则需要在 flush 的时候执行 copy-on-write，分为五个步骤执行： Copy-on-Abundant-Write 第一步，copy 该节点 第二步，转换文件对应的路径前缀（图中的 green-&gt;red） 第三步，删除无法到达的数据，图示中的蓝色 L，因为拷贝之后没有 blue 上层目录，即在该路径上该文件再也无法访问到，故可以删除 第四步，移动地址转换功能，即把原本的上一层的 red-&gt;green 移动到下一层（因为复制后的节点有一些数据为目录，如图中所示的 A 和 R，故需要指向底层文件，如果 flush 操作针对更底层的文件，那么再相应递归地执行 copy-on-write） 第五步，将数据刷入到复制后的节点中作为新的 buffer（即吸收小写） 因为 Bε-DAG 中的节点本身比较大，即可以吸收小写，且局部性比较好，所以在读写上表现都很不错，又因为采用了新的上文描述的 Copy-on-Abundant-Write 在空间上的利用率也很高，但还有一个问题没有解决，即拷贝延迟 作者实现了一种称之为 GOTO Message 的机制，用于减小 Copy 操作的延迟。存储的数据大致如 (a,b) - height - dst_node，其中 （a,b）为拷贝操作覆盖的 key 空间/范围，height 表示要拷贝的目标节点的高度，dst_node 表示要拷贝的目标节点。 如果要进行拷贝操作，如图所示，拷贝 /green 到 /violet，那么相应地会先触发 green 的 flush，确保数据一致，然后插入一条 GOTO Message。 查询操作首先查询到了 GOTO Message，那么首先判断 key 是否在相应的区间中，如果在那么就要去 dst_node 处继续查找，相应地会进行前缀转换，直到找到该数据。 相应的 GOTO message 可以和其他数据一样，通过 flush 操作进入下一层，如下图所示，直到 GOTO Message 到达了目标节点高度 + 1 的层级的时候，GOTO Message 就此时可以成为一个真正的指向下一层的目录，也就是文中的 pivot，即 GOTO 将变成紫色的目录节点 /violet 这样子下来拷贝的时间复杂度完全取决于树的高度 个人见解 这篇呢个人也只是泛泛而谈，主要参考了 PPT 的动画解释，原文其实还涉及到了更为具体的实际的解释，由于重心不在这边所以没有细看，感兴趣的小伙伴可以再深入研究，但是在研究之前最好先大致看一下之前的 btrfs 的论文，至少了解到核心数据结构的设计和读写的流程，才有助于对本篇文章的理解。 想提一下的是，我发现 btrfs 相关的论文已经上了很多次 FAST 以及 TOS 了，而且本身这个文件系统提出来也没几年，FAST15 上提出来的，后续的一些对于该文件系统的迭代都直接被拿来作为了新的 idea，譬如本篇实现的 clone，其实做文件系统同学可以关注一下这个文件系统，我自己简单看了这个文件系统的数据结构之后理解为他其实是在利用 B-Tree 和 LSM-tree 各自的优势，当然主要还是解决 B-Tree 的写的问题，后续如果有相关的工作的话可能会关注一下。 SSD and Reliability A Study of SSD Reliability in Large Scale Enterprise Storage Deployments Stathis Maneas and Kaveh Mahdaviani, University of Toronto; Tim Emami, NetApp; Bianca Schroeder, University of Toronto Awarded Best Paper! Making Disk Failure Predictions SMARTer! Sidi Lu and Bing Luo, Wayne State University; Tirthak Patel, Northeastern University; Yongtao Yao, Wayne State University; Devesh Tiwari, Northeastern University; Weisong Shi, Wayne State University A Study of SSD Reliability in Large Scale Enterprise Storage Deployments 虽然是 SSD 硬件相关的，但毕竟是 BestPaper，而且这篇其实更像是针对一些数据集的分析，不是具体的基于硬件或者协议的优化，想了想决定还是大概看看吧，万一之后有所涉及。 按照惯例放个链接： CobbLiu - FAST20论文赏析（二） 本文首次对企业存储系统中基于 NAND 的 SSD 进行了大规模的现场研究(与分布式数据中心存储系统中的驱动器形成对比)。该研究基于一组非常全面的现场数据，涵盖了一家主要存储供应商(NetApp)的 140 万份 SSD。驱动器包括三个不同的制造商，18种不同的型号，12种不同的容量，和所有主要的闪存技术(SLC, cMLC - consumer-class, eMLC - enterprise-class, 3D-TLC)。这些数据使我们能够研究很多之前没有研究过的因素，包括固件版本的影响，TLC NAND的可靠性，以及RAID系统中驱动器之间的相关性（为这些驱动器收集的数据非常丰富，包括驱动器替换(包括替换的原因)、坏块、使用情况、驱动器年龄、固件版本、驱动器角色(例如，数据、奇偶校验或备用)等信息）。本文介绍了我们的分析，以及由此得出的一些实际影响。 简要介绍 背景 以前的存储设备的可靠性研究大多基于 HDD，近年来随着 SSD 的普及，对于 SSD 的可靠性研究才不断出现。现有的 SSD 可靠性研究除了在实验室的控制条件下的的相关研究以外，还有来自如 Facebook, Microsoft, Google, and Alibaba 等大型公司基于自己的数据中心中的数据对 SSD 可靠性的分析。但作者发现现有的研究中还是有一些 critical gap，所以本文就来填坑了： 没有研究关注企业级存储系统，这些系统中的驱动器、工作负载和可靠性机制可能与云数据中心中的非常不同。比如企业级存储系统中通常使用高端 SSD 并且可靠性通常由 RAID 来保障，而不是分布式存储中的那些副本的一些策略。 现有的研究没有涵盖构建现实的故障模型所需的一些最重要的故障特征，以便计算数据丢失的平均时间等指标，例如，这包括对驱动器替换原因的分析，包括底层问题的范围和相应的修复操作(RAID重建与耗尽驱动器)，以及最重要的是对同一RAID组中驱动器之间的相关性的理解。 数据统计分析 采集的系统和数据对应的相关信息此处不表，直接看相关数据统计。 前六列描述了不同厂商对应不同系列的 SSD 的相关参数，包括匿名给出了制造商、容量、接口、闪存颗粒技术、光刻技术、PE 周期（寿命） 后四列描述了不同的 SSD 被用于了何种的环境，包括 OP 比例（用于垃圾回收的保留空间比例）、第一次部署使用该 SSD 的日期、SSD 通电的中位数年数（因为每类 SSD 对应了很多个实际的 SSD）、SSD 的额定使用寿命的平均值和中位数（SSD 所经历的 PE 循环数占其 PE 循环极限的百分比） 最后三列描述了三种不同的 SSD 健康性和可靠性的指标：空闲块的使用比例、坏的扇区的数量、每年的置换率 从上表中的相关数据统计得出以下结论： 平均 ARR 为 0.22%，在 0.07% 到 1.2% 之间波动，比数据中心中的故障率要低得多 即便是具有相同工艺的同厂商的 SSD，相似容量和相似使用期限，年故障率也非常不一样 为坏块保留的备用区域为典型的驱动器提供了大量的资源：即使对于已经在数据中心中存放了好几年的 SSD，使用的备用块的百分比平均也不到 15%。即便是第 99.9 和 第 99.99 使用空闲空间最多的盘都分别为 17% 和 33% 很多 SSD 都没达到 PE 限制，即便是使用了 2-3 年的 SSD，第 99.9 和 第 99.99 寿命消耗的也只消耗了 15% 和 33%，对于绝大多数 SSD 盘来说，因为达到最大可擦写次数而失效的可能性几乎为零。 原因分析 有不同的原因可以触发更换 SSD，存储层次结构中的不同子系统也可以检测具体触发更换 SSD 的原因。可能由 SSD 本身或者存储层或者文件系统报告相应的问题，如下表所示描述了可能触发更换 SSD 的原因，以及它们的频率，以及系统采取的恢复操作(例如，从要替换的 SSD 复制数据与使用 RAID 重新构建数据)，以及问题的范围(例如，部分数据丢失的风险，完整驱动器丢失的风险，或者没有立即的问题) 原因被大致分为 4 类，分别以 ABCD 表示，严重程度递减。 最无关紧要的是 D 类，通常是由 SSD 内部或者更高的存储层在逻辑上触发的，即一些预测 SSD 未来故障的策略，基于之前发生的错误、超时以及磁盘的 SMART 统计信息等，但实际可能没坏。 最严重的是 A 类，通常是因为 SSD 变得完全无法响应，或者 SCSI 层检测到了 SSD 的问题严重到需要立即更换 SSD 并重建构建在当前 SSD 上的 RAID 时。 B 类是指替换发生在当系统怀疑 SSD 丢失了写操作的时候，例如 SSD 根本没有执行写操作，或者将其写到错误的位置，或者以其他方式破坏了写操作，根本原因可能是 SSD 中的固件错误，尽管存储堆栈中的其他层也可能是原因。由于有许多潜在的原因，heuristic（启发式判断） 被用来决定是否触发替换;具体地说，如果一个 SSD 中出现了多个这样的错误，而其他 SSD 中没有错误，那么前一个 SSD 将被替换。 C 类通常是因为命令被丢弃或者执行超时。 抛开所有的分类看具体的错误原因，我们不难发现最常见的错误是 SCSI errors，大约有 1/3 的替换都是因为 SCSI errors 发生，同时也是最严重的错误之一；还有大约 1/3 的替换仅仅是为了预防 SSD 故障才采取的，也就是 D 类错误，预测了磁盘故障可能带来的严重影响，预测性的替换通常是统计了超时次数判断是否达到了阈值 后续测试了不同的因素对 SSD 的 annual replacement rate 的影响，主要对 eMLC 和 3D-TLC SSD 进行了分析，得出了以下发现和结论： 盘在刚开始使用的 1 年内的 ARR 是 1 年后 ARR 的 2-3 倍，并且盘片的 ARR 并没有随着使用时间的增加而增加，部分原因可能是因为绝大部分盘片的擦写次数都远没达到总擦写次数。 3D-TLC SSD 比其他类型的有更高的 ARR，但是这个差别很小，SLC、MLC 和 TLC（不同 flash 和 drive） 对 ARR 的影响比使用率对 ARR 的影响更小。 大容量 SSD 不仅整体替换率更高，而且 A 类错误概率更高，可预测的错误概率更低。 更高密度的 SSD 并不总是看到更高的替代率 ARR。事实上，我们观察到，尽管更高密度的 eMLC SSD 有更高的替代率，但这一趋势在 TLC 中是相反的 早期的固件版本可能与较高的替换率相关，这就强调了固件更新的重要性。 具有非空缺陷列表的 SSD 被替换的几率更高，不仅是因为可预测的故障，还因为其他替换原因 更多地使用其 OP 空间的 SSD 很可能在将来被替换 虽然大型 RAID 组有更多的驱动器替换，但我们没有发现每个组的多次故障率(这可能导致数据丢失)与 RAID 组大小相关的证据。原因似乎是在第一次失败后出现后续失败的可能性与 RAID 组大小无关 最终作者总结了一下： 早期的固件版本可能与较高的故障率相关，所以得及时更新固件，要保证升级过程无痛且稳定 RAID 组的大小对 SSD 盘的平均 ARR 没有明显的影响 单奇偶校验 RAID 配置(例如，RAID-5)，可能容易发生数据丢失，而实际的数据丢失分析肯定必须考虑相关的故障。 具有非常大容量的驱动器总的故障率更高，出现更严重的故障。较高的故障率可能源于 SSD 上的更多 NAND 和die，这就强调了 SSD 及其系统能够处理部分驱动器故障(如 die 故障)的重要性，NetApp 正朝着这个方向努力，通过从 OP 区域来弥补部分故障的区域带来的容量损失 我们观察到，大容量的 SSD 预测失败率更小，这也提出了一个问题，即大容量 SSD 是否需要不同类型的故障预测器，以及是否需要更多来自 SSD 的内部问题(例如，坏死或 DRAM 问题)作为输入 随着 QLC NAND 的引入，人们重新开始关注 NAND SSD 的可靠性，其 PE 循环限制明显低于当前 TLC NAND。根据我们的数据，我们预测，对于绝大多数企业用户来说，向 QLC 的 PE 周期限制迈进不会带来任何风险，因为 99% 的系统最多使用其驱动器额定寿命的 15% 人们担心 NAND 有限的 PE 周期在 RAID 系统生命周期的后期，由于相关的磨损故障，SSD 可能会对数据的可靠性造成威胁，因为 RAID 组中的驱动器老化速度相同。相反，我们观察到，早期失效导致的相关失败可能是一个更大的威胁。例如，在我们的研究中，3D-TLC 驱动器，早期失效率高峰时的失败率比后期高出 2.5 倍 在选择 SSD 时，比起 flash type（比如 eMLC 还是 3D-TLC），工艺和容量更应该是首要的考量因素。 个人见解 这篇算是分析了 SSD 的一些物理特性和参数和 SSD 的故障之间的关系，以及 SSD 的替换策略的关系，但是文章开始的部分其实限定了场景为企业级存储，我个人对于企业级存储和如阿里等互联网企业的数据中心的存储这两种场景之间的差异其实不太具体理解，而且从 SSD 本身而言，这两种场景下，关于 SSD 的经验仿佛是通用的？这里有点一知半解 确实本篇文章有大量的数据支撑，最重要的其实都是文章总结出的那些发现，有的发现确实也一定程度上颠覆了以前大家对于 SSD 故障原因的一些固有认知，但可能缺少一些更为合理的解释？也可能是自己水平有限，对于相关成因的解释看的一知半解。 Making Disk Failure Predictions SMARTer! 这篇文章针对的领域是磁盘故障预测，之前有博客简要介绍过这篇文章，这篇文章的实验做的非常充分，整个行文也比较行云流水，照例贴一下链接： shunzi - AI For System Papers Index NBJL 2020论文导读24：Making Disk Failure Predictions SMARTer! 磁盘驱动器是最常被替换的硬件组件之一，它继续对准确的故障预测提出挑战。在这项工作中，我们提出了一个最大的磁盘故障预测研究之一的分析和发现，涵盖了一个大型领先数据中心运营商的64个站点在两个月的时间里总共380,000个硬盘驱动器。我们提出的基于机器学习的模型在10天的预测周期内平均用0.95 F-measure和0.95 Matthews相关系数(MCC)预测磁盘故障 本篇文章就不展开介绍了，解决的其实就还是磁盘故障预测中的准确率低的问题和提前预警的时间较短的问题，而作者在磁盘故障预测的相关机器学习模型中引入了性能和物理位置条件的指标使得磁盘故障的预测模型更为准确，提前预警的时间也大约提升到了 10 天预警。 文章写的很清楚明朗，如果对磁盘故障预测感兴趣的同学，该文值得深入阅读。 Performance An Empirical Guide to the Behavior and Use of Scalable Persistent Memory Jian Yang, Juno Kim, and Morteza Hoseinzadeh, UC San Diego; Joseph Izraelevitz, University of Colorado, Boulder; Steve Swanson, UC San Diego An Empirical Guide to the Behavior and Use of Scalable Persistent Memory 关于这篇就不再详细展开了，资料很多，贴几个典型的其他大佬的资料。做 NVM 的肯定是都会看的 8。 zhihu: 暗淡了乌云 - An Empirical Guide For 3D XPoint Persistent Memory 阿里云开发者社区：Intel PMEM的使用经验和指南 整体: 内存控制器和硬件交互细节： 对NVDIMM的访问首先到达DIMM上的控制器（本文中称为XPController），该控制器协调对Optane介质的访问。与SSD相似，Optane DIMM执行内部地址转换以实现损耗均衡和坏块管理，并为该转换维护AIT (address indirection table)。地址转换后，将实际访问存储介质。由于3D-XPoint物理介质的访问粒度为256B（文中称为XPLine），所以，XPController会将较小的请求转换为较大的256字节的访问以提升性能。然而，因为同样的原因，小数据量的存储会变为RMW（read-modify-write）操作而导致写放大。 XPController有一个小的写合并缓冲区（在本文中称为XPBuffer），用于合并地址相邻的写操作。 由于XPBuffer属于ADR域，因此到达XPBuffer的所有更新都是持久的。 文章通过介绍 Optane PM 以及做了相关的测试得出了几个 Best Practice 避免小于 256B 的随机读写； Optane 的数据更新，在内部介质会进行 read-modify-write 操作。若更新的数据量小于内部操作的数据粒度（256B），会带来写放大，而使得更新效率低。 EWR（Effective Write Ratio，由DIMM的硬件测量）的概念：其为iMC发出的字节数除以实际写入3D-XPoint介质的字节数，即为写放大的倒数。EWR小于1表示，Optane介质写效率低。EWR也可以大于1，此时表示XP-Buffer做了写合并（在内存模式中，因为DRAM的缓存作用，EWR也可以大于1） 展示了Optane DIMM的带宽（三种store命令）与EWR的正相关的关系。一般而言，小数据量的存储使得EWR小于1。 例如，当使用单个线程执行随机的ntstore时，对于64字节的写，EWR为0.25，对于256字节访问，其EWR为0.98。值得注意的是，虽然iMC仅以64B为单位访问DIMM，但是XPBuffer可以将多个64B的写进行缓存，并合并为256B的Optane内部写，所以256字节更新是高效的。由上可知，如果Optane DIMM的访问具有足够好的局部性，同样可以高效地进行小数据量的存储。 为了得到“怎样的局部性才足够”的命题结论，我们设计了一个实验来测量XPBuffer的大小。 首先，我们分配N个XPLine大小（256B）的连续区域。 在实验中，进行循环的写数据。首先，依次更新每个XPLine的前半部分（128 B）， 然后再更新每个XPLine的后半部分。 我们测量每一轮后EWR的值。 图9显示： N 小于64（即16 kB的区域大小）时，EWR接近于1，其表明，后半部分的访问命中了XPBuffer。 N 大于64时，写放大进行了突变，其由XPBuffer miss急剧上升导致。这表明，XPBuffer的大小为16KB。进一步的实验表明，读操作也会占用XPBuffer中的空间从而造成竞争关系。 使用ntstore进行大数据（大于256B）写 一般通过下面操作进行数据写入：store操作后，程序员可以通过clflush/clflushopt操作进行高速缓存evict或通过clwb操作进行写回（write back)，以将数据写入至ADR域并最终写至Optane DIMM；或者，通过ntstore指令绕过高速缓存直接写入Optane DIMM。在进行完上述某种操作后，再进行sfence操作可确保先前的evict，write back和ntstore操作的数据变成持久的。写数据时，采用上述何种操作对性能影响很大。 对于写超过 64B 的数据，每store 64B进行cache的flush操作(相比于不进行flush操作）获得的带宽会更大 对于超过 512B 的访问，ntstore的延迟比store + clwb更低 对于超过 256B 的访问，ntstore操作的带宽也最高 当写入的大小超过 8MB时，写入后再进行刷新操作会导致性能下降，因为其导致了高速缓存容量的失效，从而使得EWR升高 限制访问 Optane DIMM 的并发线程数 XPBuffer的竞争。对XPBuffer中缓存空间的争用将导致逐出次数增加，触发写3D-XPoint介质，这将使EWR降低。 iMC中的竞争。每个线程随机访问N个DIMM（线程间分布均匀）。随着N的增加，针对每个DIMM的写入次数会增加，但是每个DIMM的带宽会下降。 当对交错的Optane DIMM进行随机 4KB 访问时，Optane带宽急剧下降。当写入的数据大小为 24kB 和 48kB，出现了性能的小峰值，其访问在6个DIMM上完美分布。 避免 NUMA 访问（尤其对于是 read-modify-write 操作序列）。 Optane的NUMA效应远大于DRAM，因此应更加努力地避免跨插槽的存储器通信。对于读写混合且包含多线程访问的情况，其成本特别高。 对于本地和远程访问，单线程带宽差距不大。而对于多线程访问，随着访问压力的提高，远程访问性能会更快下降，从而导致相对于本地访问而言性能较低。 Key Value Storage Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook Zhichao Cao, University of Minnesota, Twin Cities, and Facebook; Siying Dong and Sagar Vemuri, Facebook; David H.C. Du, University of Minnesota, Twin Cities FPGA-Accelerated Compactions for LSM-based Key-Value Store Teng Zhang, Alibaba Group, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University; Jianying Wang, Xuntao Cheng, and Hao Xu, Alibaba Group; Nanlong Yu, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University; Gui Huang, Tieying Zhang, Dengcheng He, Feifei Li, and Wei Cao, Alibaba Group; Zhongdong Huang and Jianling Sun, Alibaba-Zhejiang University Joint Institute of Frontier Technologies, Zhejiang University HotRing: A Hotspot-Aware In-Memory Key-Value Store Jiqiang Chen, Liang Chen, Sheng Wang, Guoyun Zhu, Yuanyuan Sun, Huan Liu, and Feifei Li, Alibaba Group Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook 这篇文章可能和其他文章都有所不同，主要做 Benchmarking 方面的工作，和 Facebook 合作完成，对 Facebook 中现有的 RocksDB 典型的应用场景进行了表征，同时提出了一种新的更接近实际生产环境负载的模型考虑来取代 YCSB。照例先贴几个链接： jiangyuhang17. - Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook 论文笔记 程序员大本营 - 【论文阅读】Characterizing, Modeling, and Benchmarking RocksDB Key-Value Workloads at Facebook 持久化的键值存储是现代 IT 基础设施的基石，但是现有的表征 KV 存储的真是工作负载的研究有一定的局限，主要因为缺乏跟踪/分析工具以及在操作环境中收集跟踪的困难。本文对Facebook上三个典型的RocksDB生产用例的工作负载进行了详细表征: UDB：用于MySQL中用来存储社交图数据，使用RocksDB作为底层存储； ZippyDB：用于存储分布式对象存储的元数据的分布式KV； UP2X：用于存储AI/ML数据的分布式KV。 通过分析以上三种应用，有以下发现： 键和值大小的分布与用例/应用程序高度相关 KV 对的访问具有良好的局部性，并遵循一定的特殊模式 收集的性能指标在 UDB 中显示强烈的昼夜变化模式，而其他两个没有 除此以外，作者表明现如今被广泛应用的 KV 负载 YCSB 尽管提供了各种工作负载的配置和 KV 对访问分布模型，但是忽视了键的空间局部性，和实际生产环境中的工作负载还是有一定的差距，于是本文提出了一种基于键范围的工作负载，并开发了一个可以更好地模拟真实键值存储的工作负载的基准。 简要介绍 Introduction &amp; Background 首先描述工作的意义，主要是解决现如今提升 KV 存储的性能比较困难，原因主要表现在： 对KVstores的真实工作负载表征和分析的研究非常有限，KV-stores的性能与应用程序生成的工作负载高度相关 描述KV-store工作负载的分析方法与现有的块存储或文件系统的工作负载特性研究不同（语义不同） 在评估KV-store的底层存储系统时，我们不知道KV-store基准生成的工作负载是否能代表真实的KV-store工作负载 为了解决上面的问题，本文主要就做了三方面的事情：对 RocksDB 的 workload 进行 characterize, model, and benchmark 引入了一系列工具，可以在生产环境中应用，主要是收集 KV 层的查询 traces，replay traces 和分析 traces。且已开源：https://github.com/facebook/rocksdb/wiki/RocksDB-Trace%2C-Replay%2C-Analyzer%2C-and-Workload-Generation 为了更好地了解KV工作负载及其与应用程序之间的关系，分析了 UDB， ZippyDB，UP2X，有以下发现： UDB 和 ZippyDB 中的查询主要是 read，而 UP2X 中的主要查询类型是 read-modify-write (Merge) 由于上层应用程序的键组合设计，键大小通常较小且分布狭窄，大的值大小只在某些特殊情况下出现 大多数 KV 对是冷的(访问较少)，只有一小部分 KV 对经常被访问 Get, Put, and Iterator 都有很强的针对基于键的空间局部性（比如经常访问的 KV 对通常在空间的分布上也相对较近），与上层应用程序的请求局部性密切相关的一些键范围非常热（经常被访问） UDB 中的访问显式地表现为昼夜模式 发现尽管 YCSB 可以生成与 ZippyDB 工作负载类似的键值(KV)查询统计数据，但 RocksDB 存储 I/Os 可能会有很大的不同，这个问题主要是由 YCSB 生成的工作负载忽略键的空间局部性这一事实引起的。YCSB 中热的键值对可以在整个键空间中随机分配，也可以聚集在一起，这将导致存储中访问的数据块与与 KV 查询相关的数据块之间的 I/O 不匹配。在不考虑键空间局部性的情况下，基准测试生成的工作负载将导致 RocksDB 的读放大和写放大比实际工作负载大。于是作者提出了一种基于键范围的热度的工作负载建模方法。整个键空间被划分为较小的键范围，并且我们对这些较小的键范围的热度进行建模。在新的基准测试中，将根据键范围热度的分布将查询分配给键范围，并且在每个键范围中热键将被紧密地分配。 RockDB 介绍：相比于普通的 LSM Tree 或 LevelDB，从架构上来看主要是多了一个 Column Family 的概念 三种应用场景介绍： UDB：Facebook 的社交图数据长期存储在UDB中，这是一个分片 MySQL 数据库层。UDB 依赖 MySQL 实例来处理所有的查询，查询会被 MyRocks 转换成对于 RocksDB 的查询。图数据主要被维护成点和边，相应地在 RocksDB 中使用了不同的列族来存储对应的数据。收集了 14 天的 traces，也单独分析了最后一天 24 小时的负载 Object，Assoc，Assoc_count，Object_2ry，Assoc_2ry和Non_SG ZippyDB：基于 RocksDB 的分布式 KV 存储，使用 Paxos 来保证数据一致性和可靠性。KV 对被划分为切片，每个切片由一个 RocksDB 实例支持。选择一个副本作为主切片，其他副本作为次要切片。主切片处理对某个切片的所有写操作。如果读取需要强一致性，则读取请求(如 Get 和 Scan )仅由主切片处理。一个 ZippyDB 查询被转换为一组RocksDB 查询(一个或多个)。 UP2X：Facebook 使用各种 AI/ML 服务支持社交网络，并使用大量动态变化的数据集(如用户活动统计计数器)进行AI/ML预测和推断。UP2X 是分布式的专门开发的 KV-store，用于将这种类型的数据存储为 KV 对。当用户使用 Facebook 服务时，UP2X 中的 KV 对会经常更新，例如计数器增加时。如果 UP2X 在每个 Put 之前调用 Get 来实现读-修改-写操作，由于随机 Get 的速度相对较慢，它将产生很高的开销。UP2X 利用 RocksDB Merge 接口避免在更新过程中获取 Gets。 Methodology and Tool Set 开源的 RocksDB 负载分析和表征工具 https://github.com/facebook/rocksdb/wiki/RocksDB-Trace%2C-Replay%2C-Analyzer%2C-and-Workload-Generation Tracing：工具收集 RocksDB 对外暴露的开放接口对应的查询信息，并将信息记录在 trace files 中。主要包括 query type、CF ID、key、query specific data、timestamp。对于 Put 和 Merge，将值信息存储在特定于查询的数据中，对于 Seek 和 SeekForPrev 之类的迭代器查询，扫描长度(在 Seek 或 SeekForPrev 之后调用 Next 或 Prev 的次数)存储在特定于查询的数据中。为了在跟踪文件中记录每个查询的跟踪记录，需要使用锁来序列化所有查询，这可能会带来一些性能开销，但是，根据常规生产工作负载下的生产中的性能监视统计数据，我们没有观察到跟踪工具导致的吞吐量下降或延迟增加。 Trace Replaying：回放工具根据跟踪记录信息向 RocksDB 发出查询，查询之间的时间间隔遵循跟踪中的时间戳，通过设置不同的快进和多线程参数，RocksDB 可以对不同强度的工作负载进行基准测试。但是，多线程不能保证查询顺序。Replayer 生成的工作负载可以看作是真实世界的工作负载。 Trace Analyzing：由于工作负载跟踪的潜在性能开销，很难跟踪大规模和长时间的工作负载，此外，跟踪文件的内容对其用户/所有者来说是敏感和机密的，因此，RocksDB 用户很难与其他 RocksDB 开发人员或第三方公司的开发人员共享跟踪信息。为了解决这些限制，我们提出了一种分析 RocksDB 工作负载的方法，该方法根据跟踪中的信息来分析工作负载。 针对每一个 CF 中的 KV 对、query numbers、query types 的详细统计摘要 key value 大小统计 kv 对的流行度（热度） 键的空间局部性，它将访问的键与数据库中所有现有的键按排序顺序组合在一起 查询次数/秒 统计 Modeling and Benchmarking：首先选定两个变量计算对应的相关系数，来确定变量相关性较低。通过这种方式，每个变量都可以单独建模，然后我们将收集到的工作负载匹配到不同的统计模型中，以找出哪一个具有最低的拟合误差，哪一个比总是将不同的工作负载匹配到相同的模型(如Zipfian)更精确。然后，该基准可以基于这些概率模型生成KV查询。 General Statistics of Workloads 我们将介绍每个用例的一般工作负载统计，包括每个 CF 中的查询组合，KV-pair 热度分布和每秒查询数。 Query Composition：Get 是 UDB 和 ZippyDB 中最常用的查询类型，而 Merge 在 UP2X 查询中占主导地位。在不同的 CF 中查询组合可能都会有很大不同， 如下图所示 UDB 的统计 ZippyDB 只有一个 CF，Get : Put : Delete : Iterator = 78 : 13 : 6 : 3 UP2X Merge : Get : Put = 92.53 : 7.46 : 0.01 KV-Pair Hotness Distribution：UDB 和 ZippyDB 中大部分 KV 数据是冷数据 UDB 24 小时内访问的 key 最高不超过 3%，而 14 天内访问的 key 最高不超过 15%。如下为 UDB 的 Get 与 Put 操作的 KV 数据访问的 CDF 图。对于 Get，除了 Assoc 之外，其他 CF 的数据 60% 或者以上的数据都只被访问了一次。对于 Put，超过 75% 的数据都只被访问一次，Put 次数超过 10 的数据仅占不到2%，所以UDB中的KV数据大部分很少被Update。 对于ZippyDB，大约80%的key只被访问一次，1%的key被访问超过100次，因此表现出较好的局部性。约73%的数据只被Put一次，访问次数超过10次的数据仅有0.001%，因此Put的局部性较差。 UP2X：对于UP2X，Get与Merge的访问次数分布较广，并且访问次数高的数据占比比较大。 QPS (Queries Per Second)：UDB 的部分 CF 表现出较强的昼夜模式，这跟社交网络用户习惯相关（白天上 Facebook，晚上在睡觉），ZippyDB 和 UP2X 没有表现出这样的特征。 Key and Value Sizes：key size通常比较小，value size的大小与具体数据类型有关，key size的标准差较小但是value size较大， UDB平均的value size比其他两个例子要大 Key-Space and Temporal Patterns：统计方法：对key按递增顺序编号，然后统计每个key的访问次数绘制heat-map，统计key的访问时间绘制time-series。结论：heat-map可以看到三种DB的访问具有较强的key space locality，也就是热数据往往聚集分布在某些key space内。UDB的Delete/Single Delete以及UP2X的Merge的time-series表明其访问具有时间局部性 UDB：KV数据访问并不会随机分布在整个key space，而是根据key-space进行区分，部分key-space访问热度较高，这部分数据占比较小，而部分基本没有访问。属于同一个MySQL table的数据物理上也相邻存储，部分SST和block具有较高的热度，可以考虑基于这个优化compaction以及cache。 ZippyDB：ZippyDB的访问具有较为明显的key-space locality UP2X：UP2X的访问也表现出较强的key-space locality，只有后半段key被访问，而前半段key基本没有访问。merge的time-series表现出来merge每一段时间内会集中访问一个range内的key。 Modeling and Benchmarking 一些研究使用YCSB/db_bench + LevelDB/RocksDB来基准测试KV存储的存储性能。研究人员通常认为YCSB产生的工作量接近于实际工作量。对于实际的工作负载，YCSB可以针对给定的查询类型比率，KV对热度分布和值大小分布生成具有相似统计信息的查询。但是，尚不清楚它们在实际工作负载中生成的工作负载是否与基础存储系统的I/O相匹配。 为了对此进行调查，我们集中于存储I/O统计信息，例如由RocksDB中的perf_stat和io_stat收集的块读取，块缓存命中，读取字节和写入字节。为了排除可能影响存储I/O的其他因素，我们重放跟踪并在干净的服务器中收集统计信息。基准测试也在同一服务器中进行评估，以确保设置相同。为确保重放期间生成的RocksDB存储I/O与生产环境中的I/O相同，我们在收集跟踪的同一RocksDB的快照中重放跟踪。快照是在我们开始跟踪时创建的。YCSB是NoSQL应用程序的基准测试，而ZippyDB是典型的分布式KV存储。因此，预期YCSB生成的工作量接近ZippyDB的工作量，我们以ZippyDB为例进行调查。由于特殊的插件要求以及UDB和UP2X的工作量复杂性，我们没有分析这两个用例的存储统计信息。 总的来说YCSB测workload相比于真实的trace会造成更大的读放大以及更小的写放大，同时cache命中率也更低。其中的主要原因在于忽略了真实workload的key space locality，YCSB中的热点数据随机分布在整个key范围内，访问这些key会造成大量的block读并且被缓存，而这些block可能仅包含较少的热数据，cache大小有限所以降低了cache命中率。对于put，随机的热点分布使得数据在前几层就被compaction掉，所以造成更小的写放大，update的数据具有key space locality，那么新数据会不断写入，就数据一直往下compact直到新数据遇到旧数据才会被处理掉。 Key-Range Based Modeling：整个键空间被划分成几个较小的键范围。我们不再仅仅基于整个键空间统计数据对KV对访问进行建模，而是关注这些键范围的热度。实验发现 当键范围大小接近SST文件中KV对的平均数目时，它可以保留数据块级别和SST级别的局部性。因此，我们使用每个SST文件的平均KV对数作为键范围大小。 然后将key size，value size以及QPS套到模型里面去，再处理kv的访问次数，访问顺序以及每个range的平均访问次数，然后和原有的负载进行对比测试。 Prefix_dist：基于建模构建的workload Prefix_random：随机将冷热数据分布到各个key-range All_random：热数据随机分布到整个key space All_dist：热数据集中放置 个人见解 做 Benchmarking 的工作今年来比较少，一般也只有相应的企业才能统计出相应的数据特征。Benchmarking 常常意味着一系列工具都需要去实现，如本文中的负载的 Trace 收集分析统计，意义很大，对于后续的 KV 存储研究提供了新的基准测试，也配套了相应的工具可以自己去收集相应的负载，且已开源。总之对于 KV 存储的研究，未来可能会成为新的测试基准。 FPGA-Accelerated Compactions for LSM-based Key-Value Store 本文主要还是解决 LSM Tree 中的压缩慢的问题，压缩慢还会因为资源的争用影响整个存储系统处理的性能，然后呢也是得出相同的结论，存储系统的瓶颈在往 CPU 转移，这个观点在之前看的 KVell （SOSP19）的那篇文章就已经充分说明。采用的解决办法呢其实就是替 CPU 减压，引入一个新的处理单元进来，FPGA，因为 compaction 操作其实本质就是归并排序，所以 FPGA 完全可以胜任，这个思路其实也不是很新奇，ATC20 的 Best Paper 的 PinK 其实也采用了这种方法。 其实问题和思路都不算是特别地别具一格（这里没有去追究前面文章里的具体的时间先后顺序），但是毕竟是经历了工业界的验证的，阿里巴巴自研的 X-Engine 的存储引擎就使用了本文描述的技术，所以也不妨深入读一读，看看有什么有意思的点。 照例贴链接： 阿里云-开发者社区：X-Engine 研究综述 知乎：匠心之作 | 厉害了！阿里云自研存储引擎X-Engine又发顶会啦 简书 - Glitter试做一号机：FPGA-Accelerated Compactions for LSM-based Key-Value Store 本文主要提出的方法就是将压缩给下沉到 FPGA 来执行，从而加速压缩，减小 CPU 瓶颈。测试表明该方法加速压缩 2-5 倍，系统吞吐量提升了 23%，能源效率（每瓦特处理的事务数量）提升了 31.7%。 BTW，X-Engine 的论文发表在 SIGMOD19 简要介绍 Introduction &amp; Background 首先大致介绍了基于 LSM Tree 的 KV 的应用场景，以新零售为例做了简单介绍。除此以外，KV 主要还会和其他数据库一起作为缓存或者索引来提供服务，这也是 KV 存储的关键的一些应用。然后简单介绍 LSM Tree，LSM 这种数据组织形式通常迫使查询遍历多个级别，以合并分散的记录以获得完整的答案或查找记录，甚至使用索引。这样的操作会带来跳过标记为删除的无效记录的额外开销。为了控制这些 drawbacks，后台压缩操作被引入，在相邻的层之间合并键范围重叠的数据块，并删除已经标记为删除的记录，目的是保持 LSM 树在一个适当的分层形状。 引入了一个新的概念，WPI（write and point read-intensive）负载，在长时间的 WPI 负载下，因为 LSM Tree 本身数据结构维护的不好（比如出现过大的 levels），性能就会表现得越来越差。 作者总结发现在 LSM Tree 中有一个很困难的 trade-off，即分配给关键路径上查询操作和事务处理的资源 和 分配给后台压缩线程的资源（后台压缩操作需要消耗大量的计算资源和磁盘 I/O 资源，特别是对于既包含读又包含写的 WPI 负载），如果给 compactions 分配更多的软件线程，就会在降低 CPU 实际处理查询和事务的风险下，加大了对存储的后台维护。 如图 1 所示，描绘了在 WPI （75% 点查询、25% 写操作）的负载下吞吐量随着用于 Compaction 线程增加的变化情况。线程数小于 32 之前都是随着线程数单调递增，从而带来显著的性能优势。然而，随着更多的线程用于 Compactions，CPU 逐渐饱和，然后会产生 CPU 争用，因此系统吞吐量在 32 个线程之后下降。后文将有更详细的研究表明 32 线程压缩的时候仍然不够快，无法解决上述问题。 现有的优化 Compaction 的研究主要采用了两种方法： 一种是通过利用数据分布的特点（几乎是有序的，没有重叠的范围）来减小来减少每次压缩的负载，从而避免不必要的合并。如 X-engine（SIGMOD19），VT-Tree（FAST13） 或者通过将数据分割到多个分区，并在需要时分别为每个分区调度 Compaction 操作。The partitioned exponential file for database storage management(VLDBJ07) 另外一种方法是优化压缩的时机和选择哪些数据来压缩(when and where)。bLSM (SIGMOD12)。理想情况下，为了提高性能，压缩应该在最需要的时候完成，并且它的执行与系统中其他操作的资源竞争最小 但是上述的关于 when 和 where 的条件在 WPI 负载下经常都是会有冲突的，因为对compaction 的需求和对高吞吐量的需求经常同时达到峰值。因此，对于CPU和I/O，存储系统中的资源竞争仍然是一个挑战，通过增加 CPU 线程限制了压缩速度的可伸缩性，并留下了性能问题。 本文提出讲 Compaction 的压力从 CPU 向 FPGA 转移从而加速压缩的执行。理论角度上分析， offload compaction 一定程度上将 CPU 从 I/O 密集型应用中解放了出来，从而让存储系统使用更少的 CPU 或者在使用相等数量的 CPU 的情况下增加了吞吐量，在公有云的场景下都能降低成本开销。 相比于更喜欢进行 SIMD（单指令多数据流） 类型的计算的 GPU，FPGA 更符合加速压缩操作的需求，压缩任务本质是计算任务的 pipeline。我们还发现对于比较小的 KV 对的压缩合并往往会被计算资源给限制住，可能是因为现在磁盘的 I/O 带宽越来越高。 与其他方案相比，FPGA 的高能源效率在降低总拥有成本(TCO)方面具有竞争优势。KV 存储的用户对于成本其实很敏感，因为存储往往都意味着需要永久支出这笔成本。所以在云场景下，使用 FPGA 来 offload compaction 将能显著提升 LSM Tree 的经济效益。 在 FPGA 上，我们设计并实现了压缩操作，一个包含了三个阶段的流水线操作： decoding/encoding inputs/outputs merging data managing intermediate data in buffers 我们还实现了一个 FPGA 驱动程序和异步压缩任务调度器，以方便 offload 和提高效率，该方案被集成在 X-Engine 中，使用了不同的 WPI 负载来进行测试。效果显著。 以前的 LSM Tree 结构如图 a 所示，C0 满了之后合并到 C1，合并的开销会随着 C1 的大小的增加而增加，为了限制这样的开销，更好的选择是是把一个磁盘组件分成不同层级的多个组件，每个组件然后比前一层的组件大。但是会有写放大，因为一个 KV 对可能不得不合并很多次，同时也有读放大，因为查询不得不访问多个具有重叠键范围的组件。 为了限制读写放大，许多研究提出了如图 b 所示的分层存储结构。该结构具有优化的内存数据结构、多层磁盘组件，每一层由多个文件或细粒度的数据块组成。数据首先被插入到内存的 memtables 中（常用跳表来实现），一旦 memtables 满了之后转变成 immutable memtables，然后刷入到磁盘上的 L0 层，这里的 Lk 和以前的 LSM 树中的 Ck 是类似的，最大的区别是 Lk 是被分区成很多个文件（也就是 RocksDB 中的 SSTables）或者数据块（X-Engine 中的 extents），对应的合并策略有两种类型： 将其与目标级别中的现有数据合并，即所谓的 level 策略，这种方法以压缩速度为代价，将数据按良好的排序顺序保存在某个级别中 另外一种只需将数据追加到下一层，而不进行合并，这称为 tier 策略，压缩本身速度很快，但会牺牲某个级别内的排序顺序 尽管上面介绍了最先进的优化，但我们发现，由于以下原因，压缩速度慢仍然会导致运行WPI 工作负载的 LSM-tree KV 存储的性能疲劳问题： Shattered L0：L0 层的数据块通常具有重叠的键范围，因为它们直接从主存中刷新而没有合并。除非压缩及时合并它们，否则点查找可能不得不检查多个块，以找到单个键，甚至索引也是如此。在这种压缩速度慢的情况下，随着时间的推移，L0 中存储的数据块会不断增加查找开销。这种破碎的 L0 对性能有重大影响，因为由于数据局部性，刷新到L0的记录仍然非常热(即很可能被访问) Shifting Bottlenecks：压缩操作自然由多个阶段组成:解码、合并和编码，因为KV记录通常是前缀编码的。为了确定这些阶段中的瓶颈，我们对 SSD 上由单个 CPU 线程执行的单个压缩任务进行概要分析。如图所示执行时间分解，随着 value 大小的增加，计算时间(解码、合并、编码)的百分比减少。而对于小 KV，计算占据整个压缩过程的 60% 的开销。当 value 大小达到 128 字节时，I/O 操作占用了大部分 CPU 时间。这个分解表明随着 KV 大小的增加，瓶颈从 CPU 转移到 I/O。这表明在合并小 KVs 时压缩以计算为限，在其他情况下压缩以 I/O 为限。 本文就是将上述的三个阶段给映射成 pipeline，并 offload 到专门的加速器。因为有了更快的压缩操作，L0 的数据块合并的更加频繁，因为 offloading，CPUs 从 heavy 的压缩操作中释放出来，从而腾出更多的资源用于事务和查询操作的处理。加速器选了 FPGA，因为 FPGA 它适合加速压缩等计算任务的 pipeline，且具有低 TCO 的低能耗，以及作为可插拔的 PCIe-attached 加速器的灵活性 原文还有一些关于 FPGA 本身的介绍，以及 FPGA 和 KV Store 结合的场景介绍。此处简单提及和 KVs 的结合问题，现有的研究大致有两种结合方式： bump-in-the-wire：将 FPGA 放在 CPU 和 disk 之间，该方法中，FPGA 类似于一个 data filter，在 FPGA 的片上 RAM 较小，只能临时保存数据流的一个 slice 随着片上 RAM 大小增加，FPGA 现在能像一个 co-processor 一样工作，所以可以把一些大数据量的处理任务给 offload 到 FPGA 上执行。这种方法适用于异步任务，在此期间，CPU 不会因为 offloaded 任务而 stall，本文就采用了这种方式，CPU 只用于任务的生成。Samsung 的 SmartSSD 也采用了这种方案实现计算型存储。 Design and Implementation Overview：利用 Memtables 缓冲新插入的 KV 记录，利用缓存来缓冲热 KV 对和数据块，磁盘上的每一层包含多个 extents，每个 extent 依次用关联的索引和过滤器存储 KV 记录。X-Engine 在内存中还维护了一个全局索引用于加速查询。 为了 offload compactions，首先设计了一个 Task Queue 来缓冲新触发的压缩任务，一个 Result Queue 缓冲在内存中的压缩过的 KV 记录。软件层面，引入了一个 Driver 来 offload compactions，包括管理从 host 到 FPGA 的数据传输过程，在 FPGA 上，设计并应用了多个 Compaction Units (CUs) 来负责合并 KV 对。 Managing Compaction Tasks：设计了三种线程，builder threads, dispatcher threads and driver threads 分别用于构建压缩任务、分发压缩任务到 FPGA 的 CU，install 压缩后的数据块到存储中。 Builder thread：对于每个触发的压缩，Builder thread 将 extents 分区合并到多个大小相近的组中，每个组然后形成一个压缩任务，且将数据加载在内存中。FPGA 压缩任务。将构建一个 FPGA 压缩任务，其中包含所需的元数据，包括指向任务队列的指针、输入数据、结果和一个回调函数（将压缩后的数据块从FPGA传输到主存）、返回代码（指示任务是否成功完成）和压缩后任务的其他元数据。压缩任务会被推送到 task queue，等待被分发到 FPGA 的 CU 上，Builder Thread 也会检查 result queue 并把压缩后的数据块 install 回存储设备中（如果任务被成功处理的话），如果 FPGA 处理压缩任务失败了（比如 KV 的大小达到 FPGA 的容量），将重新启动一个 CPU 压缩线程来重新处理该任务。实践表明，offload 到 FPGA 上平均只有 0.03% 的任务处理失败。 Dispatcher thread：dispatcher 消费任务队列，然后以 Round-Robin 的策略把任务分发到 FPGA 上所有的 CU。由于压缩任务大小相似，这种循环调度实现了 FPGA 上多个 CUs 之间的工作负载均衡分配。dispatcher 还会通知驱动线程将数据从设备内存传输到 FPGA。 Driver thread：Driver thread 将输入数据和一个压缩任务一起传输到 FPGA 上的 device memory，然后通知对应的 CU 开始工作，当压缩任务完成之后，Driver thread 被中断以执行回调函数，回调函数将压缩后的数据块传输到宿主机 Memory，然后把完成后的任务推送到 result queue。 在这样的实现中，需要调整压缩任务的大小以为 compaction unit 提供充分的数据，在 CUs 之间保证负载均衡，同时也可以限制重试在 FPGA 上执行任务的开销。我们还会对 builder/dispatcher/installer 线程数单独进行调整，以实现比较稳定的吞吐量表现。 Instruction and Data Paths 为了驱动 FPGA 用于压缩，我们需要通过 PCIe 总线传输指令和数据，为了最大化传输效率，设计了 Instruction and Data Paths，还设计了 Interruption Mechanism 来通知 Installer Thread，Memory Management Unit (MMU) 来管理 FPGA 上的设备内存。 Instruction Path：指令路径是为像 CU 可用性检查这样的小而频繁的数据传输而设计的 Compaction Data Path：数据路径使用 DMA。要压缩的数据通过此路径传输。这样，CPU 就不参与数据传输过程了。 Interruption Mechanism：当压缩任务完成时，将通过PCIe发送一个中断，然后在中断向量的帮助下将结果写回主机 Memory Management Unit (MMU)：MMU 在设备存储器上分配内存来存储从主机复制的输入数据 Compaction Unit 压缩单元(CU)是在FPGA上实现压缩操作的逻辑实现。很多个 CUs 被部署到同一个 FPGA 上，具体的数量取决于时机可用的资源。如下图所示了 CU 的设计，如下设计中，压缩任务由几个阶段共同组成：Decoder, Merger, KV Transfer, and Encoder。我们引入缓冲区 (即KV 环缓冲器、键缓冲器)在后续模块和控制器之间协调各模块的执行。 Decoder：KV 存储的 key 通常都是前缀编码的，为了节省空间，所以 Decoder 需要解码输入的 KV 记录。如果采用了 tiering compaction 策略，可能存在多种输入数据块的合并方式。而对于 levelling policy，只会有最多两个输入数据的方式，来自两个相邻的层。同时我们实际发现 tiering 最多也只有两个或四个输入方式，如果在一个 CU 里我们放置两个 decoders，每一个 decode 一种输入，那么我们需要构建三个 两路压缩 来实现一个 四路压缩，如果我们使用四个 decoder 会多消耗 40% 的硬件资源，而无需在 2-4 路压缩方式下执行额外的任务。因此我们在每个 CU 中放置了四个 decoders，decoder 将 解码后的 KV 对输送到了 KV Ring Buffers 中。 KV Ring Buffer：KV Ring Buffer 缓存解码后的 KV 记录，我们在实践中观察到 KV 的大小很少超过 6KB，因此我们给每一个 KV Ring Buffer 配备了 32*8KB 的 slots，额外的 2KB 可以用于存储如 KV 长度这样的元数据信息。我们还设计了三种状态信号， FLAG_EMPTY, FLAG_HALF_FULL and FLAG_FULL 来表示 buffer 的空间情况。从一个空缓冲区开始，解码器持续解码并填充这个缓冲区，当处于 FLAG_HALF_FULL 时，下游的 Merger 将被允许独去缓冲区中填充的数据然后开始合并，在 Merger 工作的同时，decoder 持续地填充数据直到填满，我们匹配 merger 和解码器的速度，这样，通过填充一半的环形缓冲区，merger 将花费相同的时间来完成它的工作。通过这种方式可以有效地将 decoder 和 merger 流水线化。如果两个模块速度不匹配，Controller 将停止 decoder。我们为 merger 维护了一个读指针来标记从哪里读取的 KV 记录，并为解码器类似地维护了写指针。 KV Transfer, Key Buffer and Merger：只有 keys 被传输到了 Key Buffer，然后 Merger 进行比较。如果一个 key 被限定为一个输出，KV Transfer 模块将相应的 KV 记录从上游的 KV Ring Buffer 传输到 KV Output Buffer，数据结构和 Ring Buffer 完全相同。Controller 收到比较结果的通知后移动 ring buffer 中相应的 read pointer。如图 9 所示，如果 way 2 上的 KV 是最小的 KV，Controller 将会通知 KV Transfer 将该条记录传输，使用对应的读指针进行寻址，然后再移动到下一个位置来拉取用于下一轮比较的 entry。 Encoder： 该模块编码合并后的 KV 记录，并放置到设备内存，因为这只有一种合并后的数据，所以每个 CU 只需要一个 encoder。 Controller：Controller 在 CU 中更多地是作为一个协调者，管理 KV ring buffer 的读写指针（分别为 merger 和 decoder 管理），也需要给每一个信号传递开始或者停止的信号。 Analytical Model for CU： 由于采用流水线设计，因此匹配不同模块的吞吐量非常重要，以避免硬件资源的过度供应和浪费，但是，很难通过如此多的调优选项来确定每个组件的资源数量。因此提出了一种分析模型来指导资源分配。参数如下： CU 的吞吐量由最慢阶段的吞吐量来决定，只有一个例外 KV Transfer，当 KV Transfer 工作时，合并 KVs 的转移与其前一阶段的 Merger 串行执行，这种情况下，他们的开销应该合并起来计算。每一阶段的成本由其计算和内存开销组成，除了在启动此阶段时消耗的恒定数量的基本周期之外。 等式 2 3 4 5 都建模了每个 KV 对在对应组件内的开销，等式 6 建模了在 host 和 device 之间数据传输的吞吐量，主要受到 PCIe 带宽的限制。通过检查硬件实现和性能分析，我们使用表1中列出的数据初始化这些模型。例如一个 KV 对，8B key，32B value，每个 KV 在每个阶段消耗的周期为 16， 55， 23， 52（Decoder, Merger, KV ransfer, and Encoder stages）。整体的吞吐量为 3.6M records/s Evaluation Hardware： two Intel Xeon Platinum 8163 2.5 GHz 24-core CPUs with two-way hyperthreading a 768 GB Samsung DDR4-2666 main memory a RAID 0 consisting of 10 Samsung SSDs a Xilinx Virtex UltraScale+ VU9P FPGA board (running at 200MHz) with a 16 GB device memory to this server through a x16 PCIe Gen 3 interface Software: Linux 4.9.79, X-Engine KV 对越小，更多的开销都是在 offload 过程，随着 KV 的增大，逐渐被稀释这部分开销，所以 KV 对越大，性能提升越明显 总吞吐量总是限制在合并和 KV 传输阶段的组合，既不不是对设备存储器的访问，也不是通过 PCIe 的数据传输 24 线程以前，压缩比 KV 存储的总写吞吐量要慢，因此它不能摄取lsm树中足够的新写数据。因此，使用更多线程加速压缩是有回报的。从24个线程到32个线程，吞吐量几乎没有变化。大于 32 个线程之后，随着线程的增加吞吐量下降。原因主要在于更多的线程也就引入了更多的资源争用，如前面图 1 所示整体的 CPU 利用率接近 100%，此时瓶颈在于 CPU。第二个原因是前面增加的线程已经让磁盘 I/O 饱和，持续增加线程只会造成更多的查询/事务处理和 compaction 之间的 I/O 争用。 FPGA-offloading 和 CPU-based 两种对比发现，FPGA-offloading 比最好的 CPU-based 的吞吐量都要高出 23%，因为不仅加速了压缩，还减少了 CPU 的争用。又因为 FPGA 上的压缩任务比较好的调度和分配，没有观察到对于内存的消耗上有太显著的区别，CPU-Only 的负载相对消耗更多的内存带宽。 吞吐量都有提升，延迟有所下降，尾延迟也有所下降，操作的响应时间也有降低，能源消耗也有所降低，整体的能源效率有所提高。 当读比例在 WPI 负载中下降到了 75% 时，也就是此时成为一个写密集的 WPI 负载，我们提出的卸载方法的性能优于基准，在所有情况下都减少了约 10% 的 CPU 消耗，并且由于其吞吐量高于 CPU，因此消耗的 I/Os 稍微多一些。 在 DBBench 和 YCSB 基准测试中，与读密集型的操作相比，当工作负载中有大量写操作时，提出的 FPGA-offload 压缩对 KV 存储的贡献更大，因为压缩在工作负载中 lsm 树只在写操作时触发 Related Work Software Optimizations of Compactions FAST13 的 VT-tree 使用拼接技术来避免排序和不重叠的键范围的不必要的磁盘 I/Os，但是这种方法受数据分布的影响，可能导致碎片化，使范围扫描和压缩的性能变差。 SIGMOD12 的 bLSM 和 VLDBJ07 的 PE 考虑了数据分布，都是将键范围分渠道多个键的子范围并限制比较热的键范围的数据压缩。 IPDPS14 的 PCP 观察到压缩可以流水线化，使用多个 CPUs 和存储设备来充分利用 CPU 和 I/O 资源从而加速压缩过程。 SIGMOD18 的 Better spacetime trade-offs for lsm-tree based key-value stores via adaptive removal of superfluous merging 通过合并尽可能少的内容来实现查找成本和空间的给定边界，从而提供了更丰富的时空权衡，并提出了一种混合压缩策略(即，对最大级别使用 levelling，对其余级别使用 tiering) 来减少写入放大 SIGMOD19 的 X-engine 提出将 LSM-tree 中的数据分割成小数据块，并在压缩过程中广泛重用键范围不重叠的数据块，以减少写放大。 所有这些软件优化和我们提出的基于 FPGA 的优化是正交的，即可以一起作用来显著提升系统性能并降低能耗。 Hardware Accelerations in Databases 数据库领域早就在尝试使用各种其他类型的硬件来加速数据的读写。 个人见解 本文还是问题找的比较关键准确，所以后续的提升方案才能带来如此大的提升，而且这个问题也基本是学术界很多学者都已经发现了的问题，只是各自的解决方式不同，但大体思路其实是相通的，即借助其他处理器资源来减少 CPU 的争用问题。不同的是不同层级的优化，如很多做 KVSSD 的研究人员更多是在 SSD 那一层去做这样的优化，本文则更多地是从系统层面来做优化，一个好的点是可能很少人关注到引入新硬件后引入的成本和能耗问题，这个确实云厂商比较关注这方面的问题，所以在评价的角度上能够多出一环。 看完这篇文章的具体感受就是，处理器现在也是百家争鸣，具有各自鲜明特点的硬件单元堆积在板子上可能将会成为一个趋势，即把更专门的运算交给更专业的人去做，而不是像以前 CPU 一锅端。 HotRing: A Hotspot-Aware In-Memory Key-Value Store 本文主要针对的内存 KVs，最主要的应用还是作为 Cache，在大量的互联网场景中都有应用，作为 Cache 需要面对和解决的问题其实就是热点数据的访问。内存中的 KVs 最常见的数据结构就是 HASH，诸如像 Redis 这样的成熟的的内存 KVs，大多都是使用 HASH 来实现。而本文立足的场景也是阿里云淘宝这种并发高的电商场景，需要强劲的缓存支撑，淘宝的 Tair 也算是业界比较知名的数据库团队。 本文主要还是针对热点数据的优化，更多地是从数据结构本身出发，因为相关参考资料也已经讲的很多了，此处主要介绍问题和大致的方案。 贴几个链接： 知乎 - 暗淡了乌云：看了几篇FAST 2020 CNBlogs - 晓乎：HotRing: A Hotspot-Aware In-Memory Key-Value Store 知乎 - 阿里技术：性能提升2.58倍！阿里最快KV存储引擎揭秘 本文主要探索针对内存索引结构的热点感知的相关设计，作者首先分析了理想的热点感知的索引的潜在收益，并讨论了有效利用热点感知可能面临的挑战（热点变化和并发访问问题）， 基于此分析提出了可以热点感知的 KVs 设计 HotRing，针对一小部分 items 进行大规模并发访问而优化的。HotRing 基于有序的一个有序的环状 HASH 索引结构，通过移动 head 指针来快速访问热点数据。也会应用一个比较轻量的策略来检测运行时的热点变化。HotRing 在设计中采用了无锁的结构，所以并发操作能够更好地利用多核架构的性能。实验表明我们的方法相比于其他内存 KVs 在高度倾斜的负载下实现了 2.58x 的提升。 简要介绍 Introduction &amp; Background 常见的内存 KVs，如 Memcached、Redis、SIGMOD18 的 FASTER(HASH)、NSDI13 的 MemC3(HASH)、NSDI14 的 MICA(HASH) 等。热点问题是一个广泛的问题，有一些集群层面上的热点问题的解决方案，如一致性 HASH、数据迁移、前端数据缓存等，同样单节点上的热点问题也需要被解决。比如计算机体系结构利用垂直的存储结构来缓存最常访问的数据在更低延迟的存储介质上。但是对于内存 KVs 内部的热点数据问题往往忽略了。作者从阿里巴巴的生产环境中收集了内存 KVs 的访问情况如图所示，可以发现 日常分布中的百分之五十和极端分布中的百分之九十的访问都只会访问到总的数据的 1%，这说明网络时代的热点问题空前严重。（具体的原因主要是在线应用的活跃用户数量持续增长，一些热门事件都会导致在短期内对于少部分数据的大量访问，所以对这些热点数据的快速访问是比较关键的，除此以外这些应用程序之下的基础设施变得复杂，一个小错误，例如由于软件错误或配置错误，可能导致(不可预测的)重复访问一个项，例如无休止地读取和返回一个错误消息。理想的效果是这些不可预测的热点不会导致整个系统崩溃或阻塞） HASH 索引是最流行的内存 KV 数据结构，特别是针对于一些不需要范围查询的场景。下图展示了 HASH 索引的一种通用结构，bucket + list。HASH 值可以分为两部分，一部分用作 HASH 表的索引，一部分用作 list 种的比对，从而减少比较过程中的需要比较的对象数据的长度。 现有的内存 HASH 索引无法感知热点数据，即未对热点数据进行区分（如上图中的 热点数据 item3 位于冲突链的末尾，就会比之前多更多的内存访问次数，在一些倾斜负载的情况下热项访问成本的轻微增加可能导致整体性能的严重下降） 采用的相同的策略管理所有数据，但是理论分析表明查询热点数据的开销比理想的策略大得多，虽然存在一些减少内存访问的机制，但它们只能提供有限的效率。 比如 CPU 利用自己的缓存来加速吗，但是只有 32M 大小，完全不足以缓存热点数据，即便是有一些缓存友好的数据结构 可以扩大哈希表(即通过重新哈希)，以减少冲突链的长度，从而查找一个热项所需的内存访问更少，Rehash 可以帮助减少冲突链的长度，但会显著增加内存占用。特别是当 HASH 表已经很大的时候，就不再推荐重 HASH。例如，对于两个连续的重哈希操作，第二个需要两倍的内存空间，但只带来一半的效率(就减少链长度而言)。 在本文中，我们提出了 HotRing，这是一种支持热点的内存 KVS，它利用 HASH 索引来优化对一小部分数据项(即热点)的大规模并发访问。最初的想法是让查找一个项所需的内存访问与它的热度负相关，即越热的项读取速度越快。但是需要解决两个问题： hotspot shif：热点项集不断变化，我们需要及时发现并适应这种变化 使用 ordered-ring 来解决，当热点变化时，bucket headers 可以直接重新指向热的数据项，而不会影响正确性 同时设计了一个轻量的机制来检测运行时的热点变化 concurrent access：热点本质上是被大量并发请求访问的，我们需要为它们维持高并发性 基于现有的无锁数据结构（无锁链表）采用了一种无锁的设计，并扩展该数据结构以支持 HotRing 所需要的其他操作，包括热点变化检测、头指针移动以及有序环 rehash Design HotRing 的 HASH 索引结构如下所示，将以往的传统的 HASH 冲突链表转变为冲突环，原本的链表最后的项将和链表的首项连接起来，当环中只有一项数据时，所有指针都指向自身。 Ordered-Ring Hash Index：环状结构的问题就是没有遍历的终止条件（当对应 bucket 内无对应数据时），就需要一个机制来安全地终止查询过程。很直观的想法就是将 HEAD 指向的第一项标记为停止遍历的标识，但是对于并发请求时会有问题，比如删除了标记的项。所以本文提出了有序环结构来决定查询的过程，使用 Key 和 Tag 进行排序。（先按照 tag 排序，然后按照 key 排序），相比于以往的链式结构，平均查询次数减少到了 (n/2) + 1 次 Hotspot Shift Identification：此处讨论的都是 bucket 内的热点数据的变化，通常冲突链上有 5-10 个数据项，然后根据热点数据的比例 0.1 到 0.2 推断大约每个 Bucket 内有一个热点数据，可以直接把 HEAD 指针指向唯一的热点数据，从而避免数据的重新组织并减少内存开销。为了获取较好的性能，需要考虑两个因素： 热点识别的准确率：热点识别的准确性是通过识别热点所占的比例来衡量的 响应延迟：响应延迟是一个新的热点出现和我们成功检测到它之间的时间跨度。 基于上述两个因素提出了两种策略： random movement 和 statistical sampling strategy Random Movement：响应时间短，但是准确率低，其基本思想是，头指针从即时决策周期性地移动到潜在热点，而不记录任何历史元数据。每个线程维护一个 ThreadLocal 变量来记录该线程执行的请求数量，每 R 个请求后，线程决定是否执行头指针的移动操作。如果第 R 次访问是对热数据的访问，那么指针不移动，如果是对冷数据的访问，那么 HEAD 指针指向该冷数据，因为可能即将成为热数据。参数 R 将影响响应延迟和识别的准确率，如果 R 太小，为了实现稳定的性能的响应延迟会很低，但是将导致频繁的且低效的头指针移动。我们的场景中，数据负载是严重倾斜的，因此头指针的移动往往不频繁，根据经验，参数R默认设置为5，该参数可以提供较低的反应延迟和可忽略的性能影响。 如果负载的倾斜状况不是很明显，该策略将变得特别低效，最重要的是该策略无法处理一个冲突环中多个热点数据的问题，多个热点数据的时候，头指针频繁地移动，并不会加速对热点数据的访问反而会对正常操作产生不利影响。 Statistical Sampling Strategy：为了实现更高的性能，我们设计了一种统计采样策略，旨在提供更准确的热点识别与稍高的反应延迟。 首先考虑数据格式：Head Pointer 和 Item。将同时记录 Ring 和 Item 级别的统计数据。 Head Pointer 由三部分组成： Active bit: 用于控制该策略的 flag Total Counter: 对该冲突环的访问总次数 Address: 为对应项的物理地址 Item 由以下几部分组成： Rehash：用于控制 rehash 过程的 flag Occupied：用于保证并发时的准确性 Counter：用于记录该项的访问次数 Statistical Sampling：如何在这么大的 HASH 表中以一个低开销的方式动态识别出热点数据是一个极具挑战性的问题，关键是尽量减少开销，同时保持精度，这是通过周期采样在HotRing实现的。每个线程维护一个 ThreadLocal 变量用于计数已经处理了的请求数，每 R 个请求之后我们决定是否要开启新一轮的采样，通过改变 Active flag 的值来控制。如果第 R 个访问时热点数据，意味着当前的热点识别仍然是准确的，采样无需触发，如果是冷数据，意味着热点发生了变化，然后我们开始采样。R 默认还是设置为 5，当 Active 位被置为 1，后续对环的访问将被记录在 Total Counter 和对应的 Item 的 Counter 中。采样的策略要求额外的 CAS 操作，可能会导致临时的性能降级。为了减少这个时间，我们将样本的数量与每个环中的项目数量设置相等，我们认为这已经提供了足够的信息来派生新的热点。 Hotspot Adjustment：采样完成后，最后一个访问线程负责频率计算和热点调整。该线程首先使用 CAS 原语 RESET Active BIt，这确保了只有一个线程将执行后续任务。然后，该线程计算环中每个项的访问频率（用于后续计算 income），对应项的 Counter 除以该环的 Total Counter，然后计算每个项被头指针指向的 income，即某个项被选中被头指针指向对应的平均内存访问次数。然后选择出 income 最小的项作为热数据从而确保热点最快被访问，头指针的移动也是 CAS 操作。对于多个热点数据的情况，可以计算出相对较优的位置从而避免热点数据之前的频繁移动。调整了热点数据之后，负责的线程重置所有的计数器便于下次采样。 Write-Intensive Hotspot with RCU：对于更新操作，如果 value 小于 8 byte，那么可以就地更新。这个例子中，reading 和 updating 一个项被视作相同热度的操作，但是对于 value 比较大的项的更新操作就完全不同了。RCU（Read-Copy-Write）协议的情况下，前项的指针需要被修改因为在更新过程中指向了新的 item，如果HEAD 指针指向的写密集型热点被修改，整个冲突链将被遍历才能到达前项，也就是说一个写密集型的热数据也会造成他的前项数据变热，基于此我们稍微修改了统计采样策略。对于 RCU 更新，改为只对它的前一项的计数器加一。 Hotspot Inheritance：当对head项执行RCU更新或删除时，我们需要将head指针移动到另一个项。但是，如果头指针随机移动，它可能指向一个冷的数据，这将导致热点识别策略被频繁触发。此外，识别策略的频繁触发会严重影响系统的性能。如果环中就一个数据项，CAS 直接修改头指针来完成更新和删除操作，如果这有很多项，HotRing 使用现有的热点信息（头指针指向的位置）来继承对应的热度。针对RCU的更新和删除操作，我们设计了不同的头指针移动策略，以保证热点调整的有效性： 对于head项的RCU更新，由于访问的时间局部性，最近更新的项有很高的被立即访问的概率，因此头指针被移动到 head 的新版本 对于头项的删除，头指针只是移动到下一个项，这是一种简单而有效的解决方案 Concurrent Operations：HEAD 可移动导致并发控制更为复杂，但其实主要控制几个关键数据的原子操作即可。如 Next Item Address 后项指针需要使用 CAS 操作来保证原子性。 Insert 操作如图 A 所示，需要保证只有一个修改后向指针的操作成功。 Update 小于 8 byte 时无需额外的保证，对于 RCU 操作才需要进行控制。还是图 A 的例子，一个插入 C 的线程需要修改 B 的后向指针，一个线程需要更新 B 的数据为 B‘，这两个操作因为更新的是不同的数据指针其实都会成功，但是因为 B 对于环而言不可见了，即便 C 插入已经成功，所以后续对 B 的操作就会出错（C 的插入更新 B 的后向指针的操作），在图 b 中也有相同的问题，所以使用 Occupied 位来确保正确性，所以使用两个步骤来完成操作。对于 Update&amp;Insert 操作，需要更新后向指针的 B 先把 Occupied 位置原子性第置为 1 ，一旦 Occupied 被置为 1，插入 C 的完整操作将会失败并需要重试。然后更新 A 的后项指针指向 B’，然后将 B‘ 的 Occupied 位重置。 Delete：删除是通过将原本指向了已删除项的指针修改为下一个项来实现的。因此需要保证要删除的项的后向指针在操作过程中不被改变，还是借助 Occupied 位来实现。如图 C 所示 Head Pointer Movement： 如何处理正常操作的和热点识别策略引起的头指针移动 形成的并发的情况？ Occupied 如何处理头部指针移动，造成更新或删除头部项目? HotRing不仅需要 occupy 准备删除的项，还需要 occupy 下一个项。因为如果在删除操作期间下一项未被占用，则下一个节点可能已被更改，这使得头指针指向一个无效的项。 Lock-free Rehash：我们在HotRing中提出了一种无锁的重新哈希策略，允许在数据量增加时灵活地重新哈希。传统的重哈希策略是由哈希表的装载因子(即链的平均长度)触发的。但是，这没有考虑热点的影响，因此不适合HotRing。为了使索引适应热点项的增长，HotRing使用访问开销(即检索一个项的平均内存访问次数)来触发重新哈希。 Initialization：创建一个后台 HASH 线程，线程通过共享 tag 的最高位来初始化新哈希表，新哈希表的大小是旧哈希表的两倍。如图 a 所示。同时，重哈希线程创建一个由两个子重哈希项组成的重哈希节点，这两个子重哈希项分别对应两个新的头指针。每个重哈希项的格式与数据项相同，只是没有存储有效的KV对。HotRing 通过每个项中的Rehash 位来标识重新散列项。在初始化阶段，两个子重散列项的标记设置不同。如图 b 所示，对应的重散列项分别将标签设置为0和T/2。 Split: 重哈希线程通过向环中插入两个重哈希项来分割环。如图 C 所示，将重新哈希项分别插入到项B和项E的前面，作为标签范围的边界来划分环。当两个插入操作完成时，新建表被激活。之后，依次访问新表需要通过比较 tag 选择相应的头指针，而以前访问(从旧的表）通过 tag 重新散列节点继续。可以在不影响并发读和写的情况下正确地访问所有数据。到目前为止，对项的访问在逻辑上被划分为两条路径。 Deletion: 如图 d 所示，在此之前，重散列线程必须维护一个过渡期，以确保从旧表发起的所有访问都已经完成，比如read-copy-update同步原语的过渡期。当所有访问结束时，重新哈希线程可以安全地删除旧表，然后重新哈希节点。注意，过渡期只阻塞重散列线程，而不阻塞访问线程。 个人见解 本文还是问题找的比较关键准确，当然主要是因为淘宝有对应的真实场景，冷热数据的访问影响极大，但相较于以往针对缓存算法中冷热数据的识别，本文将问题更多地聚焦在 HASH 冲突链内，所以方向性也就比较明确，个人觉得可以在缩小问题范围之前加一个测试来说明 HASH 冲突链内的热点问题的严重性，来进一步证明把这个大问题缩小范围的有效性和可行性。 本文设计的数据结构还是挺巧妙的，乍一看链变环仿佛没啥新东西，但其中蕴藏了很多有意思的问题，尤其是并发场景下的一些数据访问的问题，你使用了有序环的结构减少内存访问次数，但也就需要见招拆招地解决环结构本身的问题，对于并发访问的场景考虑的也很周全。 Caching BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server Shucheng Wang, Ziyi Lu, and Qiang Cao, Wuhan National Laboratory for Optoelectronics, Key Laboratory of Information Storage System; Hong Jiang, Department of Computer Science and Engineering, University of Texas at Arlington; Jie Yao, School of Computer Science and Technology, Huazhong University of Science and Technology; Yuanyuan Dong and Puyuan Yang, Alibaba Group BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server 本文是隔壁实验室大佬发的，和 Alibaba 也有一些合作（盘古），有一些生产环境中的实际问题和数据可以借鉴。发现的问题也算是比较关键。 贴几个链接： FAST20 论文学习：Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Server NBJL 2020论文导读10：BCW: Buffer-Controlled Writes to HDDs for SSD-HDD Hybrid Storage Servere 场景：本文主要针对混合存储（Hybrid Storage）场景，SSD 作为缓存为 HDD 加速。HDD 因为容量大、成本低在数据中心还是有着很多的应用，特别是对一些冷数据的存储，但对于当下对于存储设备的性能需求，HDD 又无法提供用户期待的性能，所以大多数公司会使用性能更好的 SSD 来为 HDD 加速，在保证大容量低成本的同时提供一定的性能保证。 问题：在阿里的生产环境中发现（Pangu storage nodes A (Cloud Computing), B (Cloud Storage), C and D (Structured Storage)），这种混合存储的场景下有一个很严重的问题，就是对于 HDD 和 SSD 的使用严重不均衡，即对 SSD 过度使用，而 HDD 的利用率较低，如果负载是一些写密集型的负载，那么 SSD 磨损的情况更严重，而且因为 SSD 本身的机制的原因，写密集将导致频繁的 GC 从而增大 SSD 响应的尾延迟。 写入的数据量极大，每个 SSD 每天写入的数据有接近 3TB，接近于 DWPD (Drive Writes Per Day)，严重影响 SSD 的可靠性（相当于每天满负荷运行） https://www.kingston.com/cn/ssd/dwpd 有很多突发 IO，SSD 需要处理这种突发的写密集型负载 写入到 SSD 和 HDD 的数据量差别较大，利用率有 10 个百分点的差距，而且 HDD 大多是转储 SSD 的数据，基本不直接服务用户请求 存在长尾延迟，因为队列阻塞。主要原因有两类：单个 IO 操作较大 1MB；频繁的垃圾回收 小 IO 占据了所有 IO 的很大一部分 以往的方案：以往解决上述问题的方案主要有两类，一类则简单粗暴直接增加 SSD，但成本开销巨大；另一类则是利用相对空闲的 HDD 来吸收部分写，也就是所谓的 SSD 写重定向 SSD-Write-Redirect (SWR) SoCC 2019，该方案可以一定程度上解决 SSD 队列的阻塞问题，HDD 上的延迟比 SSD 高 3-12 倍，对于大多数需要微妙级延迟的小型写操作来说，这即使不是不可接受的，也显然是不可取的。所以方案目标还是想降低 HDD 的延迟到 SSD 级别。 发现：除了上述问题以外，作者还有对于 HDD 新的发现，一系列连续的、顺序的对 HDD 的写操作呈现出周期性的、阶梯状的写延迟模式（低、中、高），原因主要是因为 HDD 控制器将写操作给缓冲了。这个发现也就意味着 HDD 本身是可以提供微妙级的 IO 写延迟的（如果进行了适当的写操作调度），这时候和 SSD 的写性能相近。这些观察结果促使作者有效地利用 HDD 的这种性能潜力来吸收尽可能多的写操作，从而避免 SSD 的过度使用而不会降低性能。 如图所示，无论是多大的 IO 操作，HDD 都表现出了低中高三种延迟模式，且具有很强的周期性，前两种可以视作微秒级的响应，因为写操作一旦被写到内置的控制器 buffer 中就已经认为该写操作完成了。 但是当写缓冲满了之后，host 写操作不得不被阻塞直到缓冲的数据被 flush 到磁盘，造成较慢的写操作，这一发现启发我们充分利用 HDD 的缓冲写提供的性能潜力，在 SSD 上提高性能的同时减少写损失。 测试了不同品牌不同容量的 HDD，发现都有相同的规律，至于为什么有这样的规律，主要是因为 HDD 中有内建的 DRAM，但是 DRAM 中只有很少的一部分容量会被用于缓冲写 IO，剩下的大部分容量主要用于 read-ahead cache 预读缓存、ECC buffer、sector remapping buffer、prefetching buffer，然而，在 HDD 中使用此内置 DRAM 的具体策略(随HDD模型的不同而不同)通常仅为 HDD 制造商专有。幸运的是，写缓冲区的实际大小可以通过分析从外部测量。 成功缓冲写操作后，HDD会立即通知主机请求完成。当缓冲的数据到达阈值之后，HDD 将强制执行刷回，这期间将阻塞后续的写入直到 buffer 被释放。值得注意的是，在空闲一段时间后，由于将数据刷新到磁盘，HDD 缓冲区可能隐式变为空。但是，要显式清空缓冲区，我们可以主动调用 sync() 来强制刷新。 贡献：本文基于上述问题和发现，对 HDD 的性能表现进行了建模，提出了一个预测模型来准确地决定下一个写延迟状态（因为 HDD 内置的控制器 Buffer 对于 HOST 而言是完全不可见的，主机只能根据当前的延迟状况判断状态，利用缓冲写周期和当前写状态信息，可以实现对下一次写状态的预测）。基于这个模型，提出了一种写方法， Buffer-Controlled Write approach, BCW 来主动和有效地控制缓冲写，所以在 HDD 的低延迟、中延迟阶段调度数据的写操作，而在高延迟阶段使用填充数据。基于 BCW，设计了一个混合的 IO 调度器（MIOS）来根据写模式、运行时队列长度和磁盘状态自适应地将传入数据引导到 SSD 和 HDD。在真实生产环境下的负载和 Benchmarks 都表明 MIOS 移除了写到 SSD 数据的 93%，减少了 65% 平均延迟和 85% 尾延迟。 Design The HDD Buffered-Write Model 建模如下，每个时间序列都以 Sync 操作开始，F 表示可以完全缓冲写入到 HDD Buffer 的阶段，M 则是该 Buffer 越来越接近满的阶段，S 则是 Buffer 已满后续的写入都会被阻塞的阶段。 Fast stage lasts for WfW_fWf​ data written Mid stage lasts for WmW_mWm​ data written Write-state Predictor F/A : The current write state is F and the buffer is available. Next write state is most likely to be F. F/U : Although the current write state is F, the buffer is unavailable. Next write state is likely to change to S. M/A : The current write state is M and the buffer is available. Next write state is most likely to remain M. M/U : Although the current write state is M, the buffer is unavailable. Next write state should be S. S : The current write state is S. Next write state will be M with a high probability. The Sync operation will force the next write state and buffer state back to be F/A in all cases 通过监视 IO 请求大小和延迟，并计算写缓冲区中的空闲空间确定了当前的写状态，F，M 或 S，也就是说，记录当前写状态(F 或 M)中的 ADW，并与 WfW_fWf​ 或 WmW_mWm​ 进行比较，以预测下一个写状态。 关于预测准确度的问题，实验表明对于 S 状态的预测相对较低，S 状态的低预测精度是由于当实际的 S 状态被错误地预测为另一种状态时，预测策略倾向于 S 状态以减少性能下降。 Buffer-Controlled Writes 缓冲区控制写(BCW)是一种 HDD 写方法，它确保用户使用 F 或 M 写状态进行写操作，并避免分配慢写操作。BCW 的核心思想是使缓冲写可控。 激活 BCW 后，将调用 sync() 操作来强制同步以主动清空缓冲区，如果预测处于 F 或 M 状态，BCW 将向 HDD 发送连续的用户写操作，否则，将非用户数据填充到 HDD，直到达到缓冲写的最大设置循环(或无限)序列。如果队列中有用户请求，BCW 按顺序写入它们。写完成后，BCW将它的写大小添加到 ADW，并相应地更新写状态。 在请求稀少的轻量级或空闲工作负载期间，HDD 请求队列将时不时为空，使写流不连续。为了确保按顺序和连续模式缓冲写操作的稳定性和确定性，BCW 将主动地将非用户数据填充到磁盘上写入。填充数据有两种类型：（即使对于每个填充写，BCW 仍然执行写状态预测算法） PF：用于用 4KB 的非用户数据填充 F 和 M 状态，小的 PF 可以尽量减少用户请求的等待时间 PS：用更大的块大小填充 S 状态。例如 64KB 的非用户数据，大的 PS 帮助更快地触发慢写操作 BCW 持续计算当前状态( F 或 M )的 ADW。当 ADW 接近 WfW_fWf​ 或 WmW_mWm​ 时，意味着硬盘驱动器缓冲写处于快速或中期阶段的末尾。S 写状态可能在多次写之后发生。此时，BCW 会通知调度程序，并使用 PS 主动触发慢写。为了避免用户写操作的长延迟，在这个阶段，所有传入的用户请求都必须被引导到其他存储设备，比如 SSD。S 状态的写完成之后，下一个写操作将为 M，然后 BCW 重新设置 ADW 并再次接受用户请求。 作者发现在 F 状态下 ADW 达到 WfW_fWf​ 之前没有必要主动地填充写操作。因为这时候物理磁盘操作尚未触发，缓冲区可以在此期间吸收用户请求，短时间之后达到该阈值，这意味着缓冲区将开始将数据刷新到磁盘，下一个写状态将更改为 S。另一方面，当 ADW 长时间小于 WfW_fWf​ 时，磁盘可以自动刷新缓冲过的数据，以便下一个写状态可能是 F。但是，它不会影响性能。 Mixed IO scheduler 调度器根据写状态预测器的结果和当前队列状态决定是否将用户写操作引导到 HDD 请求队列。 架构如下所示，MIOS 在运行时监控 SSD 和 HDD 的所有请求队列，明智地触发 BCW 进程，并确定是否应该将用户写操作定向到所选的 HDD 或 SSD。MIOS 在配置过程中在每个 HDD 中创建一个设备文件。设备文件以仅追加的方式存储BCW 写入。在 MIOS 调度之前，执行概要分析来确定写状态预测器的关键参数(WfW_fWf​ 或 WmW_mWm​等) 调度策略：SSD 在 t 时刻的请求队列长度 l(t) 是关键参数，当大于预定义的阈值 L，调度器通过预测用户的写状态是 F 或 M 来引导用户写到 HDD。阈值 L 是根据 SSD 上的实际性能测量值预先确定的。设定的标准是，我们测量不同 SSD 队列长度下的写延迟，如果队列长度为 l 的请求延迟大于在 M 状态下，我们只需设置阈值 L 为最小的 l 即可。其基本原理是，当 SSD 队列长度大于 L 时，SSD 写操作的延迟将与 BCW 的 F 或 M 写状态下 HDD 上的延迟相同。L 可以在运行时根据工作负载行为和存储设备配置实验性地确定和调整。此策略虽然不能避免，但可以缓解工作负载激增或繁重时的长尾延迟以及 SSD 上的垃圾回收，在这些情况下，SSD 请求队列长度可能是其平均长度的 8-10 倍。因此，重定向的 HDD 写操作不仅减轻了突发请求和重 gc 带来的 SSD 压力，抑制了长尾延迟，而且还降低了平均延迟。 另外，当SSD的队列长度小于 L 时，可选触发 BCW。在这种情况下，启用或禁用 BCW 分别表示为 MIOS_E 或MIOS_D。换句话说，当 SSD 的队列长度小于 L 时，MIOS_E 策略允许使用 BCW 重定向。MIOS_D 策略相反地可以在 SSD 队列长度小于 L 时关闭重定向。注意，在 M 写状态下的 HDD 的写延迟仍然比 SSD 的要高得多。重定向后的请求延迟可能会增加。因此，当 l(t) 小于 l 时，我们只重定向用户请求以利用 MIOS_E 中 HDD 的 F 写状态。 通常，一个典型的混合存储节点包含多个 SSD 和 HDD。我们把所有的磁盘分成独立的 SSD/HDD 对，每个对包含一个SSD 和一个或多个 HDD。每个 SSD/HDD 对都由一个独立的 MIOS 调度程序实例管理。 最后，MIOS 需要对 HDD 的完全控制。这意味着 BCW 中的 HDD 不会受到其他 IO 操作的干扰。当一个 HDD 正在执行 BCW 并且一个读请求到达时，MIOS 立即挂起 BCW 并提供这个读。此时，它将尝试将所有写操作重定向到其他空闲磁盘。对于以读为主的工作负载，可以禁用 BCW 以避免干扰读操作。 个人见解 SSD/HDD 混合存储的场景其实在工业界的应用极为广泛，但仿佛很少有人关注这种场景下存在的问题，即便是考虑到对于 SSD 的磨损也大多是通过对单一的 SSD 层面上进行优化来减少磨损，而从整个混合存储系统的角度出发，屏蔽器件内部的技术细节，还是在 HDD/SSD 的写调度上发力来减少对 SSD 的消耗，当然关键主要还是对于 HDD 的测试发现了 HDD 本身的性能变化的规律，还是比较巧妙的。 本文的设计与实现其实在对于 HDD 的测试之后就已经呼之欲出了，但作者还是设计了很多细节上的东西来提供了更多实现方面的细节，包括一些具体调度策略的伪代码，使得该方案在工业界落地也成为可能， Consistency and Reliability CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost Zizhong Wang, Tongliang Li, Haixia Wang, Airan Shao, Yunren Bai, Shangming Cai, Zihan Xu, and Dongsheng Wang, Tsinghua University Hybrid Data Reliability for Emerging Key-Value Storage Devices Rekha Pitchumani and Yang-suk Kee, Memory Solutions Lab, Samsung Semiconductor Inc. Strong and Efficient Consistency with Consistency-Aware Durability Aishwarya Ganesan, Ramnatthan Alagappan, Andrea Arpaci-Dusseau, and Remzi Arpaci-Dusseau, University of Wisconsin–Madison Awarded Best Paper! CRaft: An Erasure-coding-supported Version of Raft for Reducing Storage Cost and Network Cost 因为很久没单开分布式的文章的坑了，所以干脆拿这篇作为一个 Motivation，然后把 Raft、Paxos 也总结一下。虽然这篇文章跟 EC 有很大关系，但是更多的还是解决一致性的一些问题吧。 完成之后此处贴链接。 ","link":"https://blog.shunzi.tech/post/fast20-some/"},{"title":"FileMR: Rethinking RDMA Networking for Scalable Persistent Memory","content":" NSDI 2020 的文章 FileMR: Rethinking RDMA Networking for Scalable Persistent Memory 这篇论文主要结合了 RDMA 和 NVM，针对各自的特性进行了整合重组，修改相应的协议实现 Abstract 问题：NVM 和 RDMA 看起来天作之合。大容量持久性内存 + 远程内存访问。但实际上现有的 NVMM-aware 的文件系统使用文件来管理 NVM，RDMA 而是以 Memory Regions 的形式进行组织访问。因此要想构建 可使用 RDMA 访问的 NVMM 则需要实现代价较高的转换层，代价高是因为会有大量重复的工作需要做，如跨越权限、命名和地址转换。 贡献：在现有的 RDMA 协议基础上引入了两个变化：FileMR (file memory region) 和 range-based address translation。这两个优化点将内存区域和文件结合起来实现了内存抽象，即一个客户端可以通过 RDMA 直接访问一个由 NVMM 作为后端存储设备支持的文件，通过文件偏移进行内存寻址，从而消除了重复的地址转换，减小了在网卡上完成的地址转换的数量，减小了网卡中的地址转换缓存的负载，提升了命中率约 3.8x - 340x，应用性能相应提升了 1.8x - 2.0x Introduction 可伸缩计算机系统存储和访问数据的方式近年来变化迅速，而这些变化的部分原因是传统独立的系统组件之间的界限越来越模糊。NVM 模糊了内存和存储之间的界限，RDMA 模糊了本地和远端内存之间的界限。NVM+RDMA 仿佛组合起来就能整合内存、存储、网络来提供一个大规模的稳定的字节寻址的网络直连的内存。不幸的是，用于管理这些技术的现有系统同时存在重叠和不兼容的情况。 NVMMs 合并内存和存储。该技术允许应用程序使用加载/存储指令访问持久数据，避免了传统存储系统使用的基于块的接口。而 NVMM 通常被一个基于 NVMM 的文件系统来管理，文件系统来负责对存储设备的访问，应用只需要使用文件系统的相关接口即可。应用程序可以将一个文件映射到它们的地址空间，然后使用 load 和 store 指令访问它，这大大减少了访问持久性数据的延迟。 RDMA 合并了本地和远端的内存。RDMA允许客户端直接访问远程服务器上的内存。一旦远程服务器决定允许传入访问，它将其地址空间的一部分注册为 RDMA 内存区域，并向客户机发送访问该区域的密钥。使用密钥，客户端可以利用服务器的 RDMA 网络接口(RNIC) 绕过 CPU 直接读写服务器的内存。RDMA之所以流行，是因为它将大部分网络栈转移到硬件上，并提供了接近硬件的抽象，与 TCP/IP 协议相比，它表现出更好的延迟。 基于 NVM 的文件系统和 RDMA 在设计之初没有考虑过两者的结合，所有如果将他们直接结合起来就会出现很多重复的工作。虽然只有 RDMA 提供网络数据传输和只有 NVMM 文件系统提供持久内存元数据，但两个系统都实现了保护、地址转换、命名和跨不同内存抽象的分配，RDMA 基于 Memory Region，NVMM Filesystem 基于 File。天真地同时使用RDMA 和 NVMM 文件系统会导致重复工作和它们抽象之间的低效转换层。这些转换层代价极高，特别是因为 RNIC 只能为有限的内存提供转换，而 NVM 的容量可以非常大。 所以我们提出了 FileMR，通过实现新的内存区域类型，它将RDMA 需要的大多数与内存管理相关的任务卸载到 NVM 文件系统，从而实现这一目标;文件系统从而有效地成为 RDMA 的控制平面。FileMR 直接以文件代替 RDMA 访问过程中的 Memory Region，读写直接通过文件系统被定向到文件，并使用文件偏移来进行寻址。文件偏移和物理内存地址之间的转换则还是由 NVMM 文件系统来保证。对文件的访问还是由 NVM 文件系统来管理，使用原生的 ACL 来实现访问保护。为了进一步优化地址转换，我们将基于范围的转换系统集成到 RNIC 中，该系统使用地址范围(而不是页面 pages)进行转换，从而减少了地址转换所需的空间 space，并解决了 RDMA 和 NVMM 文件系统之间的内存抽象不匹配问题。 该方案相比于原生的 RDMA+NVMM 的组合，带来了以下好处： NIC 上完成的地址转换数量降低，从而减轻了 NIC 的地址转换缓存的负载，提升缓存命中率 3.8x - 340x 使用现有的文件系统 ACL 机制来替代了 RDMA 的 ad-hoc memory keys，从而简化了内存保护机制 通过使用持久性的文件代替了短暂的内存区域 IDs，从而简化了连接的管理 允许在不撤消权限或关闭连接的情况下移动或扩展网络可访问内存，从而使文件系统能够整理碎片并向文件追加内容。 RDMA Networking RDMA 硬件支持一组操作，如单边原语 read/write，无需远程 CPU 的参与就可直接访问远端内存，事实上是完全绕过了远端 CPU。还有双边原语，要求两台机器发起匹配的请求，比如 send/receive，通过本地发送方和接收方应用程序选择的地址在已注册的缓冲区之间传输数据。 为了建立 RDMA 连接，应用程序注册一个或多个内存区域(MRs)，授权本地 RNIC 访问本地地址空间的，MR 既是一个命名空间又是一个安全域：为了让客户访问一个区域，本地 RNIC 提供 MR 的虚拟地址、内存大小和一个特殊的 32 位“rkey”。Rkeys 在任何单边原语发送过程中就被发送，并允许接收端 RNIC 验证客户端是否有直接访问该区域的权限。对于双边原语，send/recv 操作要求发送方和接收方都发布匹配的请求，每个请求都附加到某个本地的、预先注册的内存区域，从而消除对 rkey 的需求。 为了管理未完成的请求，RDMA使用来自虚拟接口体系结构（VIA）的工作队列，应用在建立连接之后，可以通过本地 RNIC 发布工作队列项 WQEs 来初始化一个 RDMA 原语。这些项被写入到一组队列中，也就是所谓的 QP，一个 Queue 用于 send/write 请求，一个 Queue 用于 read/receive 请求。一旦 WQE 被写到 QP 中，RNIC 将执行 RDMA 原语并访问远程机器。一旦原语完成，RNIC 将通过放置一个 completion 标志在 completion queue (CQ) 中来确认原语成功处理。应用程序可以从完成队列轮询完成，以接收谓词成功完成的通知。 参考链接 [1] CSDN - 深入浅出全面解析RDMA NVM NVMM 是持久性的存储介质，需要管理软件提供 naming, allocation 以及 protection 等功能，NVMM 通常由文件系统来管理，但和之前基于较慢的块设备构建的文件系统有一些不一样，支持 NVMM 的文件系统在提供高效的 NVMM 访问过程中扮演了重要的角色，接近 DRAM 延迟的 NVMM 意味着软件开销将显著影响整体的性能，因此支持 NVMM 的文件系统在关键路径上尽可能减小软件开销的方式大致有两种： 支持直接访问 mmap() (DAX-mmap)功能。DAX-mmap 允许应用将 NVMM 文件直接映射到他们自己的地址空间，并通过简单的 load/stores 指令来执行数据的访问。该模式允许应用对于大多数数据访问可以直接绕过内核和文件系统，显著提高了文件访问的性能。 NVMM 驻留在内存层次结构中，这可能会导致并发症，因为缓存不是持久的，但可以保存应用程序希望持久保存的数据。为了让数据持久化，对 NVMM 的缓存写操作之后必须执行缓存行刷新或清理指令，以确保实际将数据写回 NVMM，而非临时写操作可以完全绕过 CPU 缓存。store fence 可以强制保证执行写操作的顺序，并保证数据在断电时仍然有效。 Managing RDMA and NVMM 用户空间 RDMA 访问和 NVMM mmapped-DAX 访问共享一个关键功能:它们允许直接访问内存，而不涉及内核。一般来说，我们可以将 NVMM 文件系统和 RDMA 划分为访问内存的数据平面和管理向用户应用程序公开的内存的控制平面。两者的数据平面实际上是相同的:它由直接的对内存的 load 和 store 组成。相比之下，不同系统之间的控制平面差别很大。 对于RDMA和NVMM文件系统，控制平面必须为内存管理提供四种服务。 naming: 确保应用程序能够找到要直接访问的适当内存区域 access control: 阻止应用程序访问它不应该访问的数据 allocation: 提供一种机制来分配和释放资源，扩大或缩小为应用程序可用内存 translation: 必须在应用程序级名称(例如，虚拟地址，或内存和文件偏移量)与物理内存地址之间进行转换.意味着 RDMA 和 NVMM 文件系统必须与虚拟内存子系统紧密合作 下表展示了 RDMA 和 NVMM Filesystem 已经提供的功能，总结了控制平面需要支持的元数据操作。但是内存管理功能被关联到了 RDMA 和 NVMM FS 不同的内存抽象上，RDMA 使用了 Memory Region 或者 Memory Windows，而 NVMM FS 使用了文件。 Naming Naming 提供了一种独立于硬件的方式来引用物理内存，在 RDMA 应用中，memory region 的虚拟地址是和其主机地址一起的，如 IP or GID，从而作为一个全局的有意义的物理内存区域的命名。这些命名的生命周期是短暂的，因为当创建该命名的应用退出的时候命名也就失效了；命名同时也不够灵活，因为它们防止 RDMA 公开的页面在可访问时更改其虚拟地址到物理地址的映射。为了与希望通过读和写直接访问名称的客户机共享 Naming，主机提供 MR 的元数据，对于双边原语(例如，发送/接收)，Naming 是特别的:接收方必须使用带外通道来决定将接收到的数据放在哪里。 基于 NVMM 的文件系统使用文件名来命名主机上的内存区域，由于文件的寿命比应用程序长，因此文件系统独立于应用程序管理命名，并为命名内存区域提供更复杂的管理（例如分层目录和基于文本的名称）。为了访问一个文件，客户端和应用必须通过文件来访问 Permission 权限决定哪个进程有权限访问哪一块内存。在 RDMA 中，RDMA上下文是隔离的，权限通过两种方式执行： 为了授予客户端对内存位置的直接读/写访问权，主机共享内存区域特定的“rkey”。rkey 是一个和所有的单边原语关联的 32位的 key，会被 RNIC 验证是否有权限访问对应地址的内存区域。对于每一个注册了的区域，RNIC 驱动都会维护一个 rkey，和其他 RDMA 元数据一起在 DRAM 的硬件可访问数据结构中提供隔离和保护。 当两个节点之间建立起了 RDMA 连接的时候，Permissions 也被建立，由应用程序代码在建立连接的时候授权，权限不会比进程的寿命长，如果系统重启也会丢失。保护双边原语是由接收应用程序以一种特别的方式执行:接收方使用一个带外信道来决定发送方有哪些权限。 NVMM 的访问控制使用传统的文件系统设计，权限被关联到每一个文件，并针对每个用户或者用户组单独设计。与 RDMA 内存区域和它们的 rkey 不同，权限是底层数据的一个属性，并且在进程和系统重启后仍然存在。 文件系统主要使用了 Permission Bits 和 Access Control Lists 来进行访问控制。延伸阅读 Series Three of Basic of Persistence - Files and Directories Allocation RDMA 原语和 NVMM 文件都直接访问内存，所以可用内存的分配和扩充对于两者都是一个重要的元数据操作。 NVMM 文件系统中维护了一个空闲物理页的列表，可以用于创建或者扩展文件。文件的创建涉及到编组适当的资源，并将新页面链接到现有的文件层次结构中，类似地，可以将空闲页面链接到现有文件或将其与现有文件分离，以扩大或缩小文件。通过调用 fallocate 和 mremap，也可以很容易地更改 DAX-mmap 文件的大小。 创建一个新的 RDMA 内存区域包括分配所需的内存资源、固定它们的页面和生成 rkey。需要注意的是尽管许多 RNICs 可以处理物理地址，但是内存区域的物理地址经常不受程序员控制（关键取决于 malloc 的实现方式），页面一旦被固定到了注册的区域，就将导致物理地址空间的碎片。 除此之外，改变内存区域的映射开销很大。例如，为了提升内存区域的大小，主机端的服务器通常需要先解除内存区域的注册，然后重新注册一个更大的区域，再将相应的变化发送给客户端。rereg_mr 原语包括了解除注册和重新注册的步骤，但是仍然会有很大的开销。带公共内存池的 MPI 应用使用了内存窗口在内存区域的上层来提供动态的访问控制。这种方法不能与 NVMM 文件系统混合使用，因为它仍然需要底层内存区域的静态映射。 程序员也可以把其他内存与添加到当前连接或者保护域中，由于内存区域需要不可忽略的元数据，并且 RDMA 不支持多区域访问，因此这种解决方案显著增加了复杂性。 这种固定的大小限制还阻碍了常见的文件系统操作和优化，例如附加到文件、重新映射文件内容和碎片整理 Address Translation 如下图所示，RDMA 通过将虚拟地址固定到物理地址来解决地址转换的问题，也就是说，只要注册了一个内存区域，它的虚拟地址和物理地址就不能改变。一旦这个映射被固定，RNIC 就能够直接处理在虚拟地址范围上注册的内存区域：RNIC 为传入的 RDMA 原语从虚拟地址转换为物理地址。为了完成这个转换，NIC 维护了一个内存转换表 memory translation table(MTT)，它保存了系统页表的一部分。 MTT 将 RDMA 可访问的页面相关的地址转换条目扁平化，且可以缓存在 RNIC 的板载 SRAM 来加速这个映射的查找。pin-down 缓存对于 RDMA 获得良好性能至关重要，该缓存通常很小，几个 MB，如果缓存不命中开销将会很大，因为其对应的地址转换机制，大多数 RNICs 要求一个 region 的所有页都得是相同的大小，为了回避这些限制，研究者们做了大量的工作试图尽可能地利用缓存来寻址大内存。尽管存在复杂的解决方案，但最常见的建议是减少所需的地址转换数量，例如使用大页面或物理地址来寻址大型连续内存区域来减少地址转换的次数。 NVMM 文件系统处理地址转换有两种方式，都和 RDMA 有所不同。 对于常规的读和写操作，文件系统使用偏移量来将文件名转换成物理地址，地址转换在内核中由系统调用完成。 对于映射内存的访问，mmap 从用户空间直接向 NVMM 上的文件内容建立了虚拟到物理地址的映射。只有在用户和物理地址之间缺少转换时，文件系统才会发起缺页中断，在正常的数据访问中，文件系统被绕过。 不同的转换方案相互干扰，从而产生性能问题。如果一个页通过 RDMA 访问，那么该页将被固定到了一个特定的物理地址，除此以外，内存区域内的每个页的大小还必须相同，因此文件系统不能更新打开的了的文件的数据布局（譬如对文件进行碎片整理或扩容）。 由于 RDMA 阻碍文件的碎片整理，并且禁止在 RDMA 可访问内存中混合页面大小，由文件支持的内存区域必须使用许多小页面来寻址较大的区域，压倒了 pin-down 缓存并削弱了 RDMA 性能。 下图展示了 pin-down cache 不命中对 RDMA 写吞吐量的影响。每个工作请求向随机的 8 字节对齐偏移量写入 8 字节。当内存区域大小为 16 MB 时，使用 4kB 可以达到基准性能的 61.1%(发送物理地址、没有 TLB 或 pin-down cache 缓存丢失)，而使用 2 MB 的大页面时有95.2%。当区域大小达到 16 GB 时，即使是 2 MB 的页面也不够——只能达到 61.2% 的性能。 Design FileMR 是一种扩展了 RDMA 协议的内存区域新类型，提供针对 NVMM 的基于文件的内存抽象。它需要对现有的 RDMA 协议进行较小的更改，并且不依赖于任何特定的文件系统设计，FileMR 可以与传统的 RDMA 内存区域共存，确保向后兼容性。 FileMR 通过一些创新解决了 RDMA 和 NVMM 文件系统之间的冲突，这些冲突会导致不必要的限制和性能下降。 Merged control plane：客户端使用文件偏移来寻址内存，代替虚拟或物理地址，FileMR 还利用文件系统的命名、寻址和权限来流水线化 RDMA 访问。 Range-based address translation：FileMR 利用文件系统的高效、基于区段的布局描述机制来减少 NIC 需要保留的状态数量。由于文件已经被组织成连续的区段，我们将这种寻址机制扩展到 RNIC，允许 RNIC 的 pin-down 缓存使用空间高效转换方案来寻址大量的 RDMA 可访问内存 Assumptions and Definitions FileMR 作为跨用户空间应用程序、系统软件和 RDMA 网络堆栈的高效和协调的内存管理层。本文假设 NVMM 是由系统软件主动管理的，我们将其描述为一个文件系统。注意，文件系统的概念是松散定义的:FileMR 可以与内核文件系统、用户空间文件系统或用户空间访问原始 NVMM(也称为device-DAX)并提供命名的 NVMM 库集成，其中文件映射到对应的实体。 本文假设 NVMM 在其整个生命周期中映射到应用程序地址空间：NVMM 最突出的特点是以非常低的成本获得细粒度的持久性。FileMR 的设计目标是支持远程 NVMM 访问，同时保持本地 NVMM 访问的简单性和效率。另一种方法是构建管理这两种存储的整体系统 (NVMM) 和网络 (RDMA) FileMR 新的内存抽象 FileMR 既是一个 RDMA 内存区域，又是一个 NVMM 文件，这允许 RDMA 和 NVMM 控制平面以实现顺畅的交互操作。对FileMR 的 RDMA 访问是通过文件偏移来寻址的，文件系统管理底层文件的访问权限、命名和分配，就像管理任何文件一样。NVMM 文件始终由文件系统管理的物理页面支持，因此，在使用 FileMR 时，RDMA 子系统可以简单地重用文件系统元数据中已经可用的转换、权限和命名信息，以便进行适当的检查和寻址。 如下图所示描述了 FileMR 中的元数据和数据访问。 对于元数据，初始化内存区域的流程如下 step1. 在创建 FileMR 之前，应用带着对应的权限去打开对应的后端文件 step2. 应用创建 FileMR (File Memory Region) step3. 将对应的 region 绑定到文件上，将 FileMR 绑定到文件会产生一个类似于 rkey 的 filekey，远程客户端可以使用它来访问 FileMR step4. 创建了 FileMR 并将其绑定到后端文件之后，文件系统将使文件的寻址信息与 RNIC 保持同步 对于数据：对于远程 FileMR 及其后端 NVMM 文件的数据访问，应用程序使用 FileMR (带有 filekey 来证明其权限)和文件偏移量来访问，RNIC 使用由文件系统提供的地址转换信息来在文件偏移和物理内存之间转换。除了对 FileMR 的 read/write 单边原语以外，引入了一个新的单边原语 append，来增大对应的内存区域。当发送一个 append 原语的时候，客户端不包含远程地址，服务端处理该原语类似于处理对地址等于当前 FileMR 大小的写原语，然后更新 FileMR 的大小并通知文件系统。一个优化点，为了防止在每个 append 消息上出错，文件系统可以预先分配超出文件大小的地址转换项，即使在通过 FileMR 打开并访问后端我文件时，本地应用程序仍然可以使用普通的文件系统调用或映射的地址继续访问它，对文件元数据的任何更改都将传播到 RNIC。 Range-based Address Translation NVMM 文件系统尝试在 NVMM 中以大的线性区段存储文件数据。FileMR 在 MTT 和 pin-down cache 中分别使用基于范围的地址转换 RangeMTT, range pin-down cache。这一变化与传统 RDMA 基于页的寻址方式有很大的不同。基于页面的转换使用一组固定大小的页面将虚拟地址转换为物理地址，而基于范围的转换(在cpu端转换中探索和使用)不同于基于页面的转换，它将可变大小的虚拟地址范围映射为物理地址。当寻址很大的线性内存区域的时候，基于范围的地址缓缓就十分有用，并且能够利用已经存在的基于 extent 的文件组织。 对于 FileMR，基于范围的地址转换有两个主要的好处，存储映射所需的空间和 用可变大小的区段数量(而不是固定大小的页面数量)注册映射比例所需的时间。在 MTT 和 pin-down cache 中注册一个页大概需要花费 5 微妙，该过程需要对内存描述符加锁，而且很难并行化。因此，单核只能注册 4KB 的页以 770MB/s 的速度。如果是 TB 级的 NVMM，结果注册时间会长到无法接受的程度。 Design Overview 下图灰色部分展示了原生 RDMA 堆栈，绿色部分则是在 FileMR 中做的一些必要的改变。 为了支持 FileMR 的抽象，文件系统要求实现 bind() 函数来关联一个 FileMR 和一个 File，在必要时，当绑定文件的元数据通过回调发生变化时通知 RDMA 堆栈(最终是 RNIC 的 RangeMTT 和 pin-down 缓存)。这些回调允许 RNIC 为传入的 RDMA 请求维护正确的基于范围的物理地址映射 可选地，文件系统还可以注册一组回调函数，当 RNIC 无法找到传入地址的转换项时触发。这个过程类似于按需分页，需要它来支持新的 append ，它既修改文件布局，又写入文件 支持 FileMR 抽象还需要更改 RNIC 硬件。在我们提出的 RangeMTT 中，RNIC 硬件和驱动程序需要在 MTT 和pin-down 缓存中采用基于范围的寻址。基于范围的硬件寻址方案可以用来实现基于范围的地址查找。我们使用了软件 RNIC 进行模拟测试。 FileMR 还向 RDMA 接口本身添加了增量的、向后兼容的更改。如下表所示，它为内存区域创建添加了一个新的访问标志，以标识 FileMR 的创建，创建后，FileMR 被标记为处于未配置状态。对文件系统的后续 bind() 调用将在RangeMTT 中分配 FileMR 的转换条目(通过来自文件系统的cm_bind回调)。bind() 方法可以通过 ioctl()(用于内核级文件系统)或库调用(用于用户级文件系统)实现。FileMR 还添加了新的 RDMA 单边原语 Append。将现有应用程序转换为使用 FileMRs 很容易，因为应用程序只需要更改其区域创建代码 Implementation 为了实现 RangeMTT，我们遵循了冗余内存映射中引入的设计:每个 FileMR 指向存储了偏移量和长度的b树，我们使用这些偏移量作为索引。所有 RangeMTT 条目都是页面对齐的地址，因为 OS 只能以页面粒度管理虚拟内存 与页面对齐的 RangeMTT 不同，FileMR 支持任意大小并允许子页面文件/对象。每个 RangeMTT 条目由一个页面地址、一个长度字段和必要的位组成。这些条目没有重叠，对于稀疏的文件可能会有间隙。 为了支持 append 原语，FileMR 允许超出其大小的地址转换项。Append 是单边原语，不会在 WR 具体指定远程服务器的地址。在服务器端，RNIC 总是尝试 DMA 到当前的大小 FileMR，并在成功时增加其大小。当地址转换 missing，当IOMMU可用时，服务端可能会发起一个 IO 缺页中断，并且将调用文件系统例程来完成中断条目的处理。或者，如果这种支持不可用，服务器将通过类似于接收端未就绪(RNR)错误的消息向客户端发送信号。 Soft-RoCE 将 MTT 条目管理为一个 64 位物理地址的平面数组，查找复杂度为O(1)。我们发现在硬件 RNIC 驱动程序如 mlx4 中也实现了类似的设计。对于具有范围 pin-down 缓存缺失的 FileMR，条目查找将以更高的时间复杂度遍历已注册的数据结构(O(log(n))) 由于映射是在 DRAM 中，Soft-RoCE 不具有 pin-down 缓存。为了模拟 RangeMTT，我们构建了一个 4096 项 4 路关联缓存用于模拟传统的定位缓存，以及用于 FileMR 的 4096 项 4 路关联范围定位缓存。每个范围转换条目由一个 32 位的页面地址和一个 32 位的长度组成，这允许最大的 FileMR 大小为 16TB (4 kB页)或 8PB (2 MB页)。 我们适配了两个应用程序来使用 FileMR。对于内核文件系统，我们的实现基于NOVA，这是一个成熟的内核空间 nvmm感知文件系统，具有良好的性能。我们还将 FileMR 修改为 libpmemlog (pmdk 的一部分，pmdk 是一个管理本地持久对象的用户级库)，以构建可远程访问的持久日志。 Remote File Access in NOVA NOVA 是一种符合 posix 的日志结构本地 NVMM 文件系统。在 NOVA 中，每个文件都组织为一个持久日志，其中包含大小不一的区段，这些区段驻留在持久内存中。文件数据由文件系统通过每个 cpu 空闲列表分配，并作为合并条目进行维护。 为了处理远程文件系统上的元数据操作，我们添加了一个用户级守护进程 novad，该进程打开文件以建立 FileMR，并接收来自远程应用程序的任何元数据更新（譬如目录创建），并将这些应用到本地文件系统上。 在客户端，应用程序通过与 novad 通信并接收 filekey 来远程打开文件。然后它可以发送单方面的 RDMA 原语来直接远程访问 NVMM。同时，本地运行的应用程序仍然可以使用传统的 POSIX IO 接口访问文件，或者将文件映射到它的地址空间，并发出加载和存储指令。 我们的组合系统也可以轻松处理数据复制。通过使用几个 FileMRs，我们可以简单地复制一个原语(具有相同或不同的filekey，具体取决于文件系统实现)并发送到多个主机，而无需考虑文件的物理地址(只要它们的名称相等)。 Remote NVMM Log with libpmemlog FileMR 抽象只要求后端的“文件系统”适当地实现 bind() 方法，RNIC 回调，并可以访问原始 NVMM。例如，可以由能够访问原始 NVMM 设备的应用程序创建 FileMR。在本节中，我们将利用这种灵活性并基于 libpmemlog 构建远程 NVMM 日志 我们修改 libpmemlog 的分配器以使用必要的 FileMR 回调。也就是说，每当为日志分配或释放内存时，RNIC 的RangeMTT 都会更新。客户端使用新的 append 谓词追加到日志。在服务器端，当 FileMR 大小在映射的 RangeMTT 范围内时, RNIC 绕过服务器应用程序时可以进行地址转换。如果没有，则发生范围故障，并且库通过分配和映射额外的内存来扩展该区域 Evaluation DRAM 模拟 PM 2 Intel Xeon (Broadwell) CPUs with 10 cores and 256 GB of DRAM（64 GB configured as an emulated NVMM device） Soft-RoCE, Intel X710 10GbE NIC Registration Overhead Allocated Regions 这个实验演示了当应用程序直接分配和映射文件而不更新其元数据时的用例。对于 FileMR，我们还包含了从 NOVA 日志中生成范围条目的时间，这在应用程序第一次打开文件时发生。 结果表明 注册一个大的内存区域会消耗大量的时间。主要是由于文件系统分配器的内部碎片。它花费 30 秒注册一个64gb 的持久性(文件)和易失性的具有 4 kB页面的(Alloc-4K)内存区域。使用 hugepages (alloc-2M)将注册成本降低到 20秒，而 FileMR 只需要67毫秒(低3个数量级)。 对于小文件，NOVA 只为文件创建一个或两个区段，而传统的 MRs 仍然与操作系统的虚拟内存例程交互，从而造成开销 Data Fragmentation FileMR 得益于文件数据的连续性。文件系统的内部碎片可能有两个原因:文件系统老化和使用频繁更改文件布局的POSIX IO。为了测试 FileMR 在碎片化的文件系统上的性能，我们先使用了四个敏感的发起 POSIX IO 操作的负载来让文件系统产生碎片。完成之后，我们再在所有的 NVMM 文件上创建内存区域。使用了如下负载： 结果如下所示：在碎片化的文件上运行 FileMR 仍然显示了显著的的改善，在 region 注册时间和内存消耗 MTT 条目上。 Fileserver 演示了许多文件的情况，其中FileMR 只创建传统内存区域条目的 0.5%，并且只需要 6.8% 的注册时间 Metadata heavy 工作负载(Varmail), FileMR 条目的数量只减少了 3%(由于严重的内部碎片和小文件大小), 但它仍然可以节约 20% 的注册时间，因为它拥有索引节点锁，减少争用 Redis 是一个键值存储，它在 IO 路径上持久化一个附加文件，并异步刷新数据库——少量的内部碎片意味着它只需要传统内存区域 2% 的空间和时间。 类似地，SQLite 也使用日志记录，这导致很少的碎片，并且极大地节省了空间和时间。 Translation Cache Effectiveness RDMA 在大型 NVMM 上的性能下降主要是由于 pin-down 缓存丢失造成的。由于 Soft-RoCE 在 UDP 中封装 RDMA 消息，并在 DRAM 中访问所有 RDMA 状态，因此我们不能通过端到端性能来衡量缓存的有效性。 相反，我们为 FileMR 测量模拟的向下缓存和范围向下缓存的缓存命中率。我们收集对表5中描述的工作负载的POSIX IO 系统调用的跟踪，并使用单向 RDMA 原语对远程主机 replay 它们。 下图展示了测试结果，range-based 的 pin-down cache 远好于 page-based pin-down cache。对于比较大的分配的文件，也就意味着更少的地址转换表项，range-based pin-down cache 的命中率接近达到了 100% Accessing Remote Files 为了测试数据路径上的性能，我们让客户端访问运行 novad 的远程服务器上的文件。客户机使用 RDMA 写谓词发出随机的 1KB 写操作，我们测量客户机应用程序发出谓词和远程 RNIC DMAs 到目标内存地址(memcopy for Soft-RoCE)之间的延迟。我们将 FileMR 与映射本地访问和其他提供分布式存储访问的分布式系统进行了比较。所有这些系统都通过在链路上上发送物理地址来避免转换开销。 如下所示，延迟分解结果如下。注意，所有系统的延迟都比典型的 RDMA NIC 要高，因为 Soft-RoCE 比真正的 RNIC效率低。此外，我们省略了 UDP 包封装和交付的延迟，这控制了端到端延迟。它只需要1.5 微妙就可以将 4kB 的数据存储并持久化到本地 NVMM。FileMR 延迟更低，因为它消除了任何间接层的需要（Mojim 需要系统调用 msync，LITE 需要共享内存写入，Orion 需要进行 POSIX 写 ） Accessing Remote NVMM logs 我们使用引入的远程日志实现来评估引入的新 append 谓词。我们比较基线 libpmemlog 使用本地NVMM(绕过网络),以及日志的 HERD RPC RDMA library。 如下所示了创建 64 字节日志项的延迟分解。使用 libpmemlog 花费大概 5.5 微妙来在本地进行日志记录。FileMR为远程日志和本地日志增加了 53% 的开销，HERD RPC-based 增加了 192% 的开销 Discussion 本章节讨论了软件模拟和实际硬件的差距，即该方案如果要像应用在实际的物理硬件上会有哪些问题与挑战，感兴趣请阅读原文。 Related Work 该章节介绍了大量的实验对比对象，以及一些思想的起源。 Conclusion NVMM 和 RDMA 系统之间元数据管理的冲突会导致昂贵的转换开销，并阻止文件系统更改其布局。这项工作引入了对现有RDMA协议的两个修改：:基于FileMR和范围的转换，从而提供了一种结合内存区域和文件的抽象。它通过消除无关的转换提高了 RDMA 可访问 NVMMs 的性能，同时为 RDMA 提供了其他好处，包括更有效的访问权限和更简单的连接管理。 ","link":"https://blog.shunzi.tech/post/NSDI20-FileMR/"},{"title":"RADOS 读写流程","content":" 本篇主要总结 RADOS 底层的读写流程，并结合源码进行分析 考虑基于现有的强一致性模型的读写流程是否有可以优化的点，提升 Ceph 的 IO 性能 OSD 读写流程 大致分为三个阶段： 接受请求：主要是网络模块相关 OSD 的 op_wq 处理：在工作队列 op_wq 的线程池中处理，检查 PG 状态，封装请求为事务 PGBackend 的处理：仍然在 op_wq 的线程池中处理，将事务进行分发，由对应的 PGBackend 来实现本地事务处理 PG Placement Group (PG)，是一些对象的集合，对象和 PG 的对应关系其实就是对象标识 HASH 之后取模得到对应的 pg_id（模值为对应存储池对应的 PG 数目）。 locator = object_name obj_hash = hash(locator) pg = obj_hash % num_pg OSDs_for_pg = crush(pg) # returns a list of OSDs primary = osds_for_pg[0] replicas = osds_for_pg[1:] 以 PG 为单位进行组织的目的是为了使用有限的、可控数目的 PG 来管理无限扩张的对象数据，并控制节点资源的分配。 PG 是数据备份、同步、迁移等操作的基本单位。 常见的 PG 级别的操作 Peering 指（当前或者过去曾经）归属于同一个 PG 所有的 PG 实例就本 PG 所存储的全部对象及对象相关的元数据进行协商并最终达成一致的过程。 Peering 基于 Info 和 Log 进行。Log 是指权威日志，作为数据同步的依据。Info 是指 PG 的基本元数据信息，在 Peering 过程中通过交换 Info，可以由 Primary 选举得到权威日志。 此处说的一致，并不意味着每个 PG 实例都实时拥有每个对象的最新内容 Golden Rule: 对任何 PG 的写操作只有在该 PG 的操作集的所有成员都持久化后，才会向客户端确认，且在 Peering 期间不能进行任何 IO 操作，且不能做 Recovery 涉及的其他概念 Acting Set：负责特定 PG 的(或在某些 epoch 时) OSD 的有序列表。列表第一个 OSD 为主 OSD，其余为 Replica OSD Up Set：通常情况下和 Acting Set 相同，出现 PG temp 时有所不同。Acting Set 完全由 CRUSH 决定，Up Set 会受到 PG temp 的影响 PG temp：假设一个 PG 的 acting set 为 [0,1,2] 列表。此时如果 osd0 出现故障，导致 CRUSH 算法重新分配该 PG 的 acting set 为 [3,1,2]。此时 osd3 为该 PG 的主 OSD，但是 osd3 为新加入的 OSD，并不能负担该 PG 上的读操作。所以 PG 向 Monitor 申请一个临时的 PG，osd1 为临时的主 OSD，这时 up set 变为 [1,3,2]，acting set 依然为 [3,1,2]，导致 acting set 和 up set 不同。当 osd3 完成 Backfill 过程之后，临时 PG 被取消，该 PG 的 up set 修复为 acting set，此时 acting set 和 up set 都为 [3,1,2] 列表。 Epoch：OSDMap 的版本号，Monitor 管理生成，单增。OSDMap 发生了变化，Epoch 相应地增加，但为了防止 Epoch 的剧烈变化和较快的消耗，一个特定时间段内的修改会被折叠进入一个 Epoch Interval：OSDMap 的一个连续 Epoch 间隔，该期间内的 PG 的 Active Set 和 Up Set 没有发生变化，也就意味着和 PG 是绑定的。每个 Interval 的起始 Epoch 称之为 same_interval_since 触发时机 系统初始化时，OSD 重新启动导致 PG 重新加载 PG 新创建时，PG 会发起一次 Peering 的过程 当有 OSD 失效，OSD 的增加或者删除等导致 PG 的 acting set 发生了变化，该 PG 就会重新发起一次 Peering 过程 Recovery 当 PG 完成了 Peering 过程后，处于 Active 状态的 PG 就已经可以对外提供服务了。如果该 PG 的各个副本上有不一致的对象，就需要进行修复。Ceph 的修复过程有两种：Recovery 和 Backfill。本质是针对 PG 某些实例进行数据同步的过程，最终目标是将 PG 变成 Active+Clean 状态 过程 Peering 过程产生关于缺失对象的信息，主副本和从副本对应的缺失对象信息有所不同，存储的位置不同。主 OSD 缺失的对象存储在权威日志 pg_log 的相关数据结构中，副本上缺失的对象存储在 OSD 对应的 peer_missing 的数据结构中。 对于主 OSD 缺失的对象，随机选择一个拥有该对象的 OSD，拉取数据（PULL）。（先修复主 OSD，再修复从 OSD） 对于 replica 数据缺失的情况，从主副本上把缺失的对象数据推送到副本上完成数据修复（PUSH） 快照对象有一些单独的处理 场景 OSD 暂时下线，然后又上线 OSD 硬件故障下线，更换硬盘重新上线 Pull/Push Recovery 由 Primary 主导进行，期间 Primary 通过 Pull 或者 Push 的方式进行对象间的数据同步 Backfill 是 Recovery 的一种特殊场景，指 Peering 完成后，如果基于当前权威日志无法对 Up Set 当中的某些 PG 实例实施增量同步（例如承载这些 PG 实例的 OSD 离线太久，或者是新的 OSD 加入集群导致的 PG 实例整体迁移），则通过完全拷贝当前 Primary 所有对象的方式进行全量同步。 Scrub Ceph 内部实现的数据一致性检查工具 Ceph Scrub。原理为：通过对比各个对象副本的数据和元数据完成副本的一致性检查。后台执行检查操作，可以设置相应的调度策略来触发 Scrub（立即启动/间隔一定的时间/定时） 主要包括scrub 和 deep-scrub。 其中 scrub 只对元数据信息进行扫描，相对比较快； 而 deep-scrub 不仅对元数据进行扫描，还会对存储的数据进行扫描，几乎要扫描磁盘上的所有数据并计算 crc32 校验值，相对比较慢。 PrimaryLogPG::do_request 该步骤主要是做一些 PG 级别的检查，以及一些 PG 级别的操作的分发处理。 操作最终可能因为各种各样的原因被加入到响应队列推迟处理，对应了很多种重试队列，用于区分不同的场景。 为了保证 OP 之间不会乱序，上述队列均为 FIFO 队列，且队列之间也严格有序。当对应的限制接触后，PG 会触发关联的 OP 重新进入 op_shardedwq 队列排队，等候再次被 PG 执行 最终许多普通的操作都会进入 do_op 执行 void PrimaryLogPG::do_request( OpRequestRef &amp;op, ThreadPool::TPHandle &amp;handle) { // Trace 相关配置检查 if (op-&gt;osd_trace) { op-&gt;pg_trace.init(&quot;pg op&quot;, &amp;trace_endpoint, &amp;op-&gt;osd_trace); op-&gt;pg_trace.event(&quot;do request&quot;); } // make sure we have a new enough map // 判断 waiting_for_map 队列中是否有来自相同客户端的操作 auto p = waiting_for_map.find(op-&gt;get_source()); if (p != waiting_for_map.end()) { // 有则将当前 Op 加入 waiting_for_map 队列，然后直接返回 // preserve ordering dout(20) &lt;&lt; __func__ &lt;&lt; &quot; waiting_for_map &quot; &lt;&lt; p-&gt;first &lt;&lt; &quot; not empty, queueing&quot; &lt;&lt; dendl; p-&gt;second.push_back(op); op-&gt;mark_delayed(&quot;waiting_for_map not empty&quot;); return; } // 判断当前 op 携带的 Epoch 信息是否是最新的 op-&gt;min_epoch &lt;= get_osdmap_epoch() if (!have_same_or_newer_map(op-&gt;min_epoch)) { // 如果 Op 携带的 epoch 更新，则将当前 Op 加入 waiting_for_map 队列，然后直接返回 dout(20) &lt;&lt; __func__ &lt;&lt; &quot; min &quot; &lt;&lt; op-&gt;min_epoch &lt;&lt; &quot;, queue on waiting_for_map &quot; &lt;&lt; op-&gt;get_source() &lt;&lt; dendl; waiting_for_map[op-&gt;get_source()].push_back(op); op-&gt;mark_delayed(&quot;op must wait for map&quot;); osd-&gt;request_osdmap_update(op-&gt;min_epoch); return; } // 判断是否可以丢弃掉该 op // 1. op 对应的客户端链路断开 // 2. 收到 op 时，PG 当前已经切换到一个更新的 Interval (即 PG 此时的 same_interval_since 比 op 携带的 Epoch 要大，后续客户端会重发) // 3. op 在 PG 分裂之前发送（后续客户端会进行重发） // 4. ... if (can_discard_request(op)) { return; } // pg-wide backoffs const Message *m = op-&gt;get_req(); int msg_type = m-&gt;get_type(); if (m-&gt;get_connection()-&gt;has_feature(CEPH_FEATURE_RADOS_BACKOFF)) { auto session = ceph::ref_cast&lt;Session&gt;(m-&gt;get_connection()-&gt;get_priv()); if (!session) return; // drop it. if (msg_type == CEPH_MSG_OSD_OP) { if (session-&gt;check_backoff(cct, info.pgid, info.pgid.pgid.get_hobj_start(), m)) { return; } bool backoff = is_down() || is_incomplete() || (!is_active() &amp;&amp; is_peered()); if (g_conf()-&gt;osd_backoff_on_peering &amp;&amp; !backoff) { if (is_peering()) { backoff = true; } } if (backoff) { add_pg_backoff(session); return; } } // pg backoff acks at pg-level if (msg_type == CEPH_MSG_OSD_BACKOFF) { const MOSDBackoff *ba = static_cast&lt;const MOSDBackoff *&gt;(m); if (ba-&gt;begin != ba-&gt;end) { handle_backoff(op); return; } } } // PG 是否处于 Active 或者 Peer 状态？ if (!is_peered()) { // 不处于上述状态，判断是否可以由后端直接处理。 // 1. ECBackend 该情况下不能处理 // 2. ReplicatedBackend 判断如果是 PULL 操作则可以进行处理 // Delay unless PGBackend says it's ok if (pgbackend-&gt;can_handle_while_inactive(op)) { bool handled = pgbackend-&gt;handle_message(op); ceph_assert(handled); return; } else { // 不能处理则加入 waiting_for_peered 队列，然后返回。 waiting_for_peered.push_back(op); op-&gt;mark_delayed(&quot;waiting for peered&quot;); return; } } // PG 处于 Active 或者 Peer 状态，判断是否正在进行刷回 if (recovery_state.needs_flush()) { // 正在刷回则将该 op 加入 waiting_for_flush 队列，并返回 dout(20) &lt;&lt; &quot;waiting for flush on &quot; &lt;&lt; op &lt;&lt; dendl; waiting_for_flush.push_back(op); op-&gt;mark_delayed(&quot;waiting for flush&quot;); return; } ceph_assert(is_peered() &amp;&amp; !recovery_state.needs_flush()); // 由 PGBackend 直接处理然后返回，此处只处理以下操作 // 1. MSG_OSD_PG_RECOVERY_DELETE (Common) // 2. MSG_OSD_PG_RECOVERY_DELETE_REPLY (Common) // 3. MSG_OSD_PG_PUSH (副本) // 4. MSG_OSD_PG_PULL (副本) // 5. MSG_OSD_PG_PUSH_REPLY (副本) // 6. MSG_OSD_REPOP (副本) // 7. MSG_OSD_REPOPREPLY (副本) // 8. MSG_OSD_EC_WRITE (EC) // 9. MSG_OSD_EC_WRITE_REPLY (EC) // 10. MSG_OSD_EC_READ (EC) // 11. MSG_OSD_EC_READ_REPLY (EC) // 12. MSG_OSD_PG_PUSH (EC) // 13. MSG_OSD_PG_PUSH_REPLY (EC) if (pgbackend-&gt;handle_message(op)) return; // 其余操作如下处理： switch (msg_type) { case CEPH_MSG_OSD_OP: case CEPH_MSG_OSD_BACKOFF: // 判断是否是 Active 状态 if (!is_active()) { // 即 Peer 状态，加入 waiting_for_active 队列并返回 dout(20) &lt;&lt; &quot; peered, not active, waiting for active on &quot; &lt;&lt; op &lt;&lt; dendl; waiting_for_active.push_back(op); op-&gt;mark_delayed(&quot;waiting for active&quot;); return; } // 为 Active 状态 switch (msg_type) { // 处理 CEPH_MSG_OSD_OP // 如果为 tier 相关直接报错，否则 do_op case CEPH_MSG_OSD_OP: // verify client features if ((pool.info.has_tiers() || pool.info.is_tier()) &amp;&amp; !op-&gt;has_feature(CEPH_FEATURE_OSD_CACHEPOOL)) { osd-&gt;reply_op_error(op, -EOPNOTSUPP); return; } do_op(op); break; // 处理 CEPH_MSG_OSD_BACKOFF case CEPH_MSG_OSD_BACKOFF: // object-level backoff acks handled in osdop context handle_backoff(op); break; } break; // 其他操作的处理 case MSG_OSD_PG_SCAN: do_scan(op, handle); break; case MSG_OSD_PG_BACKFILL: do_backfill(op); break; case MSG_OSD_PG_BACKFILL_REMOVE: do_backfill_remove(op); break; case MSG_OSD_SCRUB_RESERVE: { auto m = op-&gt;get_req&lt;MOSDScrubReserve&gt;(); switch (m-&gt;type) { case MOSDScrubReserve::REQUEST: handle_scrub_reserve_request(op); break; case MOSDScrubReserve::GRANT: handle_scrub_reserve_grant(op, m-&gt;from); break; case MOSDScrubReserve::REJECT: handle_scrub_reserve_reject(op, m-&gt;from); break; case MOSDScrubReserve::RELEASE: handle_scrub_reserve_release(op); break; } } break; case MSG_OSD_REP_SCRUB: replica_scrub(op, handle); break; case MSG_OSD_REP_SCRUBMAP: do_replica_scrub_map(op); break; case MSG_OSD_PG_UPDATE_LOG_MISSING: do_update_log_missing(op); break; case MSG_OSD_PG_UPDATE_LOG_MISSING_REPLY: do_update_log_missing_reply(op); break; default: ceph_abort_msg(&quot;bad message type in do_request&quot;); } } PrimaryLogPG::do_op /** do_op - do an op * pg lock will be held (if multithreaded) * osd_lock NOT held. */ void PrimaryLogPG::do_op(OpRequestRef &amp;op) { FUNCTRACE(cct); // 使用一个指针进行指向对应的请求，后续操作都使用该指针 // NOTE: take a non-const pointer here; we must be careful not to // change anything that will break other reads on m (operator&lt;&lt;). MOSDOp *m = static_cast&lt;MOSDOp *&gt;(op-&gt;get_nonconst_req()); // op 参数校验 ceph_assert(m-&gt;get_type() == CEPH_MSG_OSD_OP); // decode 请求解码状态判断，从 bufferlist 中解析数据 if (m-&gt;finish_decode()) { op-&gt;reset_desc(); // for TrackedOp m-&gt;clear_payload(); } dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: op &quot; &lt;&lt; *m &lt;&lt; dendl; const hobject_t head = m-&gt;get_hobj().get_head(); // 【PG 参数检查】判断是否包含 op 所携带的对象 if (!info.pgid.pgid.contains( info.pgid.pgid.get_split_bits(pool.info.get_pg_num()), head)) { derr &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; info.pgid.pgid &lt;&lt; &quot; does not contain &quot; &lt;&lt; head &lt;&lt; &quot; pg_num &quot; &lt;&lt; pool.info.get_pg_num() &lt;&lt; &quot; hash &quot; &lt;&lt; std::hex &lt;&lt; head.get_hash() &lt;&lt; std::dec &lt;&lt; dendl; osd-&gt;clog-&gt;warn() &lt;&lt; info.pgid.pgid &lt;&lt; &quot; does not contain &quot; &lt;&lt; head &lt;&lt; &quot; op &quot; &lt;&lt; *m; ceph_assert(!cct-&gt;_conf-&gt;osd_debug_misdirected_ops); return; } bool can_backoff = m-&gt;get_connection()-&gt;has_feature(CEPH_FEATURE_RADOS_BACKOFF); ceph::ref_t&lt;Session&gt; session; if (can_backoff) { session = static_cast&lt;Session *&gt;(m-&gt;get_connection()-&gt;get_priv().get()); if (!session.get()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; no session&quot; &lt;&lt; dendl; return; } if (session-&gt;check_backoff(cct, info.pgid, head, m)) { return; } } // op 携带了 CEPH_OSD_FLAG_PARALLELEXEC 标志，指示可以并发执行 if (m-&gt;has_flag(CEPH_OSD_FLAG_PARALLELEXEC)) { // not implemented. dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: PARALLELEXEC not implemented &quot; &lt;&lt; *m &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EINVAL); return; } { int r = op-&gt;maybe_init_op_info(*get_osdmap()); if (r) { osd-&gt;reply_op_error(op, r); return; } } // op 携带了 CEPH_OSD_FLAG_BALANCE_READS 或者 CEPH_OSD_FLAG_LOCALIZE_READS 标志， // 指示可以读取别的 OSD 节点，不一定是主 OSD，或者执行本地读 // 注意：该 Flag 支持读操作，且不支持缓存 if ((m-&gt;get_flags() &amp; (CEPH_OSD_FLAG_BALANCE_READS | CEPH_OSD_FLAG_LOCALIZE_READS)) &amp;&amp; op-&gt;may_read() &amp;&amp; !(op-&gt;may_write() || op-&gt;may_cache())) { // 当前节点既不是主节点，也不是 replicated 节点时，譬如 Stray，则报错 // balanced reads; any replica will do if (!(is_primary() || is_nonprimary())) { osd-&gt;handle_misdirected_op(this, op); return; } } else { // 正常操作的时候必须是主节点，否则报错 // normal case; must be primary if (!is_primary()) { osd-&gt;handle_misdirected_op(this, op); return; } } // 判断是否为 laggy 状态 // https://docs.ceph.com/en/latest/dev/osd_internals/stale_read/ if (!check_laggy(op)) { return; } // 检查权限 caps if (!op_has_sufficient_caps(op)) { osd-&gt;reply_op_error(op, -EPERM); return; } // 如果包含 includes_pg_op 操作（对 PG 的操作，主要是获取 PG 相关信息），则执行 do_pg_op if (op-&gt;includes_pg_op()) { return do_pg_op(op); } // 对象名称超过、key、命名空间等数据信息超过最大限制会影响存储后端 // 检查 oid 是否为空，检查对象 key // object name too long? if (m-&gt;get_oid().name.size() &gt; cct-&gt;_conf-&gt;osd_max_object_name_len) { dout(4) &lt;&lt; &quot;do_op name is longer than &quot; &lt;&lt; cct-&gt;_conf-&gt;osd_max_object_name_len &lt;&lt; &quot; bytes&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -ENAMETOOLONG); return; } if (m-&gt;get_hobj().get_key().size() &gt; cct-&gt;_conf-&gt;osd_max_object_name_len) { dout(4) &lt;&lt; &quot;do_op locator is longer than &quot; &lt;&lt; cct-&gt;_conf-&gt;osd_max_object_name_len &lt;&lt; &quot; bytes&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -ENAMETOOLONG); return; } if (m-&gt;get_hobj().nspace.size() &gt; cct-&gt;_conf-&gt;osd_max_object_namespace_len) { dout(4) &lt;&lt; &quot;do_op namespace is longer than &quot; &lt;&lt; cct-&gt;_conf-&gt;osd_max_object_namespace_len &lt;&lt; &quot; bytes&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -ENAMETOOLONG); return; } if (m-&gt;get_hobj().oid.name.empty()) { dout(4) &lt;&lt; &quot;do_op empty oid name is not allowed&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EINVAL); return; } if (int r = osd-&gt;store-&gt;validate_hobject_key(head)) { dout(4) &lt;&lt; &quot;do_op object &quot; &lt;&lt; head &lt;&lt; &quot; invalid for backing store: &quot; &lt;&lt; r &lt;&lt; dendl; osd-&gt;reply_op_error(op, r); return; } // 客户端被禁止访问 // blocklisted? if (get_osdmap()-&gt;is_blocklisted(m-&gt;get_source_addr())) { dout(10) &lt;&lt; &quot;do_op &quot; &lt;&lt; m-&gt;get_source_addr() &lt;&lt; &quot; is blocklisted&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EBLOCKLISTED); return; } // order this op as a write? bool write_ordered = op-&gt;rwordered(); // 检查集群是否已被标记为 FULL，并检查 op 是否有携带 CEPH_OSD_FLAG_FULL_TRY 和 CEPH_OSD_FLAG_FULL_FORCE 标志 // discard due to cluster full transition? (we discard any op that // originates before the cluster or pool is marked full; the client // will resend after the full flag is removed or if they expect the // op to succeed despite being full). The except is FULL_FORCE and // FULL_TRY ops, which there is no reason to discard because they // bypass all full checks anyway. If this op isn't write or // read-ordered, we skip. // FIXME: we exclude mds writes for now. if (write_ordered &amp;&amp; !(m-&gt;get_source().is_mds() || m-&gt;has_flag(CEPH_OSD_FLAG_FULL_TRY) || m-&gt;has_flag(CEPH_OSD_FLAG_FULL_FORCE)) &amp;&amp; info.history.last_epoch_marked_full &gt; m-&gt;get_map_epoch()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; discarding op sent before full &quot; &lt;&lt; m &lt;&lt; &quot; &quot; &lt;&lt; *m &lt;&lt; dendl; return; } // 检查 PG 所在 OSD 可用存储空间情况 // mds should have stopped writing before this point. // We can't allow OSD to become non-startable even if mds // could be writing as part of file removals. if (write_ordered &amp;&amp; osd-&gt;check_failsafe_full(get_dpp()) &amp;&amp; !m-&gt;has_flag(CEPH_OSD_FLAG_FULL_TRY)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; fail-safe full check failed, dropping request.&quot; &lt;&lt; dendl; return; } int64_t poolid = get_pgid().pool(); // 判断 op 是否为写操作 if (op-&gt;may_write()) { // 获取对应的 pool 并检查 const pg_pool_t *pi = get_osdmap()-&gt;get_pg_pool(poolid); if (!pi) { return; } // invalid? // 判断是否访问快照对象，若访问则报错，快照不允许写 if (m-&gt;get_snapid() != CEPH_NOSNAP) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: write to clone not valid &quot; &lt;&lt; *m &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EINVAL); return; } // too big? // 判断写入的数据大小并校验，osd_max_write_size if (cct-&gt;_conf-&gt;osd_max_write_size &amp;&amp; m-&gt;get_data_len() &gt; cct-&gt;_conf-&gt;osd_max_write_size &lt;&lt; 20) { // journal can't hold commit! derr &lt;&lt; &quot;do_op msg data len &quot; &lt;&lt; m-&gt;get_data_len() &lt;&lt; &quot; &gt; osd_max_write_size &quot; &lt;&lt; (cct-&gt;_conf-&gt;osd_max_write_size &lt;&lt; 20) &lt;&lt; &quot; on &quot; &lt;&lt; *m &lt;&lt; dendl; osd-&gt;reply_op_error(op, -OSD_WRITETOOBIG); return; } } dout(10) &lt;&lt; &quot;do_op &quot; &lt;&lt; *m &lt;&lt; (op-&gt;may_write() ? &quot; may_write&quot; : &quot;&quot;) &lt;&lt; (op-&gt;may_read() ? &quot; may_read&quot; : &quot;&quot;) &lt;&lt; (op-&gt;may_cache() ? &quot; may_cache&quot; : &quot;&quot;) &lt;&lt; &quot; -&gt; &quot; &lt;&lt; (write_ordered ? &quot;write-ordered&quot; : &quot;read-ordered&quot;) &lt;&lt; &quot; flags &quot; &lt;&lt; ceph_osd_flag_string(m-&gt;get_flags()) &lt;&lt; dendl; // missing object? // 检查对象是否不可读， // 1. 如果对象在 missing 列表（恢复过程中检查 PGLog 构建的 missing 列表）中，不可读 // 2. 数据修复过程中，在当前 acting set 对应的多个 OSD 上该对象不可读 // bool is_unreadable_object(const hobject_t &amp;oid) const // { // return is_missing_object(oid) || // !recovery_state.get_missing_loc().readable_with_acting( // oid, get_actingset()); // } if (is_unreadable_object(head)) { // 不是主节点 报错 if (!is_primary()) { osd-&gt;reply_op_error(op, -EAGAIN); return; } // 是主节点相应地判断 OSD backoff 状态 if (can_backoff &amp;&amp; (g_conf()-&gt;osd_backoff_on_degraded || (g_conf()-&gt;osd_backoff_on_unfound &amp;&amp; recovery_state.get_missing_loc().is_unfound(head)))) { add_backoff(session, head, head); maybe_kick_recovery(head); } else { // 等待对象恢复完成 wait_for_unreadable_object(head, op); } return; } // 顺序写 if (write_ordered) { // 对象处于降级状态（恢复状态） // degraded object? if (is_degraded_or_backfilling_object(head)) { if (can_backoff &amp;&amp; g_conf()-&gt;osd_backoff_on_degraded) { // 尝试启动 recovery add_backoff(session, head, head); maybe_kick_recovery(head); } else { wait_for_degraded_object(head, op); } return; } // 对象正在被 scrub，加入相应的队列 waiting_for_scrub if (scrubber.is_chunky_scrub_active() &amp;&amp; write_blocked_by_scrub(head)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: waiting for scrub&quot; &lt;&lt; dendl; waiting_for_scrub.push_back(op); op-&gt;mark_delayed(&quot;waiting for scrub&quot;); return; } if (!check_laggy_requeue(op)) { return; } // 对象被 snap // objects_blocked_on_degraded_snap 保存了 head 对象则需要等待 // head 对象在 rollback 到某个版本的快照时，该版本的 snap 对象处于确实状态，则需要等待 snap 对象恢复 // blocked on snap? if (auto blocked_iter = objects_blocked_on_degraded_snap.find(head); blocked_iter != std::end(objects_blocked_on_degraded_snap)) { hobject_t to_wait_on(head); to_wait_on.snap = blocked_iter-&gt;second; wait_for_degraded_object(to_wait_on, op); return; } // objects_blocked_on_snap_promotion 里的对象表示 head 对象 rollback 到某个版本的快照时 // 该版本的快照对象在 cache pool 层中没有，需要到 data pool 层获取 if (auto blocked_snap_promote_iter = objects_blocked_on_snap_promotion.find(head); blocked_snap_promote_iter != std::end(objects_blocked_on_snap_promotion)) { wait_for_blocked_object(blocked_snap_promote_iter-&gt;second-&gt;obs.oi.soid, op); return; } // objects_blocked_on_cache_full 该队列中的对象因为 cache pool 层空间满而阻塞了写操作 if (objects_blocked_on_cache_full.count(head)) { block_write_on_full_cache(head, op); return; } } // 检查 op 是否为重发 // dup/resent? if (op-&gt;may_write() || op-&gt;may_cache()) { // warning: we will get back *a* request for this reqid, but not // necessarily the most recent. this happens with flush and // promote ops, but we can't possible have both in our log where // the original request is still not stable on disk, so for our // purposes here it doesn't matter which one we get. eversion_t version; version_t user_version; int return_code = 0; vector&lt;pg_log_op_return_item_t&gt; op_returns; bool got = check_in_progress_op( m-&gt;get_reqid(), &amp;version, &amp;user_version, &amp;return_code, &amp;op_returns); if (got) { dout(3) &lt;&lt; __func__ &lt;&lt; &quot; dup &quot; &lt;&lt; m-&gt;get_reqid() &lt;&lt; &quot; version &quot; &lt;&lt; version &lt;&lt; dendl; if (already_complete(version)) { osd-&gt;reply_op_error(op, return_code, version, user_version, op_returns); } else { dout(10) &lt;&lt; &quot; waiting for &quot; &lt;&lt; version &lt;&lt; &quot; to commit&quot; &lt;&lt; dendl; // always queue ondisk waiters, so that we can requeue if needed waiting_for_ondisk[version].emplace_back(op, user_version, return_code, op_returns); op-&gt;mark_delayed(&quot;waiting for ondisk&quot;); } return; } } ObjectContextRef obc; bool can_create = op-&gt;may_write(); hobject_t missing_oid; // kludge around the fact that LIST_SNAPS sets CEPH_SNAPDIR for LIST_SNAPS const hobject_t &amp;oid = m-&gt;get_snapid() == CEPH_SNAPDIR ? head : m-&gt;get_hobj(); // make sure LIST_SNAPS is on CEPH_SNAPDIR and nothing else for (vector&lt;OSDOp&gt;::iterator p = m-&gt;ops.begin(); p != m-&gt;ops.end(); ++p) { OSDOp &amp;osd_op = *p; if (osd_op.op.op == CEPH_OSD_OP_LIST_SNAPS) { if (m-&gt;get_snapid() != CEPH_SNAPDIR) { dout(10) &lt;&lt; &quot;LIST_SNAPS with incorrect context&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EINVAL); return; } } else { if (m-&gt;get_snapid() == CEPH_SNAPDIR) { dout(10) &lt;&lt; &quot;non-LIST_SNAPS on snapdir&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EINVAL); return; } } } // io blocked on obc? if (!m-&gt;has_flag(CEPH_OSD_FLAG_FLUSH) &amp;&amp; maybe_await_blocked_head(oid, op)) { return; } // 当前节点不是主节点 if (!is_primary()) { // 判断当前状态下是否能处理副本节点的读请求 if (!recovery_state.can_serve_replica_read(oid)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: unstable write on replica, bouncing to primary &quot; &lt;&lt; *m &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EAGAIN); return; } dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: serving replica read on oid &quot; &lt;&lt; oid &lt;&lt; dendl; } int r = find_object_context( oid, &amp;obc, can_create, m-&gt;has_flag(CEPH_OSD_FLAG_MAP_SNAP_CLONE), &amp;missing_oid); // LIST_SNAPS needs the ssc too if (obc &amp;&amp; m-&gt;get_snapid() == CEPH_SNAPDIR &amp;&amp; !obc-&gt;ssc) { obc-&gt;ssc = get_snapset_context(oid, true); } if (r == -EAGAIN) { // If we're not the primary of this OSD, we just return -EAGAIN. Otherwise, // we have to wait for the object. if (is_primary()) { // missing the specific snap we need; requeue and wait. ceph_assert(!op-&gt;may_write()); // only happens on a read/cache wait_for_unreadable_object(missing_oid, op); return; } } else if (r == 0) { // 检查 snapdir 对象是否可读 if (is_unreadable_object(obc-&gt;obs.oi.soid)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot;: clone &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; &quot; is unreadable, waiting&quot; &lt;&lt; dendl; wait_for_unreadable_object(obc-&gt;obs.oi.soid, op); return; } // 如果是写操作需要检查 snapdir 对象是否缺失 // degraded object? (the check above was for head; this could be a clone) if (write_ordered &amp;&amp; obc-&gt;obs.oi.soid.snap != CEPH_NOSNAP &amp;&amp; is_degraded_or_backfilling_object(obc-&gt;obs.oi.soid)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot;: clone &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; &quot; is degraded, waiting&quot; &lt;&lt; dendl; wait_for_degraded_object(obc-&gt;obs.oi.soid, op); return; } } bool in_hit_set = false; // hitset 不为空，进入 cache tiering 流程 if (hit_set) { if (obc.get()) { if (obc-&gt;obs.oi.soid != hobject_t() &amp;&amp; hit_set-&gt;contains(obc-&gt;obs.oi.soid)) in_hit_set = true; } else { if (missing_oid != hobject_t() &amp;&amp; hit_set-&gt;contains(missing_oid)) in_hit_set = true; } if (!op-&gt;hitset_inserted) { hit_set-&gt;insert(oid); op-&gt;hitset_inserted = true; if (hit_set-&gt;is_full() || hit_set_start_stamp + pool.info.hit_set_period &lt;= m-&gt;get_recv_stamp()) { hit_set_persist(); } } } if (agent_state) { if (agent_choose_mode(false, op)) return; } if (obc.get() &amp;&amp; obc-&gt;obs.exists &amp;&amp; obc-&gt;obs.oi.has_manifest()) { if (maybe_handle_manifest(op, write_ordered, obc)) return; } if (maybe_handle_cache(op, write_ordered, obc, r, missing_oid, false, in_hit_set)) return; if (r &amp;&amp; (r != -ENOENT || !obc)) { // copy the reqids for copy get on ENOENT if (r == -ENOENT &amp;&amp; (m-&gt;ops[0].op.op == CEPH_OSD_OP_COPY_GET)) { fill_in_copy_get_noent(op, oid, m-&gt;ops[0]); return; } dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: find_object_context got error &quot; &lt;&lt; r &lt;&lt; dendl; if (op-&gt;may_write() &amp;&amp; get_osdmap()-&gt;require_osd_release &gt;= ceph_release_t::kraken) { record_write_error(op, oid, nullptr, r); } else { osd-&gt;reply_op_error(op, r); } return; } // 验证 object_locator 和 msg 中的是否相同 // make sure locator is consistent object_locator_t oloc(obc-&gt;obs.oi.soid); if (m-&gt;get_object_locator() != oloc) { dout(10) &lt;&lt; &quot; provided locator &quot; &lt;&lt; m-&gt;get_object_locator() &lt;&lt; &quot; != object's &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; dendl; osd-&gt;clog-&gt;warn() &lt;&lt; &quot;bad locator &quot; &lt;&lt; m-&gt;get_object_locator() &lt;&lt; &quot; on object &quot; &lt;&lt; oloc &lt;&lt; &quot; op &quot; &lt;&lt; *m; } // 检查该对象是否被阻塞 // io blocked on obc? if (obc-&gt;is_blocked() &amp;&amp; !m-&gt;has_flag(CEPH_OSD_FLAG_FLUSH)) { wait_for_blocked_object(obc-&gt;obs.oi.soid, op); return; } dout(25) &lt;&lt; __func__ &lt;&lt; &quot; oi &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; // 获取对象上下文，创建 OpContext 对 op 进行跟踪 OpContext *ctx = new OpContext(op, m-&gt;get_reqid(), &amp;m-&gt;ops, obc, this); // 根据对应的 flag 决定锁的处理方式 if (m-&gt;has_flag(CEPH_OSD_FLAG_SKIPRWLOCKS)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: skipping rw locks&quot; &lt;&lt; dendl; } else if (m-&gt;get_flags() &amp; CEPH_OSD_FLAG_FLUSH) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: part of flush, will ignore write lock&quot; &lt;&lt; dendl; // verify there is in fact a flush in progress // FIXME: we could make this a stronger test. map&lt;hobject_t, FlushOpRef&gt;::iterator p = flush_ops.find(obc-&gt;obs.oi.soid); if (p == flush_ops.end()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; no flush in progress, aborting&quot; &lt;&lt; dendl; reply_ctx(ctx, -EINVAL); return; } } else if (!get_rw_locks(write_ordered, ctx)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; waiting for rw locks &quot; &lt;&lt; dendl; op-&gt;mark_delayed(&quot;waiting for rw locks&quot;); close_op_ctx(ctx); return; } dout(20) &lt;&lt; __func__ &lt;&lt; &quot; obc &quot; &lt;&lt; *obc &lt;&lt; dendl; if (r) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; returned an error: &quot; &lt;&lt; r &lt;&lt; dendl; if (op-&gt;may_write() &amp;&amp; get_osdmap()-&gt;require_osd_release &gt;= ceph_release_t::kraken) { record_write_error(op, oid, nullptr, r, ctx-&gt;op-&gt;allows_returnvec() ? ctx : nullptr); } else { osd-&gt;reply_op_error(op, r); } close_op_ctx(ctx); return; } if (m-&gt;has_flag(CEPH_OSD_FLAG_IGNORE_CACHE)) { ctx-&gt;ignore_cache = true; } if ((op-&gt;may_read()) &amp;&amp; (obc-&gt;obs.oi.is_lost())) { // This object is lost. Reading from it returns an error. dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: object &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; &quot; is lost&quot; &lt;&lt; dendl; reply_ctx(ctx, -ENFILE); return; } if (!op-&gt;may_write() &amp;&amp; !op-&gt;may_cache() &amp;&amp; (!obc-&gt;obs.exists || ((m-&gt;get_snapid() != CEPH_SNAPDIR) &amp;&amp; obc-&gt;obs.oi.is_whiteout()))) { // copy the reqids for copy get on ENOENT if (m-&gt;ops[0].op.op == CEPH_OSD_OP_COPY_GET) { fill_in_copy_get_noent(op, oid, m-&gt;ops[0]); close_op_ctx(ctx); return; } reply_ctx(ctx, -ENOENT); return; } op-&gt;mark_started(); // 真正开始执行 op execute_ctx(ctx); utime_t prepare_latency = ceph_clock_now(); prepare_latency -= op-&gt;get_dequeued_time(); osd-&gt;logger-&gt;tinc(l_osd_op_prepare_lat, prepare_latency); if (op-&gt;may_read() &amp;&amp; op-&gt;may_write()) { osd-&gt;logger-&gt;tinc(l_osd_op_rw_prepare_lat, prepare_latency); } else if (op-&gt;may_read()) { osd-&gt;logger-&gt;tinc(l_osd_op_r_prepare_lat, prepare_latency); } else if (op-&gt;may_write() || op-&gt;may_cache()) { osd-&gt;logger-&gt;tinc(l_osd_op_w_prepare_lat, prepare_latency); } // force recovery of the oldest missing object if too many logs maybe_force_recovery(); } execute_ctx void PrimaryLogPG::execute_ctx(OpContext *ctx) { FUNCTRACE(cct); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; ctx &lt;&lt; dendl; ctx-&gt;reset_obs(ctx-&gt;obc); ctx-&gt;update_log_only = false; // reset in case finish_copyfrom() is re-running execute_ctx OpRequestRef op = ctx-&gt;op; auto m = op-&gt;get_req&lt;MOSDOp&gt;(); ObjectContextRef obc = ctx-&gt;obc; const hobject_t &amp;soid = obc-&gt;obs.oi.soid; // this method must be idempotent since we may call it several times // before we finally apply the resulting transaction. ctx-&gt;op_t.reset(new PGTransaction); // 写操作 if (op-&gt;may_write() || op-&gt;may_cache()) { // snap // 对于快照进行一些处理 if (!(m-&gt;has_flag(CEPH_OSD_FLAG_ENFORCE_SNAPC)) &amp;&amp; pool.info.is_pool_snaps_mode()) { // 更新 ctx-&gt;snapc，该值保存了该操作的客户端附带的快照相关信息 // use pool's snapc ctx-&gt;snapc = pool.snapc; } else { // 用户特定快照，通常为 RBD 快照，此时设置为消息中携带的信息 // client specified snapc ctx-&gt;snapc.seq = m-&gt;get_snap_seq(); ctx-&gt;snapc.snaps = m-&gt;get_snaps(); filter_snapc(ctx-&gt;snapc.snaps); } // 比较 SNAP_SEQ，如果客户端的更小则报错 if ((m-&gt;has_flag(CEPH_OSD_FLAG_ORDERSNAP)) &amp;&amp; ctx-&gt;snapc.seq &lt; obc-&gt;ssc-&gt;snapset.seq) { dout(10) &lt;&lt; &quot; ORDERSNAP flag set and snapc seq &quot; &lt;&lt; ctx-&gt;snapc.seq &lt;&lt; &quot; &lt; snapset seq &quot; &lt;&lt; obc-&gt;ssc-&gt;snapset.seq &lt;&lt; &quot; on &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; dendl; reply_ctx(ctx, -EOLDSNAPC); return; } // 更新 OpContext 版本号 // version ctx-&gt;at_version = get_next_version(); ctx-&gt;mtime = m-&gt;get_mtime(); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; soid &lt;&lt; &quot; &quot; &lt;&lt; *ctx-&gt;ops &lt;&lt; &quot; ov &quot; &lt;&lt; obc-&gt;obs.oi.version &lt;&lt; &quot; av &quot; &lt;&lt; ctx-&gt;at_version &lt;&lt; &quot; snapc &quot; &lt;&lt; ctx-&gt;snapc &lt;&lt; &quot; snapset &quot; &lt;&lt; obc-&gt;ssc-&gt;snapset &lt;&lt; dendl; } else { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; soid &lt;&lt; &quot; &quot; &lt;&lt; *ctx-&gt;ops &lt;&lt; &quot; ov &quot; &lt;&lt; obc-&gt;obs.oi.version &lt;&lt; dendl; } if (!ctx-&gt;user_at_version) ctx-&gt;user_at_version = obc-&gt;obs.oi.user_version; dout(30) &lt;&lt; __func__ &lt;&lt; &quot; user_at_version &quot; &lt;&lt; ctx-&gt;user_at_version &lt;&lt; dendl; { #ifdef WITH_LTTNG osd_reqid_t reqid = ctx-&gt;op-&gt;get_reqid(); #endif tracepoint(osd, prepare_tx_enter, reqid.name._type, reqid.name._num, reqid.tid, reqid.inc); } // 准备事务 // 1. 通过 do_osd_ops 生成原始 op 对应的 PG 事务 // 2. 如果 op 针对 head 对象进行操作，通过 make_writable 检查是否需要预先执行克隆操作 // 3. 通过 finish_ctx 检查是否需要创建或者删除 snapdir 对象，生成日志，并更新对象的 OI（object_info_t） 和 SS（SnapSet） 属性 // 其中涉及了大量的对克隆和快照的处理 int result = prepare_transaction(ctx); { #ifdef WITH_LTTNG osd_reqid_t reqid = ctx-&gt;op-&gt;get_reqid(); #endif tracepoint(osd, prepare_tx_exit, reqid.name._type, reqid.name._num, reqid.tid, reqid.inc); } // 异步读则将 op 加入 in_progress_async_reads 队列，完成之后再向客户端应答 bool pending_async_reads = !ctx-&gt;pending_async_reads.empty(); if (result == -EINPROGRESS || pending_async_reads) { // come back later. if (pending_async_reads) { ceph_assert(pool.info.is_erasure()); in_progress_async_reads.push_back(make_pair(op, ctx)); // 完成异步读取 ctx-&gt;start_async_reads(this); } return; } if (result == -EAGAIN) { // clean up after the ctx close_op_ctx(ctx); return; } bool ignore_out_data = false; if (!ctx-&gt;op_t-&gt;empty() &amp;&amp; op-&gt;may_write() &amp;&amp; result &gt;= 0) { // successful update if (ctx-&gt;op-&gt;allows_returnvec()) { // enforce reasonable bound on the return buffer sizes for (auto &amp;i : *ctx-&gt;ops) { if (i.outdata.length() &gt; cct-&gt;_conf-&gt;osd_max_write_op_reply_len) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; op &quot; &lt;&lt; i &lt;&lt; &quot; outdata overflow&quot; &lt;&lt; dendl; result = -EOVERFLOW; // overall result is overflow i.rval = -EOVERFLOW; i.outdata.clear(); } } } else { // legacy behavior -- zero result and return data etc. ignore_out_data = true; result = 0; } } // prepare the reply ctx-&gt;reply = new MOSDOpReply(m, result, get_osdmap_epoch(), 0, ignore_out_data); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; alloc reply &quot; &lt;&lt; ctx-&gt;reply &lt;&lt; &quot; result &quot; &lt;&lt; result &lt;&lt; dendl; // 只包含读操作或者 失败，是则向客户端发送应答 // read or error? if ((ctx-&gt;op_t-&gt;empty() || result &lt; 0) &amp;&amp; !ctx-&gt;update_log_only) { // finish side-effects if (result &gt;= 0) do_osd_op_effects(ctx, m-&gt;get_connection()); // 同步读取调用以下方法完成读操作 complete_read_ctx(result, ctx); return; } ctx-&gt;reply-&gt;set_reply_versions(ctx-&gt;at_version, ctx-&gt;user_at_version); // 后续均为写操作 ceph_assert(op-&gt;may_write() || op-&gt;may_cache()); // trim log? // 将旧的日志进行 trim // calc_trim_to_aggressive() // calc_trim_to() recovery_state.update_trim_to(); // verify that we are doing this in order? if (cct-&gt;_conf-&gt;osd_debug_op_order &amp;&amp; m-&gt;get_source().is_client() &amp;&amp; !pool.info.is_tier() &amp;&amp; !pool.info.has_tiers()) { map&lt;client_t, ceph_tid_t&gt; &amp;cm = debug_op_order[obc-&gt;obs.oi.soid]; ceph_tid_t t = m-&gt;get_tid(); client_t n = m-&gt;get_source().num(); map&lt;client_t, ceph_tid_t&gt;::iterator p = cm.find(n); if (p == cm.end()) { dout(20) &lt;&lt; &quot; op order client.&quot; &lt;&lt; n &lt;&lt; &quot; tid &quot; &lt;&lt; t &lt;&lt; &quot; (first)&quot; &lt;&lt; dendl; cm[n] = t; } else { dout(20) &lt;&lt; &quot; op order client.&quot; &lt;&lt; n &lt;&lt; &quot; tid &quot; &lt;&lt; t &lt;&lt; &quot; last was &quot; &lt;&lt; p-&gt;second &lt;&lt; dendl; if (p-&gt;second &gt; t) { derr &lt;&lt; &quot;bad op order, already applied &quot; &lt;&lt; p-&gt;second &lt;&lt; &quot; &gt; this &quot; &lt;&lt; t &lt;&lt; dendl; ceph_abort_msg(&quot;out of order op&quot;); } p-&gt;second = t; } } if (ctx-&gt;update_log_only) { if (result &gt;= 0) do_osd_op_effects(ctx, m-&gt;get_connection()); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; update_log_only -- result=&quot; &lt;&lt; result &lt;&lt; dendl; // save just what we need from ctx MOSDOpReply *reply = ctx-&gt;reply; ctx-&gt;reply = nullptr; reply-&gt;get_header().data_off = (ctx-&gt;data_off ? *ctx-&gt;data_off : 0); if (result == -ENOENT) { reply-&gt;set_enoent_reply_versions(info.last_update, info.last_user_version); } reply-&gt;add_flags(CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK); // append to pg log for dup detection - don't save buffers for now record_write_error(op, soid, reply, result, ctx-&gt;op-&gt;allows_returnvec() ? ctx : nullptr); close_op_ctx(ctx); return; } // 写操作则注册如下的回调函数：按照如下顺序要求 // 1. on_commit: 执行时，向客户端发送写入完成应答 // 2. on_success: 执行时，进行 Watch/Notify 相关的处理 // 3. on_finish: 执行时，删除 OpContext // no need to capture PG ref, repop cancel will handle that // Can capture the ctx by pointer, it's owned by the repop ctx-&gt;register_on_commit( [m, ctx, this]() { if (ctx-&gt;op) log_op_stats(*ctx-&gt;op, ctx-&gt;bytes_written, ctx-&gt;bytes_read); if (m &amp;&amp; !ctx-&gt;sent_reply) { MOSDOpReply *reply = ctx-&gt;reply; ctx-&gt;reply = nullptr; reply-&gt;add_flags(CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK); dout(10) &lt;&lt; &quot; sending reply on &quot; &lt;&lt; *m &lt;&lt; &quot; &quot; &lt;&lt; reply &lt;&lt; dendl; osd-&gt;send_message_osd_client(reply, m-&gt;get_connection()); ctx-&gt;sent_reply = true; ctx-&gt;op-&gt;mark_commit_sent(); } }); ctx-&gt;register_on_success( [ctx, this]() { do_osd_op_effects( ctx, ctx-&gt;op ? ctx-&gt;op-&gt;get_req()-&gt;get_connection() : ConnectionRef()); }); ctx-&gt;register_on_finish( [ctx]() { delete ctx; }); // 事务准备完成，由 Primary 进行副本间的本地事务分发和整体同步 // issue replica writes ceph_tid_t rep_tid = osd-&gt;get_tid(); // 创建一个 RepGather RepGather *repop = new_repop(ctx, obc, rep_tid); // 将 RepGather 提交到 PGBackend，由 PGBackend 负责将 PG 事务转为每个副本的本地事务，然后分发 // 即向各个副本发送同步操作请求 issue_repop(repop, ctx); // 评估 RepGather 是否真正完成，真正完成后则依次执行 RepGather 中注册过的一系列回调函数，最后删除 RepGather // 检查各个副本的同步操作是否已经 reply 成功 eval_repop(repop); repop-&gt;put(); } issue_repop 真正的分发逻辑，和 OSD 本地事务处理都封装在该方法中。回顾上述主流程中关于多副本写操作的处理： // 写操作则注册如下的回调函数：按照如下顺序要求 // 1. on_commit: 执行时，向客户端发送写入完成应答 // 2. on_success: 执行时，进行 Watch/Notify 相关的处理 // 3. on_finish: 执行时，删除 OpContext // no need to capture PG ref, repop cancel will handle that // Can capture the ctx by pointer, it's owned by the repop ctx-&gt;register_on_commit( [m, ctx, this]() { if (ctx-&gt;op) log_op_stats(*ctx-&gt;op, ctx-&gt;bytes_written, ctx-&gt;bytes_read); if (m &amp;&amp; !ctx-&gt;sent_reply) { MOSDOpReply *reply = ctx-&gt;reply; ctx-&gt;reply = nullptr; reply-&gt;add_flags(CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK); dout(10) &lt;&lt; &quot; sending reply on &quot; &lt;&lt; *m &lt;&lt; &quot; &quot; &lt;&lt; reply &lt;&lt; dendl; osd-&gt;send_message_osd_client(reply, m-&gt;get_connection()); ctx-&gt;sent_reply = true; ctx-&gt;op-&gt;mark_commit_sent(); } }); ctx-&gt;register_on_success( [ctx, this]() { do_osd_op_effects( ctx, ctx-&gt;op ? ctx-&gt;op-&gt;get_req()-&gt;get_connection() : ConnectionRef()); }); ctx-&gt;register_on_finish( [ctx]() { delete ctx; }); // 事务准备完成，由 Primary 进行副本间的本地事务分发和整体同步 // issue replica writes ceph_tid_t rep_tid = osd-&gt;get_tid(); // 创建一个 RepGather RepGather *repop = new_repop(ctx, obc, rep_tid); // 将 RepGather 提交到 PGBackend，由 PGBackend 负责将 PG 事务转为每个副本的本地事务，然后分发 // 即向各个副本发送同步操作请求 issue_repop(repop, ctx); // 评估 RepGather 是否真正完成，真正完成后则依次执行 RepGather 中注册过的一系列回调函数，最后删除 RepGather // 检查各个副本的同步操作是否已经 reply 成功 eval_repop(repop); repop-&gt;put(); 其中 issue_repopn 主要是调用了 submit_transaction 让对应的 PGBackend 来执行事务 pgbackend-&gt;submit_transaction( soid, ctx-&gt;delta_stats, ctx-&gt;at_version, std::move(ctx-&gt;op_t), recovery_state.get_pg_trim_to(), recovery_state.get_min_last_complete_ondisk(), std::move(ctx-&gt;log), ctx-&gt;updated_hset_history, on_all_commit, repop-&gt;rep_tid, ctx-&gt;reqid, ctx-&gt;op); } submit_transaction 此处暂时只讨论 ReplicatedBackend，即多副本情况下的事务提交 副本方式：核心处理流程是把封装好的事务分发到该 PG 对应的其他从 OSD 上 纠删码方式：核心处理流程是为主 chunk 向各个分片 chunk 分发数据的过程 通过 issue_op 分发消息到副本 OSD（异步），当前 OSD 相应地执行日志操作以及完成本地 OSD 请求的处理queue_transactions。 void ReplicatedBackend::submit_transaction( const hobject_t &amp;soid, const object_stat_sum_t &amp;delta_stats, const eversion_t &amp;at_version, PGTransactionUPtr &amp;&amp;_t, const eversion_t &amp;trim_to, const eversion_t &amp;min_last_complete_ondisk, vector&lt;pg_log_entry_t&gt; &amp;&amp;_log_entries, std::optional&lt;pg_hit_set_history_t&gt; &amp;hset_history, Context *on_all_commit, ceph_tid_t tid, osd_reqid_t reqid, OpRequestRef orig_op) { parent-&gt;apply_stats( soid, delta_stats); vector&lt;pg_log_entry_t&gt; log_entries(_log_entries); ObjectStore::Transaction op_t; PGTransactionUPtr t(std::move(_t)); set&lt;hobject_t&gt; added, removed; // 根据具体的操作类型生成相应的事务 generate_transaction( t, coll, log_entries, &amp;op_t, &amp;added, &amp;removed, get_osdmap()-&gt;require_osd_release); ceph_assert(added.size() &lt;= 1); ceph_assert(removed.size() &lt;= 1); // 构建处理中的请求记录 in_progress_ops auto insert_res = in_progress_ops.insert( make_pair( tid, ceph::make_ref&lt;InProgressOp&gt;( tid, on_all_commit, orig_op, at_version))); ceph_assert(insert_res.second); InProgressOp &amp;op = *insert_res.first-&gt;second; // 统计 commit 的副本操作数量，等待副本操作完成回调时进行清除，使用该结构方便统计是不是所有的副本都完成了操作。 op.waiting_for_commit.insert( parent-&gt;get_acting_recovery_backfill_shards().begin(), parent-&gt;get_acting_recovery_backfill_shards().end()); // 把请求发送出去 issue_op( soid, at_version, tid, reqid, trim_to, min_last_complete_ondisk, added.size() ? *(added.begin()) : hobject_t(), removed.size() ? *(removed.begin()) : hobject_t(), log_entries, hset_history, &amp;op, op_t); add_temp_objs(added); clear_temp_objs(removed); //进行日志操作，开始记录本端操作 object 的 log parent-&gt;log_operation( std::move(log_entries), hset_history, trim_to, at_version, min_last_complete_ondisk, true, op_t); // 开始注册本端的 commit 回调函数，这里回调后直接向上返回 op_t.register_on_commit( parent-&gt;bless_context( new C_OSD_OnOpCommit(this, &amp;op))); vector&lt;ObjectStore::Transaction&gt; tls; tls.push_back(std::move(op_t)); // 完成本地 OSD 的请求处理 parent-&gt;queue_transactions(tls, op.op); if (at_version != eversion_t()) { parent-&gt;op_applied(at_version); } } issue_op 该方法构造相应的写请求，以消息的方式发送到该主 OSD 对应的副本 OSD 上。 void ReplicatedBackend::issue_op( const hobject_t &amp;soid, const eversion_t &amp;at_version, ceph_tid_t tid, osd_reqid_t reqid, eversion_t pg_trim_to, eversion_t min_last_complete_ondisk, hobject_t new_temp_oid, hobject_t discard_temp_oid, const vector&lt;pg_log_entry_t&gt; &amp;log_entries, std::optional&lt;pg_hit_set_history_t&gt; &amp;hset_hist, InProgressOp *op, ObjectStore::Transaction &amp;op_t) { // 副本节点数量 &gt; 1 if (parent-&gt;get_acting_recovery_backfill_shards().size() &gt; 1) { if (op-&gt;op) { op-&gt;op-&gt;pg_trace.event(&quot;issue replication ops&quot;); ostringstream ss; set&lt;pg_shard_t&gt; replicas = parent-&gt;get_acting_recovery_backfill_shards(); replicas.erase(parent-&gt;whoami_shard()); ss &lt;&lt; &quot;waiting for subops from &quot; &lt;&lt; replicas; op-&gt;op-&gt;mark_sub_op_sent(ss.str()); } // avoid doing the same work in generate_subop bufferlist logs; encode(log_entries, logs); // 遍历所有的 replica OSDs for (const auto &amp;shard : get_parent()-&gt;get_acting_recovery_backfill_shards()) { // 如果该节点是主节点，跳过 if (shard == parent-&gt;whoami_shard()) continue; // 获取副本节点对应的 pg const pg_info_t &amp;pinfo = parent-&gt;get_shard_info().find(shard)-&gt;second; Message *wr; // 使用相应的参数构造 REPOP 请求 wr = generate_subop( soid, at_version, tid, reqid, pg_trim_to, min_last_complete_ondisk, new_temp_oid, discard_temp_oid, logs, hset_hist, op_t, shard, pinfo); if (op-&gt;op &amp;&amp; op-&gt;op-&gt;pg_trace) wr-&gt;trace.init(&quot;replicated op&quot;, nullptr, &amp;op-&gt;op-&gt;pg_trace); // 将消息发送出去到整个集群 // void OSDService::send_message_osd_cluster(int peer, Message *m, epoch_t from_epoch) // 写操作的消息发送给对应副本节点对应的 osd get_parent()-&gt;send_message_osd_cluster( shard.osd, wr, get_osdmap_epoch()); } } } do_repop 相应的副本 OSD 收到消息时，继续上述流程，从头到尾，直到执行对应的 do_request 方法。在 do_request 方法中曾介绍有对部分请求的处理，截取如下： // 由 PGBackend 直接处理然后返回，此处只处理以下操作 // 1. MSG_OSD_PG_RECOVERY_DELETE (Common) // 2. MSG_OSD_PG_RECOVERY_DELETE_REPLY (Common) // 3. MSG_OSD_PG_PUSH (副本) // 4. MSG_OSD_PG_PULL (副本) // 5. MSG_OSD_PG_PUSH_REPLY (副本) // 6. MSG_OSD_REPOP (副本) // 7. MSG_OSD_REPOPREPLY (副本) // 8. MSG_OSD_EC_WRITE (EC) // 9. MSG_OSD_EC_WRITE_REPLY (EC) // 10. MSG_OSD_EC_READ (EC) // 11. MSG_OSD_EC_READ_REPLY (EC) // 12. MSG_OSD_PG_PUSH (EC) // 13. MSG_OSD_PG_PUSH_REPLY (EC) if (pgbackend-&gt;handle_message(op)) return; 其中就包含 MSG_OSD_REPOP 和 MSG_OSD_REPOPREPLY 的处理（针对多副本）。又相应地调用了 do_repop 和 do_repop_reply 方法 bool ReplicatedBackend::_handle_message( OpRequestRef op) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot;: &quot; &lt;&lt; op &lt;&lt; dendl; switch (op-&gt;get_req()-&gt;get_type()) { case MSG_OSD_PG_PUSH: do_push(op); return true; case MSG_OSD_PG_PULL: do_pull(op); return true; case MSG_OSD_PG_PUSH_REPLY: do_push_reply(op); return true; case MSG_OSD_REPOP: { do_repop(op); return true; } case MSG_OSD_REPOPREPLY: { do_repop_reply(op); return true; } default: break; } return false; } do_repop 用于处理 repop 类型的 msg，相应地检查参数和当前 OSD 对应的状态，记录日志，注册回调函数，并执行本地事务更新。 // sub op modify void ReplicatedBackend::do_repop(OpRequestRef op) { static_cast&lt;MOSDRepOp *&gt;(op-&gt;get_nonconst_req())-&gt;finish_decode(); // 获取当前消息 auto m = op-&gt;get_req&lt;MOSDRepOp&gt;(); // 检查消息类型 int msg_type = m-&gt;get_type(); ceph_assert(MSG_OSD_REPOP == msg_type); const hobject_t &amp;soid = m-&gt;poid; dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; soid &lt;&lt; &quot; v &quot; &lt;&lt; m-&gt;version &lt;&lt; (m-&gt;logbl.length() ? &quot; (transaction)&quot; : &quot; (parallel exec&quot;) &lt;&lt; &quot; &quot; &lt;&lt; m-&gt;logbl.length() &lt;&lt; dendl; // 检查版本号和 interval // sanity checks ceph_assert(m-&gt;map_epoch &gt;= get_info().history.same_interval_since); // 检查该副本节点是否在进行 scrub 操作 dout(30) &lt;&lt; __func__ &lt;&lt; &quot; missing before &quot; &lt;&lt; get_parent()-&gt;get_log().get_missing().get_items() &lt;&lt; dendl; parent-&gt;maybe_preempt_replica_scrub(soid); // 获取消息来源 int ackerosd = m-&gt;get_source().num(); // 标记当前操作开始，设置相关参数 op-&gt;mark_started(); RepModifyRef rm(std::make_shared&lt;RepModify&gt;()); rm-&gt;op = op; rm-&gt;ackerosd = ackerosd; rm-&gt;last_complete = get_info().last_complete; rm-&gt;epoch_started = get_osdmap_epoch(); ceph_assert(m-&gt;logbl.length()); // shipped transaction and log entries vector&lt;pg_log_entry_t&gt; log; auto p = const_cast&lt;bufferlist &amp;&gt;(m-&gt;get_data()).cbegin(); decode(rm-&gt;opt, p); if (m-&gt;new_temp_oid != hobject_t()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; start tracking temp &quot; &lt;&lt; m-&gt;new_temp_oid &lt;&lt; dendl; add_temp_obj(m-&gt;new_temp_oid); } if (m-&gt;discard_temp_oid != hobject_t()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; stop tracking temp &quot; &lt;&lt; m-&gt;discard_temp_oid &lt;&lt; dendl; if (rm-&gt;opt.empty()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot;: removing object &quot; &lt;&lt; m-&gt;discard_temp_oid &lt;&lt; &quot; since we won't get the transaction&quot; &lt;&lt; dendl; rm-&gt;localt.remove(coll, ghobject_t(m-&gt;discard_temp_oid)); } clear_temp_obj(m-&gt;discard_temp_oid); } p = const_cast&lt;bufferlist &amp;&gt;(m-&gt;logbl).begin(); decode(log, p); rm-&gt;opt.set_fadvise_flag(CEPH_OSD_OP_FLAG_FADVISE_DONTNEED); bool update_snaps = false; if (!rm-&gt;opt.empty()) { // If the opt is non-empty, we infer we are before // last_backfill (according to the primary, not our // not-quite-accurate value), and should update the // collections now. Otherwise, we do it later on push. update_snaps = true; } // flag set to true during async recovery bool async = false; pg_missing_tracker_t pmissing = get_parent()-&gt;get_local_missing(); if (pmissing.is_missing(soid)) { async = true; dout(30) &lt;&lt; __func__ &lt;&lt; &quot; is_missing &quot; &lt;&lt; pmissing.is_missing(soid) &lt;&lt; dendl; for (auto &amp;&amp;e : log) { dout(30) &lt;&lt; &quot; add_next_event entry &quot; &lt;&lt; e &lt;&lt; dendl; get_parent()-&gt;add_local_next_event(e); dout(30) &lt;&lt; &quot; entry is_delete &quot; &lt;&lt; e.is_delete() &lt;&lt; dendl; } } parent-&gt;update_stats(m-&gt;pg_stats); // 更新日志 parent-&gt;log_operation( std::move(log), m-&gt;updated_hit_set_history, m-&gt;pg_trim_to, m-&gt;version, /* Replicated PGs don't have rollback info */ m-&gt;min_last_complete_ondisk, update_snaps, rm-&gt;localt, async); // 注册回调函数 C_OSD_RepModifyCommit，回调完成后调用 pg-&gt;repop_commit(rm) // C_OSD_RepModifyCommit(ReplicatedBackend *pg, RepModifyRef r) // : pg(pg), rm(r) {} // void finish(int r) override // { // pg-&gt;repop_commit(rm); // } rm-&gt;opt.register_on_commit( parent-&gt;bless_context( new C_OSD_RepModifyCommit(this, rm))); vector&lt;ObjectStore::Transaction&gt; tls; tls.reserve(2); tls.push_back(std::move(rm-&gt;localt)); tls.push_back(std::move(rm-&gt;opt)); // 本地事务更新 parent-&gt;queue_transactions(tls, op); // op is cleaned up by oncommit/onapply when both are executed dout(30) &lt;&lt; __func__ &lt;&lt; &quot; missing after&quot; &lt;&lt; get_parent()-&gt;get_log().get_missing().get_items() &lt;&lt; dendl; } queue_transactions 无论是一开始的主节点还是后面描述的副本节点执行写操作都调用了 queue_transactions 该方法，该方法是ObjectStore 层的统一入口，KVStore、MemStore、FileStore、BlueStore都相应的实现了这个接口。 该方法相应地创建事务上下文并进行保序，并执行事务状态机，事务提交后相应地执行上文注册的一系列回调函数。 // --------------------------- // transactions int BlueStore::queue_transactions( CollectionHandle &amp;ch, vector&lt;Transaction&gt; &amp;tls, TrackedOpRef op, ThreadPool::TPHandle *handle) { FUNCTRACE(cct); // on_commoit: 事务提交完成之后的回调函数 // on_applied_sync: 同步调用执行，事务应用完成之后的回调函数 // on_applied: 在 Finisher 线程里异步调用执行，事务应用完成之后的回调函数 list&lt;Context *&gt; on_applied, on_commit, on_applied_sync; ObjectStore::Transaction::collect_contexts( tls, &amp;on_applied, &amp;on_commit, &amp;on_applied_sync); // 计时开始 auto start = mono_clock::now(); Collection *c = static_cast&lt;Collection *&gt;(ch.get()); // 获取操作序列号，用于保序。 // 会判断 PG 是否已经关联 OpSeq，未关联则新建并关联 PG OpSequencer *osr = c-&gt;osr.get(); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; ch &quot; &lt;&lt; c &lt;&lt; &quot; &quot; &lt;&lt; c-&gt;cid &lt;&lt; dendl; // 创建 TransContext，并关联回调函数 on_commit // prepare TransContext *txc = _txc_create(static_cast&lt;Collection *&gt;(ch.get()), osr, &amp;on_commit, op); // With HM-SMR drives (and ZNS SSDs) we want the I/O allocation and I/O // submission to happen atomically because if I/O submission happens in a // different order than I/O allocation, we end up issuing non-sequential // writes to the drive. This is a temporary solution until ZONE APPEND // support matures in the kernel. For more information please see: // https://www.usenix.org/conference/vault20/presentation/bjorling if (bdev-&gt;is_smr()) { atomic_alloc_and_submit_lock.lock(); } // 将所有的写操作添加到 TransContext，并记录操作字节数 for (vector&lt;Transaction&gt;::iterator p = tls.begin(); p != tls.end(); ++p) { txc-&gt;bytes += (*p).get_num_bytes(); _txc_add_transaction(txc, &amp;(*p)); } // 计算开销 _txc_calc_cost(txc); // 更新 ONodes， shared_blobs _txc_write_nodes(txc, txc-&gt;t); // journal deferred items if (txc-&gt;deferred_txn) { txc-&gt;deferred_txn-&gt;seq = ++deferred_seq; bufferlist bl; encode(*txc-&gt;deferred_txn, bl); string key; get_deferred_key(txc-&gt;deferred_txn-&gt;seq, &amp;key); txc-&gt;t-&gt;set(PREFIX_DEFERRED, key, bl); } _txc_finalize_kv(txc, txc-&gt;t); #ifdef WITH_BLKIN if (txc-&gt;trace) { txc-&gt;trace.event(&quot;txc encode finished&quot;); } #endif if (handle) handle-&gt;suspend_tp_timeout(); // 记录 throttle 开始的时间 auto tstart = mono_clock::now(); // 事务提交到 Throttle (内部流控机制) if (!throttle.try_start_transaction( *db, *txc, tstart)) { // ensure we do not block here because of deferred writes dout(10) &lt;&lt; __func__ &lt;&lt; &quot; failed get throttle_deferred_bytes, aggressive&quot; &lt;&lt; dendl; ++deferred_aggressive; deferred_try_submit(); { // wake up any previously finished deferred events std::lock_guard l(kv_lock); if (!kv_sync_in_progress) { kv_sync_in_progress = true; kv_cond.notify_one(); } } throttle.finish_start_transaction(*db, *txc, tstart); --deferred_aggressive; } // 记录 throttle 完成时间 auto tend = mono_clock::now(); if (handle) handle-&gt;reset_tp_timeout(); logger-&gt;inc(l_bluestore_txc); // 处理事务状态，执行状态机，将 IO 请求交给块设备执行 // 该方法中为一系列事务状态机的转换，最终写操作完成后会执行 oncommit 的回调 // execute (start) _txc_state_proc(txc); if (bdev-&gt;is_smr()) { atomic_alloc_and_submit_lock.unlock(); } // 针对写完日志之后的回调操作，也就是所谓的 on_readable // BliueStore 只会产生少量 WAL 到 RocksDB，所以写日志先于写数据完成 // we're immediately readable (unlike FileStore) for (auto c : on_applied_sync) { c-&gt;complete(0); } if (!on_applied.empty()) { if (c-&gt;commit_queue) { c-&gt;commit_queue-&gt;queue(on_applied); } else { finisher.queue(on_applied); } } #ifdef WITH_BLKIN if (txc-&gt;trace) { txc-&gt;trace.event(&quot;txc applied&quot;); } #endif log_latency(&quot;submit_transact&quot;, l_bluestore_submit_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age); log_latency(&quot;throttle_transact&quot;, l_bluestore_throttle_lat, tend - tstart, cct-&gt;_conf-&gt;bluestore_log_op_age); return 0; } repop_commit 在写完成后的回调函数中对应地执行 repop_commit，相应地构造 MOSDRepOpReply，再发送到集群。 void ReplicatedBackend::repop_commit(RepModifyRef rm) { rm-&gt;op-&gt;mark_commit_sent(); rm-&gt;op-&gt;pg_trace.event(&quot;sup_op_commit&quot;); rm-&gt;committed = true; // send commit. auto m = rm-&gt;op-&gt;get_req&lt;MOSDRepOp&gt;(); ceph_assert(m-&gt;get_type() == MSG_OSD_REPOP); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; on op &quot; &lt;&lt; *m &lt;&lt; &quot;, sending commit to osd.&quot; &lt;&lt; rm-&gt;ackerosd &lt;&lt; dendl; ceph_assert(get_osdmap()-&gt;is_up(rm-&gt;ackerosd)); get_parent()-&gt;update_last_complete_ondisk(rm-&gt;last_complete); MOSDRepOpReply *reply = new MOSDRepOpReply( m, get_parent()-&gt;whoami_shard(), 0, get_osdmap_epoch(), m-&gt;get_min_epoch(), CEPH_OSD_FLAG_ONDISK); reply-&gt;set_last_complete_ondisk(rm-&gt;last_complete); reply-&gt;set_priority(CEPH_MSG_PRIO_HIGH); // this better match ack priority! reply-&gt;trace = rm-&gt;op-&gt;pg_trace; get_parent()-&gt;send_message_osd_cluster( rm-&gt;ackerosd, reply, get_osdmap_epoch()); log_subop_stats(get_parent()-&gt;get_logger(), rm-&gt;op, l_osd_sop_w); } do_repop_reply MSG_OSD_REPOPREPLY 消息发送到了集群，又开始从头到尾的消息处理逻辑，在 _handle_message 中对该类型的消息进行处理，相应地执行 do_repop_reply void ReplicatedBackend::do_repop_reply(OpRequestRef op) { static_cast&lt;MOSDRepOpReply *&gt;(op-&gt;get_nonconst_req())-&gt;finish_decode(); // 获取上文构造的 Reply Msg auto r = op-&gt;get_req&lt;MOSDRepOpReply&gt;(); ceph_assert(r-&gt;get_header().type == MSG_OSD_REPOPREPLY); op-&gt;mark_started(); // must be replication. ceph_tid_t rep_tid = r-&gt;get_tid(); pg_shard_t from = r-&gt;from; auto iter = in_progress_ops.find(rep_tid); if (iter != in_progress_ops.end()) { // 获取副本节点上正在处理的 op InProgressOp InProgressOp &amp;ip_op = *iter-&gt;second; const MOSDOp *m = nullptr; if (ip_op.op) m = ip_op.op-&gt;get_req&lt;MOSDOp&gt;(); if (m) dout(7) &lt;&lt; __func__ &lt;&lt; &quot;: tid &quot; &lt;&lt; ip_op.tid &lt;&lt; &quot; op &quot; //&lt;&lt; *m &lt;&lt; &quot; ack_type &quot; &lt;&lt; (int)r-&gt;ack_type &lt;&lt; &quot; from &quot; &lt;&lt; from &lt;&lt; dendl; else dout(7) &lt;&lt; __func__ &lt;&lt; &quot;: tid &quot; &lt;&lt; ip_op.tid &lt;&lt; &quot; (no op) &quot; &lt;&lt; &quot; ack_type &quot; &lt;&lt; (int)r-&gt;ack_type &lt;&lt; &quot; from &quot; &lt;&lt; from &lt;&lt; dendl; // oh, good. // 检查响应消息中的 ACK 类型 if (r-&gt;ack_type &amp; CEPH_OSD_FLAG_ONDISK) { ceph_assert(ip_op.waiting_for_commit.count(from)); ip_op.waiting_for_commit.erase(from); if (ip_op.op) { ip_op.op-&gt;mark_event(&quot;sub_op_commit_rec&quot;); ip_op.op-&gt;pg_trace.event(&quot;sub_op_commit_rec&quot;); } } else { // legacy peer; ignore } parent-&gt;update_peer_last_complete_ondisk( from, r-&gt;get_last_complete_ondisk()); // 检查 waiting_for_commit 是否为空 // 如果为空，继续向上回调。C_OSD_RepopCommit // C_OSD_RepopCommit(PrimaryLogPG *pg, PrimaryLogPG::RepGather *repop) // : pg(pg), repop(repop) {} // void finish(int) override // { // pg-&gt;repop_all_committed(repop.get()); // } // }; if (ip_op.waiting_for_commit.empty() &amp;&amp; ip_op.on_commit) { ip_op.on_commit-&gt;complete(0); ip_op.on_commit = 0; in_progress_ops.erase(iter); } } } repop_all_committed PrimaryLogPG::repop_all_committed 准备于客户端进行交互，调用 eval_repop void PrimaryLogPG::repop_all_committed(RepGather *repop) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot;: repop tid &quot; &lt;&lt; repop-&gt;rep_tid &lt;&lt; &quot; all committed &quot; &lt;&lt; dendl; repop-&gt;all_committed = true; if (!repop-&gt;rep_aborted) { if (repop-&gt;v != eversion_t()) { recovery_state.complete_write(repop-&gt;v, repop-&gt;pg_local_last_complete); } eval_repop(repop); } } eval_repop 通过执行在 execute_ctx 函数中注册的 commit 回调，从而向客户端发送应答消息。 void PrimaryLogPG::eval_repop(RepGather *repop) { dout(10) &lt;&lt; &quot;eval_repop &quot; &lt;&lt; *repop &lt;&lt; (repop-&gt;op &amp;&amp; repop-&gt;op-&gt;get_req&lt;MOSDOp&gt;() ? &quot;&quot; : &quot; (no op)&quot;) &lt;&lt; dendl; // 所有副本应答写入磁盘完成 // ondisk? if (repop-&gt;all_committed) { dout(10) &lt;&lt; &quot; commit: &quot; &lt;&lt; *repop &lt;&lt; dendl; for (auto p = repop-&gt;on_committed.begin(); p != repop-&gt;on_committed.end(); repop-&gt;on_committed.erase(p++)) { // 执行回调 // ctx-&gt;register_on_commit( // [m, ctx, this]() { // if (ctx-&gt;op) // log_op_stats(*ctx-&gt;op, ctx-&gt;bytes_written, ctx-&gt;bytes_read); // if (m &amp;&amp; !ctx-&gt;sent_reply) // { // MOSDOpReply *reply = ctx-&gt;reply; // ctx-&gt;reply = nullptr; // reply-&gt;add_flags(CEPH_OSD_FLAG_ACK | CEPH_OSD_FLAG_ONDISK); // dout(10) &lt;&lt; &quot; sending reply on &quot; &lt;&lt; *m &lt;&lt; &quot; &quot; &lt;&lt; reply &lt;&lt; dendl; // osd-&gt;send_message_osd_client(reply, m-&gt;get_connection()); // ctx-&gt;sent_reply = true; // ctx-&gt;op-&gt;mark_commit_sent(); // } // }); (*p)(); } // send dup commits, in order auto it = waiting_for_ondisk.find(repop-&gt;v); if (it != waiting_for_ondisk.end()) { ceph_assert(waiting_for_ondisk.begin()-&gt;first == repop-&gt;v); for (auto &amp;i : it-&gt;second) { int return_code = repop-&gt;r; if (return_code &gt;= 0) { return_code = std::get&lt;2&gt;(i); } osd-&gt;reply_op_error(std::get&lt;0&gt;(i), return_code, repop-&gt;v, std::get&lt;1&gt;(i), std::get&lt;3&gt;(i)); } waiting_for_ondisk.erase(it); } publish_stats_to_osd(); dout(10) &lt;&lt; &quot; removing &quot; &lt;&lt; *repop &lt;&lt; dendl; ceph_assert(!repop_queue.empty()); dout(20) &lt;&lt; &quot; q front is &quot; &lt;&lt; *repop_queue.front() &lt;&lt; dendl; if (repop_queue.front() == repop) { RepGather *to_remove = nullptr; while (!repop_queue.empty() &amp;&amp; (to_remove = repop_queue.front())-&gt;all_committed) { repop_queue.pop_front(); for (auto p = to_remove-&gt;on_success.begin(); p != to_remove-&gt;on_success.end(); to_remove-&gt;on_success.erase(p++)) { (*p)(); } remove_repop(to_remove); } } } } ","link":"https://blog.shunzi.tech/post/rados-io-path/"},{"title":"FlatStore: An Efficient Log-Structured Key-Value Storage Engine for Persistent Memory","content":" 该篇文章来自于 ASPLOS20 - FlatStore: An Efficient Log-Structured Key-Value Storage Engine for Persistent Memory 要解决的主要问题是现阶段的 PM 上的 KV 存储都没能充分利用 NVM 的带宽，出发点其实也是发现生产环境中的 KV 负载大多是小 KV 整个方案还是比较清晰的，涉及到的方方面面也在论文中有所阐述，基于 NVM 的数据结构的设计，流水线优化，RDMA 实现优化，值得一读。 Abstract 问题：新型技术 PM 和高速 NIC 的出现使得高效的键值存储系统能够得以构建。但是我们观察发现现有的 KV 存储系统中小 IO 的访问模式和 PM 的持久性保证粒度不匹配，没有充分利用 PM 的带宽。 What we do: 提出了 FlatStore，一个基于 PM 的高效 KV 存储引擎。具体而言，该引擎将 KV 存储给解耦成了一个持久的日志结构（实现高效存储）和一个易失的索引（实现快速索引）。在此基础上，FlatStore 进一步结合了两种技术： 压缩的日志格式，以最大限度的批处理的机会在日志 在创建批处理时，通过流水线水平批处理从其他内核窃取日志条目，从而提供低延迟和高吞吐量的性能 实现和测试：使用了 Hash Table 和 Masstree 构建易失的索引，在 Optane DC PM 上进行了部署测试，单服务器节点可以达到 35Mop/s，比现有的系统快 2.5-6.3 倍。 Introduction 关键问题：如何设计高效的键值存储来处理写密集型和小规模的工作负载？ 业界的解决方式和问题：运用新型技术，诸如新型存储介质（PCM, ReRAM, Optane DC PM）以及 InfiniBand 网络技术。通过分析大量的学术界中现有的对于基于 PM 的 KV 存储研究发现，其访问模式都没能匹配 PM 的访问粒度。 KV 存储本身的特点： KV 存储生成大量的小型写操作(例如，在生产工作负载中，很大一部分KV对只包含几个或几十个字节)，大多数更新操作其实只会导致一个或几个指针的更新 此外，KV 存储中的索引结构会导致写操作的放大。 基于 HASH 的索引往往需要进行重映射来解决 HASH 冲突，或者因为 HASH 表大小的变化而进行重映射 基于 Tree 的索引需要频繁地移动每个树节点中的索引条目，以保持它们的顺序，并合并/拆分树节点，以保持树的平衡 对于故障原子性，需要使用额外的刷新指令显式地持久化更新后的数据。（clflushopt/clwb）但是 CPU 刷新数据是以 cache line 为粒度的，在 AMD 和 x86 平台粒度对应的为 64B，PM 可能有更粗的内部块大小，比如 Optane DC PM 通常为 256B，在绝大多数场景下都比写的粒度要大，所以严重地浪费了硬件带宽。 简单测试：部署了 FAST&amp;FAIR，发现该方案实现了 PUT 操作 3.5 Mops/s，仅仅只达到了 Optane PM 的裸带宽的 6% 解决问题的通用思路：使用日志结构来管理 KV 存储，通过将批量的写请求进行聚合成大块的顺序读写，从而将写开销分摊到每个请求，但是这种方法受限于可以同时进行的更新的数量，在顺序 IO 友好的存储器如 HDD SSD 上效果较好。但是在 NVM 上进行构建时将会存在一些问题： 随着 I/O 大小大于 PM 最小的 I/O 单元 256B 之后，且有足够的线程数并发地执行 I/O，此时对于 Optane 的顺序和随机写的性能十分相近，此时聚合写的好处将体现不出来。PM 有更细的访问粒度(即64 B刷新大小和256 B块大小)，如果我们简单地在日志中记录每个内存更新，那么它只能容纳非常有限的日志条目 批处理不可避免地增加了延迟，阻碍了它在新兴的PMs和高速 NIC 中的应用，因为低延迟是对应硬件的最大好处之一。 现有方案的主要问题：已经有一些研究（ATC17 Log-structured non-volatile main memory 和 FAST16 的 NOVA） 将日志结构化的数据结构应用到基于 PM 的存储系统上，主要是为了利用日志结构的故障原子性保证或者减少内存的碎片，但都没有考虑使用批处理来分摊持久化的开销。 本文的方案： 目的是为了解决上述问题，实现 高吞吐、低延迟、多核扩展。核心思想是解耦成两个部分，一个是易失的索引结构，一个是存储在 PM 上的高效日志结构。还包括以下技术点： Compact log format：FlatStore 只存储索引元数据和小 KVs 到持久化的日志中。大 KVs 通过使用 NVM allocator 单独存储，因为日志结构对于大 IO 没有那么好的效果。而在日志结构上的数据组织方式，采用了 operation log technique 的方式，即简单描述每一个操作，而不是记录每个索引更新操作，从而减小索引元数据对于空间的占用， Pipelined Horizontal Batching：允许一个核在创建一个批量任务的时候可以收集来自别的核的日志项，该方法减少了积累指定数目的日志项的时间，为了缓和核与核之间的争用： 在不影响正确性的前提下，调度核提前释放锁 将核心组织成组，以平衡争用开销和批处理机会 内存的索引结构使用了两种， hash table (FlatStore-H) 和 Masstree (FlatStore-M) Background And Motivation 现有的研究都是基于模拟器实现，所以存在一定的假设，和实际的物理器件的具体表现其实有一定的差异，甚至相反的结果。 生产环境的 KV 负载：数据中心的数据统计表示 KV 存储大多都是小粒度的，读为主的访问模式也逐渐转变成为了写为主的访问模式 细粒度更新和持久化的粒度不匹配：主要分析了现有的持久性内存上的 KV 存储，由持久化的索引和实际需要存储的 KV 对两部分组成。KV 记录通常都保存在 NVM 上，但是索引的存储方式略有不同。CDDS-Tree、wB+-Tree、FAST&amp;FAIR、CCEH、Level-Hashing 都是把整个索引放在 NVM 上，FPTree、NV-Tree 是把索引结构的叶子节点放在 NVM 上（其他节点放在 DRAM 上），Bullet、HiKV 则是采用了容灾的策略（DRAM 上和 NVM 上都存，使用后台线程进行同步），这些设计中一个 PUT 操作通常都会引入很多对 NVM 的更新操作，就包括： 对实际的 KV 的更新（生产环境通常都是小 IO） 对 allocator 内部的元数据更新 对于索引结构的多次更新（可能造成严峻的写放大） CPU Cache 到 NVM 常常会以 CacheLine 的粒度进行刷回，64B，与 PM 的 256B 的块大小不匹配，浪费了 PM 的带宽 Optane DCPMM：在一些测试的基础上，我们有一些新的发现 高并发情况下顺序访问和随机访问的带宽相近。 对同一 cacheline 重复刷新会被显著延迟。 可能的原因一：clwb 异步发起，后续的 clwb 被阻塞，直到前一个完成或者超时 原因二：Optane 自带片上的负载均衡功能，当访问同一个 CacheLine 时阻塞后续的 flush 操作 挑战： 日志结构存储在PM中有较少的批处理机会。PM 粒度更小，顺序随机访问性能接近，日志结构化数据结构的更新常常伴随严峻的写放大。 批处理增加延迟。器件本身就是低延迟特性，批处理的设计与器件特性背道而驰 Design Overview 结构组成主要分为三个部分： Volatile Indexes Compacted OpLog Lazy-Persistent Allocator 优化主要体现以下两个个方面： Compacted OpLog (Pipelined) Horizontal Batching Compacted OpLog 组成：日志项会尽可能地设计得比较小以更好地支持批量操作，以便在一个 FLUSH 操作里持久化更多的日志项，所以 OpLog 中的日志项只会保存索引元数据和特别小的 KV 数据。 压缩日志项：使用 operation log technique，日志项不会记录内存的每一次更新，而是只包含描述每个操作的最少信息，如图所示，每个日志项包含五个部分： Op：操作类型 Emd: 该 KV 对是否放在日志项的末尾，用于日志类型的区分 Version: 版本用于保证日志清理的正确性 Key：8byte key。 Ptr-based: FlatStore 支持更大的 Key 存在 OpLog 之外，和 Value 一样，然后使用指针 Ptr 指向实际的存储位置。- 为了最小化日志大小，只给 Ptr 40bits，40+8 位能索引 128TB 的 NVM 空间。 每个日志项的大小被限制在 16 字节，因此可以同时刷入 16 个日志项，和刷入一个日志项开销相等。 Value：较小的 KV 会把 value size 和 Value 一同放在日志项末尾 Padding：前文提到对同一 cache line 的刷回会有性能问题，在我们对应的日志刷回过程中，也可能存在两个 Batch 使用同一个 cache line 的情况，如上图所示，Batch 1 和 Batch 2 同时使用了 Cacheline2，batch2 会被延迟，所以我们为了解决这个问题，采用填充的方式，即在每一个 Batch 后进行填充，直到填满 Cacheline，从而避免多个 Batch 使用一个 Cacheline 的情况。但是这种方式对于就地更新的存储系统较为复杂，特别是在一些倾斜负载的情况下。 Lazy-persist Allocator 思路来源：NVM 分配器需要仔细维护 NVM 元数据，即追踪哪些地址已经被使用，哪些地址仍然空闲。但是我们发现在日志项和分配元数据有一定的重复：一旦数据被插入成功，日志中的 Ptr 指针总是指向一个分配了的数据块，因此我们可以惰性地持久化分配元数据，并且可以在系统故障之后利用该重复数据进行恢复。主要的挑战是如何在恢复期间在每个日志条目中使用 Ptr 来反向定位分配元数据，因为分配器需要支持可变长度分配。 核心思想：我们提出了 Lazy-persist Allocator，将 NVM 切分成 4MB 的 chunks，然后这些 4MB 的 chunks 又被划分成不同种类的数据块，相同 chunk 里的数据块对应的数据大小相等。在每个 NVM 块准备分配时会记录对应的切分数据大小，记录在 head 中，同时还会存储一个位图，来标识未使用的 data blocks。在这种设计中，每个块的起始地址都是 4MB 对齐的，每个块的分配粒度在其头部指定，因此一个已经分配的 NVM block 在 chunk 内的偏移量可以直接通过 Ptr 计算出来，从而就能系统崩溃后恢复出对应的 bitmap。考虑到扩展性问题，这些 4MB 的 NVM chunks 被分分区到不同的 cores。一旦收到分配请求，分配器首先选择一个合适的 NVM chunk，然后分配一个空闲的数据块，修改相应的位图。对于大小大于 4MB 的分配，虽然在 KV 存储中几乎不太可能，我们也可以直接分配一个或多个连续的 chunks 来解决。 完整流程（PUT 操作）： Lazy-persist Allocator 分配一个数据块，拷贝数据到该数据块，格式位 (v_len, value) 并持久化。（小 KV 会跳过该步骤） 初始化一个日志项，并填充日志项中的数据 如果该记录放在 OpLog 之外的位置，Ptr 指向步骤一分配的数据块 如果该数据块已经存在的话，相应的版本号加一 追加日志项到日志中并进行持久化 最终更新尾指针指向日志的末尾，并持久化 更新项对应的易失的索引结构来指向该日志项 对于崩溃一致性，如果记录存在的话我们需要进行异地更新，因此除了上述步骤以外，一旦插入完成，我们还需要释放老的数据块。释放之后的空闲块可以立即被重新使用，因为 FlatStore 中不存在 删后读 异常，因为 KV 被根据 keyhash 被转发到对应的核，相同 Key 操作对应的核也相同，会使用一个冲突队列来串行化操作。只有步骤二之后，即数据被写入到日志中并持久化才认为此次操作完成，但如果此时发生了故障，FlatStore 可以通过重放 OpLog 来重建索引和分配位图，删除操作则使用一个标识。 开销分析：FlatStore 包含三个刷回操作，KV record、日志项和尾指针。因为小 KV 直接放在了日志中，刷回的次数大大减少，选择性地将 KV 记录放在 OpLog 和分配器中，也使得垃圾回收更加高效。FlatStore 需要周期性地回收陈旧的日志项占用的 NVM 空间，因为日志项很小所以其压缩过程也不会占用太多的 CPU 资源。 Horizontal Batching 为了更好地支持 batching，服务器的 core 可以接受多个客户端请求并统一处理。假设 N 个 PUT 请求到达，FlatStore 为每个请求首先分配数据块并持久化 KV 项，然后合并日志项并一起刷回到 OpLog 中，最后更新内存中的索引。因为使用了 Batching，PM 写的次数从 3N 减小到了 N+2： 因为使用了只能追加写的 OpLog，来自不同请求的日志条目有机会被刷新在一起 OpLog 尾指针的更新从每请求一次到每批处理一次 小 KV 直接存储在日志中，持久化操作进一步减少 因为上述批处理策略只会批处理每一个核各自收到的请求，称之为 Vertical Batching。该方式减小了持久化的开销，但是引入了更高的延迟，因为减少了跨核的 Batching，每个核不得不等足够的请求到了之后才能 Batch。如下图所示 a 和 b 的对比，Vertical Batching 相比于无 batching 的情况显著增加了响应延迟。亚微妙级别的硬件如 NVM,RDMA 驱使我们实现更小的延迟，于是设计了 Horizontal Batching，允许 steal 其他核的请求。 在 Pipelined Horizontal Batching (Pipelined HB) 之前首先介绍什么是 Horizontal Batching：如下图所示，为了实现日志项的跨核，我们设计了： 一个用于同步所有核的全局锁 用于核之间的通信请求池，每个核对应一个池 PUT 操作被解耦成三个阶段： l-persist：空间分配并持久化 KV 对 g-persist：持久化日志项 volatile：更新易失索引 步骤如图所示： 空间分配并持久化 KV 对 ❶/➀ 每个 CPU core 将需要持久化的日志项对应的地址放在本地的请求池中 ❷/➁ 试图请求全局锁 ❸/➂ 拿到锁的 core 成为 leader，其他的 core 成为 followers,followers 等待提交了的请求完成 ➃ leader core 从其他核那里抓取日志项 ❹。（这里通过让 leader core 扫描所有其他 cores 来抓取所有存在的日志项，而不是使用其他复杂的策略，来减少延迟） 收集到的日志项被合并到一起然后追加到 OpLog 中，以批量的形式 ❺ and ❻ 完成后释放全局锁，并通知其他核心持久化操作已经完成 ❼ 最后 leader 和 followers 都更新内存索引结构并向客户端发送响应消息 ❽/➄ 使用这种方式其实次优的。因为这三个阶段的处理是严格有序的，而且大多数 CPU 周期都花在等待日志持久化完成上。于是提出了 Pipelined HB 交叉执行每个阶段： 一旦核获取锁失败，core 转而轮询下一个请求的到达，并执行第二个水平批处理的逻辑 followers 仅异步地等待来自之前 leader 的完成消息 leader 在从其他 core 收集了日志条目之后，就会释放全局锁，因此日志持久化开销就不会在锁的逻辑里 如下图 C D 所示的效果 但是 Pipelined HB 也存在一定的问题。Pipelined HB 可能对客户端的请求重排序：一个 core 处理一组请求，后面的 Get 操作可能无法看到前面的 PUT 操作相同 Key 带来的实质影响。这样的例子发生在 PUT 仍然在被 leader 处理时，follower core 切换了开始处理下一个请求。为了解决这个问题，每个 core 会维护一个独占的冲突队列来追踪正在被服务的请求，如果 Key 冲突的话后续请求相应地都会被滞后。 Pipelined HB 和 work stealing 的不同：work stealing 通过让空闲的线程窃取繁忙的线程的任务。重新平衡了负载；Pipelined HB 依赖 work stealing，但是是以相反的方式让一个 core 获得所有的请求从而减小 Batching 的延迟。 Pipelined HB with Grouping：在 multi-socket 平台上，由于大量的 cores 都需要请求全局锁会导致严峻的同步开销，为了解决这个问题，cores 通常会被分区成不同的 groups 来执行 Pipelined HB。因此，一个合适的分组大小和 batch 大小能够较好地平衡全局同步开销。分组太小，虽然锁开销更小，但是也就意味着 batch size 变小。基于我们的实验结果，通过将一个 socket 对应的所有核安排在一个组就能提供较优的性能。 Log Cleaning FlatStore 中每一个 core 维护了一个内存中的 table 来追踪正在处理普通的 PUT 和 DELETE 请求时 OpLog 中每个 4MB chunk 的使用情况。一个 block 何时被插入到回收列表中取决于该块中有效 KV 对的比例和总共空闲的 chunks 数。每一个 HB group 启用一个后台线程来清理日志，因此日志的回收操作是在不同的组并行进行的，cleaner 线程是周期执行的，周期地扫描回收列表。 为了回收 NVM chunk，cleaner 线程首先扫描该 chunk 来判断每个日志项的活性，通过比较日志项的版本号和内存索引中存储的最新的版本号，所有仍然有效的日志项将被拷贝到新分配的 NVM chunk 中。对 tombstones (对于Del)的活性进行识别则更为复杂，只有在回收了与此KV项相关的所有日志项之后，才能安全地回收它。之后，cleaner 更新内存中索引中的对应项，以使用原子 CAS 指向它们的新位置。最后，通过将旧块放回分配器来释放它。当新的NVM chunk 被填满时，它将该块链接到相应的 OpLog。还需要跟踪新的 NVM 块的地址，以防止在系统故障时丢失它。我们把这样的地址记录在日志区域上上(PM中的预保护区域)，可以在恢复期间重新读取。 Recovery Recovery after a normal shutdown：正常关闭之前，会将内存中的索引拷贝到 NVM 上的一块预定义的区域中，将每一个 NVM chunk 的 bitmap 进行刷回，最后，写一个 shutdown 的标志来表明是一次正常的关机。在重启的时候，Flatstore 首先检查这个标志的状态并进行重置，如果是正常关机则相应地加载索引数据到内存。 Recovery after a system failure：如果标志位检查发现不是正常关机，则需要重建索引和位图，需要从头到尾地扫描 OpLogs。日志项的 key 将被用于在内存索引中定位一个 slot，如果没有相应的 slot 则在索引中执行插入操作。否则，它通过进一步比较版本号来判断是否更新该索引项的指针。使用 Ptr 项也能重建出分配元数据。实验表明恢复 10 亿个 KV 只需要花费 40s，这样的恢复时间是容许的： 许多生产环境中的负载有更多不同大小的 KV 分布，因此在大多数场景下需要恢复的索引数较少 为了缩短恢复时间，FlatStore 也支持使用 Checkpoint 方式在 CPU 不是特别繁忙的时候周期地将索引数据写入 PM Implementation FlatStore-H: FlatStore with Hash Table 索引结构使用了 CCEH 的方案，CCEH 被用单独的键散列划分为多个范围，每个核都拥有一个CCEH 实例。因此多核可以无锁开销地修改 HashTable。因为索引的持久性已经被 OpLog 来保证，所以直接将 CCEH 放在 DRAM 中且移除了他的刷回操作。客户端直接放请求发送到具体的负责该 Key 的核，每个核将只负责更新他自己的索引，但仍然需要使用 pipelined HB 持久化日志项。 每一个 Bucket 包含多个 index slots，包括 一组 keys 和相应的 version 一组指针指向在 OpLog 中对应的日志项 分区的设计可能会导致在倾斜负载下的负载不均衡，然而，水平批处理能够在很大程度上减轻这种不平衡：它将最耗时的持久化日志操作的负载分散在各个核之间。 FlatStore-M: FlatStore with Masstree 因为现有的持久化 KV 存储常常使用树来作为索引从而之支持顺序和范围查询，所以我们也使用了 Masstree 来实现。Masstree 本身就是基于多核扩展性和高效设计的，为了支持范围查询，一个全局的 Masstree 实例在启动的时候创建，也被所有的 cores 共享。Keys，Versions 和指针都被存储在叶子节点中，和 FlatStore-H 类似，客户端发送的请求也会先经过 HASH 找到一个具体的 core，同时也就减少了 Masstree 更新时的冲突。 RDMA-based FlatRPC RDMA 本身的相关介绍此处不做赘述 RDMA-based FlatRPC 让客户端直接写 core 的 message buffer，但 reply 被委托给 agent core 来发送。当一个客户端连接到每个 server node，只会在它和 agent core 之间常见一个 QP，agent core 随机选择离 NIC 近的 socket。每个 core 也会预分配一个 message buffer 给客户端来存储收到的消息，但是共享一个 QP。 如下图所示，为了发送一个消息， 客户端首先将消息内容跟直接写到一个具体的核对应的 message buffer 中 每个服务器核心都会轮询消息缓冲区以提取新消息并处理它 然后按照响应的规则准备 response 并发送： 如果当前 core 恰好是 agent core，直接使用 MMIO 发送 write 原语 不是 agent core，通过共享内存将原语委托给代理核心 NIC 收到原语，将获取对应的消息内容并发送到客户端 FlatRPC 将 QP 的数量减少到 NcN_cNc​，即客户端的数量，但是引入了额外的核与核之间的委托开销，但我们发现这样做是值得的： 原语命令是轻量级的，只有几个字节，同时 agent core 可以使用预取指令来加速委托过程 委托阶段收集了所有的离当前 NIC 较近的 Socket 的原语命令，从而减小了 MMIO 的开销。 后文有实验整明我们的 FlatRPC 相比于全连接方式有 1.5 倍的吞吐量提升。 RDMA offloading：有的研究建议使用单边原语，例如 READ，来 offload 对客户端 GET 请求的处理，从而减轻服务器 CPU 的负担。在我们的系统中，我们实验发现 RDMA read 原语对应的吞吐量相比双边原语有一定的下降。因为我们有大量的读操作。因此在 FlatStore 中 GET 和 PUT 请求在服务端都是通过 RPC 原语来处理的。 Evaluation Env 节点数目：1 server，12 clients 服务器： 内存： Optane DCPMM 1TB in total，每个 256GB 128 GB DRAM CPU：two 2.6GHz Intel Xeon Gold 6240M CPUs (36 cores in total) OS：Ubuntu 18.04 客户端： 内存：128GB DRAM CPU： two 2.2GHz Intel Xeon E5-2650 v4 CPUs (24 cores in total) OS：CentOS 7.4 网络：Mellanox MSB7790-ES2F switch，MCX555AECAT ConnectX-5 EDR HCAs，100Gbps IB 参数：默认 batch_size 8 Compare 下表所示四种持久性索引，使用默认配置，具体参见原论文 Results YCSB Facebook ETC Pool - Production Workload Multicore Scalability Internal Function Performance Evaluations 对于每个优化点带来的性能提升，原文也有测试来体现，还对日志回收机制的效率进行了测试，具体细节请查看原文。 Related Work NVM-based KV stores： Hash Based: HiKV, Bullet Tree Based: NV-Tree， FP-Tree, wB+Tree, CDDS-Tree Log-structured Persistent Memory: LSNVMM, NOVA RDMA and Persistent Memory: Octopus, Hotpot, AsymNVM ","link":"https://blog.shunzi.tech/post/ASPLOS20-FlatStore/"},{"title":"Series Five of Basic of Virtualization - Memory Virtualization - Paging","content":" 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtualization（虚拟化）。本篇为虚拟化技术的基础篇系列第三篇（Memory Virtualization），内存虚拟化。 内存虚拟化又将分为两篇，本篇为第二篇内存虚拟化之分页 Chapter Index TODO Paging: Introduction 操作系统解决空间管理的问题大致分为两种： 是将空间分割成不同长度的分片，就像虚拟内存管理中的分段。但是将空间切成不同长度的分片以后，空间本身会碎片化（fragmented），随着时间推移，分配内存会变得比较困难。 将空间分割成固定长度的分片。在虚拟内存中，我们称这种思想为分页。分页不是将一个进程的地址空间分割成几个不同长度的逻辑段（即代码、堆、段），而是分割成固定大小的单元，每个单元称为一页。相应地，我们把物理内存看成是定长槽块的阵列，叫作页帧（page frame）。每个这样的页帧包含一个虚拟内存页。 CRUX: 如何通过页来实现虚拟内存，从而避免分段的问题？ A Simple Example And Overview 如图展示了一个只有 64 字节的小地址空间，有 4 个 16 字节的页（虚拟页 0、1、2、3）。真实的地址空间肯定大得多，通常 32 位有 4GB 的地址空间，甚至有 64 位。 物理内存如图所示，也由一组固定大小的槽块组成。在这个例子中，有 8 个页 帧（由 128 字节物理内存构成，也是极小的）。从图中可以看出，虚拟地址空间的页放在物理内存的不同位置。图中还显示，操作系统自己用了一些物理内存。 可以看到，与我们以前的方法相比，分页有许多优点。可能最大的改进就是灵活性：通过完善的分页方法，操作系统能够高效地提供地址空间的抽象，不管进程如何使用地址空间。例如，我们不会假定堆和栈的增长方向，以及它们如何使用。 另一个优点是分页提供的空闲空间管理的简单性。例如，如果操作系统希望将 64 字节的小地址空间放到 8 页的物理地址空间中，它只要找到 4 个空闲页。也许操作系统保存了一个所有空闲页的空闲列表（free list），只需要从这个列表中拿出 4 个空闲页。即上图所示的物理内存的页帧 2，3，5，7 对应了地址空间中的逻辑页。 为了记录地址空间的每个虚拟页放在物理内存中的位置，操作系统通常为每个进程保存一个数据结构，称为页表（page table）。页表的主要作用是为地址空间的每个虚拟页面保存地址转换（address translation），从而让我们知道每个页在物理内存中的位置。即 VP 到 PF 的映射，上述例中即为 VP0 -&gt; PF3，VP1 -&gt; PF7，VP2 -&gt; PF5，VP3 -&gt; PF2 这个页表是一个每进程的数据结构（我们讨论的大多数页表结构都是每进程的数据结构，我们将接触的一个例外是倒排页表，inverted page table）。如果在上面的示例中运行另一个进程，操作系统将不得不为它管理不同的页表，因为它的虚拟页显然映射到不同的物理页面（除了共享之外）。 地址转换示例，设想拥有这个小地址空间（64 字节）的进程正在访问内存： movl &lt;virtual address&gt;, %eax 为了转换（translate）该过程生成的虚拟地址，我们必须首先将它分成两个组件：虚拟页面号（virtual page number，VPN）和页内的偏移量（offset）。对于这个例子，因为进程的虚拟地址空间是 64 字节，我们的虚拟地址总共需要 6 位（2^6 = 64）。因此，虚拟地址可以表示如下，因为需要选择四个页，故使用前两位表示属于哪个页，剩余的作为偏移量。 假设我们访问的虚拟地址为 21，对应的二进制为 010101，即为虚拟页 1 的偏移量为 5 的地址。此时检索页表，将虚拟页转换成物理页，偏移量保持不变，即可获取到最终的物理地址 1110101（10进制为 117） Where Are Page Tables Stored? 页表可以变得非常大，比我们之前讨论过的小段表或基址/界限对要大得多。例如，想象一个典型的 32 位地址空间，带有 4KB 的页。这个虚拟地址分成 20 位的 VPN 和 12 位的偏移量。一个 20 位的 VPN 意味着，操作系统必须为每个进程管理 220个地址转换（大约一百万）。假设每个页表格条目（PTE）需要 4 个字节，来保存物理地址转换和任何其他有用的东西，每个页表就需要巨大的 4MB 内存！这非常大。现在想象一下有 100 个进程在运行：这意味着操作系统会需要 400MB 内存，只是为了所有这些地址转换！ 由于页表如此之大，我们没有在 MMU 中利用任何特殊的片上硬件，来存储当前正在运行的进程的页表，而是将每个进程的页表存储在内存中。现在让我们假设页表存在于操作系统管理的物理内存中，稍后我们会看到，很多操作系统内存本身都可以虚拟化，因此页表可以存储在操作系统的虚拟内存中（甚至可以交换到磁盘上） What’s Actually In The Page Table? 页表就是一种数据结构，用于将虚拟地址（或者实际上，是虚拟页号）映射到物理地址（物理帧号）。因此，任何数据结构都可以采用。最简单的形式称为线性页表（linear page table），就是一个数组。操作系统通过虚拟页号（VPN）检索该数组，并在该索引处查找页表项（PTE），以便找到期望的物理帧号（PFN）。现在，我们将假设采用这个简单的线性结构。 至于每个 PTE 的内容，我们在其中有许多不同的位，值得有所了解。有效位（valid bit）通常用于指示特定地址转换是否有效。例如，当一个程序开始运行时，它的代码和堆在其地址空间的一端，栈在另一端。所有未使用的中间空间都将被标记为无效（invalid），如果进程尝试访问这种内存，就会陷入操作系统，可能会导致该进程终止。因此，有效位对于支持稀疏地址空间至关重要。通过简单地将地址空间中所有未使用的页面标记为无效，我们不再需要为这些页面分配物理帧，从而节省大量内存。 还可能有保护位（protection bit），表明页是否可以读取、写入或执行。同样，以这些位不允许的方式访问页，会陷入操作系统。存在位（present bit）表示该页是在物理存储器还是在磁盘上（即它已被换出，swapped out）。脏位（dirty bit）也很常见，表明页面被带入内存后是否被修改过。参考位（reference bit，也被称为访问位，accessed bit）有时用于追踪页是否被访问，也用于确定哪些页很受欢迎，因此应该保留在内存中。 图 18.5 显示了来自 x86 架构的示例页表项。它包含一个存在位（P），确定是否允许写入该页面的读/写位（R/W） 确定用户模式进程是否可以访问该页面的用户/超级用户位（U/S），有几位（PWT、PCD、PAT 和 G）确定硬件缓存如何为这些页面工作，一个访问位（A）和一个脏位（D），最后是页帧号（PFN）本身。 Paging: Also Too Slow 还是以上述例子为例，movl 21, %eax 我们只看对地址 21 的显式引用，而不关心指令获取。在这个例子中，我们假定硬件为我们执行地址转换。要获取所需数据，系统必须首先将虚拟地址（21）转换为正确的物理地址（117）。因此，在从地址 117 获取数据之前，系统必须首先从进程的页表中提取适当的页表项，执行转换，然后从物理内存中加载数据。硬件必须知道当前正在运行的进程的页表的位置。现在让我们假设一个页表基址寄存器（page-table base register）包含页表的起始位置的物理地址。为了找到想要的 PTE 的位置，硬件将执行以下功能： VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT PTEAddr = PageTableBaseRegister + (VPN * sizeof(PTE)) 在我们的例子中，VPN MASK 将被设置为 0x30（十六进制 30，或二进制 110000），它从完整的虚拟地址中挑选出 VPN 位；SHIFT 设置为 4（偏移量的位数），这样我们就可以将VPN 位向右移动以形成正确的整数虚拟页码。例如，使用虚拟地址 21（010101），掩码将此值转换为 010000，移位将它变成 01，或虚拟页 1，正是我们期望的值。然后，我们使用该值作为页表基址寄存器指向的 PTE 数组的索引。 一旦知道了这个物理地址，硬件就可以从内存中获取 PTE，提取 PFN，并将它与来自虚拟地址的偏移量连接起来，形成所需的物理地址。具体来说，你可以想象 PFN 被 SHIFT 左移，然后与偏移量进行逻辑或运算，以形成最终地址，如下所示。 offset = VirtualAddress &amp; OFFSET_MASK PhysAddr = (PFN &lt;&lt; SHIFT) | offset 最后，硬件可以从内存中获取所需的数据，并将其放入寄存器eax中。程序现在已经成功地从内存中加载了一个值。整个过程如下所示 1 // Extract the VPN from the virtual address 2 VPN = (VirtualAddress &amp; VPN_MASK) &gt;&gt; SHIFT 3 4 // Form the address of the page-table entry (PTE) 5 PTEAddr = PTBR + (VPN * sizeof(PTE)) 6 7 // Fetch the PTE 8 PTE = AccessMemory(PTEAddr) 9 10 // Check if process can access the page 11 if (PTE.Valid == False) 12 RaiseException(SEGMENTATION_FAULT) 13 else if (CanAccess(PTE.ProtectBits) == False) 14 RaiseException(PROTECTION_FAULT) 15 else 16 // Access is OK: form physical address and fetch it 17 offset = VirtualAddress &amp; OFFSET_MASK 18 PhysAddr = (PTE.PFN &lt;&lt; PFN_SHIFT) | offset 19 Register = AccessMemory(PhysAddr) 对于每个内存引用（无论是取指令还是显式加载或存储），分页都需要我们执行一个额外的内存引用，以便首先从页表中获取地址转换。工作量很大！额外的内存引用开销很大，在这种情况下，可能会使进程减慢两倍或更多。故存在两个问题：系统运行速度过慢，并占用太多内存 A Memory Trace 简单内存访问示例如下： int array[1000]; ... for (i = 0; i &lt; 1000; i++) array[i] = 0; 对应的汇编代码为： 第一条指令将零值（显示为$0x0）移动到数组位置的虚拟内存地址，这个地址是通过取%edi 的内容并将其加上%eax 乘以 4 来计算的。因此，%edi 保存数组的基址，而%eax 保存数组索引（i）。我们乘以 4，因为数组是一个整型数组，每个元素的大小为 4 个字节。 第二条指令增加保存在 %eax 中的数组索引 第三条指令将该寄存器的内容与十六进制值 0x03e8 或十进制数 1000 进行比较。如果比较结果显示两个值不相等（这就是 jne 指令测试），第四条指令跳回到循环的顶部。 0x1024 movl $0x0,(%edi,%eax,4) 0x1028 incl %eax 0x102c cmpl $0x03e8,%eax 0x1030 jne 0x1024 针对如上例子，我们假设一个大小为 64KB 的虚拟地址空间（不切实际地小）。我们还假定页面大小为 1KB。现在需要知道页表的内容，以及它在物理内存中的位置。假设有一个线性（基于数组）的页表，它位于物理地址 1KB（1024）。 我们只需要关心为这个例子映射的几个虚拟页面。首先，存在代码所在的虚拟页面。由于页大小为 1KB，虚拟地址 1024 驻留在虚拟地址空间的第二页（VPN = 1，因为 VPN = 0 是第一页）。假设这个虚拟页映射到物理帧 4（VPN 1→PFN 4）。接下来是数组本身。它的大小是 4000 字节（1000 整数），我们假设它驻留在虚拟地址40000 到 44000（不包括最后一个字节）。它的虚拟页的十进制范围是 VPN = 39……VPN = 42。因此，我们需要这些页的映射。针对这个例子，让我们假设以下虚拟到物理的映射： (VPN 39 → PFN 7), (VPN 40 → PFN 8), (VPN 41 → PFN 9), (VPN 42 → PFN 10) 下图展示了前 5 次循环迭代的整个过程。最下面的图显示了 y 轴上的指令内存引用（黑色虚拟地址和右边的实际物理地址）。中间的图以深灰色展示了数组访问（同样，虚拟在左侧，物理在右侧）；最后，最上面的图展示了浅灰色的页表内存访问（只有物理的，因为本例中的页表位于物理内存中）。整个追踪的 x 轴显示循环的前 5 个迭代中内存访问。每个循环有 10 次内存访问，其中包括 4 次取指令，一次显式更新内存，以及 5 次页表访问，为这 4 次获取和一次显式更新进行地址转换。 Summary 我们已经引入了分页（paging）的概念，作为虚拟内存挑战的解决方案。与以前的方法（如分段）相比，分页有许多优点。首先，它不会导致外部碎片，因为分页（按设计）将内存划分为固定大小的单元。其次，它非常灵活，支持稀疏虚拟地址空间。 然而，实现分页支持而不小心考虑，会导致较慢的机器（有许多额外的内存访问来访问页表）和内存浪费（内存被页表塞满而不是有用的应用程序数据）。 ","link":"https://blog.shunzi.tech/post/basic-of-virtualization-five/"},{"title":"Optane Persisten Memory And UPS Memory","content":" 项目新技术预研。Intel Optane Persistent Memory 和 备电（UPS）内存 主要是资料整理，并考虑应用此类硬件技术到 Ceph. Optane Persistent Memory Prepare Intel®Optane™persistent memory是一种创新的内存技术，它提供了一个可负担的大容量和对数据持久性的支持的独特组合。它可以帮助企业通过增加容量和独特的内存模式推动创新，在最大化 VM 密度的同时降低总体TCO，并通过自动硬件级加密提高内存安全性。 与 DRAM 的区别 PMem具有比传统DRAM更高的容量。PMem模块有128GB、256GB和512GB的容量，比DRAM模块大很多，DRAM模块通常从4GB到32GB，不过也有更大的容量。 PMem还可以在不通电的情况下以持久模式存储数据，这就增加了保护数据安全的安全性。尽管PMem模块不如DRAM模块快，但以 成本/GB 为基础，相比于 DRAM 能够提高上限，TCO 得到了极大的改善。 与 Optane SSD 的区别 虽然 Intel® Optane™ 持久化存储器(PMem)和Intel®Optane™ssd使用相同的Intel®Optane™存储介质，但它们是非常不同的产品。 Intel® Optane™ 持久性内存是一个 DIMM package，在 DRAM 总线上运行，可以是易失的或持久的，并作为DRAM的替代品。 Intel® Optane™ SSD，严格用于快速存储，驻留在标准的 NAND Package 模型中(AIC, M.2, U.2, EDSFF，…)，驻留在PCIe总线上，使用NVMe*协议，由于持久化存储的特性，作为一个快速存储替代品。 Intel® Optane™ SSD 不同于标准 Intel® 3D NAND SSD; 所有队列深度大小情况下都有突破性的性能(比NAND快6倍) 队列深度较小时可能有一些差异 持续载荷响应(写压力下 63 x比NAND快) 高服务质量(60 x比NAND) 非常高的耐力寿命(20 x比NAND总字节写)。 与 NVDIMM 的区别 NVDIMM(非易失性双内联内存模块)是一种混合内存，在宕机期间保留数据。NVDIMMs 通常将非易失性NAND闪存与DRAM和单个内存子系统上的专用备份电源集成在一起。虽然它提供了持久性，但它通常是一个更昂贵的系统，因为你需要和DRAM模块一样多的NAND存储以及额外的电池备份子系统。当发生断电时，保存和恢复状态也会有延迟。此外，如果电池没有维护或坏了，就失去了备份能力。 使用 Intel® Optane™ 持久内存，你有即时的持久性，不需要额外的组件，不担心更换电池，不需要额外的存储，所以你的成本更低，而且在恢复上没有性能损失。 Optane PM 优势 持久性——即使在关机时也能存储数据 大容量，低于DRAM价格——随着更大的持久内存容量的出现，更大的数据集可以存在更靠近CPU的更快的处理，这意味着更大的洞察力。更高容量的Intel® Optane™持久存储器创造了一个更实惠的解决方案，加速了整个行业向着IMDB的趋势。在第二代Intel Xeon可升级处理器上交付的大容量工作负载将为快速数据处理带来显著的性能提升。 运作模式——英特尔®Optane™持久内存有两种操作模式:内存模式(内存模式非常适合大内存容量和不需要更改应用程序的内存，是易失的)和 AppDirect 模式(软件和应用程序能够直接和英特尔®Optane™持久内存交互,但是需要应用程序更改)。凭借独特的操作模式，客户可以灵活地利用英特尔®Optane™的持久内存优势，跨越多种工作负载。 硬件加密——是更安全的。加密密钥存储在模块的安全元数据区域，只能由Intel®Optane™持久内存控制器访问。如果重新使用或丢弃模块，则使用安全的加密擦除和DIMM覆盖来防止数据被访问 Persistent memory as Ceph cache in Openstack Ceph librbd librbd 是 Ceph 提供的用户态的块设备访问库 librbd 包含了一些内存缓存 RAM Cache，存在一些问题 基于对象 故障造成数据丢失 故障后可能造成数据不一致 实现的新的 write-back cache，基于 Persistent Memory （基于 librabd 的 ImageCache 接口实现） 基于 LBA 能够保证崩溃一致性 Writeback cache in Ceph 使用 PMem 作为 Cache 是因为 Pmem 相比之下对于小写更友好，缓存对于元数据的 IO 操作的作用是比较大的 PMDK 支持事务 可以使用 RDMA+PMDK 较好地实现 PMem 的数据复制 对块设备的写请求当持久化到了 Optane 上时就可以结束了，所以可以大大缩小延迟 可以保证写操作的数据最终会被 flush 到 RADOS 层 Replicated write back cache (RWL) RWL 使用持久内存为 RBD 存储回写缓存。RWL 可以通过 RDMA 将日志复制到另一个节点的持久内存中。 RWL 使用 libpmemobj(来自PMDK，请参阅http://pmem.io) 管理 PMDK 池中的缓存数据。RWL 在已配置的目录中查找并创建池，该目录应该位于持久内存支持的支持 DAX 的文件系统上。任何后备设备都可以使用，但是除了 pmem 之外的其他存储设备的性能将很差。RWL 复制是通过 PMDK 池复制来完成的，PMDK 池复制通过 RDMA 将一个本地池镜像为一个或多个远程池(因此写入副本不会占用任何CPU周期)。 RWL 将每个写操作附加到一个日志中，这个日志在 pmem 中是一个小结构环。每次写操作的数据都保存在 pmem 中单独分配的缓冲区中。许多线程的写操作可以在绑定它们附加到日志的顺序之前独立地分配和持久化它们的有效负载数据。为了原子性，写日志条目及其关联的数据缓冲区分配被持久化到一个 pmem 事务中。 RWL延迟与任何正在进行的写操作重叠的写操作，以防止对同一位置的写操作出现冲突。它从不延迟读操作，一旦写操作的附加顺序确定，那么读操作就会触及该日志条目。 RWL提供了两种持久性保证模式： persist-on-write：RWL 只有在所有日志副本中持久化后才能完成写入，并且在崩溃后仍然可读。如果启用，则在看到第一个rbd_aio_flush()时进入该模式 persist-on-flush mode：当 RWL 不再需要调用者的数据缓冲区来完成写操作时，它就会完成写操作，但不能保证在崩溃后写操作是可读的。 测试 测试环境 测试结果 PMDK Crimson Ceph 新的 OSD 实现，为了支持 PMem 和快速的 nvme storage Seastar Seastar 是一个应用框架，它几乎将操作系统所提供的抽象完整地搬移到了用户态中，以减少操作系统的抽象开销，实现软硬件一体化。从总体架构而言，Seastar是一个完全分片（share-nothing）的设计：每个logic core 一个thread，每个core有自己的资源：CPU, network, disk I/O, memory。多个core之间没有资源的竞争，随着core数量的增加，扩展性和性能也随之提升。 Seastar 将每个核抽象成一台单核计算机，每个单核计算机上运行着许多执行流，一个单核计算机上的多个执行流可以共享数据，不同单核计算机上的执行流只能通过消息来共享数据。在每个用户态CPU上运行一个调度器，来调度一系列的微任务。 Share-nothing的用户态执行流的抽象降低了切换开销以及同步开销，然而，同一进程内，内存是共享的，分配与释放内存时，依然会有同步的存在。为了避免此问题，Seastar在应用启动时，将整个虚拟地址空间按照CPU核数等分为若干块，每个CPU使用自己的内存块进行内存分配与释放，从而避免同步。 Seastar 是一个异步框架，任何一个核阻塞都会造成核上的待调度的微任务严重超时。然而，令人无奈的是，传统文件系统操作是同步阻塞的。好在AIO的存在解决了这一问题（虽然现在AIO还是一堆坑）。AIO有一些固有的限制，它必须以O_DIRECT方式打开文件，导致不能使用pagecache以及读写必须对齐。为了解决AIO的问题，Seastar维护用户态PageCache，从而实现了Zero copy的文件操作。并且，它维护自己的IO调度策略，从而更好地使用磁盘。 Seastar 支持多种形式的网络操作，一是传统的epoll方式，这种方式已经非常成熟，并且在业内有广泛应用。另一种是用户态网络栈+DPDK，从而实现Zero copy与Zero switch的网络操作，进一步提高了网络的性能。 Seastar in Crimson 线程模型发生了变化 Seastore BlueStore 的基础上使用 Seastar 除此以外还需要支持新型存储设备。 ZNS &amp; PMem PMem 在 Seastore 中主要用于元数据的存储和日志数据的存储 参考文献 [1] Intel - optane-dc-persistent-memory. [2] SDC2018 - Using Persistent Memory and RDMA for Ceph Client Write-back Caching [3] OpenStack - Persistent memory as Ceph cache in Openstack [4] Vault20 - Crimson: A New Ceph OSD for the Age of Persistent Memory and Fast NVMe Storage [5] Vault20 - Introduction to Client-side Caching in Ceph [6] Zhihu - 用户态操作系统之一 Seastar简介 ","link":"https://blog.shunzi.tech/post/Optane-Memory-UPS/"},{"title":"OC SSD & ZNS","content":" 项目新技术预研。OpenChannel SSD 和 ZNS SSD 主要是资料整理，并考虑应用此类硬件技术到 Ceph. OC SSD &amp; ZNS Prepare NVM Express™ Specification: NVM Express™(NVMe™) 基础规范最初是为 SSD 设计的，用于帮助定义主机软件如何通过PCI Express®(PCIe®)总线与非易失性内存通信。它已经迅速发展成为各种形式的PCIe固态硬盘(SSD)的工业标准(U.2、M.2、AIC、EDSFF)。NVMe 基本规范提供了一个有效的接口，提供了更低的延迟，比串行ATA (SATA)等传统接口更适合 SSD。https://nvmexpress.org/developers/nvme-specification/ 此规范目的在于充分利用PCI-E通道的低延时以及并行性，还有当代处理器、平台与应用的并行性，在可控制的存储成本下，极大的提升固态硬盘的读写性能，降低由于AHCI接口带来的高延时，彻底解放SATA时代固态硬盘的极致性能。NVMe是为SSD而生。在此之前SSD都用SATA接口。SATA接口采用AHCI规范，其已经成为制约SSD速度的瓶颈。AHCI只有1个命令队列，队列深度32；而NVMe可以有65535个队列，每个队列都可以深达65536个命令。NVMe也充分使用了MSI的2048个中断向量优势，延迟大大减小。 Zoned Block Commands (ZBC)：定义了SMR盘相关的术语和行为，成为SAS接口SMR盘所必须遵守的规范。 Zoned-device ATA Commands (ZAC)：与ZBC类似，是SATA接口SMR盘所必须遵守的标准。 参考文献 [1] 博客园：NVMe概述 [2] 杂说闪存一：关公战秦琼之 UFS VS NVMe Open-Channel SSD 为什么使用 OC SSD？: 以 Ceph 为例。 传统的 FileStore 是基于本地文件系统设计的，在索引结构、空间管理、垃圾回收以及日志等方面，FileStore 的不同层级之间其实是有功能的冗余的，这种冗余带来的影响就是较高的软件开销。 BlueStore 则 bypass 了文件系统，直接在用户空间分配和回收 SSD 的数据块，使用 RocksDB 来进行元数据的管理从而保证一致性，RocksDB 在定制的 BlueFS 上运行，相比于 FileStore，简化的 IO 路径带来了显著的性能提升。但在 BlueStore 和 FTL 中还是有功能冗余，导致两部分彼此隔离，而在实际进行决策的时候可能产生冲突。SSD 的性能和持久性还是未得到充分的利用。 Object Store Architecture in Ceph 普通 NVMe SSD Open-Channel SSD Open-Channel将本来位于NVMe SSD上Firmware中的对Flash管理和控制的部分功能，交给了Host上的应用软件。让应用根据自身的业务特点，对盘上的Flash进行有效的管理。很多情况下，Host上应用的管理，可以避免在垃圾回收等各类对前端应用IO请求的影响。 Open-Channel 向 Host 展示出内部 NAND 布局的细节，Host 可以决定数据实际存放的物理位置。这样，Host 就可以根据 IO 请求的发起方，将 IO 数据写到不同的位置，实现不同应用、用户数据的物理隔离，达到更好的 QOS 效果，如图所示。 但就从实际应用的部署情况来看，Open-Channel的使用者需要实现一个复杂的FTL(Flash Translation Layer), 替代SSD中本已健壮成熟的Firmware层实现的功能，来管理NAND flash和数据存放。而且Open-Channel Specification 仅仅定义了Open-Channel涉及的最为通用的部分。不同厂商的SSD产品特性不同，它们或者难以统一，或者涉及敏感内容，不便公开，实际Open-Channel产品往往在兼容Open-Channel Spec的基础上，各有一部分私有定义。不同业务方的需求独特，往往需要在自己的FTL内加入定制化的内容。因此，至今尚未有通用的Open-Channel SSD和针对独特业务的通用FTL。这些制约严重影响了Open-Channel的发展速度。 Ecosystem LightNVM &amp; PBLK 为了方便的管理和操作 Open-Channel SSD，LightNVM 应运而生。LightNVM 是在 Linux Kernel 中一个针对 Open-Channel SSD 的 Subsystem。LightNVM 提供了一套新的接口，用于管理 Open-Channel SSD，以及执行 IO 操作。为了和 Kernel 中现有的 IO 子系统协同工作，还存在 pblk（Physical Block Device）层。他在 LightNVM 的基础上实现了 FTL 的功能，同时对上层暴露传统的 Block 层接口，使得现有的文件系统可以通过 pblk 直接运行在 Open-Channel SSD 上。 目前 LightNVM 已经被合并入 Kernel 的主线。而对于用户态的程序来说，可以通过 liblightnvm 操作 Open-Channel SSD。 PPA Addressing Physical Page Addressing (PPA) Interface 物理页寻址。 通过这个接口暴露SSD的几何结构 逻辑/物理几何结构（取决于供应商） 性能 特定介质的元数据（需要的情况下） 控制器功能 展示分层的地址空间 编码并行的几何结构单元（PUs）到地址空间 向量 I/Os 读/写/擦除（有效访问给定的新的地址空间） NVMe Device drivers 检测open-channel SSD 并实现 PPA 接口 启用 LightNVM 的 NVMe 设备驱动程序使内核模块可以通过 PPA I/O 接口访问 open-channel SSD。设备驱动程序将设备作为传统的 Linux 设备公开给用户空间，从而允许应用程序通过 ioctl（专用于设备输入输出操作的系统调用） 与设备进行交互。 PBLK 处理控制器和特定介质之间的约束 （例如，缓存必要的数据量来对Flash页进行编程） 将逻辑地址映射到物理地址（4KB粒度），并确保完整性，最终在面对关联映射表（L2P）崩溃时进行恢复 处理错误和垃圾回收 处理flush操作：因为典型的闪存页面大于4KB，flush会强制pblk的运行中数据在完成之前存储在设备上 Write Buffering 数据缓冲区，用于存储4KB用户数据入口（4KB对应于一个扇区的大小） 上下文缓冲区，用于存储 Per entry 元数据。 缓冲区的大小是闪存页大小（FPSZ），要写入的闪存页数（下/上页）和PU数（N）的乘积。如果FPSZ = 64KB，PP = 8，N = 128，则写缓冲区为64MB。多生产者-单消费者模型 Solution QBLK 多队列缓冲 每个通道的地址管理 无锁地址映射 细粒度的刷回 OCStore 基于 OC SSD（华为定制的 OC SSD，32 flash channels，每个 block 包含 512 个 pages，每个页大小为 4KB） 和 PFTL 构建的对象存储，直接在原始闪存上管理对象，减少了对象存储、文件系统和FTL层的冗余功能。提供了流事务更新，这不仅确保了利用非覆盖flash写入特性的多页原子性，而且还为独立的I/O流提供隔离，同时支持对不同通道的并行访问。OCStore 还协调不同的通道来支持事务感知调度，从而减少事务级别的延迟，并为分布式存储提供较低的响应时间。显著降低了延迟和写流量。 Zoned Namespace OC SSDs 由于缺乏官方标准，一些供应商引入了不同的实现 OpenChannel SSD 接口的方法，导致实现支离破碎。为了防止这种情况发生，主要的供应商联合起来引入了一个新的 NVMe 标准，称为 Zoned Namespace (ZNS)，该标准定义了在不使用 FTL 情况下管理 SSD 的接口。 Concepts Zoned Namespace（ZNS）是NVMe工作组为增加对定制化需求提供灵活性，而计划加入NVMe协议的新特性。 Zoned Namespace中的zone名称，源自于为SMR硬盘(Shingled Magnetic Recording, 叠瓦式磁记录)所做的设计。相关标准化组织曾制定了ZBC(Zoned Block Commands，分区块命令集)与ZAC(Zoned-device ATA Commands， 区设备ATA指令集)，来对SCSI和ATA协议进行拓展。所以zone所涉及的状态转换与软件栈，都与之前的SMR硬盘相符。 相对于正常的 NVMe Namespace, Zoned Namespace将一个Namespace的逻辑地址空间切分成一个个的zone。如图所示，zone是Namespace内的一种固定大小的子区间。每个zone都有一段LBA(Logical Block Address, 逻辑地址空间)区间，这段区间只能顺序写，而且如果要覆盖写，则必须显示的进行一次擦除操作。这样，namespace就可以把NAND内部结构的边界透露给外界。NVMe SSD也就能够将地址映射表等内部管理工作交由host去处理，从而减少写放大、选择合适的GC(Garbage Collection, 垃圾回收)时机。 States And Operations Zone的基本操作有Read, Append Write，Zone Management 以及Get Log Page。Zone大小可以采用Open-Channel中Chunk的大小为单位，即与NAND介质的物理边界为单位。Zone Log Page也会与Open-Channel Spec 2.0中的Chunk Info Log相似。 与Open-Channel相比，最大的区别就是在Zoned Namespace中，Zone的地址是LBA（Logical Block Address, 逻辑块地址），Zone X的起始地址与Zone X-1的结束地址相连，Zone X的结束地址与Zone X+1的起始地址相连。Zone的容量总是小于等于Zone的逻辑大小。这样一来，Zone Namespace就可以避免Open-Channel里繁琐的各类地址转换。 Zone 状态： Emptyzone 内无有效数据 Explicitly Openedzone 被 Management 命令显式的打开 Implicitly Openedzone 处在打开状态，但并非是由 Management 命令打开的 Closedzone 被 Management 命令显式的关闭 Fullzone 处于写满状态 Offlinezone 无法被读写 一个zone只有在Open状态下才可以写入数据；擦除可以使写满的zone回退到Empty状态；在zone的内部Nand介质达到磨损极限后处于Offline状态。它的正常工作状态变化过程为：Empty → Open → Full → Empty ZM（Zone Management，zone管理）command 有 ZM Open, ZM Close, ZM Reset, ZM Finish 等，可以控制zone的状态变化。例如，在正常工作过程中，zone的状态变化需要ZM命令的参与：Empty → &lt;ZM Open&gt; → Explicitly Opened → &lt;Append Write Operations&gt;/&lt;ZM finish&gt; → Full → &lt;ZM Reset&gt; → Empty ZNS SSD 限制 顺序写约束: 写操作需要按照顺序进行，就像SMR驱动器一样。 主机需要直接控制 Zones，如 Zones 打开、关闭、重置和 Zones 垃圾收集。 空间利用率较高的时候垃圾收集开销很大，随着利用率增加，垃圾收集的开销增加尤为显著 分组访问比单独访问快得多，因为每个 Zone 可能被映射到多个通道，可以利用内部并行性适当地增大 IO 大小来提升性能。 Ecosystem 业界也早已为SMR硬盘的推广，添加了很多软件生态。如图所示， 例如在Linux体系中， 有用户态函数库libzbc；文件系统中加入针对SMR的优化；SCSI/ATA驱动增加对SMR相关指令集的支持。 Linux内核中块设备有对zone支持的zoned block device模型，通用设备映射器（Device Mapper）中加入的dm-zoned device mapper, 可以将一个zoned block device映射为不受追加写限制的通用块设备。开启此类映射需要用到两个工具命令，一个是Device Mapper的工具dmsetup，另一个是zoned block device 专用工具dmzadm (dm-zoned admin). # dmzadm 格式化 zoned block device /dev/sdb dmzadm –format /dev/sdb # 创建通用块设备 -&gt; /dev/mapper/dmz-sdb dmsetup create dmz-sdb 文件系统诸如F2FS、BTRFS中已经有了对zoned block device的直接支持，所以它们可以直接操作zoned block device，绕过DM-zoned。如果一个NVMe SSD的一个Namespace是Zoned Namespace时，F2FS和BTRFS可以直接配置使用它，而无需额外的开发工作。 SPDK zonedev layer，它为FTL提供所有所需的操作集，从而屏蔽底层Open-Channel, Zoned Namespace, 以及其他各类第三方的非标准Spec的差异 OC/Zonedev adapter: 考虑到Zoned Namespace将会被NVMe Spec接纳，zonedev layer 会以Zoned Namespace的定义为标准。Open-Channel设备则需再加入一个OC/Zonedev adapter去将Open-Channel的操作封装适配到zonedev layer中。 参考链接 [1] Zoned Namespace: NVMe Spec对标Open-Channel的解决方案（上篇） [2] Zoned Namespace_NVMe Spec对标Open-Channel的解决方案（下篇） [3] DBLP: q=Open-Channel [4] A New LSM-style Garbage Collection Scheme for ZNS SSDs [5] Open-Channel SSD Read The Docs [6] ICDCS19 - Lu Youyou: OCStore: Accelerating Distributed Object Storage with Open-Channel SSDs [7] Souhu: 获NVMe Express认可，ZNS标准化又进了一步 [8] ATC16 - Jiacheng Zhang: ParaFS: A Log-Structured File System to Exploit the Internal Parallelism of Flash Devices [9] FAST17 - LightNVM: The Linux Open-Channel SSD Subsystem [10] DATE2019 - QBLK: Towards Fully Exploiting the Parallelism of Open-Channel SSDs [11] ZONED STORAGE [12] 阿里云 - 【阿里云总监课】存储系统设计——NVMe SSD性能影响因素一探究竟 [13] HotStoreage20 - A New LSM-style Garbage Collection Scheme for ZNS SSDs ","link":"https://blog.shunzi.tech/post/OC-SSD-AND-ZNS/"},{"title":"Series Four of Basic of Virtualization - Memory Virtualization - Basic And Segmentation","content":" 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtualization（虚拟化）。本篇为虚拟化技术的基础篇系列第三篇（Memory Virtualization），内存虚拟化。 内存虚拟化又将分为两篇，本篇为第一篇内存虚拟化之基础和分段 Chapter Index TODO Dialogue Every address generated by a user program is a virtual address. The Abstraction: Address Spaces Early Systems 早期的系统未提供太多内存的抽象给用户，物理内存的分布如图所示。操作系统曾经是一组函数（实际上是一个库），在内存中（在本例中，从物理地址 0 开始），然后有一个正在运行的程序（进程），目前在物理内存中（在本例中，从物理地址 64KB 开始），并使用剩余的内存。 Multiprogramming and Time Sharing 多道程序（multiprogramming）：多个进程在给定时间准备运行，比如当有一个进程在等待 I/O 操作的时候，操作系统会切换这些进程，这样增加了 CPU 的有效利用率（utilization）。 时分共享（Time Sharing）：因为厌倦了长时间的（因此也是低效率的）编程—调试循环。交互性（interactivity）变得很重要，因为许多用户可能同时在使用机器，每个人都在等待（或希望）他们执行的任务及时响应。 一种实现时分共享的方法，是让一个进程单独占用全部内存运行一小段时间，如早期的系统一样，然后停止它，并将它所有的状态信息保存在磁盘上（包含所有的物理内存），加载其他进程的状态信息，再运行一段时间，这就实现了某种比较粗糙的机器共享。但该方法的问题在于太慢了，主要因为需要将内存信息保存到磁盘，磁盘速度较慢，所以考虑在进程切换时的进程信息仍然保存在内存中可以高效地实现时分共享。 如图所示，三个进程 ABC，每个进程拥有从 512KB 物理内存中切出来给它们的一小部分内存。假定只有一个 CPU，操作系统选择运行其中一个进程（比如 A），同时其他进程（B 和 C）则在队列中等待运行。 将进程信息都保存在内存中也就引入了新的问题，内存区域之间需要进行隔离或保护，防止其他进程修改了当前进程的内存区域。 The Address Space 操作系统为了解决上述问题，对物理内存提供一层抽象，也就是地址空间（Address Space），即运行的程序看到的系统的内存。 一个进程的地址空间包含运行的程序的所有内存状态。 程序的代码（code，指令）必须在内存中，因此它们在地址空间里。 当程序在运行的时候，利用栈（stack）来保存当前的函数调用信息，分配空间给局部变量，传递参数和函数返回值。 堆（heap）用于管理动态分配的、用户管理的内存，就像你从 C 语言中调用 malloc()或面向对象语言（如 C ++ 或 Java）中调用 new 获得内存。 还有其他的区域（例如，静态初始化的变量），但现在假设只有这 3 个部分：代码、栈和堆。 如下图所示例子中，有一个 16KB 的地址空间，顶部 0-1KB 为代码对应的分段，因为指令在运行过程中所占用的空间不会发生变化，即静态的，所以分配一个固定大小的空间即可。但是堆和栈是动态的，堆在顶部，栈在底部，同时向中间部分增长，当用户动态分配内存的时候，堆从 1KB 开始向下增长，当用户进行程序调用的时候，栈从 16KB 向上增长。 需要注意的是，我们描述地址空间时，所描述的是操作系统提供给运行程序的抽象（abstract）。程序不在物理地址 0～16KB 的内存中，而是加载在任意的物理地址。CRUX：操作系统如何在单一的物理内存上为多个运行的进程（所有进程共享内存）构建一个私有的、可能很大的地址空间的抽象？ Goals 虚拟内存（VM）系统的一个主要目标是透明（transparency） 操作系统实现虚拟内存的方式，应该让运行的程序看不见。因此，程序不应该感知到内存被虚拟化的事实，相 反，程序的行为就好像它拥有自己的私有物理内存。在幕后，操作系统（和硬件）完成了所有的工作，让不同的工作复用内存，从而实现这个假象。 虚拟内存的另一个目标是效率（efficiency）。操作系统应该追求虚拟化尽可能高效（efficient），包括时间上（即不会使程序运行得更慢）和空间上（即不需要太多额外的内存来支持虚拟化）。在实现高效率虚拟化时，操作系统将不得不依靠硬件支持，包括 TLB 这样的硬件功能。 虚拟内存第三个目标是保护（protection）。操作系统应确保进程受到保护（protect），不会受其他进程影响，操作系统本身也不会受进程影响。当一个进程执行加载、存储或指令提取时，它不应该以任何方式访问或影响任何其他进程或操作系统本身的内存内容（即在它的地址空间之外的任何内容）。因此，保护让我们能够在进程之间提供隔离（isolation）的特性，每个进程都应该在自己的独立环境中运行，避免其他出错或恶意进程的影响。 Summary 虚拟内存系统负责为程序提供一个巨大的、稀疏的、私有的地址空间的假象，其中保存了程序的所有指令和数据。操作系统在专门硬件的帮助下，通过每一个虚拟内存的索引，将其转换为物理地址，物理内存根据获得的物理地址但获取所需的信息。操作系统会同时对许多进程执行此操作，并且确保程序之间互相不会受到影响，也不会影响操作系统。整个方法需要大量的机制（很多底层机制）和一些关键的策略。 Interlude: Memory API Types of Memory 栈 Stack 内存，它的申请和释放操作是编译器来隐式管理的，所以有时也称为自动（automatic）内存。通过在函数中声明即可，剩余事情由编译器完成，进入函数时开辟栈上的内存空间，退出函数时，释放对应的内存。但是对于某些函数调用之外的内存，则需要借助堆。 void func() { int x; // declares an integer on the stack ... } 堆（heap）内存，其中所有的申请和释放操作都由程序员显式地完成。如下代码所示展示了如何在堆上分配一个整数，得到指向它的指针。关于这一小段代码有两点说明。首先，你可能会注意到栈和堆的分配都发生在这一行：首先编译器看到指针的声明（int * x）时，知道为一个整型指针分配空间，随后，当程序调用 malloc()时，它会在堆上请求整数的空间，函数返回这样一个整数的地址（成功时，失败时则返回 NULL），然后将其存储在栈中以供程序使用。 void func() { int *x = (int *) malloc(sizeof(int)); ... } The malloc() AND free() Call malloc 函数非常简单：传入要申请的堆空间的大小，它成功就返回一个指向新申请空间的指针，失败就返回 NULL #include &lt;stdlib.h&gt; ... void *malloc(size_t size); 要释放不再使用的堆内存，程序员只需调用 free()，该函数接受一个参数，即一个由 malloc()返回的指针。 int *x = malloc(10 * sizeof(int)); ... free(x); Common Errors 很多高级语言支持自动内存管理，即采用 new 分配内存，不用手动显式释放，会有专门的 GC 来进行垃圾回收。 忘记分配内存 segmentation fault char *src = &quot;hello&quot;; char *dst; // oops! unallocated strcpy(dst, src); // segfault and die 没有分配足够的内存 buffer overflow char *src = &quot;hello&quot;; char *dst = (char *) malloc(strlen(src)); // too small! strcpy(dst, src); // work properly 忘记初始化分配的内存 uninitialized read 忘记释放内存 memory leak 在用完之前释放内存 dangling pointer 反复释放内存 double free Underlying OS Support malloc() free() 都是库调用，malloc 库管理虚拟地址空间内的空间，但是它本身是建立在一些系统调用之上的，这些系统调用会进入操作系统，来请求本多内存或者将一些内容释放回系统。 一个这样的系统调用叫作 brk，它被用来改变程序分断（break）的位置：堆结束的位置。它需要一个参数（新分断的地址），从而根据新分断是大于还是小于当前分断，来增加或减小堆的大小。另一个调用 sbrk 要求传入一个增量，但目的是类似的。 你还可以通过 mmap()调用从操作系统获取内存。通过传入正确的参数，mmap() 可以在程序中创建一个匿名（anonymous）内存区域——这个区域不与任何特定文件相关联，而是与交换空间（swap space）相关联 Mechanism: Address Translation CRUX：如何实现高效的内存虚拟化？如何提供应用程序所需的灵活性？如何保持控制应用程序可访问的内存位置，从而确保应用程序的内存访问受到合理的限制？如何高效地实现这一切？ 我们利用了一种通用技术，有时被称为基于硬件的地址转换（hardware-based address translation），简称为地址转换（address translation）。它可以看成是受限直接执行这种一般方法的补充。利用地址转换，硬件对每次内存访问进行处理（即指令获取、数据读取或写入），将指令中的虚拟（virtual）地址转换为数据实际存储的物理（physical）地址。因此，在每次内存引用时，硬件都会进行地址转换，将应用程序的内存引用重定位到内存中实际的位置。 仅仅依靠硬件不足以实现虚拟内存，因为它只是提供了底层机制来提高效率。操作系统必须在关键的位置介入，设置好硬件，以便完成正确的地址转换。因此它必须管理内存（manage memory），记录被占用和空闲的内存位置，并明智而谨慎地介入，保持对内存使用的控制。 An Example 假设用户的地址空间必须连续地放在物理内存中。同时，为了简单，我们假设地址空间不是很大，具体来说，小于物理内存的大小。最后，假设每个地址空间的大小完全一样。别担心这些假设听起来不切实际，我们会逐步地放宽这些假设，从而得到现实的内存虚拟化。 void func() { int x; x = x + 3; // this is the line of code we are interested in x86 下的汇编如下：假定 x 的地址已经存入寄存器 ebx，之后通过 movl 指令将这个地址的值加载到通用寄存器 eax（长字移动）。下一条指令对 eax 的内容加 3。最后一条指令将 eax 中的值写回到内存的同一位置。 128: movl 0x0(%ebx), %eax ;load 0+ebx into eax 132: addl $0x03, %eax ;add 3 to eax register 135: movl %eax, 0x0(%ebx) ;store eax back to mem 代码和数据都位于进程的地址空间，3 条指令序列位于地址 128（靠近头部的代码段），变量 x 的值位于地址 15KB（在靠近底部的栈中）。 如果这 3 条指令执行，从进程的角度来看，发生了以下几次内存访问： 从地址 128 获取指令； 执行指令（从地址 15KB 加载数据）； 从地址 132 获取命令； 执行命令（没有内存访问）； 从地址 135 获取指令； 执行指令（新值存入地址 15KB）。 从程序的角度来看，它的地址空间（address space）从 0 开始到 16KB 结束。它包含的所有内存引用都应该在这个范围内。然而，对虚拟内存来说，操作系统希望将这个进程地址空间放在物理内存的其他位置，并不一定从地址 0 开始。因此我们遇到了如下问题：怎样在内存中重定位这个进程，同时对该进程透明（transparent）？怎么样提供一种虚拟地址空间从 0 开始的假象，而实际上地址空间位于另外某个物理地址？ 如图所示展示了一个例子，说明这个进程的地址空间被放入物理内存后可能的样子。从图 15.2 中可以看到，操作系统将第一块物理内存留给了自己，并将上述例子中的进程地址空间重定位到从 32KB 开始的物理内存地址。剩下的两块内存空闲（16～32KB 和 48～64KB）。 Dynamic (Hardware-based) Relocation 每个 CPU 需要两个硬件寄存器：基址（base）寄存器和界限（bound）寄存器，有时称为限制（limit）寄存器。 这组基址和界限寄存器，让我们能够将地址空间放在物理内存的任何位置，同时又能确保进程只能访问自己的地址空间。 采用这种方式，在编写和编译程序时假设地址空间从零开始。但是，当程序真正执行时，操作系统会决定其在物理内存中的实际加载地址，并将起始地址记录在基址寄存器中。 在上面的例子中，操作系统决定加载在物理地址 32KB 的进程，因此将基址寄存器设置为这个值。当进程运行时，该进程产生的所有内存引用，都会被处理器通过以下方式转换为物理地址： physical address = virtual address + base 进程中使用的内存引用都是虚拟地址（virtual address），硬件接下来将虚拟地址加上基址寄存器中的内容，得到物理地址（physical address），再发给内存系统。如上面例子中的第一条指令 128: movl 0x0(%ebx), %eax: 程序计数器（PC）首先被设置为 128。 当硬件需要获取这条指令时，它先将这个值加上基址寄存器中的 32KB(32768)，得到实际的物理地址 32896，然后硬件从这个物理地址获取指令。 接下来，处理器开始执行该指令。 这时，进程发起从虚拟地址 15KB 的加载，处理器同样将虚拟地址加上基址寄存器内容（32KB），得到最终的物理地址 47KB，从而获得需要的数据。 将虚拟地址转换为物理地址，这正是所谓的 地址转换（address translation）技术。也就是说，硬件取得进程认为它要访问的地址，将它转换成数据实际位于的物理地址。由于这种重定位是在 运行时 发生的，而且我们甚至可以在进程开始运行后改变其地址空间，这种技术一般被称为 动态重定位（dynamic relocation） 界限寄存器提供了访问保护。在上面的例子中，界限寄存器被置为 16KB。如果进程需要访问超过这个界限或者为负数的虚拟地址，CPU 将触发异常，进程最终可能被终止。界限寄存器的用处在于，它确保了进程产生的所有地址都在进程的地址“界限”中。 这种基址寄存器配合界限寄存器的硬件结构是芯片中的（每个 CPU 一对）。有时我们将CPU 的这个负责地址转换的部分统称为 内存管理单元（Memory Management Unit，MMU）。随着我们开发更复杂的内存管理技术，MMU 也将有更复杂的电路和功能。 Hardware Support: A Summary 首先，正如在 CPU 虚拟化的章节中提到的，我们需要两种 CPU 模式。 操作系统在特权模式（privileged mode，或内核模式，kernel mode），可以访问整个机器资源。 应用程序在用户模式（user mode）运行，只能做有限的操作。 只要一个位，也许保存在处理器状态字（processor status word）中，就能说明当前的 CPU 运行模式。在一些特殊的时刻（如系统调用、异常或中断），CPU 会切换状态。 硬件还必须提供基址和界限寄存器（base and bounds register），因此每个 CPU 的内存管理单元（Memory Management Unit，MMU）都需要这两个额外的寄存器。用户程序运行时，硬件会转换每个地址，即将用户程序产生的虚拟地址加上基址寄存器的内容。硬件也必须能检查地址是否有用，通过界限寄存器和 CPU 内的一些电路来实现。 硬件应该提供一些特殊的指令，用于修改基址寄存器和界限寄存器，允许操作系统在切换进程时改变它们。这些指令是 特权（privileged）指令，只有在 内核模式 下，才能修改这些寄存器。 最后，在用户程序尝试非法访问内存（越界访问）时，CPU必须能够产生异常（exception）。在这种情况下，CPU 应该阻止用户程序的执行，并安排操作系统的“越界”异常处理程序（exception handler）去处理。操作系统的处理程序会做出正确的响应，比如在这种情况下终止进程。类似地，如果用户程序尝试修改基址或者界限寄存器时，CPU 也应该产生异常，并调用“用户模式尝试执行特权指令”的异常处理程序。CPU 还必须提供一种方法，来通知它这些处理程序的位置，因此又需要另一些特权指令。 Operating System Issues 硬件支持和操作系统管理结合在一起，实现了一个简单的虚拟内存。具体来说，在一些关键的时刻操作系统需要介入，以实现基址和界限方式的虚拟内存 在进程创建时，操作系统必须采取行动，为进程的地址空间找到内存空间。由于我们假设每个进程的地址空间小于物理内存的大小，并且大小相同，这对操作系统来说很容易。它可以把整个物理内存看作一组槽块，标记了空闲或已用。当新进程创建时，操作系统检索这个数据结构（常被称为空闲列表，free list），为新地址空间找到位置，并将其标记为已用。 在图 15.2 中，操作系统将物理内存的第一个槽块分配给自己，然后将例子中的进程重定位到物理内存地址 32KB。另两个槽块（16～32KB，48～64KB）空闲，因此空闲列表（free list）就包含这两个槽块。 在进程终止时（正常退出，或因行为不端被强制终止），操作系统也必须做一些工作，回收它的所有内存，给其他进程或者操作系统使用。在进程终止时，操作系统会将这些内存放回到空闲列表，并根据需要清除相关的数据结构。 在上下文切换时，操作系统也必须执行一些额外的操作。每个 CPU 毕竟只有一个基址寄存器和一个界限寄存器，但对于每个运行的程序，它们的值都不同，因为每个程序被加载到内存中不同的物理地址。因此，在切换进程时，操作系统必须保存和恢复基础和界限寄存器。具体来说，当操作系统决定中止当前的运行进程时，它必须将当前基址和界限寄存器中的内容保存在内存中，放在某种每个进程都有的结构中，如进程结构（process structure）或进程控制块（Process Control Block，PCB）中。类似地，当操作系统恢复执行某个进程时（或第一次执行），也必须给基址和界限寄存器设置正确的值。 当进程停止时（即没有运行），操作系统可以改变其地址空间的物理位置，这很容易。要移动进程的地址空间，操作系统首先让进程停止运行，然后将地址空间拷贝到新位置，最后更新保存的基址寄存器（在进程结构中），指向新位置。当该进程恢复执行时，它的（新）基址寄存器会被恢复，它再次开始运行，显然它的指令和数据都在新的内存位置了。 操作系统必须提供异常处理程序（exception handler），或要一些调用的函数，像上面提到的那样。操作系统在启动时加载这些处理程序（通过特权命令）。例如，当一个进程试图越界访问内存时，CPU 会触发异常。在这种异常产生时，操作系统必须准备采取行动。 Summary 通过虚拟内存使用的一种特殊机制，即地址转换（address translation），扩展了受限直接访问的概念。利用地址转换，操作系统可以控制进程的所有内存访问，确保访问在地址空间的界限内。这个技术高效的关键是硬件支持，硬件快速地将所有内存访问操作中的虚拟地址（进程自己看到的内存位置）转换为物理地址（实际位置）。所有的这一切对进程来说都是透明的，进程并不知道自己使用的内存引用已经被重定位. 一种特殊的虚拟化方式，称为基址加界限的动态重定位。基址加界限的虚拟化方式非常高效，因为只需要很少的硬件逻辑，就可以将虚拟地址和基址寄存器加起来，并检查进程产生的地址没有越界。基址加界限也提供了保护，操作系统和硬件的协作，确保没有进程能够访问其地址空间之外的内容。保护肯定是操作系统最重要的目标之一。没有保护，操作系统不可能控制机器（如果进程可以随意修改内存，它们就可以轻松地做出可怕的事情，比如重写陷阱表并完全接管系统）。 遗憾的是，这个简单的动态重定位技术有 效率低下 的问题。重定位的进程使用了从 32KB 到 48KB 的物理内存，但由于该进程的栈区和堆区并不很大，导致这块内存区域中大量的空间被浪费。这种浪费通常称为 内部碎片（internal fragmentation），指的是已经分配的内存单元内部有未使用的空间（即碎片），造成了浪费。 在我们当前的方式中，即使有足够的物理内存容纳更多进程，但我们目前要求将地址空间放在固定大小的槽块中，因此会出现内部碎片。所以，我们需要更复杂的机制，以便更好地利用物理内存，避免内部碎片。第一次尝试是将基址加界限的概念稍稍泛化，得到分段（segmentation）的概念. Segmentation 如果我们将整个地址空间放入物理内存，那么栈和堆之间的空间并没有被进程使用，却依然占用了实际的物理内存。因此，简单的通过基址寄存器和界限寄存器实现的虚拟内存很浪费。另外，如果剩余物理内存无法提供连续区域来放置完整的地址空间，进程便无法运行。这种基址加界限的方式看来并不像我们期望的那样灵活。 CRUX：怎样支持大地址空间？ Segmentation: Generalized Base/Bounds 在 MMU 中引入不止一个基址和界限寄存器对，而是给地址空间内的每个逻辑段（segment）一对。一个段只是地址空间里的一个连续定长的区域，在典型的地址空间里有 3 个逻辑不同的段：代码、栈和堆。分段的机制使得操作系统能够将不同的段放到不同的物理内存区域，从而避免了虚拟地址空间中的未使用部分占用物理内存。 如图所示，64KB 的物理内存除去操作系统占用的 16KB 内存外可以划分成三个段，给每个段都分配一对基址和界限寄存器。 下表记录了上图例中的寄存器值，每个界限寄存器记录了一个段的大小。 地址转换举例： 假设现在要引用虚拟地址 100（在代码段中），MMU 将基址值加上偏移量（100）得到实际的物理地址：100 + 32KB = 32868。然后它会检查该地址是否在界限内（100 小于 2KB），发现是的，于是发起对物理地址 32868 的引用。 一个堆中的地址，虚拟地址 4200。（参考图16.1） 如果用虚拟地址 4200 加上堆的基址（34KB），得到物理地址 39016，这不是正确的地址。我们首先应该先减去地址空间中堆的偏移量，即该地址指的是这个段中的哪个字节。因为堆从虚拟地址 4K（4096）开始，4200 的偏移量实际上是 4200 减去 4096，即 104，然后用这个偏移量（104）加上基址寄存器中的物理地址（34KB），得到真正的物理地址 34920。 如果我们试图访问非法的地址，例如 7KB，超出了堆的边界呢。硬件会发现该地址越界，因此陷入操作系统，很可能导致终止出错进程。从而产生段异常（segmentation violation）或段错误（segmentation fault）。 Which Segment Are We Referring To? 硬件在地址转换时使用段寄存器。它如何知道段内的偏移量，以及地址引用了哪个段？ Explicit 一种常见的方式，有时称为显式（explicit）方式，就是用虚拟地址的开头几位来标识不同的段，VAX/VMS 系统使用了这种技术。上述例子中有三个段，相应的需要两位就可以表示了。如果我们用 14 位虚拟地址的前两位来标识，那么前两位即为对应的段号，后12位则为偏移量。上述的 4200 （堆段+104）就标识为 01 0000 0110 1000 00 代码段 01 堆段 11 栈段（因为起始地址为 00 0000 0000 0000，地址空间如图所示 0-2KB 为代码段，二进制表示前四位为 0000-0010，4KB 为堆的起始地址，二进制表示前四位为 0100） 偏移量简化了对段边界的判断。我们只要检查偏移量是否小于界限，大于界限的为非法地址。因此，如果基址和界限放在数组中（每个段一项），为了获得需要的物理地址，硬件会做下面这样的事：在上述 14 位虚拟地址的例子中，对应 SEG_MASK 即为 0x3000，SEG_SHIFT 为 12，OFFSET_MASK 为 0xFFF。 1 // get top 2 bits of 14-bit VA 2 Segment = (VirtualAddress &amp; SEG_MASK) &gt;&gt; SEG_SHIFT 3 // now get offset 4 Offset = VirtualAddress &amp; OFFSET_MASK 5 if (Offset &gt;= Bounds[Segment]) 6 RaiseException(PROTECTION_FAULT) 7 else 8 PhysAddr = Base[Segment] + Offset 9 Register = AccessMemory(PhysAddr) 由于用了两位来表示段号，但其实是可以表述四个段的，浪费了一个表示，所以有些系统会将堆和栈放在一个段中，因此就只需要一位来表示。 Implict 隐式（implicit）方式中，硬件通过地址产生的方式来确定段。例如，如果地址由程序计数器产生（即它是指令获取），那么地址在代码段。如果基于栈或基址指针，它一定在栈段。其他地址则在堆段。 What About The Stack? 上述例子中未涉及到栈的地址转换。在表 16.3 中，栈被重定位到物理地址 28KB。但有一点关键区别，它 反向增长。物理内存中，它始于 28KB，增长回到 26KB，相应虚拟地址从 16KB 到 14KB。因此地址转换方式和其他段会有所不同。 所以对于段的硬件支持除了基地址寄存器和界限寄存器以外，还需要知道段地址的增长方向，可以用一位来表示。更新后的寄存器结果如下：1 表示正向增长（从小到大），0 表示反向增长。 假设要访问虚拟地址 15KB，根据图 16.1 知道这是个栈地址，以及根据物理内存的对应关系不难发现物理地址为 27KB。虚拟地址的二进制形式为 11 1100 0000 0000，前两位表示为 stack 段，偏移量为 3KB，但此时非正确偏移，只是二进制地址表示中的地址偏移。为了得到正确的反向偏移，我们必须从 3KB 中减去最大的段地址：在这个例子中，段可以是 4KB，因此正确的偏移量是 3KB 减去 4KB，即−1KB。只要用这个反向偏移量（−1KB）加上基址（28KB），就得到了正确的物理地址 27KB。用户可以进行界限检查，确保反向偏移量的绝对值小于段的大小。 Support for Sharing 随着分段机制的不断改进，系统设计人员很快意识到，通过再多一点的硬件支持，就能实现新的效率提升。具体来说，要节省内存，有时候在地址空间之间共享（share）某些内存段是有用的。尤其是，代码共享很常见，今天的系统仍然在使用。 为了支持共享，需要一些额外的硬件支持，这就是保护位（protection bit）。基本为每个段增加了几个位，标识程序是否能够读写该段，或执行其中的代码。通过将代码段标记为只读，同样的代码可以被多个进程共享，而不用担心破坏隔离。虽然每个进程都认为自己独占这块内存，但操作系统秘密地共享了内存，进程不能修改这些内存，所以假象得以保持。 如下表所示。代码段的权限是可读和可执行，因此物理内存中的一个段可以映射到多个虚拟地址空间。有了保护位，前面描述的硬件算法也必须改变。除了检查虚拟地址是否越界，硬件还需要检查特定访问是否允许。如果用户进程试图写入只读段，或从非执行段执行指令，硬件会触发异常，让操作系统来处理出错进程。 Fine-grained vs. Coarse-grained Segmentation 到目前为止，我们的例子大多针对只有很少的几个段的系统（即代码、栈、堆）。我们可以认为这种分段是粗粒度的（coarse-grained），因为它将地址空间分成较大的、粗粒度的块。但是，一些早期系统更灵活，允许将地址空间划分为大量较小的段，这被称为细粒度（fine-grained）分段。 支持许多段需要进一步的硬件支持，并在内存中保存某种段表（segment table）。这种段表通常支持创建非常多的段，因此系统使用段的方式，可以比之前讨论的方式更灵活。例如，像 Burroughs B5000 这样的早期机器可以支持成千上万的段，有了操作系统和硬件的支持，编译器可以将代码段和数据段划分为许多不同的部分。当时的考虑是，通过更细粒度的段，操作系统可以更好地了解哪些段在使用哪些没有，从而可以更高效地利用内存。 OS Support 系统运行时，地址空间中的不同段被重定位到物理内存中。与我们之前介绍的整个地址空间只有一个基址/界限寄存器对的方式相比，大量节省了物理内存。具体来说，栈和堆之间没有使用的区域就不需要再分配物理内存，让我们能将更多地址空间放进物理内存。 分段也带来了一些新的问题： 操作系统在上下文切换时应该做什么？各个段寄存器中的内容必须保存和恢复。显然，每个进程都有自己独立的虚拟地址空间，操作系统必须在进程运行前，确保这些寄存器被正确地赋值。 管理物理内存的空闲空间。新的地址空间被创建时，操作系统需要在物理内存中为它的段找到空间。之前，我们假设所有的地址空间大小相同，物理内存可以被认为是一些槽块，进程可以放进去。现在，每个进程都有一些段，每个段的大小也可能不同。一般会遇到的问题是，物理内存很快充满了许多空闲空间的小洞，因而很难分配给新的段，或扩大已有的段。这种问题被称为外部碎片 在这个例子中，一个进程需要分配一个 20KB 的段。当前有 24KB 空闲，但并不连续（是3 个不相邻的块）。因此，操作系统无法满足这个 20KB 的请求。该问题的一种解决方案是紧凑（compact）物理内存，重新安排原有的段。例如，操作系统先终止运行的进程，将它们的数据复制到连续的内存区域中去，改变它们的段寄存器中的值，指向新的物理地址，从而得到了足够大的连续空闲空间。这样做，操作系统能让新的内存分配请求成功。但是，内存紧凑成本很高，因为拷贝段是内存密集型的，一般会占用大量的处理器时间。 一种更简单的做法是利用空闲列表管理算法，试图保留大的内存块用于分配。相关的算法可能有成百上千种，包括传统的最优匹配（best-fit，从空闲链表中找最接近需要分配空间的空闲块返回）、最坏匹配（worst-fit）、首次匹配（first-fit）以及像伙伴算法（buddy algorithm）这样更复杂的算法。 无论算法多么精妙，都无法完全消除外部碎片，因此，好的算法只是试图减小它。 Summary 分段解决了一些问题，帮助我们实现了更高效的虚拟内存。不只是动态重定位，通过避免地址空间的逻辑段之间的大量潜在的内存浪费，分段能更好地支持稀疏地址空间。它还很快，因为分段要求的算法很容易，很适合硬件完成，地址转换的开销极小。分段还有一个附加的好处：代码共享。如果代码放在独立的段中，这样的段就可能被多个运行的程序共享。 但我们已经知道，在内存中分配不同大小的段会导致一些问题，我们希望克服。首先，是我们上面讨论的外部碎片。由于段的大小不同，空闲内存被割裂成各种奇怪的大小，因此满足内存分配请求可能会很难。用户可以尝试采用聪明的算法，或定期紧凑内存，但问题很根本，难以避免。 第二个问题也许更重要，分段还是不足以支持更一般化的稀疏地址空间。例如，如果有一个很大但是稀疏的堆，都在一个逻辑段中，整个堆仍然必须完整地加载到内存中。换言之，如果使用地址空间的方式不能很好地匹配底层分段的设计目标，分段就不能很好地工作。因此我们需要找到新的解决方案。 Free-Space Management 如果要管理的空闲空间由大小不同的单元构成，管理就变得困难（而且有趣）。这种情况出现在用户级的内存分配库（如 malloc()和 free()），或者操作系统用分段（segmentation）的方式实现虚拟内存。在这两种情况下，出现了外部碎片（external fragmentation）的问题：空闲空间被分割成不同大小的小块，成为碎片，后续的请求可能失败，因为没有一块足够大的连续空闲空间，即使这时总的空闲空间超出了请求的大小。 CRUX：要满足变长的分配请求，应该如何管理空闲空间？什么策略可以让碎片最小化？不同方法的时间和空间开销如何？ Assumptions 假定基本的接口就像 malloc()和 free()提供的那样。具体来说，void * malloc(size tsize)需要一个参数 size，它是应用程序请求的字节数。函数返回一个指针（没有具体的类型，在 C 语言的术语中是 void 类型），指向这样大小（或较大一点）的一块空间。对应的函数 void free(void *ptr)函数接受一个指针，释放对应的内存块。请注意该接口的隐含意义，在释放空间时，用户不需告知库这块空间的大小。因此，在只传入一个指针的情况下，库必须能够弄清楚这块内存的大小。 进一步假设，我们主要关心的是外部碎片（external fragmentation），如上所述。当然，分配程序也可能有内部碎片（internal fragmentation）的问题。如果分配程序给出的内存块超出请求的大小，在这种块中超出请求的空间（因此而未使用）就被认为是内部碎片（因为浪费发生在已分配单元的内部），这是另一种形式的空间浪费。但是，简单起见，这里主要讨论外部碎片。 还假设，内存一旦被分配给客户，就不可以被重定位到其他位置。例如，一个程 序调用 malloc()，并获得一个指向堆中一块空间的指针，这块区域就“属于”这个程序了，库不再能够移动，直到程序调用相应的 free()函数将它归还。因此，不可能进行压缩（compaction）空闲空间的操作，从而减少碎片。但是，操作系统层在实现分段（segmentation）时，却可以通过压缩来减少碎片。 最后我们假设，分配程序所管理的是连续的一块字节区域。在一些情况下，分配程序可以要求这块区域增长。例如，一个用户级的内存分配库在空间快用完时，可以向内核申请增加堆空间（通过 sbrk 这样的系统调用），但是，简单起见，我们假设这块区域在整个生命周期内大小固定。 Low-level Mechanisms Splitting and Coalescing 空闲列表包含一组元素，记录了堆中的哪些空间还没有分配。假设有下面的 30 字节的堆： 这个堆对应的空闲列表会有两个元素，一个描述第一个 10 字节的空闲区域（字节0～9），一个描述另一个空闲区域（字节 20～29）： 任何大于 10 字节的分配请求都会失败（返回 NULL），因为没有足够的连续可用空间。而恰好 10 字节的需求可以由两个空闲块中的任何一个满足。但是，如果申请小于 10 字节空间，分配程序会执行所谓的 **分割（splitting）**动作：它找到一块可以满足请求的空闲空间，将其分割，第一块返回给用户，第二块留在空闲列表中。 假设分配 1 字节，分配程序选择使用第二块空闲空间，对 malloc()的调用会返回 20（1 字节分配区域的地址），空闲列表会变成这样： 许多分配程序中因此也有一种机制，名为合并（coalescing）。还是看前面的例子（10字节的空闲空间，10 字节的已分配空间，和另外 10 字节的空闲空间）。 对于这个（小）堆，如果应用程序调用 free(10)，归还堆中间的空间，会发生什么？如果只是简单地将这块空闲空间加入空闲列表，不多想想，可能得到如下的结果： 问题出现了：尽管整个堆现在完全空闲，但它似乎被分割成了 3 个 10 字节的区域。这时，如果用户请求 20 字节的空间，简单遍历空闲列表会找不到这样的空闲块，因此返回失败。 为了避免这个问题，分配程序会在释放一块内存时合并可用空间。想法很简单：在归还一块空闲内存时，仔细查看要归还的内存块的地址以及邻它的空闲空间块。如果新归还的空间与一个原有空闲块相邻（或两个，就像这个例子），就将它们合并为一个较大的空闲块。通过合并，最后空闲列表应该像这样： Tracking The Size Of Allocated Regions free(void *ptr)接口没有块大小的参数。因此它是假定，对于给定的指针，内存分配库可以很快确定要释放空间的大小，从而将它放回空闲列表。 要完成这个任务，大多数分配程序都会在头块（header）中保存一点额外的信息，它在内存中，通常就在返回的内存块之前。如下图所示的例子中，检查一个 20 字节的已分配块，由 ptr 指着，设想用户调用了 malloc()，并将结果保存在ptr 中：ptr = malloc(20)。 该 head 块中至少包含所分配空间的大小（这个例子中是 20）。它也可能包含一些额外的指针来加速空间释放，包含一个幻数来提供完整性检查，以及其他信息。我们假定，一个简单的 head 块包含了分配空间的大小和一个幻数： typedef struct header_t { int size; int magic; } header_t; 用户调用 free(ptr)时，库会通过简单的指针运算得到头块的位置： void free(void *ptr) { header_t *hptr = (void *)ptr - sizeof(header_t); } 获得 head 块的指针后，库可以很容易地确定幻数是否符合预期的值，作为正常性检查（assert（hptr-&gt;magic == 1234567）），并简单计算要释放的空间大小即 head 块的大小加区域长度）。实际释放的是 head 块大小加上分配给用户的空间的大小。因此，如果用户请求 N 字节的内存，库不是寻找大小为 N 的空闲块，而是寻找 N 加上 head 块大小的空闲块。 Embedding A Free List 这个简单的空闲列表还只是一个概念上的存在，它就是一个列表，描述了堆中的空闲内存块。但如何在空闲内存自己内部建立这样一个列表呢？ 假设我们需要管理一个 4096 字节的内存块（即堆是 4KB）。为了将它作为一个空闲空间列表来管理，首先要初始化这个列表。开始，列表中只有一个条目，记录了大小为 4096 的空间（减去 head 块的大小）。下面是该列表中一个节点描述： typedef struct node_t { int size; struct node_t *next; } node_t; 现在来看一些代码，它们初始化堆，并将空闲列表的第一个元素放在该空间中。假设堆构建在某块空闲空间上，这块空间通过系统调用 mmap()获得。这不是构建这种堆的唯一选择，但在这个例子中很合适。下面是代码： // mmap() returns a pointer to a chunk of free space node_t *head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0); head-&gt;size = 4096 - sizeof(node_t); head-&gt;next = NULL; 执行这段代码之后，列表的状态是它只有一个条目，记录大小为 4088。head 指针指向这块区域的起始地址，假设是 16KB（尽管任何虚拟地址都可以）。现在，假设有一个 100 字节的内存请求。为了满足这个请求，库首先要找到一个足够大小的块。因为只有一个 4088 字节的块，所以选中这个块。然后，这个块被分割（split）为两块：一块足够满足请求（以及头块，如前所述），一块是剩余的空闲块。假设记录 head 块为 8 个字节（一个整数记录大小，一个整数记录幻数），堆中的空间如图所示。 对于 100 字节的请求，库从原有的一个空闲块中分配了 108 字节，返回指向它的一个指针（在上图中用 ptr 表示），并在其之前连续的 8 字节中记录头块信息，供未来的free()函数使用。同时将列表中的空闲节点缩小为 3980 字节（4088−108）。 现在再来看该堆，其中有 3 个已分配区域，每个 100（加上头块是 108）。这个堆如图所示。可以看出，堆的前 324 字节已经分配，因此我们看到该空间中有 3 个头块，以及 3 个 100 字节的用户使用空间。空闲列表只有一个节点（由 head 指向），但在 3 次分割后，现在大小只有 3764 字节。 但如果用户程序通过 free()归还一些内存，在这个例子中，应用程序调用 free(16500)，归还了中间的一块已分配空间（内存块的起始地址 16384 加上前一块的 108，和这一块的头块的 8 字节，就得到了 16500）。这个值在前图中用 sptr 指向。库马上弄清楚了这块要释放空间的大小，并将空闲块加回空闲列表。假设我们将它插入到空闲列表的头位置，该空间如图所示。 现在的空闲列表包括一个小空闲块（100 字节，由列表的头指向）和一个大空闲块（3764字节）。 现在假设剩余的两块已分配的空间也被释放。没有合并，空闲列表将非常破碎，如图 17.7 所示。我们忘了合并（coalesce）列表项，虽然整个内存空间是空闲的，但却被分成了小段，因此形成了碎片化的内存空间。解决方案很简单：遍历列表，合并（merge）相邻块。完成之后，堆又成了一个整体。 Growing The Heap 如果堆中的内存空间耗尽，应该怎么办？最简单的方式就是返回失败。在某些情况下这也是唯一的选择，因此返回 NULL 也是一种体面的方式。 大多数传统的分配程序会从很小的堆开始，当空间耗尽时，再向操作系统申请更大的空间。通常，这意味着它们进行了某种系统调用（例如，大多数 UNIX 系统中的 sbrk），让堆增长。操作系统在执行 sbrk 系统调用时，会找到空闲的物理内存页，将它们映射到请求进程的地址空间中去，并返回新的堆的末尾地址。这时，就有了更大的堆，请求就可以成功满足。 Basic Strategies 理想的分配程序可以同时保证快速和碎片最小化。遗憾的是，由于分配及释放的请求序列是任意的（毕竟，它们由用户程序决定），任何特定的策略在某组不匹配的输入下都会变得非常差。所以我们不会描述“最好”的策略，而是介绍一些基本的选择，并讨论它们的优缺点。 Best Fit 最优匹配（best fit）策略非常简单：首先遍历整个空闲列表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块。这就是所谓的最优匹配（也可以称为最小匹配）。只需要遍历一次空闲列表，就足以找到正确的块并返回。 最优匹配背后的想法很简单：选择最接它用户请求大小的块，从而尽量避免空间浪费。然而，这有代价。简单的实现在遍历查找正确的空闲块时，要付出较高的性能代价。 Worst Fit 最差匹配（worst fit）方法与最优匹配相反，它尝试找最大的空闲块，分割并满足用户需求后，将剩余的块（很大）加入空闲列表。最差匹配尝试在空闲列表中保留较大的块，而不是向最优匹配那样可能剩下很多难以利用的小块。但是，最差匹配同样需要遍历整个空闲列表。更糟糕的是，大多数研究表明它的表现非常差，导致过量的碎片，同时还有很高的开销。 First Fit 首次匹配（first fit）策略就是找到第一个足够大的块，将请求的空间返回给用户。同样，剩余的空闲空间留给后续请求。 首次匹配有速度优势（不需要遍历所有空闲块），但有时会让空闲列表开头的部分有很多小块。因此，分配程序如何管理空闲列表的顺序就变得很重要。一种方式是基于地址排序（address-based ordering）。通过保持空闲块按内存地址有序，合并操作会很容易，从而减少了内存碎片。 Next Fit 不同于首次匹配每次都从列表的开始查找，下次匹配（next fit）算法多维护一个指针，指向上一次查找结束的位置。其想法是将对空闲空间的查找操作扩散到整个列表中去，避免对列表开头频繁的分割。这种策略的性能与首次匹配很接近，同样避免了遍历查找。 Examples 设想一个空闲列表包含 3 个元素，长度依次为 10、30、20（我们暂时忽略头块和其他细节，只关注策略的操作方式）： 假设有一个 15 字节的内存请求。最优匹配会遍历整个空闲列表，发现 20 字节是最优匹配，因为它是满足请求的最小空闲块。结果空闲列表变为：本例中发生的情况，在最优匹配中常常发生，现在留下了一个小空闲块。 最差匹配类似，但会选择最大的空闲块进行分割，在本例中是 30。结果空闲列表变为：在这个例子中，首次匹配会和最差匹配一样，也发现满足请求的第一个空闲块。不同的是查找开销，最优匹配和最差匹配都需要遍历整个列表，而首次匹配只找到第一个满足需求的块即可，因此减少了查找开销。 Other Approaches Segregated Lists 一直以来有一种很有趣的方式叫作分离空闲列表（segregated list）。基本想法很简单：如果某个应用程序经常申请一种（或几种）大小的内存空间，那就用一个独立的列表，只管理这样大小的对象。其他大小的请求都一给更通用的内存分配程序。 这种方法的好处显而易见。通过拿出一部分内存专门满足某种大小的请求，碎片就不再是问题了。而且，由于没有复杂的列表查找过程，这种特定大小的内存分配和释放都很快。 但也引入了新的复杂性。例如，应该拿出多少内存来专门为某种大小的请求服务，而将剩余的用来满足一般请求？超级工程师 Jeff Bonwick 为 Solaris 系统内核设计的厚块分配程序（slab allocator），很优雅地处理了这个问题 具体来说，在内核启动时，它为可能频繁请求的内核对象创建一些对象缓存（object cache），如锁和文件系统 inode 等。这些的对象缓存每个分离了特定大小的空闲列表，因此能够很快地响应内存请求和释放。如果某个缓存中的空闲空间快耗尽时，它就向通用内存分配程序申请一些内存 slab（总量是页大小和对象大小的公倍数）。相反，如果给定 slab 中对象的引用计数变为 0，通用的内存分配程序可以从专门的分配程序中回收这些空间，这通常发生在虚拟内存系统需要更多的空间的时候。 slab 分配程序比大多数分离空闲列表做得更多，它将列表中的空闲对象保持在预初始化的状态。Bonwick 指出，数据结构的初始化和销毁的开销很大。通过将空闲对象保持在初始化状态，slab 分配程序避免了频繁的初始化和销毁，从而显著降低了开销。 Buddy Allocation 因为合并对分配程序很关键，所以人们设计了一些方法，让合并变得简单，一个好例子就是二分伙伴分配程序（binary buddy allocator） 在这种系统中，空闲空间首先从概念上被看成大小为 2^N 的大空间。当有一个内存分配请求时，空闲空间被递归地一分为二，直到刚好可以满足请求的大小（再一分为二就无法满足）。这时，请求的块被返回给用户。在下面的例子中，一个 64KB 大小的空闲空间被切分，以便提供 7KB 的块： 在这个例子中，最左边的 8KB 块被分配给用户（如上图中深灰色部分所示）。请注意，这种分配策略只允许分配 2 的整数次幂大小的空闲块，因此会有内部碎片（internal fragment）的麻烦。 伙伴系统的漂亮之处在于块被释放时。如果将这个 8KB 的块归还给空闲列表，分配程序会检查“伙伴”8KB 是否空闲。如果是，就合二为一，变成 16KB 的块。然后会检查这个 16KB 块的伙伴是否空闲，如果是，就合并这两块。这个递归合并过程继续上溯，直到合并整个内存区域，或者某一个块的伙伴还没有被释放。 伙伴系统运转良好的原因，在于很容易确定某个块的伙伴。怎么找？仔细想想上面例子中的各个块的地址。如果你想得够仔细，就会发现每对互为伙伴的块只有一位不同，正是这一位决定了它们在整个伙伴树中的层次。（2n 和 2n+1，2n 对应的二进制表示最低位肯定为 0，2n+1 该位即为 1） Other Ideas 上面提到的众多方法都有一个重要的问题，缺乏可扩展性（scaling）。具体来说，就是查找列表可能很慢。因此，更先进的分配程序采用更复杂的数据结构来优化这个开销，牺牲简单性来换取性能。例子包括平衡二叉树、伸展树和偏序树。 考虑到现代操作系统通常会有多核，同时会运行多线程的程序（本书之后关于并发的章节将会详细介绍），因此人们这了许多工作，提升分配程序在多核系统上的表现。这只是人们为了优化内存分配程序，在长时间内提出的几千种想法中的两种。感兴趣的话可以深入阅读。或者阅读 glibc 分配程序的工作原理，你会更了解现实的情形。 Summary 在本章中，我们讨论了最基本的内存分配程序形式。这样的分配程序存在于所有地方，与你编写的每个 C 程序链接，也和管理其自身数据结构的内存的底层操作系统链接。与许多系统一样，在构建这样一个系统时需要这许多折中。对分配程序提供的确切工作负载了解得越多，就越能调整它以更好地处理这种工作负载。在现代计算机系统中，构建一个适用于各种工作负载、快速、空间高效、可扩展的分配程序仍然是一个持续的挑战。 ","link":"https://blog.shunzi.tech/post/basic-of-virtualization-four/"},{"title":"Series Three of Basic of Virtulization - Mechanism and Policy Part.2","content":" CPU 虚拟化被分为两篇，此篇是第二篇，Policy 今天阅读到项目代码时候，涉及到了 Python 协程的概念，由于才疏学浅一时半会儿未能参透协程的实际意义。查阅了很多资料，都是和线程、进程对照着一起阐述，理解起来实在有难度，有了比较浅薄的了解之后决定还是先继续了解进程这一块的知识。 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtulization（虚拟化）。本篇为虚拟化技术的基础篇系列第二篇（Mechanism and Policy），机制和策略。（结合第一篇给出的虚拟化两层实现） CPU 虚拟化被分为两篇，此篇是第二篇，Policy Chapter Index TODO Scheduling: Introduction CRUX：如何设计与开发调度策略？ Pre-Scheduling Workload Assumptions 对操作系统中运行的进程（有时也叫工作/任务）做出如下的假设： 每一个工作运行相同的时间 所有的工作同时到达 一旦开始，每个工作保持运行直到完成 所有的工作只是用 CPU（即它们不执行 IO 操作） 每个工作的运行时间是已知的 Scheduling Metrics turnaround time 周转时间：$$T_{turnaround}=T_{completion}-T_{arrival}$$ 因为我们假设了同时到达，Tarrival=0T_{arrival}=0Tarrival​=0，所以 Tturnaround=TcompletionT_{turnaround}=T_{completion}Tturnaround​=Tcompletion​ First In, First Out (FIFO) 先进先出，又称 First Come, First Served (FCFS) 优点：简单，易于实现；假设情况下，效果很好。 考虑两种情况： 假设 A B C 都需要运行 10s，但由于必须有一个先执行，周转时间即为 (10+20+30)/3 = 20s; A B C 执行时间不相等，A 100s，B 和 C 10s，周转时间变为（100 + 110 + 120）/ 3 = 110s 上述第二种情况对应的问题通常被称为 convoy effect，即一些耗时的任务排队在前造成了后面的任务等待时间较长。 Shortest Job First (SJF) 最短任务优先：耗时更短的先执行。 回到前面的例子，SJF 可以使得周转时间变为 (10+20+120)/3=50。 但此时如果考虑任务不是同时到达的，A 先到达，BC 在 10s 时刻到达，这时候的周转时间变为 (100+110-10+120-10)/3=103.33，即还是出现了上文提到的 convoy effect Shortest Time-to-Completion First (STCF) 为了实现该策略，需要放宽任务必须一直执行直到完成的限制条件。 SJF 策略本身是一种非抢占式的调度，如果我们向 SJF 中引入抢占的策略，也就是 STCF 或者叫 Preemptive Shortest Job First ，PSJF。每当新任务进入系统时，会确定剩余任务和新任务谁的时间消耗最少，然后执行调度。 在 SJF 的第二个例子中，周转时间将变为 (10+20+90+10+20)/3=50s，周转时间大幅降低 A New Metric: Response Time 如果我们知道任务长度，而且任务只使用 CPU，而我们唯一的衡量是周转时间，STCF 将是一个很好的策略。对于早期的批处理系统，这样的策略比较有效，但是分时系统的引入，产生了一个新的度量 响应时间（response time） 响应时间定义为从任务到达系统到首次运行的时间 $$T_{response}=T_{firstrun}-T_{arrival}$$ 回到最初的例子，假设 A 最先到达，B 和 C 第10s到达，那么对应的响应时间为0，0，10。平均响应时间3.33 STCF 和相关方法在响应时间上并不是很好。例如，如果 3 个工作同时到达，第三个工作必须等待前两个工作全部运行后才能运行。这种方法虽然有很好的周转时间，但对于响应时间和交互性是相当糟糕。 针对一些对响应时间特别敏感的任务就需要设计一种新的调度策略。 Round Robin Round Robin 轮转调度，RR 在一个时间片（time slice，有时称为调度量子，scheduling quantum）内运行一个工作，然后切换到运行队列中的下一个任务，而不是运行一个任务直到结束。它反复执行，直到所有任务完成。因此，RR 有时被称为时间切片（time-slicing）。时间片长度必须是时钟中断周期的倍数。 假设 ABC 同时到达，都希望运行 5s，SJF 调度程序必须运行完当前任务才可运行下一个任务，而 RR 可以快速地循环工作。平均响应时间分别为 SJF - 5，RR - 1。 时间片长度对于 RR 是至关重要的。越短，RR 在响应时间上表现越好。，时间片太短是有问题的：突然上下文切换的成本将影响整体性能。需要权衡时间片的长度，使其足够长，以便摊销（amortize）上下文切换成本，而又不会使系统不及时响应。 上下文切换的成本不仅仅来自保存和恢复少量寄存器的操作系统操作。程序运行时，它们在 CPU 高要缓存、TLB、分支预测器和其他片上硬件中建立了大量的状态。切换到另一个工作会导致此状态被刷新，且与当前运行的作业相关的新状态被引入，这可能导致显著的性能成本。 而对于周转时间，平均值变成了 14，这时候 RR 表现就很差。RR 所做的正是延伸每个工作，只运行每个工作一小段时间，就转向下一个工作。因为周转时间只关心作业何时完成，RR 几乎是最差的，在很多情况下甚至比简单的 FIFO 更差。 任何公平（fair）的政策（如 RR），即在小规模的时间内将 CPU 均匀分配到活动进程之间，在周转时间这类指标上表现不佳。事实上，这是固有的权衡：如果你愿意不公平，你可以运行较短的工作直到完成，但是要以响应时间为代价。如果你重视公平性，则响应时间会较短，但会以周转时间为代价。 第一种类型（SJF、STCF）优化周转时间，但对响应时间不利。第二种类型（RR）优化响应时间，但对周转时间不利。 Incorporating I/O 放宽假设 4：当然所有程序都执行 I/O。 调度程序显然要在工作发起 I/O 请求时做出决定，因为当前正在运行的作业在 I/O 期间不会使用 CPU，它被阻塞等待 I/O 完成。如果将 I/O 发送到硬盘驱动器，则进程可能会被阻塞几毫秒或更长时间，具体取决于驱动器当前的 I/O 负载。因此，这时调度程序应该在 CPU 上安排另一项工作。 调度程序还必须在 I/O 完成时做出决定。发生这种情况时，会产生中断，操作系统运行并将发出 I/O 的进程从阻塞状态移回就绪状态。当然，它甚至可以决定在那个时候运行该项工作。 假设任务 A 和 B，都需要 50ms CPU，但是 A 运行 10ms 后发起 I/O，假设 I/O 需要 10ms，而 B 不执行 I/O。先运行 A 再运行 B。STCF 将 A 分解成 5 个 10ms CPU 和 5 个 10ms I/Os，STCF 选择执行时间短的 A，执行完后执行 B，然后出现了 A 的新子任务抢占 B，从而重叠运行。一个进程在等待另一个进程的 I/O 完成时使用 CPU，系统因此得到更好的利用。 No More Oracle 放宽假设 4：不知道每个任务的执行时间。 实际上，在通用操作系统中(比如我们关心的那些)，操作系统通常对每个作业的长度知之甚少。因此，在没有这种先验知识的情况下，我们如何构建行为类似于SJF/STCF的方法呢?此外，如何将我们在RR调度器中看到的一些思想结合起来，使响应时间也相当好呢? Multi-level Feedback Queue Scheduling: The Multi-Level Feedback Queue 1962 年 Corbato 提出的多级反馈队列调度方法，应用到了兼容时分共享系统（CTSS），因此还获得了图灵奖。（mo大佬）多级反馈队列调度方法在这里提出，显然目标就是对上文提到的两个性能指标进行优化：周转时间和响应时间。SJF（或STCF）对周转时间比较友好，但需要知道任务所需时间，RR 对响应时间友好，但 RR 的粒度也很关键，同样需要任务所需时间来进行决策。所以引出了 CRUX：没有任务长度的先验知识，如何设计一个能同时减少响应时间和周转时间的调度程序？ 开始之前，简单构思一下解决办法，大概率就是经验主义。即通过参考已经发生了的任务的一些数据来对后面的任务进行预测，在生活中最常见的例子就是中医，而在计算机系统中这个其实也应用的很广泛，譬如分支预测以及一些缓存算法（经常访问的多半之后还是会经常访问） MLFQ: Basic Rules MLFQ 有很多实现，但核心思想大体相同。MLFQ 有很多独立的队列，每个队列又有相应的优先级（优先级队列），一个任务只能在一个队列中，更高优先级的队列中的任务首先被运行。而每个队列可能有多个任务，优先级相同，那么该队列里的任务则采用轮转的方式进行调度。简而言之：优先级队列 + 轮转 Rule 1：如果 A 的优先级 &gt; B 的优先级，运行 A（不运行 B）。 Rule 2：如果 A 的优先级 = B 的优先级，轮转运行 A 和 B 基本运行规则如上所述，但 MLFQ 的关键在于任务的优先级的设置，因为优先级决定了任务的分布。不会为每个任务设置固定的优先级，而是根据观察到的行为动态调整优先级。eg. 一个任务不断放弃 CPU 等待键盘 IO 输入，MLFQ 会给他高优先级，如果某个程序占用 CPU 时间特别长，则降低优先级。 Attempt #1: How To Change Priority 负载中既有运行时间很短、频繁放弃 CPU 的交互型工作，也有需要很多 CPU 时间、响应时间却不重要的长时间计算密集型工作。 Rule 3：任务进入系统时放置在最高优先级队列 Rule 4： a. 任务用完整个时间片后，降低其优先级，进入下一个队列 b. 如果任务在时间片内主动放弃 CPU，优先级不变 Example 1: A Single Long-Running Job 长时间任务不断被降级，直到最低优先级 Example 2: Along Came A Short Job 在实例 1 的基础上，T = 100 时来了一个短任务20ms。经过两个时间片，在被移入最低优先级队列之前，B 执行完毕。然后 A 继续运行（在低优先级）。 该算法的主要目标：假设一开始进入的任务是短任务，给予最高优先级，确实是短任务则很快执行完毕，否则被移动到更低优先就的队列中，也就被看成长任务了，通过该种方式使得 MLFQ 近似于 SJF Example 3: What About I/O? 灰色所示则为需要大量进行交互的操作，与长时间任务黑色表示竞争 CPU，MLFQ 保持需要大量交互的操作为最高优先级，因为总是主动让出 CPU，对应地则实现了对响应时间敏感的进程的优化。 Problems With Our Current MLFQ starvation 大量的交互型任务会造成长任务无法得到 CPU，也就是饿死。需要让 CPU 密集型进程也能一定程度上的运行 用户设计操纵进程，在每一个时间片结束之前执行 I/O 来主动释放 CPU，保证自己的高优先级，占用更多的 CPU 时间。 无法处理程序行为变化的情况。譬如计算密集型变成交互型，目前没有对优先级进行提升。 Attempt #2: The Priority Boost 为了避免饿死现象，可以考虑周期性地提升所有任务的优先级。 Rule 5：经过一段时间 S，就将系统中所有任务重新加入最高优先级队列。 该规则解决了饿死问题，使得提升优先级后的进程能够以轮转的方式共享 CPU；其次如果任务行为发生了变化，变成了IO密集型，优先级提升后也能得到正确的处理。 如下例子，长任务与两个交互型短任务竞争 CPU。左图无优先级提升，长任务在两个短任务进入之后饿死，右图每 50ms 进行提升，保证长任务能有一定的进展。但也就引入了新的问题，S 周期的设定。过长会导致饿死，过短导致交互型任务不能得到合适的 CPU 时间比例。也被称之为 voo-doo constants Attempt #3: Better Accounting 还有一个问题没解决，防止用户程序的故意行为欺骗。造成有机可乘的原因主要是 Rule 4，导致任务可以通过主动放弃 CPU 来保证自己的高优先级。 解决方案是为 MLFQ 的每层队列提供更完善的 CPU 计时方式（accounting）。即调度程序记录某一个进程在某一个队列中消耗的总时间，而不是每次调度之后重新计时，进程总时间配额用完之后进行降级。这样就能阻止用户进程肆无忌惮地花费时间片。改写规则 4 Rule 4：一旦工作用完了其在某一层中的时间配额（无论中间主动放弃了多少次 CPU），就降低其优先级（移入低一级队列） 如下例子对比了规则 4 修改前后的两种情况。没有规则 4 的保护时，进程可以在每个时间片结束前发起一次 I/O 操作，从而垄断 CPU 时间。有了这样的保护后，不论进程的 I/O 行为如何，都会慢慢地降低优先级，因而无法获得超过公平的 CPU 时间比例。 Tuning MLFQ And Other Issues MLFQ 调度算法涉及到了一些参数，这些参数的设置会影响在对应负载下的性能表现。主要有队列数量，队列时间片长度，提升优先级周期等。 大多数的 MLFQ 变体都支持不同队列可变的时间片长度。高优先级队列通常只有较短的时间片（比如 10ms 或者更少），因而这一层的交互工作可以更快地切换。相反，低优先级队列中更多的是 CPU 密集型工作，配置更长的时间片会取得更好的效果。如图所示 Solaris 的 MLFQ 实现（时分调度类 TS）很容易配置。它提供了一组表来决定进程在其生命周期中如何调整优先级，每层的时间片多大，以及多久提升一个工作的优先级。管理员可以通过这些表，让调度程序的行为方式不同。该表默认有 60 层队列，时间片长度从 20ms（最高优先级），到几百 ms（最低优先级），每一秒左右提升一次进程的优先级。 FreeBSD 调度程序（4.3 版本），会基于当前进程使用了多少 CPU，通过公式计算某个工作的当前优先级。另外，使用量会随时间衰减，这提供了期望的优先级提升，但与这里描述方式不同。 有些调度程序将最高优先级队列留给操作系统使用，因此通常的用户工作是无法得到系统的最高优先级的。有些系统 允许用户给出优先级设置的建议（advice），比如通过命令行工具 nice，可以增加或降低工作的优先级（稍微），从而增加或降低它在某个时刻运行的机会。 Summary MLFQ 有趣的原因是：它不需要对工作的运行方式有先验知识，而是通过观察工作的运行来给出对应的优先级。通过这种方式，MLFQ 可以同时满足各种工作的需求：对于短时间运行的交互型工作，获得类似于 SJF/STCF 的很好的全局性能，同时对长时间运行的CPU 密集型负载也可以公平地、不断地稳步向前。 Scheduling: Proportional Share 又称 fair-share，该调度程序的最终目标：确保每个工作获得一定比例的 CPU 时间，而不是优化周转时间和响应时间。有一个实际的例子 lottery scheduling：每隔一段时间，都会举行一次彩票抽奖，以确定接下来应该运行哪个进程。越是应该频繁运行的进程，越是应该拥有更多地赢得彩票的机会。 CRUX：如何设计调度程序来按比例分配 CPU？其关键的机制是什么？效率如何？ Basic Concept: Tickets Represent Your Share 一个进程拥有的彩票数占总彩票数的百分比，就是它占有资源的份额。 假设有两个进程 A 和 B，A 拥有 75 张彩票，B 拥有 25 张。因此我们希望 A 占用 75%的 CPU 时间，而 B 占用 25%。通过不断定时地（比如，每个时间片）抽取彩票，彩票调度从概率上（但不是确定的）获得这种份额比例。调度程序知道总共的彩票数（在我们的例子中，有 100 张）。调度程序抽取中奖彩票，这是从 0 和 99①之间的一个数，拥有这个数对应的彩票的进程中奖。假设进程 A 拥有 0 到 74 共 75 张彩票，进程 B 拥有 75 到 99 的 25 张，中奖的彩票就决定了运行 A 或 B。调度程序然后加载中奖进程的状态，并运行它。 彩票调度中利用了随机性，这导致了从概率上满足期望的比例，但并不能确保。随着任务运行的时间越长，得到的 CPU 时间就越来越接近期望。 Ticket Mechanisms ticket currency：允许拥有一组彩票的用户以他们喜欢的某种货币，将彩票分给自己的不同工作。之后操作系统再自动将这种货币兑换为正确的全局彩票。简而言之就是按比例分配 ticket transfer：通过转让，一个进程可以临时将自己的彩票交给另一个进程。这种机制在客户端/服务端交互的场景中尤其有用，在这种场景中，客户端进程向服务端发送消息，请求其按自己的需求执行工作，为了加速服务端的执行， 客户端可以将自己的彩票转让给服务端，从而尽可能加速服务端执行自己请求的速度。服务端执行结束后会将这部分彩票归还给客户端。 ticket inflation：利用通胀，一个进程可以临时提升或降低自己拥有的彩票数量。当然在竞争环境中，进程之间互相不信任，这种机制就没什么意义。一个贪婪的进程可能给自己非常多的彩票，从而接管机器。但是，通胀可以用于进 程之间相互信任的环境。在这种情况下，如果一个进程知道它需要更多 CPU 时间，就可以增加自己的彩票，从而将自己的需求告知操作系统，这一切不需要与任何其他进程通信。 Implementation 只需要一个不错的随机数生成器来选择中奖彩票和一个记录系统中所有进程的数据结构（一个列表），以及所有彩票的总数。 假设如图所示 ABC 三个进程，每个进程有一定数目的彩票。 首先要从彩票总数 400 中选择一个随机数（中奖号码），假设选了 300，然后遍历链表，来寻找对应的中奖者。 1 // counter: used to track if we’ve found the winner yet 2 int counter = 0; 3 4 // winner: use some call to a random number generator to 5 // get a value, between 0 and the total # of tickets 6 int winner = getrandom(0, totaltickets); 7 8 // current: use this to walk through the list of jobs 9 node_t *current = head; 10 while (current) { 11 counter = counter + current-&gt;tickets; 12 if (counter &gt; winner) 13 break; // found the winner 14 current = current-&gt;next; 15 } 16 // ’current’ is the winner: schedule it... 要让这个过程更有效率，建议将列表项按照彩票数递减排序。这个顺序并不会影响算法的正确性，但能保证用最小的迭代次数找到需要的节点，尤其当大多数彩票被少数进程掌握时。 An Example 两个互相竞争的任务，每个任务都有 100 张彩票，以及相同的运行时间 R。 这种情况下，我们希望两个工作在大约同时完成，但由于彩票调度算法的随机性，有时一个工作会先于另一个完成。我们定义不公平指标 U 来量化这种区别，两个任务完成时刻相除为 U。比如 A 任务 10s 完成，B 任务 20s 时完成，那么 U=10/20=0.5。同时完成则 U 接近于 1，也就是我们的目标。 用模拟器测试了运行时间变化的情况下，30 次试验的平均 U 值。不难发现任务短时，不公平现象比较严峻，只有时间较长的任务才能达到近似理想的效果。 How To Assign Tickets? 系统的运行严重依赖于彩票的分配。假设用户自己知道如何分配，因此可以给每个用户一定量的彩票，由用户按照需要自主分配给自己的工作。然而这种方案似乎什么也没有解决——还是没有给出具体的分配策略。因此对于给定的一组工作，彩票分配的问题依然没有最佳答案。 Why Not Deterministic? 虽然随机方式可以使得调度程序的实现简单（且大致正确），但偶尔并不能产生正确的比例，尤其在工作运行时间很短的情况下。Waldspurger 提出了步长调度（stride scheduling），一个确定性的公平分配算法 系统中的每个工作都有自己的步长，这个值与票数值成反比。在上面的例子中，A、B、C 这 3 个工作的票数分别是 100、50 和 250，我们通过用一个大数分别除以他们的票数来获得每个进程的步长。比如用 10000 除以这些票数值，得到了 3 个进程的步长分别为 100、200 和 40。我们称这个值为每个进程的步长（stride）。每次进程运行后，我们会让它的计数器 [称为行程（pass）值] 增加它的步长，记录它的总体进展。之后，调度程序使用进程的步长及行程值来确定调度哪个进程。基本思路很简单：当需要进行调度时，选择目前拥有最小行程值的进程，并且在运行之后将该进该进程的行程值增加一个步长。 current = remove_min(queue); // pick client with minimum pass schedule(current); // use resource for quantum current-&gt;pass += current-&gt;stride; // compute next pass using stride insert(queue, current); // put back into the queue 如下表所示展示了例子中的进程调度情况。C 运行了 5 次、A 运行了 2 次，B 一次，正好是票数的比例——200、100 和 50。彩票调度算法只能一段时间后，在概率上实现比例，而步长调度算法可以在每个调度周期后做到完全正确。 彩票调度算法的优势在于不需要全局状态。假如一个新的进程在上面的步长调度执行过程中加入系统，应该怎么设置它的行程值呢？设置成 0 吗？这样的话，它就独占 CPU 了。而彩票调度算法不需要对每个进程记录全局状态，只需要用新进程的票数更新全局的总票数就可以了。因此彩票调度算法能够更合理地处理新加入的进程。 The Linux Completely Fair Scheduler (CFS) Linux 使用了另外一种方法实现了 fair-share 的目标。Completely Fair Scheduler (or CFS)，以高效和可扩展的方式实现了 fair-share。 CFS 的目标是花费很少的时间来制定调度决策，这是通过其固有的设计和它对非常适合该任务的数据结构的巧妙使用实现的。因为近年来的研究表明调度效率是非常重要的， Google datacenters, Kanev et al 表明即使在主动优化之后，调度也会使用大约 5% 的数据中心 CPU 时间。因此，尽可能减少这种开销是现代调度器体系结构的一个关键目标。 Basic Operation 目标：公平地将 CPU 划分给进程进行竞争。它通过一种称为虚拟运行时(vruntime)的简单的基于计数的技术来实现。 当每个进程运行时，它会累积 vruntime。在最基本的情况下，每个进程的 vruntime 以相同的速度增长，与物理(实时)时间成比例。当一个调度决策发生时，CFS 将选择 vruntime 最低的进程下一个运行。此时引入了一个问题：调度器如何知道何时停止当前进程开启另一个进程？如果 CFS 切换的太频繁，公平性就会增加，因为需要确保每个进程都能获得它的 CPU 份额，即使是在极短的时间内，当然也就引入了大量切换导致的性能开销。如果 CFS 切换更少，性能会提高(减少上下文切换)，但会以近期的公平性为代价。 CFS 引入了一些参数来控制： sched_latency：在考虑切换之前，一个进程应该运行多长时间？(有效地确定它的时间片，但以动态方式)。通常设置为 48ms，CFS 用这个值除以 CPU 上运行的进程数(n)来确定进程的时间片，从而确保在这段时间内，CFS 是完全公平的。 eg. 4 个进程在运行，那么每个进程的时间片为 12ms，CFS 调度第一个任务并执行直到花光 12ms，这时候检查是否有更小的 vruntime，该例子中有那么执行切换，选择剩下三个中的一个，如下图所示。 图中四个任务都首先执行了两个时间片，然后 CD 结束了，AB 还未执行完，这时候时间片变为 24ms，然后 RR 轮转执行。 出现了新的问题，如果进程太多，那么时间片就会很小，就会造成频繁的上下文切换，性能就不太好 min_granularity：为了解决上述问题出现了该参数，通常设置为 6ms，也就是说 CFS 设置的时间片大小不会低于该值，确保不会花费大量时间在调度上。 eg. 10 个进程，时间片则为 4.8ms，但是因为设置了 min_granularity，那么时间片变成 6ms，尽管 CFS 在目标调度延迟(sched_latency)上不会(完全)公平，但也十分接近，仍然能实现较高的 CPU 效率。 请注意，CFS 使用了周期性的计时器中断，这意味着它只能在固定的时间间隔做出决策。这个中断经常发生(例如，每1毫秒)，给 CFS 一个机会来唤醒并确定当前的作业是否已经到达它运行的终点。如果一个任务的时间片不是计时器中断间隔的完美倍数，那也没关系;CFS 精确地跟踪 vruntime，这意味着从长期来看，它最终将接近理想的 CPU 共享。 Weighting (Niceness) CFS 还支持对进程优先级的控制，使用户或管理员可以为某些进程提供更高的 CPU 份额。通过 UNIX 中的一种 nice level of process 机制来实现的。对于进程，nice 参数可以在 -20 到 +19 之间任意设置，默认值为 0。正 nice 值表示低优先级，负 nice 值表示高优先级; CFS将每个进程的 nice 值映射到一个权重 static const int prio_to_weight[40] = { /* -20 */ 88761, 71755, 56483, 46273, 36291, /* -15 */ 29154, 23254, 18705, 14949, 11916, /* -10 */ 9548, 7620, 6100, 4904, 3906, /* -5 */ 3121, 2501, 1991, 1586, 1277, /* 0 */ 1024, 820, 655, 526, 423, /* 5 */ 335, 272, 215, 172, 137, /* 10 */ 110, 87, 70, 56, 45, /* 15 */ 36, 29, 23, 18, 15, }; 这些权重允许我们计算每个过程的有效时间片，但是现在引入了优先级，计算公式如下：$$timeSlice_{k}=\\frac{weight_k}{\\sum_{i=0}^{n-1}weight_k}* schedLatency$$ 假设两个任务 A 和 B，A 的优先级 nice 值为 -5，B 为默认优先级 nice 值为 0，那么 weightAweight_AweightA​ 即为 3121，weightBweight_BweightB​ 即为 1024。公式计算时间片，A 的时间片大约为 schedLatency 的四分之三，36ms，B 的时间片大约为四分之一，12ms。 除了推广时间片计算，CFS 计算 vruntime 的方式也必须调整。会考虑到进程的实际运行时间 runtimeiruntime_iruntimei​，与进程的权重成反比。在上述例子中，A 的 vruntime 的累积速度是 B 的三分之一。$$vruntime_i=vruntime_i+\\frac{weight_0}{weight_i}*runtime_i$$ 上面权重表构造的一个聪明之处在于，当 nice 值的差为常数时，该表保持了 CPU 比例比率。比如 A 的 nice 值为 5，B 的 nice 值为 10，因为对应的权重差值较小，CFS 会以与之前完全相同的方式安排它们。 Using Red-Black Trees CFS 调度算法重点关注效率，效率又涉及很多方面，而上文中描述的效率是指选择合适的进程来提升整体的效率。而对于如何快速地找到已经确定的下一个需要执行的任务的效率也是至关重要的。列表这样的简单数据结构是无法伸缩的，现代系统有时由1000个进程组成，因此每隔这么多毫秒就搜索一个长列表是很浪费的。 CFS 采用了红黑树来解决这个问题。相比于二叉树，红黑树是一个平衡树，平衡树在降低树的深度上做了一些额外的工作，从而保证操作的时间复杂度是对数级别的，而不是线性的（二叉树在最坏的情况下可能退化成链表） CFS 不会在数据结构中保留所有的进程，仅仅保留部分运行中的进程，如果进程即将处于休眠状态，譬如等待 I/O 完成或者等待网络包到达，这样的线程将从红黑树中移除。 假设现在有 10 个任务，对应的 vruntime 分别为 1，5，9，10，14，18，17，21，22，24。如果使用有序列表保存这些任务，那么找到下一个执行的任务就很简单，直接从列表中取出第一个即可。但是当将任务放回列表中的时候，就必须扫描 list 里，因为需要找到正确的位置进行插入，时间复杂度对应地变为 O(n)，在列表中任何搜索都是非常低效的，平均花费的时间也是线性的。 把对应的值保存在红黑树中可以让操作更为高效，如图所示，进程根据 vruntime 在树中也是有序的，大多数操作在时间复杂度上都为 O(log n)。当 n 特别大的时候，比如数以千计，红黑树就显得更为高效。 Dealing With I/O And Sleeping Processes 在选择最小的 vruntime 对应的进程时候存在另外一个问题：即任务已经休眠了很长一段时间。假设有任务 AB，其中一个(A)连续运行，另一个(B)已经休眠很长一段时间(比如，10秒)。当 B 被唤醒，它的 vruntime 会比 A 晚 10 秒（比 A 小 10s），因此(如果我们不小心的话)，B 现在会在接下来的 10 秒内垄断 CPU，在它赶上 A 之前，这实际上是在饿死 A。 CFS 通过在一个任务被唤醒时改变它的 vruntime 来处理这种情况，具体来说，CFS 将该任务的 vruntime 设置为树中找到的最小值(请记住，树只包含正在运行的任务)。通过这种方式，避免了饿死现象的发生，但也不是没有代价: 睡眠时间短的工作可能就经常不能公平地共享 CPU。 Summary 彩票调度通过随机值，聪明地做到了按比例分配。步长调度算法能够确定的获得需要的比例。虽然两者都很有趣，但由于一些原因，并没有作为 CPU 调度程序被广泛使用。一个原因是这两种方式都不能很好地适合 I/O，另一个原因是其中最难的票数分配问题并没有确定的解决方式，例如，如何知道浏览器进程应该拥有多少票数？通用调度程序（像前面讨论的 MLFQ 及其他类似的 Linux 调度程序）做得更好，因此得到了广泛的应用。 比例份额调度程序只有在这些问题可以相对容易解决的领域更有用（例如容易确定份额比例）。例如在虚拟（virtualized）数据中心中，你可能会希望分配 1/4 的 CPU 周期给 Windows 虚拟机，剩余的给 Linux 系统，比例分配的方式可以更简单高效。 CFS 是本章中讨论的唯一的“真正的”调度器，它有点像带有动态时间片的加权轮询，但它的构建是为了在负载下扩展和执行良好；据我们所知，它是目前使用最广泛的公平共享调度器。 没有调度策略是万能的，都有一些各自的问题。一个问题就是这些方法并不能很好地与 I/O 结合，如上所述，偶尔执行 I/O 的任务可能无法获得公平的 CPU 份额。另一个问题是，它们留下了票或优先级分配的难题，比如你的应用的 nice 值到底设置为多少比较合适？其他通用调度器(如前面讨论的 MLFQ 和其他类似的 Linux 调度器)自动处理这些问题，因此可能更容易部署。 Multiprocessor Scheduling (Advanced) 前面章节更多地都是在介绍单个 CPU 上的调度，而现实中常常是多 CPU 协同工作的，那么多 CPU 的调度又是什么样的呢？ Background: Multiprocessor Architecture 多 CPU 与单 CPU 的区别：核心在于对硬件缓存（cache）的使用，以及多处理器之间共享数据的方式。 在单 CPU 系统中，存在多级的硬件缓存（hardware cache），一般来说会让处理器更快地执行程序。缓存是很小但很快的存储设备，通常拥有内存中最热的数据的备份。相比之下，内存很大且拥有所有的数据，但访问速度较慢。通过将频繁访问的数据放在缓存中，系统似乎拥有又大又快的内存。 多 CPU 的情况下缓存要复杂得多。例如，假设一个运行在 CPU 1 上的程序从内存地址 A 读取数据。由于不在 CPU 1 的缓存中，所以系统直接访问内存，得到值 D。程序然后修改了地址 A 处的值，只是将它的缓存更新为新值 D'。将数据写回内存比较慢，因此系统（通常）会稍后再做。假设这时操作系统中断了该程序的运行，并将其交给 CPU 2， 重新读取地址 A 的数据，由于 CPU 2 的缓存中并没有该数据，所以会直接从内存中读取，得到了旧值 D，而不是正确的值 D'。这就是缓存一致性问题 硬件提供了这个问题的基本解决方案：通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。在基于总线的系统中，一种方式是使用总线窥探（bus snooping）。每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果 CPU 发现对它放在缓存中的数据的更新，会作废（invalidate）本地副本（从缓存中移除）， 或更新（update）它（修改为新值）。 Don’t Forget Synchronization 跨 CPU 访问（尤其是写入）共享数据或数据结构时，需要使用互斥原语（比如锁），能保证正确性（其他方法，如使用无锁（lock-free）数据结构，很复杂，偶尔才使用。例如，假设多 CPU 并发访问一个共享队列。如果没有锁，即使有底层一致性协议，并发地从队列增加或删除元素，依然不会得到预期结果。需要用锁来保证数据结构状态更新的原子性。 如下所示代码，用于删除共享链表的一个元素。假设两个 CPU 上的不同线程同时进入这个函数。如果线程 1 执行第一行，会将 head 的当前值存入它的 tmp 变量。如果线程 2 接着也执行第一行，它也会将同样的 head 值存入它自己的私有 tmp 变量（tmp 在栈上分配，因此每个线程都有自己的私有存储）。因此，两个线程会尝试删除同一个链表头，而不是每个线程移除一个元素，这导致了各种问题 1 typedef struct __Node_t { 2 int value; 3 struct __Node_t *next; 4 } Node_t; 5 6 int List_Pop() { 7 Node_t *tmp = head; // remember old head ... 8 int value = head-&gt;value; // ... and its value 9 head = head-&gt;next; // advance head to next pointer 10 free(tmp); // free old head 11 return value; // return value at head 12 } 让这类函数正确工作的方法是加锁（locking）。这里只需要一个互斥锁（即pthread_mutex_t m;），然后在函数开始时调用 lock(&amp;m)，在结束时调用 unlock(&amp;m)，确保代码的执行如预期。我们会看到，这里依然有问题，尤其是性能方面。具体来说，随着 CPU 数量的增加，访问同步共享的数据结构会变得很慢。 One Final Issue: Cache Affinity 在设计多处理器调度时遇到的最后一个问题，是所谓的缓存亲和度（cache affinity）。这个概念很简单：一个进程在某个 CPU 上运行时，会在该 CPU 的缓存中维护许多状态。下次该进程在相同 CPU 上运行时，由于缓存中的数据而执行得更快。相反，在不同的 CPU 上执行，会由于需要重新加载数据而很慢（好在硬件保证的缓存一致性可以保证正确执行）。因此多处理器调度应该考虑到这种缓存亲和性，并尽可能将进程保持在同一个 CPU 上。 Single-Queue Scheduling 最基本的方式是简单地复用单处理器调度的基本架构，将所有需要调度的工作放入一个单独的队列中，我们称之为单队列多处理器调度（Single Queue Multiprocessor Scheduling，SQMS）。这个方法最大的优点是简单。它不需要太多修改，就可以将原有的策略用于多个 CPU，选择最适合的工作来运行（例如，如果有两个 CPU，它可能选择两个最合适的工作）。 然而，SQMS 有几个明显的短板。第一个是缺乏可扩展性（scalability）。为了保证在多CPU 上正常运行，调度程序的开发者需要在代码中通过加锁（locking）来保证原子性，如上所述。在 SQMS 访问单个队列时（如寻找下一个运行的工作），锁确保得到正确的结果。锁可能带来巨大的性能损失，尤其是随着系统中的 CPU 数增加时。随着这种单个锁的争用增加，系统花费了越来越多的时间在锁的开销上，较少的时间用于系统应该完成的工作。 SQMS 的第二个主要问题是缓存亲和性。比如，假设我们有 5 个工作（A、B、C、D、E）和 4 个处理器。调度队列如下： 一段时间后，假设每个工作依次执行一个时间片，然后选择另一个工作，下面是每个CPU 可能的调度序列： 由于每个 CPU 都简单地从全局共享的队列中选取下一个工作执行，因此每个工作都不断在不同 CPU 之间转移，这与缓存亲和的目标背道而驰。为了解决这个问题，大多数 SQMS 调度程序都引入了一些亲和度机制，尽可能让进程在同一个 CPU 上运行。保持一些工作的亲和度的同时，可能需要牺牲其他工作的亲和度来实现负载均衡。例如，针对同样的 5 个工作的调度如下： 这种调度中，A、B、C、D 这 4 个工作都保持在同一个 CPU 上，只有工作 E 不断地来回迁移（migrating），从而尽可能多地获得缓存亲和度。为了公平起见，之后我们可以选择不同的工作来迁移。但实现这种策略可能很复杂。 SQMS 调度方式有优势也有不足。优势是能够从单 CPU 调度程序很简单地发展而来，根据定义，它只有一个队列。然而，它的扩展性不好（由于同步开销有限），并且不能很好地保证缓存亲和度。 Multi-Queue Scheduling 在 MQMS 中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则，比如轮转或其他任何可能的算法。当一个任务进入系统后，系统会依照一些启发性规则（如随机或选择较空的队列）将其放入某个调度队列。这样一来，每个 CPU 调度之间相互独立，就避免了单队列的方式中由于数据共享及同步带来的问题。 例如，假设系统中有两个 CPU（CPU 0 和 CPU 1）。这时一些工作进入系统：A、B、C和 D。由于每个 CPU 都有自己的调度队列，操作系统需要决定每个工作放入哪个队列。可能像下面这样做： 根据不同队列的调度策略，每个 CPU 从两个工作中选择，决定谁将运行。例如，利用轮转，调度结果可能如下所示： MQMS 比 SQMS 有明显的优势，它天生更具有可扩展性。队列的数量会随着 CPU 的增加而增加，因此锁和缓存争用的开销不是大问题。此外，MQMS 天生具有良好的缓存亲和度。所有工作都保持在固定的 CPU 上，因而可以很好地利用缓存数据。但是，如果稍加注意，你可能会发现有一个新问题（这在多队列的方法中是根本的），即负载不均（load imbalance）。假定和上面设定一样（4 个工作，2 个 CPU），但假设一个工作（如 C）这时执行完毕。现在调度队列如下： 如果对系统中每个队列都执行轮转调度策略，会获得如下调度结果： 从图中可以看出，A 获得了 B 和 D 两倍的 CPU 时间，这不是期望的结果。更糟的是，假设 A 和 C 都执行完毕，系统中只有 B 和 D。调度队列看起来如下： 为了解决负载不均的问题，考虑使用迁移的策略。通过工作的跨 CPU 迁移，可以真正实现负载均衡。 在图示这种情况下，期望的迁移很容易理解：操作系统应该将 B 或 D 迁移到 CPU0。这次工作迁移导致负载均衡 在图示情况下，单次迁移并不能解决问题。 不断地迁移一个或多个工作。一种可能的解决方案是不断切换工作，如下面的时间线所示。可以看到，开始的时候 A 独享 CPU 0，B 和 D 在 CPU 1。一些时间片后，B 迁移到 CPU 0 与 A 竞争，D 则独享 CPU 1 一段时间。这样就实现了负载均衡。 如何进行迁移？ 一个基本的方法是采用一种技术，名为工作窃取（work stealing）。通过这种方法，工作量较少的（源）队列不定期地“偷看”其他（目标）队列是不是比自己的工作多。如果目标队列比源队列（显著地）更满，就从目标队列“窃取”一个或多个工作，实现负载均衡。但如果太频繁地检查其他队列，就会带来较高的开销，可扩展性不好，而这是多队列调度最初的全部目标！相反，如果检查间隔太长，又可能会带来严重的负载不均。 Linux Multiprocessor Schedulers 在构建多处理器调度程序方面，Linux 社区一直没有达成共识。存在 3 种不同的调度程序： O(1)调度程序：该调度器引入了每CPU的优先级队列组，每组队列包含 active 和 expire 两个队列，采用启发式算法动态调整优先级，尽可能的进行时间片补偿和惩罚等动态计算，多CPU之间的优先级队列负载均衡。 完全公平调度程序（CFS） BF 调度程序（BFS） O(1) CFS 采用多队列，而 BFS 采用单队列，这说明两种方法都可以成功。当然它们之间还有很多不同的细节。例如，O(1)调度程序是基于优先级的（类似于之前介绍的 MLFQ），随时间推移改变进程的优先级，然后调度最高优先级进程，来实现各种调度目标。交互性得到了特别关注。与之不同，CFS 是确定的比例调度方法（类似之前介绍的步长调度）。BFS 作为三个算法中唯一采用单队列的算法，也基于比例调度，但采用了更复杂的方案，称为最早最合适虚拟截止时间优先算法（EEVEF） 参考链接 [1] CSDN - 返璞归真的Linux BFS调度器 [2] IBM Blog - Linux 调度器 BFS 简介 ","link":"https://blog.shunzi.tech/post/basic-of-virtualization-three/"},{"title":"Series Two of Basic of Virtulization - Mechanism and Policy Part.1","content":" CPU 虚拟化被分为两篇，此篇是第一篇，Mechanism 今天阅读到项目代码时候，涉及到了 Python 协程的概念，由于才疏学浅一时半会儿未能参透协程的实际意义。查阅了很多资料，都是和线程、进程对照着一起阐述，理解起来实在有难度，有了比较浅薄的了解之后决定还是先继续了解进程这一块的知识。 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtulization（虚拟化）。本篇为虚拟化技术的基础篇系列第二篇（Mechanism and Policy），机制和策略。（结合第一篇给出的虚拟化两层实现） CPU 虚拟化被分为两篇，此篇是第一篇，Mechanism Chapter Index TODO Mechanism: Limited Direct Execution 实现 CPU 虚拟化的主要思想为运行一个进程一段时间，然后运行另一个进程，轮流执行。即时分共享的方式来利用 CPU。这样的方式也就存在着一列问题： Performance: 如何在不引入特别多的开销的情况实现虚拟化？ Control: 当持有 CPU 的执行（控制）权时，如何高效运行当前进程？ CRUX：如何高效、可控地虚拟化 CPU？ Basic Technique: Limited Direct Execution Direct Execution：是指在 CPU 上直接运行程序。即当操作系统希望启动程序运行时，会在进程列表中为其创建一个进程条目，为其分配一些内存，将程序代码（从磁盘）加载到内存中，找到入口点（main()函数或类似的），跳转到那里，并开始运行用户的代码。协议如下： 这种直接的方式衍生了一些问题： 操作系统如何保证这个程序只做规定范围内的事情（即我们在代码里描述的事情），还要很高效地去执行？ 在程序运行过程中，操作系统如何暂停这个程序切换到另一个进程上去执行，从而实现时分复用 CPU？ 因此需要为这种直接执行的机制引入一定的限制 Problem #1: Restricted Operations 上述机制的优点在于执行很快，因为直接在 CPU 上运行。但是如果进程希望执行一些其他受限的操作，比如发起磁盘 I/O 或者申请更多的内存和 CPU 等，则需要进行一定的限制和处理。 针对 I/O 类型的操作，如果放任进程做任何事情，就无法构建许多我们想要构建的系统。比如构建一个带权限控制和访问的文件系统。针对进程的 I/O 操作需要进行权限的校验，放任进程的话就可能导致进程读写整个磁盘，就完全失去了权限的保护。 实际上采用的方法是引入一个新的处理器模式，即 User Mode。该模式下的代码运行中会受到限制。比如，在该模式下，进程不能发起 I/O 请求，如果发起将引发异常然后操作系统可能终止进程。与用户模式对应的模式叫做 Kernel Mode，操作系统（或内核）就以这种模式运行，该模式下，运行的代码可以做任何事情，即该模式下无限制。 如果进程确实需要执行某些限制的指令，譬如发起 I/O，那么此时就引入了新的概念 System call。系统调用其实是内核向用户暴露的某些关键功能，譬如文件系统的相关操作，进程的创建销毁通信等，内存的分配回收等。程序为了执行系统调用，需要首先执行 trap 指令，该指令将进入到内核中并将权限级别提升到内核模式，一旦在内核中，程序就可以执行想要执行的受限制的操作，当执行结束后，操作系统会调用 return-from-trap 指令，回到发起调用的用户程序并将权限级别调整回用户模式。 在执行 trap 指令时有一些硬件上的细节需要注意，即为了保证之后操作系统发起 return-from-trap 指令后能够正确地回到原来的程序，需要保存原有的用户程序的寄存器数据，即原有进程的上下文。在 x86 上，处理器将会把 PC，flags 以及其他寄存器压栈到每个进程对应的内核栈上，return from trap 的时候才出栈恢复用户态下的程序执行。其他平台可能使用了不同的实现，但基本原理大体如此。 trap 指令如何知道运行操作系统中哪部分的代码？借助系统启动时构建的 trap table，由于启动时处于内核态，很容易配置机器硬件的行为，操作系统需要在 trap table 中维护当发生某些异常事件时对应的硬件需要执行什么样的操作，从而告诉硬件这些 trap handlers 的位置，然后硬件会记住对应的处理程序的位置直到下次系统启动。为了指定准确的系统调用，通常为每个系统调用分配一个系统调用号。因此，用户代码负责将所需的系统调用号放入寄存器或堆栈上的指定位置;当操作系统在trap handler 中处理系统调用时，检查这个数字，确保它是有效的，如果是，就执行相应的代码。这种间接性可以作为一种保护形式;用户代码不能指定要跳转到的确切地址，但必须通过号码请求特定的服务。 如下就是 Limited Direct Execution 的协议，被分成了两个阶段。 第一阶段即启动阶段，初始化 trap table，每个硬件对应的记住系统调用处理程序的位置。 第二阶段即进程运行阶段，首先需要设置一些内容，分配内存等操作，返回用户态执行程序，当程序需要进行系统调用的时候再次进入内核态，执行对应的系统调用处理程序，执行完后返回主程序，直到程序执行完再次进入内核态释放内存。 Problem #2: Switching Between Processes 假设只有一个 CPU，如果一个进程在运行，那就意味着操作系统此时不在运行，那这时候操作系统就不能做很多事情，也就引出了关键问题 CRUX: 操作系统如何重新获得控制权以便在进程间切换？ A Cooperative Approach: Wait For System Calls 早期的一些操作系统采用的方式是协作式，等待系统调用。即相信进程会合理运行，并且假设进程运行太长时间的话会定时地放弃 CPU 的使用并由操作系统决定运行别的进程。 友好的进程如何放弃 CPU 的执行权？大多数进程通常都是通过系统调用将 CPU 的执行权转给操作系统，比如打开文件后读取文件，或者向另一台机器发送数据，又或者创建新进程，这样的系统通常显式地包含了一个 yield 系统调用，它什么都不干，只是将控制权交给操作系统，以便系统可以运行其他进程。 如果应用程序进行了一些非法的操作也会把 CPU 的控制权转移给操作系统，比如除以 0 或者访问不能被访问的内存空间，都会导致 trap 的执行，然后操作系统就获取到了 CPU 的执行权，并可能终止这样的违规进程。 因此，在协作调度系统中，操作系统通过等待系统调用或某种非法操作的发生来重新获得对 CPU 的控制。但是这样太过理想，即对进程太过自信，假设一个进程有很多问题，且是个死循环，但是从不进行系统调用，这个时候操作系统就永远都拿不到 CPU 了。 A Non-Cooperative Approach: The OS Takes Control 如上所述，如果没有硬件的额外帮助，进程不进行系统调用，也不出错的话，CPU 控制权根本无法交给操作系统，要是程序死循环执行，就只能重启了。所以这时候的 CRUX：即使进程不协作，操作系统如何获得 CPU 的控制权？操作系统可以做什么来确保流氓进程不会占用机器？ timer interrupt 时钟中断是指每隔几毫秒发生一次中断，产生中断时当前正在运行的进程停止，执行操作系统中预先定义的中断处理程序，此时操作系统重新获得了 CPU 的执行权，这时候就可按照逻辑执行，譬如停止当前进程，运行另一个进程。 时钟中断程序和前面讨论的系统调用一样，需要在系统启动时就通知硬件哪些代码在发生中断时执行。除此以外系统还需要启动时钟，从而保证操作系统最终能够获取到 CPU 的执行权。时钟当然也可以关闭。时钟的启动和关闭都是一些权限极高的操作。 中断操作发生时对硬件也会有一定的要求，跟系统调用时对硬件的要求类似，即需要保存原有的用户程序所对应的寄存器数据，也就是所谓的进程上下文。 Saving and Restoring Context 既然操作系统已经重新获得了控制权，无论是通过系统调用协作，还是通过时钟中断更强制执行，都必须决定：是继续运行当前正在运行的进程，还是切换到另一个进程。这个决定是由调度程序（scheduler）做出的，它是操作系统的一部分，即调度策略。 进程的切换其实就是我们常常说的上下文切换 context switch，就是指操作系统要做的就是为当前正在执行的进程保存一些寄存器的值（例如，到它的内核栈），并为即将执行的进程恢复一些寄存器的值（从它的内核栈）。这样一来，操作系统就可以确保最后执行从陷阱返回指令时，不是返回到之前运行的进程，而是继续执行另一个进程。 为了保存当前正在运行的进程的上下文，操作系统会执行一些底层汇编代码，来保存通用寄存器、程序计数器，以及当前正在运行的进程的内核栈指针，然后恢复寄存器、程序计数器，并切换内核栈，供即将运行的进程使用。通过切换栈，内核在进入切换代码调用时，是一个进程（被中断的进程）的上下文，在返回时，是另一进程（即将执行的进程）的上下文。当操作系统最终执行从return-from-trap 指令时，即将执行的进程变成了当前运行的进程。至此上下文切换完成。 如下所示，进程 A 运行时被中断时钟中断，硬件保存寄存器数据到内核栈中，进入内核模式，时钟中断处理程序中，操作系统决定从正在运行的进程 A 切换到进程 B。此时，它调用 switch()例程，该例程仔细保存当前寄存器的值（保存到 A 的进程结构），恢复寄存器进程 B（从它的进程结构），然后切换上下文（switch context），具体来说是通过改变栈指针来使用 B 的内核栈（而不是 A 的）。最后，操作系统从 trap 返回，恢复 B 的寄存器并开始运行它。 图中涉及到了两种类型的寄存器的保存和恢复。 第一种是时钟中断时，硬件隐式地保存了 user registers 到进程的内核栈中 第二种是内核中上下文切换的时候，操作系统显式地保存了 kernel registers 到进程的内存数据结构中 如下给出了 xv6 的上下文切换代码 1 # void swtch(struct context **old, struct context *new); 2 # 3 # Save current register context in old 4 # and then load register context from new. 5 .globl swtch 6 swtch: 7 # Save old registers 8 movl 4(%esp), %eax # put old ptr into eax 9 popl 0(%eax) # save the old IP 10 movl %esp, 4(%eax) # and stack 11 movl %ebx, 8(%eax) # and other registers 12 movl %ecx, 12(%eax) 13 movl %edx, 16(%eax) 14 movl %esi, 20(%eax) 15 movl %edi, 24(%eax) 16 movl %ebp, 28(%eax) 17 18 # Load new registers 19 movl 4(%esp), %eax # put new ptr into eax 20 movl 28(%eax), %ebp # restore other registers 21 movl 24(%eax), %edi 22 movl 20(%eax), %esi 23 movl 16(%eax), %edx 24 movl 12(%eax), %ecx 25 movl 8(%eax), %ebx 26 movl 4(%eax), %esp # stack is switched here 27 pushl 0(%eax) # return addr put in place 28 ret # finally return into new ctxt Worried About Concurrency? CRUX：在系统调用期间发生了时钟中断的话会怎么样？以及在处理一个中断的时候发生了另一个中断又会怎么样？ 具体的细节会在并发部分进行详细阐释，简单地讲 操作系统会在中断期间禁止中断，即保证在一个中断处理过程中不会交出 CPU，但是需要保证其他中断不会因为等待时间过长而丢失。除此以外还有一些加锁的方案，以保护共享的数据结构的并发访问。 Summary limited direct execution：让你想运行的程序在 CPU 上运行，但首先确保设置好硬件，以便在没有操作系统帮助的情况下限制进程可以执行的操作。 操作系统会在启动时加载 trap 处理程序并启动时钟中断，仅在受限模式下运行进程，从而保证进程高效执行，只有在需要执行高级别的操作如系统调用或者占用 CPU 时间过长的时候，才会由操作系统接管 CPU 进行干预。 ","link":"https://blog.shunzi.tech/post/basic-of-virtualization-two/"},{"title":"Series One of Basic of Virtulization - Processes and API","content":" 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtulization（虚拟化）。本篇为虚拟化技术的基础篇系列第一篇（Processes and API），进程和进程API。 突然看到知乎上 关于七龙珠里龟仙人和鹤仙人的讨论，相比之下龟仙人的徒弟之所以更强除了天赋以外，更多的是龟仙人的信奉的理念。基础打好了，一拳一招就是招式。龟仙流的训练方法就是 好好锻炼、好好学习、好好玩耍、好好吃饭、好好睡觉。 龟仙流的训练方法字面上仿佛与平安经无异，但传递的其实是习武最本质的内涵。以无法为有法，以无限为有限。做学术亦是如此，所以比起漫无目的地看论文，还是再回顾回顾一知半解的基础吧。 威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Virtulization（虚拟化）。本篇为虚拟化技术的基础篇系列第一篇（Processes and API），进程和进程API。 Chapter Index TODO Virtualization Dialogue Conversation 以桃子为例，即多人共享一个桃子，但是每个人可能都以为自己完整拥有这个桃子，然后联系到计算机领域中的资源，如 CPU。物理上就有限的 CPU（比如只有一个 CPU），但是当很多用户或者进程使用时，完全以为这个 CPU 就是自己的，这就是虚拟化技术。 Processes 进程：运行的程序。 程序本身无生命。就是存储在磁盘上的一系列指令的集合，包括一些静态数据，然后等待执行。操作系统会从磁盘中读取出这些指令并执行，从而运行程序。 一次可能会运行很多个程序，一个系统看起来可能同时运行了上百上千个程序，系统的易用性使得程序不用考虑是否有 CPU 空闲，所以就引出了关键问题：如何表现出有成百上千个 CPU 的假象？ 答案就是 虚拟化。 CPU 资源常常会使用到 时分共享（time sharing）的技术，即允许一个实体使用该资源一段时间后停止，由另一个实体使用该资源持续一段时间，如此循环往复，来保证资源共享。CPU 上的时分共享最终就能让人感觉到大量程序并发地被执行。 资源共享技术中除了时分共享以外，还有空分共享。如磁盘就是空分共享资源，磁盘上的空间被释放后则可以被其他实体使用。 虚拟化需要两层实现： 底层实现称之为 mechanism，主要是实现一些低级方法和协议。 上层实现为 policy，即需要在操作系统中需要借助一定的数据依据做出某种决策，也称之为调度策略。 抽象：进程 操作系统对运行的程序的抽象即为进程。 进程的组成，也就是所谓的 machine state: memory，正在运行的程序、读取或写入的数据都在内存中，进程可以访问的内存即为进程的一部分（也称地址空间 address space） register，寄存器。许多指令需要读取或更新寄存器。还包括一些比较重要的寄存器，如 program counterm (或者叫 instruction pointer) 来指示程序运行的下一个位置，还有 stack pointer 以及 frame pointer 用于管理函数参数、局部变量、返回值地址等。 还有一些打开的 I/O 描述符（eg. open file table） 进程 API 分类 create destroy wait：等待进程停止运行 miscellaneous control：常用于暂停某个进程，然后恢复执行 status：获取进程状态信息 进程创建 操作系统运行程序必须做的第一件事是将代码和所有静态数据（例如初始化变量）加载（load）到内存中，加载到进程的地址空间中。程序最初以某种可执行格式驻留在磁盘上（disk，或者在某些现代系统中，在基于闪存的 SSD 上）。因此，将程序和静态数据加载到内存中的过程，需要操作系统从磁盘读取这些字节，并将它们放在内存中的某处（见图 4.1）。 在早期的（或简单的）操作系统中，加载过程尽早（eagerly）完成，即在运行程序之前全部完成。现代操作系统惰性（lazily）执行该过程，即仅在程序执行期间需要加载的代码或数据片段，才会加载。 将代码和静态数据加载到内存后，操作系统在运行此进程之前还需要执行其他一些操作。必须为程序的运行时栈（run-time stack 或 stack）分配一些内存。C 语言程序使用栈存放局部变量、函数参数和返回地址，操作系统分配这些内存，并提供给进程。操作系统也可能会用参数初始化栈。具体来说，它会将参数填入 main() 函数，即 argc 和 argv 数组。 操作系统也可能为程序的堆（heap）分配一些内存。在 C 程序中，堆用于显式请求的动态分配数据。程序通过调用 malloc()来请求这样的空间，并通过调用 free()来明确地释放它。数据结构（如链表、散列表、树和其他有趣的数据结构）需要堆。起初堆会很小。随着程序运行，通过 malloc()库 API 请求更多内存，操作系统可能会参与分配更多内存给进程，以满足这些调用。 操作系统还会执行一些 I/O 相关的初始化任务。 通过将代码和静态数据加载到内存中，通过创建和初始化栈以及执行与 I/O 设置相关的其他工作，OS 现在（终于）为程序执行搭好了舞台。然后它有最后一项任务：启动程序，在入口处运行，即 main()。通过跳转到 main() 例程，OS 将 CPU 的控制权转移到新创建的进程中，从而程序开始执行。 进程状态 running ready block 数据结构 如下所示为 xv6 内核中进程的数据结构定义。 context 包含了所有寄存器的信息，便于进程的恢复，也就是我们常常说到的 context switch。所以进程中的上下文切换的本质其实就是寄存器数据的切换。 proc_state 除了上文所述的三种状态以外还有其他状态，如 EMBRYO 表示进程在创建时处于的状态；ZOMBIE 表示进程已经退出但尚未清理的状态，该状态将允许其他进程检查该进程的执行情况来决定下一步操作。 // the registers xv6 will save and restore // to stop and subsequently restart a process struct context { int eip; int esp; int ebx; int ecx; int edx; int esi; int edi; int ebp; }; // the different states a process can be in enum proc_state { UNUSED, EMBRYO, SLEEPING, RUNNABLE, RUNNING, ZOMBIE }; // the information xv6 tracks about each process // including its register context and state struct proc { char *mem; // Start of process memory uint sz; // Size of process memory char *kstack; // Bottom of kernel stack for this process enum proc_state state; // Process state int pid; // Process ID struct proc *parent; // Parent process void *chan; // If !zero, sleeping on chan int killed; // If !zero, has been killed struct file *ofile[NOFILE]; // Open files struct inode *cwd; // Current directory struct context context; // Switch here to run process struct trapframe *tf; // Trap frame for the current interrupt }; 进程列表 process list 被用于跟踪系统中正在运行的所有程序，任何能够同时运行多个程序的操作系统都包含这样的数据结构，进程列表中的每一个进程信息块常常又被称为进程控制块（Process Control Block, PCB） Process API 主要涉及到一组系统调用 fork() 和 exec() 以及 wait() 的使用。 CRUX：OS 为了创建以及控制进程需要提供什么样的接口？如何设计这些接口以实现强大的功能、易用性和高性能？ fork() 用于创建一个新进程。 1 #include &lt;stdio.h&gt; 2 #include &lt;stdlib.h&gt; 3 #include &lt;unistd.h&gt; 4 5 int main(int argc, char *argv[]) { 6 printf(&quot;hello world (pid:%d)\\n&quot;, (int) getpid()); 7 int rc = fork(); 8 if (rc &lt; 0) { 9 // fork failed 10 fprintf(stderr, &quot;fork failed\\n&quot;); 11 exit(1); 12 } else if (rc == 0) { 13 // child (new process) 14 printf(&quot;hello, I am child (pid:%d)\\n&quot;, (int) getpid()); 15 } else { 16 // parent goes down this path (main) 17 printf(&quot;hello, I am parent of %d (pid:%d)\\n&quot;, 18 rc, (int) getpid()); 19 } 20 return 0; 21 } 输出如下：p1 进程首先输出了自己的进程 ID (process identifier)，然后执行 fork 创建新进程。新创建的进程叫子进程。原进程为父进程。此例中即为 29146 进程创建了子进程 29147。父进程按照原有的逻辑继续执行，子进程则是从 fork 调用后返回，所以才出现如下的输出结果。 此时的系统中相当于有两个 p1 进程，但是子进程和父进程不完全相同，除了拥有各自的地址空间、寄存器、PC 以外，fork 的返回值也是不同的。父进程的 fork 返回的是子进程的值，子进程的 fork 返回的是 0。 prompt&gt; ./p1 hello world (pid:29146) hello, I am parent of 29147 (pid:29146) hello, I am child (pid:29147) prompt&gt; fork 之后其执行顺序就不太确定了，即此时存在两个活动的进程，如果是单个 CPU，则任何一个进程都有可能去执行，所以输出结果的顺序就不确定了。 prompt&gt; ./p1 hello world (pid:29146) hello, I am child (pid:29147) hello, I am parent of 29147 (pid:29146) prompt&gt; CPU Scheduler 决定了某个时刻执行哪个进程。但由于调度程序很复杂，并不能假设某个进程会先执行。这种不确定性就会导致一系列问题，特别是在多线程程序中，也就是我们说的并发。 wait() 有时候父进程需要等待子进程执行完毕再决定下一步操作，这时候 wait 就发挥作用了。 如下代码中，父进程调用了 wait 延迟自己的执行，直到子进程执行完毕。子进程结束时，wait 才返回父进程。 1 #include &lt;stdio.h&gt; 2 #include &lt;stdlib.h&gt; 3 #include &lt;unistd.h&gt; 4 #include &lt;sys/wait.h&gt; 5 6 int 7 main(int argc, char *argv[]) 8 { 9 printf(&quot;hello world (pid:%d)\\n&quot;, (int) getpid()); 10 int rc = fork(); 11 if (rc &lt; 0) { // fork failed; exit 12 fprintf(stderr, &quot;fork failed\\n&quot;); 13 exit(1); 14 } else if (rc == 0) { // child (new process) 15 printf(&quot;hello, I am child (pid:%d)\\n&quot;, (int) getpid()); 16 } else { // parent goes down this path (main) 17 int wc = wait(NULL); 18 printf(&quot;hello, I am parent of %d (wc:%d) (pid:%d)\\n&quot;, 19 rc, wc, (int) getpid()); 20 } 21 return 0; 22 } 输出结果的顺序一下就明确了，如果父进程先执行，会执行 wait，该调用会在子进程运行结束后才返回，因此还是会等待子进程先执行。 prompt&gt; ./p2 hello world (pid:29266) hello, I am child (pid:29267) hello, I am parent of 29267 (wc:29267) (pid:29266) prompt&gt; exec() 用于执行和父进程不同的程序。 如下所示代码中则调用了 execvp() 来执行字符计数程序 wc，并使用 p3.c 作为输入参数 1 #include &lt;stdio.h&gt; 2 #include &lt;stdlib.h&gt; 3 #include &lt;unistd.h&gt; 4 #include &lt;string.h&gt; 5 #include &lt;sys/wait.h&gt; 6 7 int 8 main(int argc, char *argv[]) 9 { 10 printf(&quot;hello world (pid:%d)\\n&quot;, (int) getpid()); 11 int rc = fork(); 12 if (rc &lt; 0) { // fork failed; exit 13 fprintf(stderr, &quot;fork failed\\n&quot;); 14 exit(1); 15 } else if (rc == 0) { // child (new process) 16 printf(&quot;hello, I am child (pid:%d)\\n&quot;, (int) getpid()); 17 char *myargs[3]; 18 myargs[0] = strdup(&quot;wc&quot;); // program: &quot;wc&quot; (word count) 19 myargs[1] = strdup(&quot;p3.c&quot;); // argument: file to count 20 myargs[2] = NULL; // marks end of array 21 execvp(myargs[0], myargs); // runs word count 22 printf(&quot;this shouldn't print out&quot;); 23 } else { // parent goes down this path (main) 24 int wc = wait(NULL); 25 printf(&quot;hello, I am parent of %d (wc:%d) (pid:%d)\\n&quot;, 26 rc, wc, (int) getpid()); 27 } 28 return 0; 29 } 输出如下所示。exec() 会从可执行程序中加载代码和静态数据，并覆盖写自己的代码段，栈、堆以及其他内存空间也会被初始化，然后操作系统执行该程序，将参数通过 argv 传递给该进程。因此，它并没有创建新进程，而是直接将当前运行的程序（以前的 p3）替换为我同的运行程序（wc）。子进程执行 exec()之后，几乎就像 p3.c 从未运行过一样。对 exec() 的成功调用永远不会返回。 prompt&gt; ./p3 hello world (pid:29383) hello, I am child (pid:29384) 29 107 1030 p3.c hello, I am parent of 29384 (wc:29384) (pid:29383) prompt&gt; 为什么这样设计？ 分离 fork 和 exec 便于 shell 在 fork 之后 exec 之前调整新程序的环境。 shell 也是一个用户程序，它首先显示一个提示符（prompt），然后等待用户输入。你可以向它输入一个命令（一个可执行程序的名称及需要的参数），大多数情况下，shell 可以在文件系统中找到这个可执行程序，调用 fork() 创建新进程，并调用 exec() 的某个变体来执行这个可执行程序，调用 wait() 等待该命令完成。子进程执行结束后，shell 从 wait()返回并再次输出一个提示符，等待用户输入下一条命令。 shell 还可以实现一些有趣的功能，譬如重定向。如下所示将输出结果重定向到文件中 prompt&gt; wc p3.c &gt; newfile.txt shell 实现结果重定向的方式也很简单，当完成子进程的创建后，shell 谁调用 exec() 之前先关闭了标准输出（standard output），打开了文件 newfile.txt。这样，即将运行的程序 wc 的输出结果就被发送到该文件，而不是打印在屏幕上。 如下代码展示了重定向的工作原理，是基于对操作系统管理文件描述符方式的假设。具体来说，UNIX 系统从 0 开始寻找可以使用的文件描述符。谁这个例子中，STDOUT_FILENO 将成为第一个可用的文件描述符，因此谁 open()被调用时，得到赋值。然后子进程向标准输出文件描述符的写入（例如通过 printf()这样的函数），都会被透明地转向新打开的文件，而不是屏幕。 1 #include &lt;stdio.h&gt; 2 #include &lt;stdlib.h&gt; 3 #include &lt;unistd.h&gt; 4 #include &lt;string.h&gt; 5 #include &lt;fcntl.h&gt; 6 #include &lt;sys/wait.h&gt; 7 8 int 9 main(int argc, char *argv[]) 10 { 11 int rc = fork(); 12 if (rc &lt; 0) { // fork failed; exit 13 fprintf(stderr, &quot;fork failed\\n&quot;); 14 exit(1); 15 } else if (rc == 0) { // child: redirect standard output to a file 16 close(STDOUT_FILENO); 17 open(&quot;./p4.output&quot;, O_CREAT|O_WRONLY|O_TRUNC, S_IRWXU); 18 19 // now exec &quot;wc&quot;... 20 char *myargs[3]; 21 myargs[0] = strdup(&quot;wc&quot;); // program: &quot;wc&quot; (word count) 22 myargs[1] = strdup(&quot;p4.c&quot;); // argument: file to count 23 myargs[2] = NULL; // marks end of array 24 execvp(myargs[0], myargs); // runs word count 25 } else { // parent goes down this path (main) 26 int wc = wait(NULL); 27 } 28 return 0; 29 } 输出结果如下。 prompt&gt; ./p4 prompt&gt; cat p4.output 32 109 846 p4.c prompt&gt; UNIX 管谁也是用类似的方式实现的，但用的是 pipe()系统调用。在这种情况下，一个进程的输出被链接到了一个内核管谁（pipe）上（队列），另一个进程的输入也被连接到了同一个管谁上。因此，前一个进程的输出无缝地作为后一个进程的输入，许多命令可以用这种方式串联谁一起，共同完成某项任务。比如通过将 grep、wc 命令用管谁连接可以完成从一个文件中查找某个词，并统计其出现次数的功能：grep -o foo file | wc -l 其他 除了上述接口以外，还有很多其他系统调用接口用于进程的管理。譬如 kill() 向进程发送信号，还有如暂停等其他指令。为了方便起见，在大多数 UNIX shell中，配置某些按键组合以向当前运行的进程发送特定的信号，如常用的 ctrl-c 来中断进程，ctrl-z 来暂停进程 整个信号子系统提供了丰富的基础设施来向流程交付外部事件，包括在各个流程内接收和处理这些信号的方法，以及向各个流程和整个流程组发送信号的方法。为了使用这种通信方式，进程应该使用 signal() 系统调用来“捕捉”各种信号;这样做可以确保当一个特定的信号被传递给一个进程时，它会暂停它的正常执行，并运行一个特定的代码片段来响应这个信号。 谁可以发送信号？用户可以启动一个或多个进程，并对它们进行完全控制(暂停它们，杀死它们，等等)。 用户通常只能控制自己的进程;操作系统的工作是将资源(如CPU、内存和磁盘)分配给每个用户(及其进程)，以满足总体系统目标。 ps、top、kill、killall “A fork() in the road” by Andrew Baumann, Jonathan Appavoo, Orran Krieger, Timothy Roscoe. HotOS ’19, Bertinoro, Italy. 该文指出了 fork 存在的一些问题，考虑使用 spawn() 来代替 fork(). 参考：fork() 成为负担，需要淘汰 spawn ","link":"https://blog.shunzi.tech/post/basic-of-virtualization-one/"},{"title":"GearDB: A GC-free Key-Value Store on HM-SMR Drives with Gear Compaction","content":" 该篇文章来自于 FAST2019 - GearDB: A GC-free Key-Value Store on HM-SMR Drives with Gear Compaction 该篇文章来自于 FAST2019 - GearDB: A GC-free Key-Value Store on HM-SMR Drives with Gear Compaction 本文主要针对 HW-SMR 上的 KV 存储引擎的 GC 存在的问题进行了分析并进行了针对性的优化 由于场景相对单一，没有深入阅读，大致理解思想即可 Abstract HM-SMR 磁盘的出现顺应了数据爆炸增长的趋势，其主要优点就在于容量大，成本低，性能可预测，因此很适合顺序写、随机读的应用，如 LSM-tree。但是 LSM-tree 的 compaction 和 HM-SMR 的垃圾回收在数据清理过程中是有一定的冗余的，为了消除磁盘内部的 GC 开销并提升 compaction 的效率，我们提出了 GearDB，为 HM-SMR 定制的无 GC KV 存储。主要提出了三种技术： 提出了一种新的磁盘布局 compation windows 一种新的压缩算法 gear compaction 我们在真实的 HM-SMR Drive 上实现并运行了基于 LevelDB 的 GearDB。在性能和空间效率上表现均较好。 Introduction &amp; Background SMR 通过增大磁盘表面密度来增大容量，相同的面积下，进行类似于瓦片的交叠技术来增大盘面密度。 HM-SMR 随机写性能较差，因为需要将重叠部分的数据拷贝到空白区域之后修改才能保证不影响别的数据。因此这种顺序的要求使得 LSM-tree 成为比较好的存储引擎选择，但也是也就引入了垃圾回收的问题。GC 通过迁移有效数据来为新的写操作生成空区域，从而回收磁盘空间。GC的数据迁移开销严重降低了系统性能。 SMR 根据随机写的复杂性处理的位置被分为三种类型：（其中 HM-SMR 因为较大的容量、可预测的性能以及较低的成本应用尤为广泛，特别是针对顺序写随机读比较显著的负载） drive-managed（DM-SMR：在固件中实现了一个转换层，以适应顺序和随机写操作）主要为了替代现有的 HDD，但是性能较差且不可预测。 host-managed（HM-SMR：要求主机端软件修改来利用对应器件的优势，它只允许顺序写操作，并通过公开内部驱动器状态来提供可预测的性能） host-aware（HA-SMR：介于 HM-SMR 和 DM-SMR 之间，然而，它是最复杂的，并获得最大的好处和可预测性） 许多生产中的 HA/HM SMR 被分成了 256MB 大小的 Zones，每个区域通过维护一个写指针来恢复后续的写操作来适应严格的顺序写操作。两个相邻的 Zones 被 guard 分开。没有有效数据的区域可以通过将该区域的写指针重设到该区域的第一个块来重用为空区域。所有复杂的 HM-SMR 都通过一个新的命令集 T10 Zone Block Commands 暴露给软件 为了遵守 SMR 顺序写限制，应用程序或操作系统需要以日志结构的方式写数据。但是，log-structured 布局以垃圾收集(GC)的形式增加了额外的开销。并且由于实时数据迁移，GC 会阻塞前台请求并降低系统性能。 LSM-Tree LSM-Tree 的介绍此处省略。 KV 存储容量的需求促进了基于 HM-SMR 构建 KV 存储的想法产生。现有的方案有 希捷的 Kinetic，华为的 SMR-based KV Store，Netapp 的 SMORE 等等。 但是在 HM-SMR 上构建 KV Store 的挑战主要表现在重复的数据清理过程对性能产生的严重影响。LSM 树中，压缩操作清理无效数据并排序。在 HM-SMR 驱动器中，由于应用程序使数据失效，具有日志结构数据布局的区域被分割。因此需要在 HM-SMR 上执行垃圾回收。HM-SMR 驱动器上的现有应用程序要么没有解决垃圾收集问题，要么使用简单的贪婪策略从部分空的区域迁移实时数据。冗余清理过程，特别是存储设备的垃圾收集，会显著降低系统性能。 为了演示磁盘上垃圾收集的影响，我们实现了一种成本较低和贪婪垃圾收集策略，类似于日志结构文件系统和 SSD 中的空闲空间管理，结果表明垃圾回收不仅仅带来系统延迟上很大的开销，也会对 HM-SMR 空间利用率产生影响。传统的 KV 存储在 HM-SMR 驱动器上面临困境：要么在性能差的情况下获得较高的空间效率，要么在空间利用率差的情况下获得良好的性能。（空间利用率定义为磁盘上有效数据卷与分配的磁盘空间之间的比率） Motivation HM-SMR驱动器所需的日志结构写方式可能导致磁盘碎片过多，因此需要进行开销较大的垃圾收集来处理它们。具体而言，在基于 HM-SMR 的 LSM KV 存储中，压缩操作常常清理无效的 KV 项但是在磁盘上会留下无效的 SSTables。特别地，对常规日志的任意顺序写操作会导致 在一个 Zone 中来自多个层级的 SSTables 拥有不同的压缩频率。如图所示。 因为压缩操作在 LSM 的生命周期中持续地在进行，HM-SMR 的碎片将遍布到各个 Zones，就需要大量的片段 GC 操作。因为严峻的写放大系数（大于12x）以及压缩造成的大量的分散的碎片，被动的 GCs 变得不可避免。当空闲的磁盘空间低于阈值 20% 时候将触发 GC，GC 将先在 Zone 之间迁移有效数据，然后清理对应的 Zone。 为了证明上述的 HM-SMR 上的 LSM KV 存储在垃圾回收过程中的问题，我们在真实的 HM-SMR 实现了 LevelDB，实现了贪婪和利于成本的 GC 策略来管理 HM-SMR 空间。 贪婪 GC 通过将无效数据迁移到其他区域来清除无效数据最多的区域。 Cost-benefit GC 考虑区域的年龄和空间利用度(u)，使用如下公式选择区域。 Zone Age：将 SSTables 的 Level 进行求和，因为 SSTable 在更底层也就意味着存在的时间更长，就有着更小的压缩频率 n 是该 Zone 中 SSTables 的个数 Li 即为一个 SSTable 的层数 开销包括读一个 Zone 然后写回 u 的有效数据的开销 通过实现，我们有如下发现： 发现1：在加载过程中，我们每十分钟计算一次有效数据量和GC时间。当有效数据增长到大约磁盘空间的 70% 时，十分钟的采样周期中有超过一半的时间都被用于 GC，当有效数据的比例增长到 76% 的时候，GC 执行时间占据了超过 80%。测试结果表明，垃圾收集占用了总执行时间的很大一部分，从而显著降低了系统性能。 解释：因为来自一个区域中不同级别的sstable是混合的，多个区域在 Cost-benefit GC 策略中显示相似的年龄。相似的区域年龄使得贪婪策略和 Cost-benefit GC 策略表现出几乎相同的性能，因为它们都倾向于回收使用最多无效数据的区域 发现2：我们在 HM-SMR 驱动器上记录了加载 1000 万KV项后每个占区的空间利用情况，图 a 显示了有效数据在每个 Zone 中的比例，图 b 显示了 Zone 空间利用率的累计分布函数。两种 GC 策略平均空间效率达不到 60%，更具体地讲，85% 的 Zones 的空间利用率大约在 45% 到 80%。我们认为，正如上面所讨论的，这种空间利用率分布导致大量时间花费在GC上。 解释： 迁移的区域中的实时数据越多，清理所需的磁盘带宽就越多，而写入新数据则无法使用磁盘带宽。更好、更友好的空间利用方式是双峰分布，其中大多数区域几乎都已满，少数区域为空，而cleaner始终可以处理空区域，从而消除GC的开销。既可以实现高磁盘空间利用率，又可以消除磁盘上的垃圾收集开销。 、 发现3：通过将GC的阈值从100%更改为50%)的110 GB限制磁盘容量，我们测试了6组80 GB的随机写操作，以显示性能随磁盘空间利用率的变化。磁盘空间利用率或空间效率定义为磁盘上有效数据卷与分配的磁盘空间的比率，如下图所示，系统性能随空间利用率而降低。 解释：在HM-SMR驱动器上运行的LevelDB面临着一个困境，即它只能在性能和空间利用率之间折衷，我们的设计 GearDB 旨在同时实现更高的性能和更好的空间效率。 Design 一个新的磁盘布局，为 HM-SMR 驱动器提供特定于应用程序的数据管理。一个 Zone 只服务于来自于同一个 Level 的 SSTable 来避免不同 Level 的数据混合分布导致大量分散的碎片。 基于新的磁盘布局，为每一个 LSM-trees Level 设计了一个压缩窗口。只有在压缩窗口中，压错操作才会被执行，从而讲碎片限制在了压缩窗口中 提出了一个新的压缩算法，gear compaction，基于上述两种设计，该算法将每次压缩合并了的数据分成三部分，并在下一级的压缩窗口中与重叠的数据进一步压缩。通过让压缩窗口中的 SSTables 全部无效实现了自动地清空 SMR Zones，从而不需要额外的垃圾回收操作。 A New On-disk Data Layout 初始的时候，每一个 Level 被关联到一个 Zone，随着一个 Level 的数据量的增长，其他的 Zones 也会被分配给这个 Level，即 Zone 和 Level 的关系是多对一的关系。 一旦一个 Zone 关联到了一个 level，该 Zone 就只能存储来自 Li 层的顺序写入的 SSTables，直到该 Zone 被作为一个 Empty Zone 释放。 每一层对应的多个 Zone 之间，只有一个 Zone 接受进来的写入，被称之为 Writing Zone，每个 Zone 的顺序写入严格遵循 HM-SMR 驱动器的重叠约束。 因为一个 Zone 都属于同一个 Level，所以对应的压缩频率也就相同，这样的数据布局就能减少碎片化的磁盘空间并为后面的设计提供便利，可能导致期望的双峰分布，从而使我们以低成本获得高系统性能。除此以外，读的性能也能因为空间局部性的原因而有所提升。 Compaction Windows 为了进一步减少碎片，为每一层提出了 Compaction Window，一个 Compaction Window 就是由一组属于该 Level 的 Zones 组成的。构建该窗口的时候，以旋转的方式从属于该级别的区域中挑选一定数量的区域，每一层对应的窗口大小常常使用如下方式计算：即每一层的 1/K，因此随着容量的增大每一层的窗口也就相应地增大，默认的 K 为 4，需要注意的是，在我们的研究中，L0 和 L1 的窗口大小就为整个 Level 的大小，因为这两层不够占据一个 Zone。 压缩窗口不是直接设计成用于提高系统性能，而是在压缩窗口中限制压缩过程，相应的碎片也就被限制在压缩窗口中而不是整个磁盘上进行分布。因为一个全是无效数据的 Zone 可以直接被置空然后进行重用，那么充满了无效 SSTables 数据的压缩窗口也就可以直接被释放从而让一组空的 Zone 直接服务于未来的写操作。当压缩窗口对应的 Zones 被释放，另外一组 Zones 被选出来形成新的压缩窗口，层内的不同区域旋转构成压缩窗口，保证各层均匀参与压缩。 如下所示为 Zone 的状态转换图： 一旦 writing zone 被写满了就成为了 Full Zone 2.3. writing zone 和 full zone 可以直接通过旋转被添加到压缩窗口中 当在压缩窗口中的所有 SSTables 经过 gear compaction 之后变为无效，则转换为 Empty 等待服务写请求，而不用触发设备级的垃圾回收 Gear Compaction 该算法首先压缩 L0 和 L1，称之为 Active Compaction，Active Compaction 触发连续的被动 Compaction，压缩从较低的级别进展到较高的级别。LevelDB 中两层压缩之后的结果将直接写入更大的一层，然而对于该算法而言，压缩后的数据会根据 Key 的范围被分成三个部分。假设压缩的层级是 LiL_iLi​ 和 Li+1L_{i+1}Li+1​，将分成 Li+2L_{i+2}Li+2​ 压缩窗口以外的部分 Li+2L_{i+2}Li+2​ 键范围以外的部分 Li+2L_{i+2}Li+2​ 压缩窗口以内的部分 合并数据的这三个部分不会保存在内存中，而是分别 写到 Li+1L_{i+1}Li+1​， dump 到 Li+2L_{i+2}Li+2​ （能够减小写放大） 或者被处理成被动压缩（例如和 Li+2L_{i+2}Li+2​ 的 CW 中重叠的 SSTable 进行压缩） 为了避免数据直接被压缩到最高层，可以让 Li+2L_{i+2}Li+2​ 只有在 Li+1L_{i+1}Li+1​ 到达大小限制且 Li+2L_{i+2}Li+2​ 到达压缩窗口的大小的时候才加入 gear compaction。因为 GearDB 还保证了时间局部性，更新的数据仍然保留在更低的层次。 下图描述了整个过程：（该过程递归执行，一层一层地执行，直到最高层或者新生成的数据不再和下一层的压缩窗口重叠） Step1. 执行 L0L_0L0​ 和 L1L_{1}L1​ 的 active compaction，生成的数据分成三个部分 Step2. 压缩窗口以外的部分写回 L1L_1L1​ Step3. 压缩窗口以内的部分 dump 到 L2L_2L2​，避免后续的压缩引入的写放大 Step4. 压缩窗口以外的部分保留在内存中以便于后续和 L2L_2L2​ 的压缩窗口一起进行压缩 该算法伪代码描述如下： 针对于根据键范围进行划分的操作进行了一定的优化，将每个级别划分为大粒度的键范围。具体而言，针对 CW 范围内和范围外的 SSTable 单独处理，如果 SSTables 之间的键范围差距不与该级别的其他 SSTables 重叠，我们将键范围合并成一个大的连续范围，因此，有序的数据只需要和该范围的最大最小值进行比较就能进行区分。 举例说明，Li+2L_{i+2}Li+2​ 有两个 SSTables，键范围分别为 a-b，c-d，我们检查 Li+2L_{i+2}Li+2​ 中的其他 SSTables，看是否有和 b-c 范围重叠的，如果没有，那么就可以修正 Li+2L_{i+2}Li+2​ 的压缩窗口的范围为 a-d 从而减少键的比较。 如何回收压缩窗口？当 CW 中的数据都无效时直接置空该 CW 就能进行垃圾回收，为了让 CW 为空，LiL_{i}Li​ 层的 SSTables 中和 Li+1L_{i+1}Li+1​ 的 CW 具有重叠键范围的就必须进行 gear compaction。一旦所有的Li区域旋转地加入压缩窗口，我们在 Li 中得到这些 SSTable。当 LiL_{i}Li​ 的所有 CWs 被回收了，Li+1L_{i+1}Li+1​ 才被回收，如下图所示。当 LiL_{i}Li​ 清理了 K 个 CWs，Li+1L_{i+1}Li+1​ 才清理一个 CW。该过程意味着，释放 CWs 的机制就像齿轮一样。 ","link":"https://blog.shunzi.tech/post/GearDB/"},{"title":"PinK: High-speed In-storage Key-value Store with Bounded Tails","content":" 该篇文章来自于 ATC2020 Best Paper PinK: High-speed In-storage Key-value Store with Bounded Tails 论文其实是基于一种新的 KV 存储形式 KV-SSD 开展研究的，KV-SSD 近年来常被提及，未来可能作为一种新型存储器件在键值存储系统中使用。 本文的工作主要实现了基于 LSM-Tree 的 KV-SSD，和基于哈希的 KV-SSD 进行了对比。 Abstract KV 存储的实现主要有 LSM-tree 和 HASH 两种，但相关学术研究和工业应用中 LSM Tree 因为支持更多的操作以及更好的写性能，相比于 HASH 更受欢迎。但是，在资源有限的 KV-SSD 环境下，LSM-tree 很难实现，故 KV-SSD 的环境下常常使用 HASH 实现。 我们提出了 PinK，一种基于 LSM-tree 实现的 KV-SSD，相比于基于 HASH 实现的 KV-SSD，99th 尾延迟降低了 73%，平均读延迟增加了 42% 但吞吐量提升了 37%。 在资源受限的环境下提升 LSM-tree 的性能的核心思想是 避免使用 BloomFilter，使用少量的 DRAM 来 keep/pin LSM-tree 的顶层数据。 Introduction &amp; Background KV Store 应用广泛 现有的优化方案 Algorithm: ATC19 Best Paper - SILK：通过调度 compaction 和带宽分配来集中优化 write 长尾延迟 SIGMOD18 - Dostoevsky：控制 merge 频率在 tiering、leveling 和 lazying leveling 几种不同策略下转换以适应不同的workload SIGMOD17 - Monkey：对每一层的布隆过滤根据数据规模的大小分配不同的内存空间，从而最小化所有层布隆过滤的假阳率。 System: VLDB10 - FlashStore：内存维护 HashTable 索引，使用签名代替 Key，CuckooHash 解决冲突，KV 以日志结构形式存储，使用 Flash Memory 作为非易失缓存 FAST16 - WiscKey：KV 分离，Key 存 LSM-Tree，Value 存 Log Eurosys14 - LOCS： Architecture: VLDB16 - Bluecache：基于闪存的分布式KV Store系统，使用了 FPGA NAND Flash-based SSD SSD 的具体结构请参考其他博文。eg. 一个比较关键的点，也是本文涉及到的点：FTL 中维护了由 LBA 索引的映射表，对应地指向响应的 flash page。映射表保存在 SSD 控制器 DRAM 中，DRAM 大小通常为 SSD 容量的 0.1%。映射表需要持久化，所以会使用到 SSD 内置的电容来防止突然的断电故障。 KV-SSD 近年来学术和工业界都在考虑将 KV 存储的相关功能从软件层面转移到硬件存储设备层面。 为什么要引入 KV-SSD？传统的 KV 存储不能完全发挥新型存储器件的性能，如很多支持 NVMe 的设备，结合之前 SOSP19 的一篇论文 [2]，发现 KV Store 的 bound 已经从磁盘转移到了 CPU，故考虑引入 KV-SSD 来降低 KV 存储对 host 侧的 CPU 和 DRAM 的高占用率，同时减小 I/O 延迟并提高吞吐量。 KV SSD 支持的接口对应的规范：全球网络存储工业协会发布的 https://www.snia.org/tech_activities/standards/curr_standards/kvsapi 设计实现 KV-SSD 的两大挑战：有限的 DRAM 资源和有限的 CPU 资源 工业界 Samsung KV-SSD：直接处理 KV 请求，参考文献 [1]。通过简化软件存储堆栈和合并冗余，KV-SSD 提供了更好的可伸缩性和性能，从而降低了总体 CPU 使用量并将内存释放给用户应用程序。 学术界 ASPLOS19 LightStore: Software-defined Network-attached Key-value Drives：使用 LSM-Tree 实现协议转换，并实现了和数据中心网络直连 KAML: A Flexible, High-Performance Key-Value SSD 实现了内部事务处理、更细粒度的锁和和缓存层 Towards Building a High-performance, Scale-in Keyvalue Storage System. NVMKV: A Scalable and Lightweight Flash Aware Key-value Store Bluecache: A Scalable Distributed Flash-based Key-value Store HASH-Based KV-SSD SSD DRAM 中维护一个 hashtable， hashtable 有很多个 buckets，每个 bucket 对应的保存了元数据（key 和指向 value 的指针）。假设 Key 为 32B，Value 为 1KB，假设 4TB 的 SSD 中的 bucket 数量为 2322^{32}232，即 4TB 全部用于存 1KB 的 Value，那么实际对应需要 DRAM 大小为 36B∗232=144GB36B * 2^{32}=144 GB36B∗232=144GB 才能装下对应的 HashTable，但是按照之前的 0.1% 的计算逻辑来看，最多提供 4GB DRAM。显然是不够的。 为了降低 DRAM 使用率，可以使用哈希签名来代替 KEY，实际的 KEY 和 Value 一起存储在 flash 中，即便是使用 16bit 的签名还是需要 24GB 的 DRAM，且可能产生签名冲突。所以只能保存部分热数据索引到 DRAM 中，剩余部分需要保存在 flash 中，但是引入了额外的 flash 读取开销。 现在的 KV-SSD 存在的问题 从现如今的 KV-SSD 表现出来的效果来看，在尾延迟和吞吐量方面表现都是 inconsistent （注：至于这个 incosistent 怎么理解，可以看后文的具体描述，大胆猜测一下应该是类似于 unpredictable 的意思） 现在的 KV-SSD 主要都是基于 HASH 实现的，因为 HASH 实现起来相对容易，但是也就带来了一些局限。基于 HASH 实现的 KV-SSD 主要是在对应的磁盘控制器（DRAM）中维护了一个 HashTable，相应的操作本质就是查表以及对表的管理，但是由于 KV-SSD 的 DRAM 容量有限，数据量大的时候肯定会有部分数据需要放在 Flash 上，简单快速的 DRAM 内查表工作就可能退化成开销较大的访问 FLASH 的查表，同时还需要实现比较复杂的空间管理机制（何时访问 FLASH 进行空间置换，诸如此类）。如果产生了 HASH 冲突，可能就需要访问很多次 FLASH，导致长尾延迟以及吞吐量的下降。 光纸上谈兵不行，做个实验：4TB KV-SSD prototype (KV-PM983)，KV pools 从 1GB 到 3TB，平均大小 Key-32B，Value-1KB，3TB 的池对应就可以有 30 亿个 KV。负载 KVBench 10 分钟随机 GET（期间无 GC）。对比测试，对照了使用 4TB Block SSD 和 FIO 测试。 实验结论：在基于哈希的 KV-SSD 实现中，随着存储的 KV 对总数的增加，性能和尾延迟会变得更差，还不如 Block SSD 稳定。（分别测试了 CDF 累积分布函数和吞吐量） incosistent 的性能表现可能是因为低效的 HASH 冲突解决策略引起的。可以考虑使用最坏情况下常数级的查询的哈希冲突解决策略，如 Cuckoo Hash, Hopscotch 等，来避免长尾延迟，但是这种好处是以降低写入速度和/或频繁重复哈希为代价的。且 HASH 不支持 range query。 性能随着容量的增加显著下降的原因主要是 DRAM 容量的限制导致索引数据太大，不得不频繁访问 Flash 解决方案 从 DRAM 资源有限的角度来考虑，LSM-tree 提供了更好的性能。 在 KV-SSD 中比较常用的 LSM-tree 实现为 LightStore [ [3]] 和 iLSM-SSD [4] 考虑使用 LSM-tree 替代 HASH，因为 LSM-tree 需要使用的 DRAM 更小，对范围查询的支持也较好。但是实验显示，使用了常见的 LSM-Tree 实现的 KV-SSD 并没有表现出预期的效果，在有些场景下甚至比 HASH 性能还差。 测试结果，如图所示。 没有 BloomFilter，LSM-tree KV-SSD 显示出了比 BlockSSD 更高的延迟。 使用 BloomFilter 会显著降低读的层级数，平均进行一次 flash 上的 page read 就可以读取到数据，读延迟比不带 BloomFilter 更好，但是尾延迟更严重（因为概率性的数据结构导致大约有 1.4% 的读操作会需要超过一次的 flash lookup）。 YCSB-C 的负载下，吞吐量的表现上，带 BloomFilter 的 LSM-tree KV-SSD 不到 Block-SSD 的一半（因为为了获取KV索引来为GET()服务， Monkey 需要两次 flash 读取）。 YCSB-Load 负载下，吞吐量严重下降，主要是因为 GC 产生了影响。我们发现为 GC 移动有效页面涉及到 LSM 树维护的 KV 索引的级联更新。 CPU 的开销，重建 BloomFilter 的开销占据了很大的比重。 通用的 LSM-tree 方案存在的问题 尾延迟： 虽然很多 LSM-tree 实现都使用了 BloomFilter 来改善平均读延迟，但因为 BloomFilter 是概率型数据结构，对于最差的情况下的尾延迟还是没有办法改善，就会出现和 HASH KV-SSD 一样的长尾延迟的情况。 严峻的写放大： LSM-tree 本身的问题（压缩会导致 GC，GC 导致更多的 I/O，加剧写放大），应用在 KV-SSD 中可能还会加剧 FTL 的 GC 开销。 CPU 占用率高： 这个也是 LSM-tree 本身存在的问题，现在用于 KV-SSD，本身存储设备的处理性能就有限，LSM-Tree 的 filter rebuild 以及 KV 对的压缩排序都需要花费大量的时钟周期，很容易导致 KV-SSD 的控制器处理单元过载，从而显著影响 I/O 性能 本文的方案 Pink：核心思想为 level pinning，即通过将 top levels 的键值索引 pin 到 DRAM 中来代替 BloomFilter (注：看着是不是很懵，可能直接看设计部分好点，但就暂时理解为 DRAM 里放了个索引来代替了布隆过滤器吧) 因为没了 BloomFilter，使用了 DRAM 内的索引，带来了两个直接的好处：延迟变得 predictable；减少了资源占用。 其他优点主要体现在： 压缩过程减小了对 flash 的 I/O，因为 DRAM 索引排序不需要 I/O 操作，且该索引由内置的电容来保证持久性； 可以批量延迟更新 DRAM 中的 LSM-tree 索引，从而减小 GC I/O 次数； 添加硬件比较器来参与 sort 操作，而无需 CPU 介入，减小 CPU 开销。 注：这儿可能看着还比较抽象，等着看下文的详细设计部分吧 Design Data Structure DRAM：a skiplist 和 level lists （由 SSD 电容保护） skiplist：类似于 write buffer，通常配置的很大以便充分利用 NAND channels 的并行性来向 Flash 刷入数据。对应包含数据 &lt;key size, key, value size, value&gt; level lists: 每一层被组织成为了具有确定大小指针对组成的数组，第一个指针指向 meta segment 在 flash 中的物理位置，第二个指针指向对应 meta segment 的开始 key。所以 meta segment 的起始 key 被单独存储在了 DRAM 中， FLASH：meta segments 和 data segments skiplists 满了之后，元数据和数据分离，刷入到 Flash，元数据包含指向对应数据的指针，数据段中存储了 KEY 以及 Key Size 便于 GC。元数据段大小被固定为 flash page size，但数据段可以是任意大小 meta segment: 由一组 &lt;key, pointer&gt; 组成，按 key 排序，加上一个 header，key 大小可变 16B-128B，，为了利用二分查找，所以维护了 &lt;key, pointer&gt; 的起始位置在 header 中。 和 HASH 相比，PinK 需要使用的 DRAM 更少 Improving I/O Speed with Level Pinning 缓解读延迟：最差的情况下，PinK 需要 O(h−1)O(h-1)O(h−1) 的 flash 查询，为了防止这种情况， PinK 采用了 Level pinning 来代替布隆过滤器，简单粗暴地可以理解如果 LSM-tree 有 h 层，PinK 直接把 top-k 层对应的 meta segments 给放在 DRAM 中。对于读操作，首先看 DRAM 中的 top-k 层有没有该 Key，没有再访问 flash。此时最糟糕的情况下的复杂度降低为 O(h−k−1)O(h-k-1)O(h−k−1) Level-pinning 内存需求：假设 4TB SSD，共 5 层，可以根据 SSD 中 DRAM 的大小来考虑实际需要缓存多少层 meta segement。 减少压缩 I/O：因为 meta segment 被缓存在内存中，所以可以直接更新，且不用刷回，因为有电容保护。 理解 图 5 表示了压缩后的数据布局。假设 L1 被固定到了 DRAM，在 L0 刷回之前，首先从 L1 中找到相应的 meta segments（读次数 - 1）。即图示中的 Page 0，然后进行归并排序，从而得到两个有序的 meta segments，这两个 meta segments 再相应地刷到 L1（写 - 2）（即图5 中的 Page 3 和 4）。相应地更新 level lists。然后发现 L1 层满了，那么需要刷回到 L2，这时候读取 L1 和 L2 的各自两个 meta segments（读 - 2）（即图 5 中的 Page 1~4，其中 Page 1 和 2 来自 L2，Page 3 和 4 来自 L1），归并排序后然后写到 L2，得到对应的 Page 5~7。 因为 L1 被固定在 DRAM 上，所以省略了对 L1 的三次 flash 读和两次 flash 写 Optimizing Search Path 假设 L1L_{1}L1​ 层有 N 个 entries，层级之间容量比为 T，因此 Lh−1L_{h-1}Lh−1​ 有 N∗Th−2N * T ^{h-2}N∗Th−2。计算最坏的时间复杂度为 O(h2∗log(T))O(h^2 * log(T))O(h2∗log(T)). 带 BloomFilter 的 LSM-tree 实现对应的复杂度可能更小，因为能够跳过一些不必要的二分查找。 为了减小查询的开销，PinK 使用了两个技术： 一个是减少字符串比较的开销，通过使用 key 的前缀，level list 的每个条目都有两个指针，每个指针指向一个 meta segment 和一个 start key string，我们还包含了前缀（即 start key 的前四个字节）。二分查找时，首先比较前缀，只有匹配了才会匹配整个字符串。 通过借用分级级联技术来减少 level list 的搜索范围。level list 的每一个 entry 都会再有一个 4-byte 指针，称之为范围指针，该指针指向下一层的最大的起始 key 但是小于或等于当前 entry 的 key 的位置。例如，在 L1L_1L1​ 中找到了一个 entry eL1ie_{L_1}^ieL1​i​，如果该 entry 指向的 meta segment 不包含匹配的 KV index，那么将会寻找下一层，eL1ie_{L_1}^ieL1​i​ 将成为 L2L_2L2​ 层搜索的下界， 而 eL1i+1e_{L_1}^{i+1}eL1​i+1​ 对应的 range pointer 将成为搜索的上界，从而缩小搜索的范围。复杂度变为 O(logT)O(logT)O(logT)，因此平均时间复杂度降低为 O(h∗log(T))O(h * log(T))O(h∗log(T)) Speeding up Compaction 虽然减少了压缩对应的 I/O 次数，但是在压缩过程中的排序操作仍然需要计算资源的开销。考虑使用 HW accelerator（硬件加速）来缓解主机端的压力。HW 加速器放置在闪存和主机数据总线之间，可以很容易地合并两个位于闪存的 level，因为两个 level 的 meta segment 能够从 flash 中以速度较快的流的形式进行传输。该加速器写合并后的 meta segments 时不需要 CPU 的介入。硬件加速器的引入不仅减小了计算的开销，同时提高了 I/O 总线的利用率，因为不再需要以前那样的 等待合并的以及合并后的数据传输。 如图所示工作流程，PinK 软件首先请求加速器来执行压缩，并提供两个 level 的 meta segments 地址以及合并后写回的 mete segements 地址。flash request generator 会进行调度等待多个读请求以便充分利用 flash 带宽，由于不同 flash 通道的数据包是交错的，我们需要为每个 level 使用每个通道的 reorder buffer 来序列化 meta segment。序列化之后送入 compaction engine，本质是个比较器进行归并排序，将比较后的更小的数据以输出流的形式，使用了小的 write buffer，最终输出到写回地址中，当两个 key 相等的时候，上层对应的数据因为最新将取代另外的数据。操作完成后，将使用了的 meta segments 页号返回，从而由上层决定回收哪些未使用的空间。 对于 pinned levels 的合并是有一个类似的机制，只是使用了 DMA 代替 flash request generator，以及交互的不是 flash 而是 DRAM。 Optimizing Garbage Collection 压缩会产生两种垃圾数据： 一种是老旧的 meta segment，当执行压缩时，PinK 写新的 meta segments 数据来替代老旧的数据，如下图所示的 pages 0,1,2。 另一种是过期的 KV 对象，该对象被更新了或者被客户端删除了都变成过期对象，过期对象的索引在压缩过程中会被直接丢弃，所以将没有 meta segments 指向。但是数据部分仍然存储在 data segments 中。 为了擦除老旧的数据，PinK 将在空间接近枯竭的时候触发 GC，选择一个 flash block，将其中的有效数据拷贝到空闲空间，然后擦除整个块。因为冷热分离的原因，meta segment 通常和 data segment 位于不同的块，PinK 通常针对不同块类型采用不同的 GC 策略。 GC for Meta Segment 读取页对应的起始键，然后查询 level list 中是否有 entry 指向该键，如果没有则说明是老旧的 segment，直接跳过，如果有则需要将该页迁移到空闲页，然后对该 entry 的更新也将定位到新的页。清理 meta segment 的开销较小，因为只涉及到有效页的拷贝以及 DRAM 中对 level list 的更新。 GC for Data Segment 从选出来的块中扫描一个 data segments，需要将 value 对应的键提取出来，利用 key 才能查询到 level lists 并找到 meta segments，才能检查该 value 的有效性，如果一个 meta segment 没有被 pin 到 DRAM 中，就必须从 flash 中去读，从而可以收集到很多的有效 values。 最简单的方法就是 WiscKey 的垃圾回收思想，收集有效的然后拷贝，然后整块擦除，对应 value 的 meta segment 也需要更新并刷回，从而指向新的位置。如果被 pin 到 DRAM 里了，则不需要写 flash。这种方法会造成会多对 flash 上的元数据更新，其次就是对于一些很久以前写的数据，压缩之后，很有可能一个 meta segment 中包含的对象不在同一个 value segement 或者 block 中，即可能只有很少的 value 被拷贝，但是却要更新对应的 meta segment。 所以 PinK 采用了延迟写的方式，把有效的 KV 再写到 L0，然后擦除整块。垃圾数据静待回收，但是对原有有效数据的访问将被更高 level 处理，即不进行元数据的更新，全部交给 GC，虽然增加了 compaction 开销，但是减少了 mete segment 的更新，因为有效的 KV 对应的元数据被重新整合到了一个 meta segment 中。 GC 过程中会把有的数据放在 L0 层会一定程度上地影响读延迟。 Durability and Scalability Issues Durability with Limited Capacitor 前面我们默认了使用 SSD 里的电容来保证 SSD DRAM 里的数据不丢，但其实很多 SSD 没有足够的电容来保证 DRAM 里的所有元数据，此时就会有持久性的问题。可以通过将 DRAM 中的数据结构写入 flash 中来解决。L0 的数据可以在处理请求之前写入日志，即写前日志；在执行压缩之后，level list 和 pinned levels 会变成脏数据，需要被持久化到 flash。 无论是写前日志还是脏数据刷回，在 HASH-KV-SSD 中一样需要处理，但是在 LSM-tree-KV-SSD 中可能刷回操作的开销更小，因为写性能 LSM-tree 更好。 DRAM Scalability 我们一开始就假设了 SSD 的 DRAM 容量会随着 flash 的容量的增加而增加，但是 DRAM 的容量增长速度比 flash 要慢，所以可能就没把所有的 levels 给 pin 在 DRAM 中，即只能 pin 部分 levels，相应地也就会增加最坏情况下的查询开销，但即便是最坏情况下，也比使用 HASH 的 KV-SSD 或者使用通用 LSM-tree 的 KV-SSD 表现要好。 另外可以考虑减小树的高度，即让除了最后一层以外的上层结构都放在 DRAM 中，虽然会因为压缩开销的增大一定程度上牺牲写性能，但是能在最坏的情况下将查询复杂度限制在 O(1)O(1)O(1) Evaluation 器件：FPGA-based SSD platform with quad-core ARM Cortex-A53 (Xilinx ZCU102)。即使用 FPGA 做硬件加速 负载：YCSB 对比：HASH-KV-SSD, LightStore-KV-SSD, W/O HW PinK 分析 吞吐量显然 PinK 更高，以及 HW 的引入相应地带来了提升。对于有很多写操作的工作负载，PinK 的这些好处是显而易见的，对于读比较多的，就没什么提升。纯粹的 LSM-tree 相比于 HASH 在写敏感负载下表现更差，反而在读上表现更好。 PinK 大幅减小了 flash 的读次数，压缩对应的 I/O 次数也比原始的 LSM-tree 小了很多 延迟测试情况 查询优化效果，ALL 是指既包含 range 指针又包含前缀匹配。效果比未优化的 PinK 肯定好很多，但比起 LSM-tree 其实只有尾延迟有一些改善。 垃圾回收：优化之后效果比 PinK 好很多，甚至比 HASH 还要好 写延迟和高度的关系：PinK 的读延迟更稳定 Conclusion PinK: 基于 LSM-tree 的 KV-SSD pinning top level indices 改善了读延迟 使用 HW accelerate 显著减少了压缩的 I/O 次数 未来考虑在 RocksDB 这种通用的 KVS 中贯彻 pinned levels 思想，但可能需要借助 Persistent Memory 来保证主机端的持久性。 References [1] Towards building a high-performance, scale-in key-value storage system [2] KVell: the Design and Implementation of a Fast Persistent Key-Value Store [3] LightStore: Software-defined Network-attached Key-value Drives [4] iLSM-SSD: An Intelligent LSM-tree based Key-Value SSD for Data Analytics ","link":"https://blog.shunzi.tech/post/PinK/"},{"title":"KVell: the Design and Implementation of a Fast Persistent Key-Value Store","content":" 该篇文章来自于 SOSP2019 的论文 KVell: the Design and Implementation of a Fast Persistent Key-Value Store 论文提到了一个比较犀利的观点：KV 存储不再像以往主要受限于存储设备，而是逐渐更多的受 CPU 的性能影响。即 storage device bound 转移到了 CPU bound 其实别的文章也略有提及现阶段 KV 存储存在的问题，但这篇文章更多是提出了新的设计范式来指导后续 KVS 的设计，值得一读。 Abstract 按块寻址的 NVMe SSD 相比以前的块设备提供了更高的带宽和更相近的随机和顺序读写的性能。持久化的 KV 存储是为早期的存储设备设计的，使用 LSM-tree 或者 B-tree，没有充分利用新型设备的优势。起初设计中为了避免随机访问的逻辑导致了保持磁盘上数据排序这样的开销巨大的操作以及同步操作，这些操作使得这些 KVs 在NVMe SSD 上受到 CPU 限制，成为新的瓶颈。 我们提出了一种新的持久性 KV 设计，与早期设计不同的是，不再追求顺序访问，数据存储在磁盘上时无序。采用无共享的理念来避免同步开销，与设备访问的批处理一起，这些设计决策使读写性能接近设备带宽，最后，在内存中维护一个开销较小的的部分有序可以产生足够的扫描性能。 我们设计了 KVell 作为第一个能充分利用现代 NVMe SSD 最大带宽的持久性 KV 存储系统。和最先进的 LSM-tree KV 和 B tree KV 进行了对比，KVell 在以读为主的工作负载上比其他类型的 KV 吞吐量至少高了 2x，以写为主的工作负载上高了 5x。对于以 scan 为主的负载，KVell 和其他类型相比性能接近甚至更好。KVell 的最大延迟也是最低的。 Introduction KVS 应用广泛，缓存、元数据管理、消息通信、在线购物等场景。本文针对的是基于提供持久化保证的以块为单位寻址的存储设备上构建的 KVS，且数据集不能全部装在内存中的情况。 传统意义上，存储设备与 CPU 设备之间的速度差距是很大的，因此充分利用 CPU 周期来优化存储设备的访问是一个很有效的的 tradeoff。所以，一些 KV 存储包含了复杂的索引结构，如 B+ tree，LSM KV 强制顺序写，因为顺序写通常比随机写更高效。所有的 KV 一般都包含了某种形式的缓存，为了支持检索给定键范围内的所有项的高效扫描，KVs 在内存和磁盘上有序地维护了相应的索引。尽管所有这些优化都需要花费一些 CPU 周期，但存储设备仍然是瓶颈，而这些优化被证明是有益的，因为它们缓解了瓶颈。 当我们测量在按块寻地址的 NVMe SSD 设备上运行的最先进的 KVs 的性能时，我们发现 CPU 是瓶颈，而不再是存储设备，这证实了之前的定性观察。NVMe SSD 在随机和顺序访问方面表现出非常高的带宽和相当的性能，因此，许多为传统存储设备开发的优化不再有用，甚至可能适得其反，因为它们在 CPU 周期方面的成本加剧了 CPU 瓶颈。例如 LSM KV 现如今对于顺序写的要求的优化不再像以往那么带来特别大的好处，反而是一些维护过程的操作如压缩等加剧了 CPU 的资源使用，对性能产生了影响。一个更令人惊讶的例子是，在共享(内存中)数据结构上进行同步以进行缓存和维护顺序会导致 CPU 成为瓶颈。 我们的结论是 存储设备的特性的变化和发展迫使 KVs 的设计思想和范式必须发生转变，需要更多地聚焦在 CPU 的流水线式的使用上。为此总结了四条设计原则： Shared-nothing：所有的数据结构都在 CPU 核之间分区，这样每个 core 几乎可以完全独立运行而不需要任何同步。 Items 不在磁盘上排序。每个分区都维护一个内存中有序的索引，以便进行高效扫描。 不用强制顺序访问存储设备，但是 I/O 操作需要进行批处理来减小系统调用的次数。批处理同时为设备等待队列的提供了较好的控制，从而可以采用一些策略进行任务的调度来实现低延迟和高吞吐量。 无需 commit log：只有在更新操作被持久化到磁盘上的最终位置后，操作才相应地得到确认。 这些原则消除了很多复杂的维护过程，除了提高平均吞吐量以外，还实现了吞吐量和延迟的可预测性。一些设计决策也做了相应的权衡，比如 Shared-nothing 会有负载不均衡的风险；不保证有序会影响 scan 的性能。我们的测试结果显示，对于主要由大中型键值对组成的工作负载，精简的 CPU 操作带来的好处超过了这些缺点。我们和四种最先进的 KVs 做了对比：分别是 LSM tree 的代表 RocksDB 和 PebblesDB；B tree 的代表 TokuMX 和 WiredTiger。 Contributions： 对传统的 LSM 和 B 树型 KVs 进行了分析，指出 NVMe SSD 场景下的 CPU 瓶颈问题 持久 KVs 利用 NVMe SSD 的属性的新范例，重点关注 CPU 的流水线使用。 将 KVell 与最新的 LSM 和 B 树 KVs 在合成基准和生产工作负载上进行深入对比测试。 Evolution of SSD Performance SSD 性能测试 Hardware： Config-SSD：32-core 2.4GHz Intel Xeon，128GB of RAM, and a 480GB Intel DC S3500 SSD (2013). Config-Amazon-8NVMe：An AWS i3.metal instance, with 36 CPUs (72 cores) running at 2.3GHz, 488GB of RAM, and 8 NVMe SSD drives of 1.9TB each (brand unknown, 2016 technology). Config-Optane：A 4-core 4.2GHz Intel i7, 60GB of RAM, and a 480GB Intel Optane 905P (2018). IOPS：表1 显示了这三种设备的最大读写 IOPS 数以及随机和顺序带宽。读写性能提升迅猛，顺序和随机之间的差距越来越小，混合随机读写不再会像老式设备那样导致性能低下 Latency and bandwidth：表2 展示了三个设备的延迟和带宽测量值，它们是设备队列深度的函数，一个核心执行随机写操作。可以发现以前的存储设备只有在请求数较小的时候才能保证较低的延迟，新的存储设备因为并行性更好往往在较多请求情况下仍然能保证毫秒级别的延迟，通过和上面的测试数据对比，发现在请求数较少的时候但其实完全没有跑满存储设备的实际带宽。所以即便是在现代设备的应用中，也需要做一个请求等待处理时间（延迟）和并发处理请求数（充分利用带宽）之间的 tradeoff。（请求太少带宽有余，请求太多延迟较高） Throughput degradation：图 1 显示了这三种设备上的 IOPS 随时间的变化。Config-SSD 可以维持大约 40min 的 50K IOPS，随后性能下降到 11K IOPS，而新的存储设备基本不会有这个问题，可以进行长时间的持续的 IO Latency spikes：图 2 展示了 4K 写的延迟，队列深度为 64。在写密集的负载下，老一代的 SSD 常常受到 latency spike 的影响（正常写延迟通常为 1.5ms，五小时后延迟将变成 100ms）。Config-Amazon-8NVMe 驱动器也会受到周期性 Latency spikes 的影响。最大观察到的延迟是 15ms。在 Config-Optane 驱动器上，潜伏期峰值不规则地发生，它们的幅度通常小于1毫秒，最大可观测到3.6毫秒 NVMe SSD 上当前 KVs 的问题 在持久性 KV 存储中有两个主流的思想： LSM KVs 被视为写密集负载的最好的解决方案，即对写更友好，如 RocksDB，Cassandra B tree KVs 被视为读密集负载的最好的解决方案，即对读更友好，如 MongoDB 但其实这两种设计在 NVMe SSD 上都会逐渐变为 CPU 瓶颈，并遭受严重的性能波动。 CPU is the bottleneck 图 3 显示了在 Config-Optane 上构建的 RocksDB 和 Wired Tiger 在负载为 YCSB-A（写密集）请求分布为 uniform 下的磁盘利用率和 CPU 利用率。两种方案都几乎榨干了 CPU，但是都还是没有完全发挥出设备实际的最大带宽。 CPU is the bottleneck in LSM KVs LSM KVs 通过使用内存缓冲区吸收更新操作，主要针对写密集型负载进行了优化。缓冲区满了之后，刷入到磁盘，然后由后台线程将刷回的数据维护在一个持久性存储设备中的树状结构中。磁盘上的数据结构包含了很多层，随着层数的增加，容量也在增加，每一层都包含多个 immutable sorted files，每一层中的每个 file 包含不同的键范围，除了第一层。为了维护这样的数据结构，LSM KVs 实现了压缩机制，该机制对 CPU 和 I/O 都比较敏感，即是比较占据 CPU 时钟周期和磁盘带宽的。 压缩操作在以前老的设备上主要是占据磁盘带宽，但是合并/索引/内核代码等也会占据一定的 CPU 时钟周期，但是在新设备上，CPU 成为了主要瓶颈。 研究分析表明 RocksDB 大概会占据 60% 的 CPU 时钟周期，其中 28% 数据合并，15% 索引构建，剩下的主要是数据传输过程（对磁盘进行数据的读写）中的 CPU 时钟周期占用。压缩需求源于 LSM 的设计要求，即顺序磁盘访问和在磁盘上保持数据排序，这种设计对于以前的存储设备是有益的，即在以往的存储设备上花费时钟周期来保证顺序执行提升存储设备的访问性能是值得的。 CPU is the bottleneck in LSM KVs 针对持久化存储的 B Tree 有两种变体 B+ tree 和 Bϵ tree。 B+ tree 在叶子节点中存储 KV 对，中间节点只包含用于路由的 Key，通常中间节点会被保存在内存中加速访问，叶子节点常常会被放在持久化的存储设备中。每一个叶子节点通常是以一个页的大小，存储了一个范围内的 KV 对。有的 B+ Tree 还会将每一个叶子节点使用指针链接起来，形成一个链表，从而提供较好的 scan 性能。WiredTiger 依赖内存缓存以实现更好的性能，更新操作首先写入到每一个线程对应的 commit log 中然后写入内存缓存，当更新的数据从缓存中被逐出时，树将使用新信息更新。更新使用序列号，这是扫描所必需的。读操作遍历缓存，只有在 KV 对未被缓存时才访问树。 将数据持久化到树中，主要有两种操作：checkpointing 和 eviction。 Checkpointing 周期性进行，或者当日志中数据大小到达一个阈值时就执行，从而确保日志文件不会过大。 Eviction 将脏数据从缓存写入到树中，当缓存中的脏数据达到一定的比例就执行 Eviction Bϵ trees 是 B+ tree 的一个变种，增加缓冲区，临时存储每个节点上的键和值，最终，当缓冲区满了时，KV 对沿着树结构向下延伸，并被写入持久存储。 B 树的设计容易产生同步开销，根据对 WiredTiger 的分析发现花费总时间的大约 47% 等待日志中的 slots。这个问题源于不能足够快地推进序列号进行更新。在更新的主代码路径中，不包括内核所花费的时间，WiredTiger 只花费18% 的时间来执行客户端请求逻辑，其余时间用于等待。WiredTiger 还需要执行后台操作: 从页面缓存中清除脏数据占总时间的 12%，管理提交日志占总时间的 5%。内核中只有 25% 的时间用于读写调用，而其余的时间用于 futex 和 yield 函数。 Bϵ trees 也受同步性能开销的严重影响。因为 Bϵ trees 需要保证磁盘上的数据有序，工作线程最终会修改共享的数据结构，从而导致争用。对 TokuMX 的分析表明线程将 30% 的时间花费在锁或用于保护共享页面的原子操作上。缓冲也是 Bϵ trees 的一个比较大的开销来源, TokuMX 在工作负载为 YCSB-A 时花费了大约 20% 的时间从缓冲区向正确的叶子节点的正确位置传输数据。这些同步开销大大超过了其他开销。 在现代的 NVMe SSD 的条件下，为了避免与同步相关的 CPU 开销，尽可能减少共享是比较好的方式。以前用于提供快速持久性保证的日志最终成为主要的瓶颈，对内存中 KVs 的观察发现缓冲也引入了开销，对于持久性存储也是一样。 Performance fluctuations in LSM and B tree KVs 除了受制于 CPU 外，LSM 和 B树 KVs 都有显著的性能波动。如图所示，展示了性能波动的情况，每秒记录一次吞吐量，在 LSM RocksDB 和 B 树 WiredTiger KVs 中，基本问题是相似的: 客户端更新会因为数据结构的维护操作而停止 LSM KVs 中，吞吐量下降更多地是因为有时更新需要等待压缩完成。当 LSM tree 的第一层满了之后，更新需要等待，直到压缩后空间可用。但是，当 LSM KVs 在现代驱动器上运行时，压缩会遇到 CPU 瓶颈，随着时间的推移，吞吐量方面的性能差异会上升一个数量级，RocksDB 的平均请求数为 63K /s，但在写入停止时下降到 1.5K /s，分析显示，写线程大约有 22% 的时间被搁置，等待内存组件被刷回。因为压缩无法跟上更新的速度，所以内存组件的刷回被延迟。 为了减小压缩过程中对性能产生的影响，大致有延迟压缩（SOSP17-PebblesDB）或者在系统空闲的时候进行压缩（ATC19-SILK）两种方案，但是这两种方案都不适用于高端 SSD。例如 Optane 刷新内存组件的速度为 2GB/s，延迟压缩超过几秒钟会导致大量请求积压和空间浪费。 B tree 中性能也会受到用户负载的停顿的影响，因为 evictions 不能足够快地进行导致用户写入被延迟，Stalls 导致吞吐量下降了一个数量级，从 120 Kops/s 下降到 8.5 Kops/s。 我们的结论是，在这两种情况下，压缩和驱逐等数据结构的维护操作严重干扰用户的工作负载，造成的停顿可能会持续数几秒。因此，新的 KV 设计应避免数据结构的维护操作。 KVell design principles 为了有效地利用现代的持久存储设备，KVs 现在需要重视 CPU 开销。如下原则将是在现代存储设备上实现高性能 KVs 的关键： Share nothing. 在 KVell 中，是指支持并行性和最小化 KV 工作线程之间的共享状态，以减少 CPU 开销。 Do not sort on disk, but keep indexes in memory. KVell 将未排序的数据保存在磁盘的最终位置，避免了昂贵的重新排序操作 Aim for fewer syscalls, not for sequential I/O. KVell 的目标不再是追求顺序访问，因为顺序 IO 的优势不再明显，目标转变为通过批量执行 I/O 来最小化系统调用带来的 CPU 开销 No commit log. KVell 不缓冲更新，因此不需要依赖于提交日志，避免了不必要的 I/O。 Share nothing 无论是读还是写，工作线程处理请求时无需和其他线程进行同步，每一个线程处理给定子集的键，并在线程中维护一个私有的数据集合对应管理该子集的键。键的数据结构： 一个轻量级的内存 B 树索引，用于跟踪键在持久存储中的位置 I/O 队列，负责高效地存储和检索来自持久化存储设备中的信息 空闲列表，部分在内存中的磁盘块列表，其中包含用于存储 items 的空闲位置 页缓存，KVell 使用自己的内部页面缓存，不依赖于 os 级结构。 Scan 是在内存中的 B 树索引上需要最小同步的唯一操作。 与传统 KV 设计相比，这种无共享方法是一个关键的区别，在传统 KV 设计中，所有或大多数主要数据结构都由所有工作线程共享，传统的方法需要对每个请求进行同步，这是 KVell 完全避免的。分区请求可能会导致负载不平衡，但我们发现，如果使用合适的分区，这种影响很小。 Do not sort on disk, but keep indexes in memory KVell 不会对工作线程的工作集中的数据进行排序，因此可以直接将数据存储在磁盘上，且该位置不会发生改变。因为不要求有序，插入 KV 对的开销相对减小，因为不需要找到对应的位置来执行插入操作，同时能消除为了维护磁盘上的数据结构有序对应的开销，无序情况下，对写操作的性能优化效果很好，可以帮助实现较低的尾延迟。 在 scan 操作时，连续的 keys 可能不再位于同一个磁盘块中，scan 的性能似乎会有所下降。但实际上对于中等大小的键值对的负载，以及单个键值对大小较大的情况下，scan 的性能并未受到严重的影响。 Aim for fewer syscalls, not for sequential I/O 在 KVell 中所有操作包括 scan 都对磁盘执行随机访问，因为随机访问和顺序访问一样高效，KVell 不会浪费 CPU 周期来强制执行顺序 I/O。 和 LSM 相似的是，KVell 将请求批处理写到磁盘，不同的是 LSM KVs 批量执行 I/O 且把 KV 对排序是为了利用顺序的磁盘访问能力，而 KVell 批量执行 I/O 的目的是为了减少系统调用的次数，从而减少 CPU 开销。 批处理其实也是做了一种权衡，在 Introduction 部分中已经简单提及，磁盘需要长时间的保持运行忙碌的状态以实现最大的 IOPS，但是只有在硬件队列中包含的请求少于给定数量时，才会以亚毫秒级的延迟响应（如 Optane 要求 256 个 I/O 请求）。一个高效的系统需要把足够的请求发送给磁盘以充分利用磁盘的带宽，但是不能超过队列的最大服务请求数太多，否则可能将导致较高的尾延迟。 在具有多个磁盘的配置中，每个 worker 只在一个磁盘上存储文件，这种设计决策对于限制每个磁盘挂起的请求数量非常关键。事实上，因为 worker 彼此之间不会进行通信，所以他们不知道其他 workers 已经发送给磁盘多少个请求。如果 workers 都在一个磁盘上存储数据，然后一个磁盘对应的请求数量将被限制为 （batch size * 每个磁盘的 workers 数量）。如果 workers 可以访问所有磁盘，然后一个磁盘可能最多处理 （batch size * 所有 workers 的数量） 因为请求是根据它们的键分配给 worker 的，而且 worker 只访问一个磁盘，所以可以设计一个工作负载，主要访问一个磁盘上的数据，而让其他磁盘空闲。在表现出倾斜行为的工作负载中，数据倾斜被内部页面缓存吸收。因此，大多数负载不平衡不会导致磁盘I/O。 No commit log KVell 只有在更新操作被持久化到磁盘上的最终位置才确认更新完成，而不依赖于 commit log。一旦更新被提交给工作线程，它将在下一个 I/O 批处理中持久化到磁盘上。移除 commit log 允许 KVell 仅将磁盘带宽用于有用的客户机请求处理。 KVell implementation 尽管 KVell 的设计看起来很简单粗暴，但其实实现起来还是很复杂的。 源码地址：https://github.com/BLepers/KVell Client operations interface 实现了和 LSM KVs 一样的核心接口， 写 Update(k, v)。Value 和 Key 是组合在一起的，只有 Value 持久化到了磁盘上才会返回操作成功。 读 Get(k) 扫描(范围查询) scan(k1, k2) On-disk data structures 为了避免碎片化，在相同大小范围内的 KV 对会被存储在相同的文件中，这些文件称之为 slab。KVell 以块的粒度访问 slabs，在论文中的机器上使用 4KB 大小的页。 如果 KV 对比页小（即好几个 KV 对才能填充满一个页），KVell 用时间戳、键大小和值大小作为 slab 中的 KV 项的前缀。如果比页大，在磁盘上每个块的开头都有一个时间戳头。 对于小于页面大小的项，将在适当的位置进行就地更新。大于页面的项的更新，会把更新操作追加到 slab 然后写一个 tombstone 来标识原有的项所在的位置。当改变 KV 项的大小，KVell 首先将更新的项写入到新的 slab，然后删除旧的 slab 里对应的项。 In-memory data structures Index KVell 依赖于快速而轻量级的内存索引，这些索引具有可预测的插入和查找时间来查找磁盘上项的位置。KVell 为每个 worker 使用一个内存中的 B 树来存储磁盘上键值对的位置。Items 根据其键(前缀)建立索引。我们使用前缀而不是哈希来保持范围扫描的键的顺序。B-tree 在磁盘上存储比较大的 items 性能较差，但是如果 item 较小且数据大多都能装在内存中的话性能就很好。KVell 利用这个特性，就只使用 B 树存储查找信息(前缀和位置信息占13B) KVell 树实现平均每个 KV 项大约使用 19B 的空间，存储对应的前缀/地址信息/B tree 结构，1.7G RAM 可以存储 100M 项。YCSB 负载（1KB 大小的项）下，索引大概占据数据库总大小的 1.7%，这样的比例是可以接受的。KVell 目前不显式地支持将 B 树的一部分刷回到磁盘，但是 B 树数据是从 mmap-ed 文件中分配的，可以由内核以页的形式置换出去。 Page Cache KVell 维护自己的内部页面缓存，以避免从持久存储中获取频繁访问的页面。缓存的页面大小是一个系统参数。页面缓存会记住哪些页面缓存在索引中，并按照 LRU 顺序从缓存中回收页面。 确保索引中的查找和插入具有最小的 CPU 开销对于获得良好的性能至关重要。我们的第一个页面缓存实现使用快速uthash 哈希表作为索引。但是，当页面缓存很大时，散列中的插入可能会花费高达 100ms 的时间(增长散列表的时间)，从而提高了尾部延迟。切换到 B 树可以消除这些延迟峰值。 Free list 当从一个 slab 中删除一个项时，它在该 slab 中的位置将插入到每个 slab 的内存堆栈中，我们称之为 slab 的空闲列表。然后在磁盘上的 item 位置写入一个 tombstone。为了限制内存的使用，内存中只保留最近 N 个空闲的位置，在本文的例子中 N 被设置为 64。其目标是限制内存使用，同时保持在不需要额外磁盘访问的情况下重用每批 I/O 的多个空闲 spots 的能力。 当第 N+1 项被释放了，KVell 使其磁盘 tombstone 指向第一个释放的位置，然后 KVell 从内存堆栈中删除第一个被释放的位置，并插入第 N + 1 个被释放的位置，当第 N + 2 个项被释放，磁盘 tombstone 指向第二个空闲位置。简而言之，KVell 维护 N 个独立的栈，这些栈的头驻留在内存中，剩余部分驻留在磁盘上。允许 KVell 重用每批 I/O 最多 N 个空闲点，如果只有一个栈，KVell 就必须从磁盘上依次读取 N 个 tombstone，以找到下一个被释放的 N 个位置。 Efficiently performing I/O KVell 依赖 Linux AIO 提供的异步 I/O API 来发送请求到磁盘，最多批量处理 64 requests。通过批处理，KVell 能平摊多个客户端请求对应的系统调用的开销。我们选择使用 Linux AIO 是因为其提供了在一个系统调用里执行多个 I/Os 的方法。我们估计，如果这样的调用在同步 I/O API中可用，性能将大致相同 我们没有采用两种常用的执行 I/O 的方法： 依赖于操作系统页缓存的 mmap （RocksDB 采用的） 通过 direct I/O 标识的读写系统调用来执行 （如 TokuMX 采用的） 这两种技术的效率都不如使用 AIO 接口， 如下表所示，展示了在 Optane 上的不同系统调用对应随机写 4K 的块（对设备 read-modify-write）的最大 IOPS，访问的数据集是 RAM 大小的三倍。 表中所示第一种方式依赖于操作系统级别的页缓存，在单线程的情况下，这种方法的性能不是最优的，因为当发生页中断时，它一次只能发出一个磁盘读取，此外，脏页只定期刷新到磁盘，这在大多数情况下会导致次优化的队列深度，然后是 I/Os 的爆发，当数据集不能完全装入内存时，内核就不得不从进程的虚拟地址空间中对置换出的页进行 map 和 unmap，将导致严峻的 CPU 开销。对于多线程，在刷新 LRU 时，页面缓存会受到 locking 开销以及系统使远程核的 TLB 项失效的速度的影响。实际上，当一个页面从虚拟地址空间中取消映射时，需要在所有访问过该页面的核上使虚拟到物理的映射失效，从而导致 IPI 通信的巨大开销。 第二种方式是使用 direct I/O。但是，当请求以同步方式完成时，direct I/O 读/写系统调用不会填满磁盘队列，因为没有必要处理复杂的从虚拟地址空间映射和取消映射页面的逻辑，该技术的性能优于 mmap 方法。 相反，批处理 I/O 每个批处理只需要一个系统调用，并允许 KVell 控制设备队列长度，以实现低延迟和高带宽。 尽管理论上 I/O 批处理技术可以应用于 LSM 和 B 树 KVs，但实现将需要很大的努力。在 B 树中，不同的操作可能会对 I/O 产生冲突的影响(例如，由插入引起的叶的分裂，然后是两个叶的合并)。此外，数据可能由于重新排序而在磁盘上移动，这也使得异步批处理请求难以实现。写请求的批处理已经通过内存组件在 LSM KVs 中实现，然而，批处理略微增加了读取路径的复杂性，因为 workers 必须确保所有要读的文件没有被压缩线程删除。 Client operation implementation 如图所示算法展示了单页 KV 对的处理。当一个请求访问该系统时，首先根据其 Key 分配给一个 worker。Worker 线程执行磁盘 I/O 并处理客户端请求的逻辑。 Get(k) Get 操作主要包含从索引中读取到对应的磁盘上的地址和读取到对应的页。如果页已经被缓存，没有必要访问持久的存储，直接同步的返回对应的 value。如果未被缓存，则相应地由 worker 将请求入队到 I/O 请求队列中异步执行。 17 case GET : 18 if (!location || page. contains_data) 19 callback (..., page) // synchronous call 20 else 21 read_async (page , callback ) // enqueue I/O 22 break; Update(k,v) 更新一个 KV 对的操作主要包含：首先读取到对应的页（即 key 存储在的页），修改值，然后将页写回磁盘。删除一个 KV 对其实就是将一个 tombstone 写入并将该项对应的位置添加到 slab 空闲列表中。当写入新的 KV 对时，空闲的位置得以重用，如果没有空闲位置则进行追加写。KVell 只在更新的项完全持久化到磁盘后确认更新已经完成（io_getevents 系统调用通知我们，与更新对应的磁盘写操作已经完成）脏数据会被立即刷新到磁盘，因为 KVell 的页面缓存不用于缓冲区更新，因此 KVell 比起其他 KVs 提供了更强的持久性保证。比如，RocksDB 只在 commit log 同步到磁盘的粒度上保证持久性，且通常同步操作只会在一定数量的更新之后才执行。 24 case UPDATE : 25 file = get_file_based_on_size ((k,v)) 26 if (!location) 27 ... // asynchronously add item in file 28 else if(location. file != file) // size changed 29 ... // delete from old slab , add in new slab 30 else if (!page. contains_data) // page is not cached 31 // read data asynchronously first... 32 get ({ (k,v), UPDATE , callback }); 33 else // ...then update &amp; flush page asynchronously 34 update cached page 35 write_async (location , callback ) 36 37 events = get processes I/O (io_getevents async I/O) 38 foreach ( e in events) 39 callback (..., e.page) Scan(k1,k2) 范围查询操作只要包含：从索引中获取 keys 的位置，然后读取到对应的页。为了得到 keys list，需要扫描整个索引（线程依次锁定、扫描和解锁所有 workers 的索引，最后合并结果以获得一个要从 KV 读取的键列表）。读取到键后使用 Get() 命令直接根据索引中存储的物理位置访问磁盘进行查询。 范围查询是唯一要求线程间共享的操作，KVell 返回与扫描触及的每个键相关联的最新值。相比之下，RocksDB 和WiredTiger 都在 KV 快照上执行扫描。 Failure model and recovery KVell 的当前实现为无故障操作进行了优化。当 crash 时，所有的 slabs 会被扫描并重建内存中的索引。即使扫描使顺序磁盘带宽最大化，在非常大的数据集上恢复仍然需要几分钟。 如果某一项在磁盘上出现了两次（故障发生在了将某个 KV 对从一个 slab 迁移到另一个 slab 时），只有最新的项被保存在了内存索引中，且另一项对应的位置也已经被插入到空闲列表中。如果该项大于块大小，KVell 使用时间戳来丢弃只被部分更新了的项。 KVell 是针对于可以支持 4KB 页的原子写入的设备设计的。通过避免对页面进行就地修改，在新页面中写入新值，然后在第一次写入操作完成后在 slab 的空闲列表中添加旧位置，可以解除该约束。 Evaluation Goals 我们用各种生产和合成工作负载来评估 KVell，并将其与最先进的 KVs 进行比较。评估开始回答以下问题： KVell 对比现有的运行在现代 SSD 上的 KVs 在读写以及是扫描的吞吐量、性能波动和尾延迟情况表现如何？ KVell 在大型数据库和生产环境中的工作负载下表现如何？ 在超出其设计范围的工作负载中使用 KVell 的利弊和限制是什么(小型项目、内存限制极端的环境和较旧的驱动器)？ Experimental setting Hardware： Config-SSD：32-core 2.4GHz Intel Xeon，128GB of RAM, and a 480GB Intel DC S3500 SSD (2013). Config-Amazon-8NVMe：An AWS i3.metal instance, with 36 CPUs (72 cores) running at 2.3GHz, 488GB of RAM, and 8 NVMe SSD drives of 1.9TB each (brand unknown, 2016 technology). Config-Optane：A 4-core 4.2GHz Intel i7, 60GB of RAM, and a 480GB Intel Optane 905P (2018). Workloads: YCSB：测试使用了 uniform 和 zipfian 的请求分布。KV 项大小为 1024B，总的数据集大小分别为 100GB（小规模）和 5TB（大规模） two production workloads from Nutanix：两个写敏感负载，write:read:scan 57:41:2，KV 项大小在 250B 到 1K 之间，中位数为 400B。数据集大小总的为 256GB，两个负载之间的区别在于数据的倾斜。Production Workload 1 更接近于 uniform 的分布， Production Workload 2 更倾斜。 Existing KVs： LSM：RocksDB 和 PebblesDB B+ tree: TokuMX 和 WiredTiger System configuration：使用 cgroups 为所有的系统限制了相同大小的内存，即数据集大小的三分之一，确保从内存和持久存储同时为请求提供服务。当数据库大于 RAM 大小的三倍的时候，将不再使用 cgroups。 对于 B Tree，块大小设置为 4KB LSM：5 levels 和两个 128MB 的内存组件，WAL Buffer 1MB Results in the main experimental setting Throughput Write-intensive workload (YCSB A and F)： TokuMX 15x WiredTiger and RocksDB by 8× and PebblesDB by 5.8× on the YCSB A workload。 该负载下未缓存的 read 导致一次 I/O，缓存了的读导致 0 次 I/O，未缓存的写导致 2 次I/O（1 read + 1 write），缓存了的写导致一次 I/O。因为页缓存大约未数据集的 1/3，所以平均一次请求大约需要 1.17 I/Os，意味着最大理论吞吐量为 500K IOPS/1.17 = 428K requests/s。而 KVell 的平均吞吐量约为 420K，因此利用了纯设备带宽的 98%，且没有被 CPU 限制住。 在这个工作负载中，KVell 将 20% 的时间花在由页面缓存和内存中索引完成的 B树查找上，20% 的时间花在 I/O 函数上，60% 的时间花在等待上。 LSM KVs 被压缩的开销给限制住，WiredTiger 被日志的争用给限制住（大约消耗 50% 的时钟周期），TokuMX 被共享数据结构的争用和不必要的 buffering 给限制住（大约消耗 50% 的时钟周期） Read-intensive workloads (YCSB B, C, D) KVell outperforms existing KVs by 2.2× on YCSB B and 2.7× on YCSB C。接近完全利用磁盘的 IOPS。 YCSB-C KVell 花费 40% 的时间查询，20% 的时间执行 I/O，40% 的时间等待。 在这些工作负载中，现有 KVs 的执行不是最优的，因为它们共享缓存，或者因为它们没有将读请求批处理到磁盘(每个读一个系统调用)。例如，RocksDB 将 41% 的时间花费在 pread() 系统调用上，并且是受 CPU 限制的。 Scan workloads (YCSB E) KVell 在扫描密集型工作负载中表现良好，在 uniform 和 Zipfian key 的分布中都是如此。注意，为了公平起见，我们修改了 YCSB 工作负载，以便它在 KVell 中以随机顺序插入键。默认情况下，YCSB 按顺序插入键。 Zipfian 下 KVell 领先至少 25%，因为负载是倾斜的，热数据被缓存在 KVell 的缓存中。shared-nothing 和低开销的缓存实现都给予了 KVell 优势。 uniform 下，KVell 由于 PebblesDB 5x，WiredTiger by 33%，和 RocksDB 相近。因为 KVell 数据在磁盘上并非有序，每个扫描项平均就要访问一个页面。相反 RocksDB 下数据有序存储，大概访问每页上的三个条目（页大小为 4K，每个 KV 项为 1024B）。因此，RocksDB 的最大吞吐量大约是 KVell 的三倍，如图 7 显示了吞吐量的时间线上的变化。RocksDB 最大吞吐量约为 55Kops/s，对比 KVell 的 18Kops/s。然而，RocksDB 的维护操作会干扰客户端负载，导致大的波动，而 KVell 的吞吐量保持稳定在 15Kops/s 左右。因此，平均而言，RocksDB 和 KVell 的表现相似。 Throughput over time 下图展示了各种系统吞吐量随着时间的变化，每秒测一次吞吐量，KVell 除了提供更高的平均吞吐量以外，还不会遭受因为维护数据结构带来的性能波动。对于其他存储系统，性能下降的都很频繁。 相比之下，KVell 的性能在一个短暂的爬升阶段(页面缓存被填满的时间)之后保持不变。KVell 在 YCSB A 上至少保持 400K/s 的请求，在过渡阶段之后保持 15K/s 的扫描。 Tail latency 下表展示了最大 99th 尾延迟，LSM 最大尾延迟 9s，B Tree 3s，这是由于维护操作直接影响了LSM和B树KVs的尾部延迟。相比之下，KVell提供了较低的最大延迟(3.9ms)，同时提供了强大的持久性保证。 Alternative configurations and workloads 下图展示了对于 uniform 负载下在 NVMe SSD 上的 YCSB 平均吞吐量，所有的系统都配备了一个 30G 的缓存，大约占 1/3 的数据库大小 KVell outperforms RocksDB by 6.7× and PebblesDB by 8×, TokuMX by 13× and WiredTiger by 9.3× on average on YCSB A 对于 YCSB-E，KVell 略优于 RocksDB，但比其他系统更快，虽然结果和 Optane 是类似的，但是我们可以看到 KVell 和竞争对手之间的性能差距是如何拉大的，因为与其他受 CPU 限制的系统相比，KVell能够更好地利用磁盘。 下图展示了两个生产环境中的真实负载下的性能表现，结果和 YCSB 其实都是类似的 图 B 显示了使用更大数据量的 YCSB 负载下的性能表现 Trade-offs and Limitations Average latency KVell 分批提交 I/O 请求。在饱和 IOPS(大批次)和最小化平均延迟(小批次)之间进行权衡。 Impact of item size 项的大小不会影响 Get() 和 Update()请求未缓存的项的性能。但是，对于保持条目有序的 KVs，条目大小会影响扫描速度。 图10 显示了 RocksDB 的平均吞吐量，RocksDB 在执行压缩时，KVell 在 YCSB E 上改变项目大小(扫描为主)。 由于KVell不对磁盘上的项进行排序，因此无论项的大小如何，它平均为每个扫描项读取一个4KB的页面，并且性能稳定。对于小条目，RocksDB优于KVell，因为它读取的页面比KVell少(64B条目少64倍)。随着条目大小的增长，保持条目排序的好处就会减少。 在运行compactions时，对于所有的项大小，RocksDB只能维持其吞吐量的一小部分。KVell 没有专门针对小 KV 设计，在所有配置下提供了可预测的性能。 Memory size impact 当索引无法装入RAM时，就会成为瓶颈 KVell只支持将索引部分刷新到磁盘，以避免索引超过可用RAM时崩溃，但没有优化到在这种情况下工作。在实践中，索引的内存开销非常低，因此它们可以在RAM中满足大多数工作负载(例如，对于YCSB来说，100GB数据集的索引为1.7GB)。 Older drives (Config-SSD) 在Config-SSD上，KVell在读写方面与LSM KVs相当，但是在扫描方面提供了相对较低的平均性能。在旧的驱动器上，花费用于优化磁盘访问的 CPU 周期平均来说是有益的，而且没有系统此时是受 CPU 限制的。压缩仍然会争夺磁盘资源，造成延迟峰值(18s以上)和吞吐量波动。KVell 的延迟仅受磁盘本身延迟的限制。因此，在旧的驱动器上使用KVell和LSM KVs是一种权衡:如果稳定性和延迟的可预测性很重要，那么KVell是比LSM更好的选择，但是对于扫描为主的工作负载，LSM KVs在旧驱动器上提供了更高的平均性能。 Recovery time KVell的恢复时间取决于数据库的大小，而其他系统的恢复时间主要取决于提交日志的最大大小。对于所有系统，我们都使用默认的提交日志配置，并测量YCSB数据库上的恢复时间(100M Key，100GB)。我们通过在YCSB A工作负载中间终止数据库来模拟崩溃，并测量Config-Amazon-8NVMe上数据库的恢复时间。 KVell 需要 6.6 秒扫描数据库并从崩溃中恢复，最大限度地提高磁盘带宽。RocksDB和WiredTiger平均恢复时间分别为18s和24s。这两个系统主要花费时间回放来自提交日志的查询和重新构建索引。尽管KVell为无故障操作进行了优化，必须扫描整个数据库才能从崩溃中恢复，它的恢复时间比现有系统的恢复时间要短。 Related Work KVs for SSDs WiscKey, HashKV PebblesDB, TRIAD SlimDB, NoveLSM, PapyrusKV, NVMRocks BetrFS, TokuMX SILT, Udepot, Tucana, Kreon LOCS, BlueCache, NVMKV KVs for byte-addressable persistent memory HiKV, Bullet, SLM-DB KVs for in-memory data stores MICA, Minos, Masstree, RAMCloud Conclusion 现有的 KV 存储设计在老一代 SSD 上是有效的，但在现代 SSD 上执行不是最优的。我们已经展示了一种依赖于 share nothing 架构、磁盘上未排序的数据和随机 I/Os 批处理的流线化方法在快速驱动器上的性能优于现有的 KVs，甚至对于扫描工作负载也是如此。我们已经在 KVell 中建立了这些想法的原型。KVell 提供了高且可预测的性能和强大的延迟保证。 ","link":"https://blog.shunzi.tech/post/KVell/"},{"title":"TinyKV 学习笔记","content":" PingCAP Talent Plan 记录一些学习路径中的关键知识点 Mini Key-Value Database badger Github Repo ","link":"https://blog.shunzi.tech/post/tiny-kv-tutorial/"},{"title":"MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with Matrix Container in NVM","content":" 该篇文章来自于 ATC2020 上非易失主题下的论文 MatrixKV: Reducing Write Stalls and Write Amplification in LSM-tree Based KV Stores with Matrix Container in NVM 由于很久没看论文了，前段时间也看了论文作者的一些分享，特别优秀的学姐，恰好对相关方向比较感兴趣，好像还和 PingCAP 合作。故拜读了这篇 Paper 本文主要结合了 NVM，然后针对 LSM 优化领域中少有的对写停顿的优化进行了分析与设计，基于 RocksDB 进行了实现，且已开源，使用了最新的器件 Optane 进行了测试。 Abstract 现有的基于 LSM-tree 的 KV 存储性能表现欠佳，且性能常常无法预测，主要原因就是其严峻的 写放大和写停顿 的问题（Write Stalls）。实验结果表明： 写停顿主要源于 L0-L1 层之间的大量数据的压缩过程 写放大会随着 LSM-trees 的深度增加而不断增大 利用 NVM 基于 LSM-trees 提出了一种 KV 存储的设计 MatrixKV（包含了多级存储：DRAM-NVM-SSD），设计原则表现为： 让 L0-L1 层之间的压缩开销更小从而减少写停顿 减小 LSM-tree 的深度来减小写放大 核心思想主要表现为以下四点： 利用自己提出的 matrix container 在 NVM 上管理 L0 层 设计了新的 column compaction 引用到了 L0-L1 的压缩中来减少压缩数据的量 增加每一层的宽度来减少 LSM-tree 的深度 引入了 cross-row hint search 来改善读性能 和 RocksDB 以及 KVS NoveLSM 对比， 99th 尾延迟降低了 5x 和 1.9x，随机写吞吐量提升了 3.6x 和 2.6x Introduction 为什么引入 NVM？：考虑到随机写操作在流行的 OLTP 工作负载中很常见，不管是突发的还是持续的随机写都是用户很关注的问题，DRAM-SSD 这种存储结构形式就是想利用快速的 DRAM 和持久的 SSD 来提供高性能的数据库访问操作，但是如 cell size、功耗、成本以及 DIMM 槽数量的限制使得不能只通过提升 DRAM 大小来提升性能。因此，在混合存储系统中使用 NVM 成为了现代存储系统的一个新方向。 原有的 LSM-tree 存在的问题：在 DRAM-SSD 的存储结构的基础上使用 RocksDB 进行测试，观察实验结果并分析原因： 写入停顿会导致应用程序吞吐量周期性地降至几乎为零，从而导致性能的剧烈波动和长尾延迟。写停顿的主要原因是每次 L0-L1 压缩处理的数据量大，合并时会合并 L0 和 L1 的所有数据，但是 L0 无序，合并过程占用较多的 CPU 资源和 SSD 带宽，从而导致写停顿和较高的尾延迟。 写入放大(WA) 降低了系统性能和存储设备的耐久性。 关键技术 Matrix container：Matrix Container 通过在 NVM 上使用一个 receiver 和一个 compactor 来管理无序的 L0。Receiver 维护来自 DRAM 刷回的 MemTable，一行一个 MemTable。Compactor 选择并合并 L0 中的数据子集合到 L1。（选择具有相同键范围的数据），每次压缩一列。 Column compaction：列压缩是 L0 和 L1 之间的细粒度压缩，每次压缩一个小的键范围，因为处理的数据量有限，并迅速释放 NVM 中存放的列从而继续接受来自 DRAM 刷回的数据，从而减小写停顿。 Reducing LSM-tree depth：增大每一层 LSM-tree 的容量来减少层数 Cross-row hint search：为每个键提供一个指针，用于对 Matrix Containier 中的所有键进行逻辑排序，从而加速搜索过程 Background and Motivation Background NVM 的出现：存储介质的发展，PCM、memristors、3D Xpoint、STT-MRAM 等存储介质的发展使得 NVM 成为新的选择。字节寻址、持久化、快速，接近 DRAM 的性能，类似于磁盘的持久性，比 DRAM 更低成本获得更大的容量，比 SSD 更低的读写延迟以及更高的带宽。研究表明 NVM 作为存储设备，使用 PCIe 总线时，只实现了边际的性能改进，浪费了 NVM 的介质带来的高性能，所以很多都将 NVM 作为单级内存使用。最常用的是 DRAM-NVM-SSD 三级存储： NVM 有望在未来几年与大容量 SSD 共存 与 DRAM 相比，NVM 仍然有 5 倍的低带宽和 3 倍的高读取延迟 混合系统平衡了 TCO（the total cost of ownership） 和系统性能。 现有 LSM-trees 机制： 几个关键点：L0 层无序是为了保证刷回操作的快速执行，磁盘组件上需要进行压缩来减小读操作和扫描的开销。 压缩过程大致如下： LiL_iLi​ 层的一个 SSTable 和 Li+1L_{i+1}Li+1​ 层的多个 SSTables （具有重叠键范围的）被选作需要压缩的数据 LiL_iLi​ 中也在该键范围内的 SSTable 会被反选 前面两步选择的 SSTable 将被加载到内存进行归并排序 新生成的 SSTables 被写回到 Li+1L_{i+1}Li+1​ 层 因为 L0L_0L0​ 层无序，每一个 SSTable 都有比较宽的键范围，就会来回执行上述步骤中的前两步，就很有可能导致两层中的所有的 SSTable 都被选中用于压缩，导致一个 all-to-all 压缩。 读请求的处理，首先查询内存组件中的 MemTable，继续查询磁盘组件的各个层次，但是因为 L0L_0L0​ 层无序，即键的范围是重叠的，所以可能该层数据检索时遍历了所有 SSTables 现有的 LSM-trees 优化方案：减小写放大、提升内存管理、支持自动调优、使用混合存储结构优化 LSM-tree.其中随机写是最受关注的，因为受压缩的影响较大。对比对象 PebblesDB，SILK，NoveLSM 减小写放大：PebblesDB，Lwc-tree，WiscKey，LSM-trie，VTtree，TRIAD。然而，几乎所有这些工作都忽略了性能差异和写停顿。 减少写停顿：SILK 引入了一个I/O调度器，通过将刷新和压缩延迟到低负载时期、对刷新和较低级别的压缩进行优先级排序以及抢占压缩，可以减轻写暂停对客户端写的影响。Blsm 提出了一个新的合并调度程序，称为“spring and gear”，以协调多级别的压缩。但该方案只是限制了最大写处理延迟，忽略了排队延迟。KVell 使磁盘上的 KV 无序，以减少 CPU 计算成本，从而减轻基于 NVMe SSD 的 KV 存储的写停顿，这不适用于具有一般 SSD 的系统。 引入 NVM 到 LSM 中：SLM-DB，MyNVM，NoveLSM，NVMRocks。 Challenges and Motivations 主要表现为两方面：写停顿和写放大。 测试结果表明：系统性能经历高峰和低谷，而吞吐量的低谷表现为写停顿。显著的波动表明性能不可预测和不稳定 Write Stalls RocksDB 中主要有三种可能的停顿： Insert stall：在完成后台刷回之前，如果 Memtable 被填满，所有的插入操作将停顿。 Flush stall：L0L_0L0​ 层有太多的 SSTables 并达到了大小限制，从内存刷回到 L0L_0L0​ 的操作将停顿 Compaction stalls：前面操作有太多待定的压缩数据块 所有这些停顿都会对写性能产生级联影响，并导致写停顿。 通过记录不同 level 的 flush 和 compaction 周期来测试这三种类型的停顿。L0L_0L0​-L1L_1L1​ 压缩的周期与观察到的写停顿近似匹配，如图 2 所示，图中的每一个短红线表示一次 L0L_0L0​-L1L_1L1​ 压缩，红线的长度表示压缩的延迟，纵轴对应此次压缩过程中处理的数量。压缩的数据量平均大小为 3.10 GB，大量压缩数据会导致大量的读合并写，占用 CPU 周期和 SSD 带宽，从而阻塞前台请求，使L0-L1压缩成为写停顿的主要原因。 写停顿不仅导致系统吞吐量低，而且还导致了高写延迟，从而导致长尾延迟问题。CDF 累积分布函数如图所示，76% 的写请求延迟低于 48 微秒，但是尾延迟达到了 1ms，高延迟会显著降低用户体验的质量，特别是对于延迟关键型应用程序 Write Amplification 系统吞吐量随着数据集大小的增加而降低。写放大(WA)定义为写入存储设备的数据量与用户写入的数据量之间的比率。由于相邻 Level 的大小从低到高以放大倍数(AF = 10)呈指数增长，所以从 LiL_iLi​ 到 Li+1L_{i+1}Li+1​ 的压缩可能会导致平均 AF 倍的写放大，数据集的增长会增加 LSM 树的深度，也会增加总体 WA。增加的 WA 消耗更多的存储带宽，与刷新操作竞争，并最终降低应用程序吞吐量。因此，系统吞吐量随着 LSM 树深度的增加而降低。 NoveLSM 设计思想主要表现为： 采用 NVM 替代 DRAM，从而增加可以存储的 Memtable 和 Immutable Memtable 的容量 允许 NVM 上的 Memtable 被直接更新从而减少压缩 然而，这些设计选择只是推迟了写停顿，当数据集大小达到了 NVM Memtable 的容量，还是会产生 flush stall，（这里有个疑问，为啥是 flush stall，不应该是 insert stall 吗?）NVM 中扩大的 memtable 被刷新到 L0L_0L0​，并显著增加了 L0L_0L0​ - L1L_1L1​ 压缩中的数据量，导致更严重的写停顿。 使用 8GB 的 NVM 和 80GB 的数据集测试了 NoveLSM，测试结果如图所示：相比于 RocksDB 整体 load 时间下降了 1.7 倍，但是写停顿的周期变得更长，主要是因为 L0L_0L0​ - L1L_1L1​ 压缩中的数据量较大，如图所示超过了 15GB，和图二中 RocksDB 的数据量对比 4.86x。当 L0L_0L0​ - L1L_1L1​ 压缩开始就会产生写停顿，在压缩开始之前可能还要等待更高级别或者更高优先级的压缩完成，如图中的灰色短线所示，压缩完成后性能再次提升。显然，NoveLSM 加剧了写停顿。 Summary 从上述分析中我们不难看出造成写停顿的主要原因是 L0L_0L0​ - L1L_1L1​ 压缩过程中的数据量太大，造成写放大系数较大的原因是 LSM-trees 的深度较大。正是两者的共同作用使得系统吞吐量较低并增大了尾延迟。 NoveLSM 试图解决这些问题时却加剧了写停顿，基于以上挑战，我们提出了 MatrixKV，致力于通过巧妙地使用 NVM 来提供一个稳定的低延迟的 KV 存储。 MatrixKV Design 四个关键技术： the matrix container in NVMs to manage the L0 of LSM-trees column compactions for L0 and L1 reducing LSM-tree levels the cross-row hint search Matrix Container NoverLSM 使用 NVM 来增大 MemTable 的大小，但是因为有了更大的 L0 层加剧了写停顿，并未解决瓶颈。因此为了减少写停顿，需要遵循的基本原则就是减少 L0L_0L0​ - L1L_1L1​ 单次压缩过程中的数据量或粒度。基于这样的原则，MatrixKV 从 SSD 上将 L0 层提升到了 NVM，并重新组织 L0 到 matrix container 中，想要充分利用 NVM 的字节寻址和快速的随机访问。matrix container 是在 NVM 上的 L0 的数据管理结构，如图所示，主要由 Receiver 和 Compactor 组成。 Receiver Receiver 接收从 DRAM 刷回的 MemTables，每一个 Memtable 被序列化成 Receiver 中一个单独的行，称为 RowTable。在 Matrix Container 中 RowTable 按行排列，有对应的序列编号，从 0 到 n，Receiver 初始容量为一个 RowTable，当 Receiver 的大小达到阈值时，比如 Matrix Container 的百分之六十，并且 Compactor 为空的时候，Receiver 将停止接受 flush 的 MemTables 并转变为 Compactor。与此同时，会创建一个新的 Receiver 用于接受 Flushed Memtables，在整个逻辑转换过程中不会发生任何数据迁移。 RowTable：由数据和元数据组成，为了构造 RowTable，我们必须首先序列化来自 Immutable Memtable 的 KV 对并进行排序，然后存储到到对应的数据区。然后使用一个有序的数组为所有 KV 对构建元数据，包含对应的页号，在页内的偏移，一个前置指针 pnp_npn​。 在 RowTable 中查找 KV，首先进行二分查找找到对应的目标 Key 和对应的页号、偏移量。 每个元素的 forward pointer 是用于 cross-row hint 搜索的，从而提升读性能。后面章节会讨论。 和传统的 LSM-trees 中的 SSTable 进行对比，SSTable 是以块为单位进行组织的，RowTable 是以 NVM 页为单位进行组织的，除此以外就是元数据的组织方式上略有不同，因此构建 SSTable 和 RowTable 的开销时接近的。 Compactor：用于以较细的粒度选择和合并 SSD 中从 L0L_0L0​ 到 L1L_1L1​ 的数据。利用 NVM 字节寻址的特性以及我们设计的 RowTable 可以实现开销更小的压缩。合并特定的键范围从 L0L_0L0​ 到 L1L_1L1​ 上的 SSTables 子集，而不需要合并所有 L0L_0L0​ 和所有 L1L_1L1​。我们称之为 Column Compaction。 在 Compactor 中，KV 对由逻辑上的列来组织管理，一列就是一个具有一定数量的 Key 的子集，也就是 Column Compaction 的基本单元。具体而言，来自不同 RowTables 的 KV 键值对在一个列对应的键范围内形成了一个逻辑上的列。 这些位于一列里的 KV 对的数量即为一个列的大小，因此一个列的大小不是固定的，但是会有一个由 Column Compaction 控制的阈值大小。 Space Management：在一个列的数据被压缩之后，对应的占据的 NVM 的空间将会被释放。我们使用了页算法来管理这些空闲的空间。由于列压缩会旋转键范围，因此每个行表最多只分段一个页面。列压缩后完全释放的 NVM 页面将作为一组页面大小的单元添加到空闲列表中。为了在 Receiver 中存储传入的 RowTable，我们应用空闲列表中的空闲页面。 8GB 的 matrix container 包含 2112^{11}211 个 4K 大小的页，每一个页由页号（无符号整数）标识，为每个列表元素添加 8 字节的指针，每个页面的元数据大小即为 12 字节。因此 NVM 上空闲列表最多占据 24KB。 Column Compaction 该压缩过程是指 L0L_0L0​ 到 L1L_1L1​ 的一种细粒度压缩方式。因此减少了 L0L_0L0​ 到 L1L_1L1​ 压缩过程中的数据量，从而减少写停顿。该压缩过程大致如下： MatrixKV 将 L1L_1L1​ 的键空间分隔为多个连续的键范围：因为 L1L_1L1​ 的 SSTables 有序，每一个 SSTable 都有对应的最小和最大键，所有的 SSTables 的最小键和最大键形成一个有序的键列表，每两个相邻的键代表一个键范围，就能得到多个连续的键范围。 Column Compaction 从上述步骤的第一个键范围开始，即选择一个 L1L_1L1​ 中的键范围作为要压缩的键范围。 在 Compactor 中，对应压缩键范围的 KV 对被并发地从多个 Rows 中选出来。假设由 N 个 RowTables，K 个线程用于并行地取对应范围的键，那么每个线程负责 N/K 个RowTables，我们为了保证足够的并发访问，用 8 个线程去 NVM 上操作。 如果取到的数据个数没有达到压缩的阈值下界，那么将会加入第二个键范围，对应的 K 个线程将继续在 N 个 Row 中寻找对应的 KV 对，直到取到的键值对数目位于压缩的阈值下界和上届之间时 （即 12AF∗Ssst\\frac{1}{2} AF*S_{sst}21​AF∗Ssst​ 到 AF∗SsstAF*S_{sst}AF∗Ssst​ 之间）。这两个边界值保证了 Column Compaction 合理的开销 上述步骤选择出的 KV 对在 Compactor 中形成一个逻辑的列 列中的数据和对应的具有重叠键范围的 L1L_1L1​ 层的 SSTables 在内存中进行合并 最终，合并后的 SSTables 被写回 L1L_1L1​ 层 Column Compaction 继续执行，选择下一个键范围，下一列。Column Compaction 选择键范围将形成一个环，旋转一直执行，从而保证平衡。 如图所示：首先选择了 0-3 的 SSTable 范围，然后搜索 RowTable 元数据找到对应的键，个数低于下界，即不足以执行一次压缩，那么继续选择一个键范围，比如 3-5，那么合并之后就是 0-5，如果还是未到下界，继续扩大，再选择 5-8，此时达到阈值 10，最终选择 0-8。此时就形成了逻辑上的一列。 Reducing LSM-tree Depth 传统的 LSM-trees 对应的 AF 一般为 10，其深度会随着数据量的增长而不断增大，写放大系数对应的可以表示为 WA=N∗AFWA=N*AFWA=N∗AF，其中 N 为 LSM-trees 的层级数。因此 Matrix 的另一个设计思路就是通过减小 LSM-tree 的深度来减小写放大。MatrixKV 通过固定比例增加每一层的大小限制，使相邻层的 AF 保持不变来减少 LSM-tree 的层数。 但是这样单层扩大带来了两个负面影响： 扩大后的 L0L_0L0​ 与 L1L_1L1​ 有更多的重叠键范围，那么压缩过程中的数据量将增大，不会增加压缩的开销但是会延长写停顿的时间。 遍历更大的 L0L_0L0​ 会降低查询操作的效率。 通过使用 Column Compaction 能够较好地解决第一个负面影响，因为该压缩方式不会受到每一层的数据容量的影响 Matrix 提出了 cross-row hint search 来改善增大单层容量操作带来的查询开销 Cross-row Hint Search 在 MatrixKV 的 L0L_0L0​，每一个 RowTable 是有序的，不同的 RowTables 的键范围可能重叠。为每一个 table 构建布隆过滤器是减小查询开销的一个可能的方案，然而会引入构建布隆过滤器的开销而且不支持 range scan。为了提供更好的读和 scan 性能，我们构建了 cross-row hint searches Constructing cross-row hints：在 RowTable 的结构中我们为每一个元数据元素添加了前置指针。RowTable i 中的 key x，对应的前置指针指向了 RowTable i-1 的 key y，y 是第一个不小于 x 的 key。这些前向指针提供了在不同行中对所有键进行逻辑排序的提示，类似于分层级联。因为每个前置指针只记录上一个行表的数组索引，所以只占据 4bytes，因此开销很低。（疑问：存储开销是很低，但是具体为每一个元素维护指针不是应该还挺耗时的吗？） Search process in the matrix container：一个查询操作从最近最新的 RowTable i 开始，如果目标 key 不在对应的键范围内，则相应地去检索 RowTable i-1。在 RowTable 内部执行二分查找。使用前向指针，我们可以缩小在前面的 RowTables 中的搜索范围。因此查询和 scan 操作将无须遍历所有的 tables。该方式通过显著减少查询操作对应的 table 和元素的数量来提升了整体的读效率。 如图所示，假设我们检索 k=12 的 key，首先二分查找 RowTable 3，得到一个范围 10-23，然后根据前置指针 hint 找到 RowTable 2 中的 13-30，因为不包含 12，此时需要把 13 之前的 8 给纳入进来，变成 8-13-30。再进行二分查找，没找到对应的 key，8-13, 移动到 RowTable 1, 9-10-13，二分查找 10-13，无对应的 key，继续看 RowTable 0，11-12-14，二分查找找到了 12. Implementation MatrixKV 通过使用 PMDK 库来访问 NVMs，使用 POSIX API 来访问 SSDs。 Write： 来自用户的写请求首先插入了一个 WAL 日志到 NVMs 以保证崩溃一致性 数据在 DRAM 中被批量处理，形成 MemTable 和 Immutable Memtable （Change）Immutable Memtable 被刷入到 NVM 并以 RowTable 的形式保存在 matrix container 的 receiver 中 （Change）当 RowTables 的数量到达了大小限制，例如 matrix container 的百分之六十，且 compactor 为空，receiver 将变成 compactor （Change）Compactor 中的数据将和 SSTable 数据以 Column Compaction 的形式进行压缩，同时新的 receiver 将继续接受刷回的 Memtables SSDs 上的压缩还是和原有的 RocksDB 保持一致 Read：同 RocksDB 的读操作处理方式大致相同，顺序变为 DRAM &gt; NVM &gt; SSD。在 NVMs 中，cross-row hint 搜索有助于更快地在 L0 的不同 RowTables 之间进行搜索。通过在不同的存储设备中并发地搜索，可以进一步提高读取性能。 Consistency：NVM 中的数据结构必须避免由系统故障引起的不一致性。在 MatrixKV 中，对 NVM 的写或更新操作只发生在两个阶段，flush 和 column compaction。 对于 flush 操作，如果在写 RowTable 的过程中发生了错误，MatrixKV 可以重新处理被记录在 WAL 中的事务 对于 Column Compaction 操作，为了实现一致性和可靠性的低开销，Matrix 采用了 RocksDB 的版本机制，使用一个 mainfest 文件记录数据库的状态，compaction 的操作会被持久化到 mainfest 文件中，作为一个版本的变化。如果系统发生了故障，则使用版本号恢复到最近的一个一致的状态。MatrixKV 会添加 RowTables 的状态到 mainfest 文件中，例如第一个 key 对应的偏移，key 的数量，文件的大小，元数据的大小等。MatrixKV 使用延迟删除来保证在一个一致的新版本完成之前，由于列压缩而失效的陈旧列不会被删除。 Evaluation 测试环境 硬件： 2 Genuine Intel(R) 2.20GHz 24-core processors 32 GB of memory 800 GB Intel SSDSC2BB800G7 SSD 256 GB NVMs of two 128 GB Intel Optane DC PMM 软件： The kernel version is 64-bit Linux 4.13.9 OS：Fedora 27 对照组 RocksDB (including RocksDB-SSD and RocksDB-L0-NVM(8G) ) NoveLSM 8GB NVM MatrixKV 8 GB L0, SSD L1 8GB PebblesDB DRAM-NVM SILK DRAM-NVM 其他配置 RocksDB：64 MB MemTables/SSTables, 256 MB L1 size, and AF of 10. The default key-value sizes are 16 bytes and 4 KB 负载 db_bench: the micro-benchmark released with RocksDB. YCSB macro-benchmarks 测试结果 db_bench 写性能 随机写 RocksDB-SSD 和 RocksDB-L0-NVM 之间的对比表明：通过将 L0 放置在 NVM 上带来了大约 65% 的提升 Matrix 相比 RocksDB-L0-NVM 和 NoveLSM 吞吐量在所有的 value 大小都有提升，相比于 RocksDB-L0-NVM 提升了大约 1.86x 到 3.61x，相比于 NoveLSM 提升了 1.72x 到 2.61x 顺序写 因为顺序写不会导致压缩，所以四种存储的顺序写吞吐量都要比随机写的吞吐量要高。 RocksDB-SSD 性能更好是因为其他三种存储引擎都使用了 NVM，就多了一次 NVM 到 SSD 上的数据迁移开销 MatrixKV 优于 NoveLSM 是因为 缩小的 RowTable 的开销比在 NoveLSM 上的大的跳表更新的开销小。 db_bench 读性能 因为 NVM 中只保存了数据量百分之十的数据，所以化石 SSD 的读性能占据了更大的比重。四种方案总体表现接近，MatrixKV 优化之后并没有造成读性能降级甚至在顺序读上有一定的提升：因为 cross-row hint 搜索减少了大 L0 的检索开销，还拥有更小的层数，进一步使得查询的开销变小。 YCSB MatrixKV 在写密集的负载下提升效果明显 MatrixKV 在读密集的负载下保持了平均水平 MatrixKV 和 NoveLSM 在负载 D 下表现得最好是因为 latest 的分布，使得在 NVM 中就能命中很多数据 Tail latency 通过减少写停顿和写放大，MatrixKV 极大降低了尾延迟并提升了用户服务质量。 性能分析 写停顿 MatrixKV 使用了更少的时间处理对应的负载，因为随机写吞吐量更高 RocksDB 和 NoveLSM 都有严重的由于 L0L_0L0​ 到 L1L_1L1​ 之间的压缩造成的写停顿问题 MatrixKV 相比之下实现了最稳定的性能 写放大 MatrixKV 的写放大系数更小 MatrixKV Enabling Techniques Column compaction 的效率对比 Overall compaction efficiency： MatrixKV 压缩次数最少，因为树的层级降低 MatrixKV 中的所有压缩处理的数据量相近，因为我们减少了 L0-L1 上的压缩数据量，而没有增加其他级别上的压缩数据量。 NoveLSM and RocksDB-L0-NVM 比 RocksDB-SSD 压缩次数少的原因是 NoveLSM 使用了比较大的 Memtable 来处理写请求，吸收了一部分的 update 请求 RocksDB-L0-NVM 在 NVM 上存放了 8GB L0 数据，存储了更多的数据 NoveLSM 和 RocksDB 大量的压缩数据来自于 L0-L1 压缩。 Reducing LSM-tree depth：RocsksDB 和 MatrixKV 随着单层容量的增加都减小了写放大，但是对性能的影响有所不同，RocksDB 随着单层的增大性能反而降低，因为增大了 L0L_0L0​ 到 L1L_1L1​ 压缩的数据量。MatrixKV 使用了独立的压缩策略不受大小变化的影响才得以实现更高的性能。 Extended Comparisons on NVMs MatrixKV 不仅仅是因为 NVM 的引入使得性能变得很好，还有设计思路和方案的加持。故和其他使用了 NVM 的方案进行了对比： 通过对比发现，MatrixKV 的方案是很适合 NVM 的，相比于其他如 PebblesDB 把 NVM 作为块设备的方案。 Conclusion 提出了基于 LSM-trees 的稳定的、低延迟的 KV 存储方案 使用了多级存储结构 DRAM-NVM-SSD 在 NVM 上设计实现了 matrix container 来管理 L0L_0L0​ 层，并采用适当的粒度进行 Column Compaction 来减小写停顿 通过增大每一层的容量来减小树的高度，从而减小写放大 使用 cross-hint search 来保证原有的读性能 ","link":"https://blog.shunzi.tech/post/MatrixKV/"},{"title":"Series Six of Basic of Persistence -  Log-structured File System","content":" 本篇为持久化技术的基础篇系列第六篇（日志结构化文件系统），有对应的原论文 SOSP1991 - The design and implementation of a log-structured file system 可以参考 知乎: Logician - Log-structured File System Chapter Index Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System Motivation 内存容量不断变大：随着内存的变大，更多的数据可以被缓存在内存中，随着更多的数据被缓存，磁盘通信越来越多地由写组成，因为读是由缓存提供的。因此，文件系统性能在很大程度上取决于它的写性能。 随机 I/O 性能和顺序 I/O 性能之间存在很大的差距：硬盘传输带宽在过去几年里增加了很多，当更多的位被压缩到驱动器表面时，访问的带宽增加，然而，Seek 和旋转延迟成本已经缓慢下降，要让廉价的小型马达更快地旋转磁盘或更快地移动磁盘臂，这是一个挑战。所以 seek 和旋转操作相对较少的顺序读写会比随机读写性能好很多。 现有的文件系统在许多常见工作负载上执行得很差：FFS 会执行大量的写操作来创建一个大小为一个块的新文件（一次写 new inode，一次更新 inode bitmap，一次写该文件所在的目录对应的数据块，一次写目录对应的 inode，一次写文件的新数据块，一次更新数据块对应的位图）。因此，尽管FFS将所有这些块放在同一个块组中，但FFS会导致许多短寻和随后的旋转延迟，因此性能远远低于连续带宽的峰值 文件系统无法感知 RAID：RAID-4 RAID-5 都会有小写问题，逻辑上的一个对一个块的写操作会造成物理上四个实际的 I/O 发生，现有的文件系统无法避免 RAID 下的最坏的写情况。 因此理想的文件系统需要更加关注写性能，同时不仅需要支持较好的数据写，还要支持元数据的更新操作也比较高效。对于单盘和 RAID 都应该表现得比较好。所以出现了 LFS，LFS 首先将写操作缓冲到一个内存段 segment 中，当 segment 满了之后，顺序地将该段写入磁盘。LFS 从不更新已经存在得数据，及永远都是追加写的方式，所以每次都是将段写到磁盘的新位置，因为段很大，所以磁盘或 RAID 能够更加接近本身的带宽性能。 LSF CRUX: 如何让所有的写变成顺序写？对于读操作，因为可能读取磁盘的任何位置，所以要求读操作顺序很难，但写操作是可以保证顺序的。 Writing To Disk Sequentially 如何将所有对文件系统状态的更新转换为一系列对磁盘的连续写操作？ 例如写入数据块 D 到磁盘上的地址 A0 写数据的同时，还会有元数据的写入，譬如写入 inode 数据，inode 数据指向数据块。即顺序写入是想将对应的数据块和元数据都顺序地写入到磁盘。 Writing Sequentially And Effectively 直接顺序写入到磁盘并不能保证高效。例如，假设我们写入了一个单独的数据块到地址 A，在时刻 T，等了一段时间之后，需要顺序写到磁盘的 A+1，这时候的时刻为 T + t。在第一次写和第二次之间，磁盘旋转了，当发起第二次写操作的时候，在真正写入之前坑需要等待一个完整的旋转周期（具体的等待时间为 t - 旋转周期）。因此简单的顺序写入到磁盘是无法解决性能较差的问题的，所以必须发起一连串的连续的读写操作或者一个大的写操作，才能实现比较好的性能。 为了实现上述的连续写操作或者一个较大的写操作，LFS 引入了一个古老的技术 写缓冲（write buffering）。在写入磁盘之前，LFS 在内存中追踪对应的写操作，当到达一定数量的更新操作之后，再一次性地将这些写操作写入磁盘，从而保证写入磁盘的高效性。 而 LFS 中批量写入的单位就是所谓的 segment，即 LFS 将一些要写入到磁盘中的写操作首先缓冲到内存的 segement 中，然后一次性地写入该段到磁盘。如果这个段足够大，那么磁盘的写入也将足够高效。 如图所示例子中，LFS 将两组更新操作合并到一个小段中，实际的段可能比图中更大，以 MB 为单位。第一组更新为写数据到文件 j 的四个块，第二组操作是向文件 k 中新写入一个块，LFS 然后把这七个块一次性写入磁盘，磁盘上的数据结构如图所示： How Much To Buffer? LFS 在写入磁盘之前需要缓冲多少写操作？取决于磁盘本身，主要由磁盘的定位开销和传输速率决定。假设 seek 开销 TpositionT_{position}Tposition​，磁盘传输速率 RpeakR_{peak}Rpeak​ MB/S，写入数据 DDD MB，那么写入该数据块的总时间 TwriteT_{write}Twrite​ 为 $$T_{write}=T_{position}+ \\frac {D}{R_{peak}}$$ 因此写入效率 ReffectiveR_{effective}Reffective​ 即为 $$R_{effective} = \\frac {D}{T_{write}}= \\frac {D}{T_{position}+ \\frac {D}{R_{peak}}}$$ 我们想要实现的是 ReffectiveR_{effective}Reffective​ 尽可能地接近 peak rate，假设我们使用系数 FFF 来表示，Reffective=F∗RpeakR_{effective} = F *R_{peak}Reffective​=F∗Rpeak​，所以我们可以推导出 $$D = \\frac{F}{1-F} * R_{peak} * T_{position}$$ 假设 TpositionT_{position}Tposition​ 为 10ms，peak transfer 为 100MB/S，假设我们想实现原设备的百分之九十的带宽，FFF = 0.9，计算出 DDD = 9MB。为了实现 95% 的带宽，对应需要缓冲 19MB，99% 的带宽需要 99MB Problem: Finding Inodes 以前的文件系统中将 inode 数据组织成了数组并存放在了磁盘上固定的位置，可以通过起始地址和对应的inode 号计算得出 inode 的具体位置，FFS 通过在每个 group 中也能计算得出，但是 LFS 中寻找到 inode 很困难。 Solution Through Indirection: The Inode Map 引入数据结构 inode map，inode number 作为输入 key，value 输出则为对应的最新的 inode 地址。因此，可以想象它通常被实现为一个简单的数组，每个条目有4个字节(一个磁盘指针)。每当一个inode被写到磁盘时，imap就会更新它的新位置。 imap 需要持久化来保证磁盘崩溃后的恢复也能正常进行，因此产生新的问题：持久化到磁盘的什么位置？如果持久化到磁盘的一个固定位置，因为 imap 可能会被频繁更新，性能上可能会严重下降。 因此 LFS 将 imap 放在了写入的所有新数据的最右侧，因此在向文件 k 中追加写入一个数据块时，LFS 实际会写入新的数据块、其对应的 inode 和 imap 的一部分数据到磁盘上。imap 告诉 LFS inode k 在对应的地址 A1 上，而 inode k 告诉 LFS 数据块 D 在对应的地址 A0 上。 Completing The Solution: The Checkpoint Region 尽管已经将 imap 持久化散布到了磁盘的各个部分，但是文件系统还是不知道如何去查询 imap 找到对应的文件。LFS 因此设置了一个确定的位置 checkpoint region (CR)，该区域包含 inode map 最近的部分，因此 inode map 可以通过首先读取 CR 查询到 inode map，CR 定期更新，大约 30s 更新一次，因此性能上不会有太大的影响。因此整个磁盘的结构将如图所示： Reading A File From Disk: A Recap 假设内存中无任何缓存数据，读操作的步骤大致如下: 首先读取 CR，从而获取到整个 inode map，将其缓存在内存中 根据文件的 inode 号和缓存的 inode map，很容易读取到对应的文件 inode 地址信息，需要读取到最新版本的地址 读取到文件的 inode 之后，和原有的文件系统一样，使用 inode 中存储的数据块指针对应地读取数据块 LFS 和普通的文件系统读取磁盘上的文件的开销应该是大致一样的，因为整个 imap 都会被缓存，因此 LFS 在读取期间所做的额外工作就是在 imap 中查找 inode 的地址。 What About Directories? 目录结构基本上与经典的 UNIX 文件系统相同，其中目录只是(名称、inode编号)映射的集合。例如创建一个文件时，LFS 需要写 inode，data，以及目录的 inode 和 data，结构大致如下，创建 /home/remzi/foo 最右边的 imap 将同时包含文件和目录的映射信息，因此访问文件 foo 时，首先查询缓存到内存中的 imap，找到目录对应 inode A3，从而找到目录对应的数据块 A2，读取数据发现 foo 对应了 inode number 为 k，再去 imap 中查询 k 对应的 inode A1，再找到数据块 A0，从而得到 foo 的数据。 inode map 还解决了递归更新问题，该问题存在于所有异地更新的文件系统中。因为每当 inode 更新，通常都是需要写入到一个新的位置，可能会导致对应的目录也发生更新，更严重的直接递归到根目录。使用了 inode map 就能避免这个问题，inode 的地址可能发生改变，但是改变不会反映到对应的目录中，当目录保持相同的名称到 inode-number映射时，会更新 imap 结构，而不会级联更新。 A New Problem: Garbage Collection 异地更新的策略会将最新版本的数据写入到新的位置，但是原本位置的老数据仍然分布在磁盘中，但此时已经是垃圾数据，假设现在有一个 inmuber 为 k 的文件，指向了单一的数据块 D0，现在更新数据块 D0，将会生成一个新的数据块 D0 和 inode，磁盘布局如下所示，省略了 imap 如果在原有的文件的基础上进行追加的方式写入新版本数据，布局如下：（version file system） LFS只保存文件的最新活动版本;因此（在后台），LFS 必须定期查找这些文件数据、索引节点和其他结构的旧版本，并清除它们。回收操作可以让磁盘上的数据块再一次 free，并用于之后顺序的读写。前面我们讨论了段的重要性，因为它们是LFS中支持对磁盘进行大规模写操作的机制，事实证明，它们也是有效清洁不可或缺的一部分。在磁盘上分配的空间之间混合了一些空闲孔的文件系统。写性能会显著下降，因为LFS将无法找到一个大的连续区域以高性能顺序写入磁盘。 LFS 回收器是逐段工作的，从而为后续的写入清理大块空间。基本的回收过程如下 LFS 回收器定期读入许多旧的(部分使用的)段，确定这些段中活动的块， 然后写出一组新的段，其中只有活动块，释放旧的用于编写。希望读入 M 个现有片段，将其内容压缩为 N 个新片段(其中N &lt; M)，然后把这 N 段写到磁盘的新位置。 然后释放旧的 M 段，供文件系统用于后续的写操作。 LFS 如何分辨段中哪些块是活动的，哪些是老旧无效的？以及多久清理一次？ Determining Block Liveness LFS 在每一个段中添加了一些额外的数据信息，用于描述数据块，LFS 包含每个数据块的 inode number（属于哪个文件） 和偏移（属于该文件的哪个块），这部分数据信息位于段头，称之为 Segment Summary Block。 例如地址 A 上的数据块 D，查看 segment summary block 并找到对应的 inode number N 和偏移量 T。查询 imap 发现 N 对应的文件 inode 的实际地址，并将该 inode 读取上来。最后使用偏移量 T 看这个 inode 中对应 T 位置的数据块的地址，如果该地址等于 A，那说明该块在使用中，如果不等于，则说明该块已经是垃圾数据。伪代码如下： (N, T) = SegmentSummary[A]; inode = Read(imap[N]); if (inode[T] == A) // block D is alive else // block D is garbage 如图所示：summray block 标识为 ss，记录了地址 A0 对应的 inode number 为 K，偏移量为 0，查询 imap 找 K 对应的地址，为 A1，读取 A1 对应的 inode，查看偏移量 0 对应的数据块地址，判断是否等于 A0 LFS 可以采用一些捷径来更有效地确定数据块的活性，比如当一个文件被标记为删除，LFS 增加对应的版本号并记录最新的版本号到 imap 中，通过比较磁盘上的版本号和imap中的版本号，LFS可以缩短上述较长的检查，从而避免额外的读取。 A Policy Question: Which Blocks To Clean, And When? 清理哪些块是许多学术研究重点讨论的问题。原始的 LFS Paper 中描述了一种方式来区分段的冷热，经常被覆盖写的段被称为热数据段，对于热段，最好的策略就是等待一段时间之后再进行清理，因为越来越多的块被覆盖写从而就可以被释放用于新的写。冷数据段是指还有少量的垃圾数据块，但是其他数据块还很稳定，因此作者建议更早地清理冷数据段，更晚地清理热数据段。但是该方法不够完美。 对于何时清理，大概有两种思路。一种是定期清理，一种是在达到一定的阈值之后磁盘必须这样做。 Crash Recovery And The Log 当 LFS 往磁盘上写的时候发生了系统故障如何处理？在正常的操作中，LFS 缓存了写操作到段中，然后将段刷入到磁盘。这些写操作都是以日志的形式组织的，例如段中的 CR 区域指向了段的头部和尾部，每一个段指向了下一个段。LFS 周期性地更新 CR 区域的数据，故障可能发生在写段的操作或者写 CR 区域的操作中。 为了保证 CR 区域的原子更新，LFS 实际上保存了两个 CR 区域，分别在磁盘的两端，交替地对他们进行写操作。LFS 还设计了一个协议用于指向 inode 映射和其他信息的最新指针更新 CR 操作。具体表现为：首先写 header，带时间戳，然后写 CR 的 body，最后写最后一个数据块的时候也带上时间戳。如果在写 CR 过程中系统挂掉了，LFS 可以检查对应的前后时间戳是否一致来判断是否发生了故障，LFS 将选择使用带一致的时间戳的 CR，从而实现 CR 的一致性更新。 因为 LFS 大约每 30s 写一次 CR，所以出现不一致的时候可能 CR 中的数据已经很老了，重启的时候，将会读取 CR 中一致的那一部分数据，从而进行恢复，但自一致状态之后的写操作将会丢失。为了改善该现象，LSF 通过使用数据库领域中常用的 roll forward 技术，基本思想是从最后一个检查点区域开始，找到日志的末端(包含在CR中)，然后使用它读取下一个片段，看看其中是否有任何有效的更新，如果对应的更新有效则相应的更新文件系统，从而恢复自上一个检查点以来写入的大量数据和元数据。Redo Log ","link":"https://blog.shunzi.tech/post/basic-of-persistence-six/"},{"title":"Series Five of Basic of Persistence -  Locality and The Fast File System","content":" 本篇为持久化技术的基础篇系列第五篇（局部性和 Fast File System），将以 FFS 为例，可以参考原版论文 A fast file system for UNIX Chapter Index Basic of Persistence Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System Locality and The Fast File System 最早的文件系统其数据结构设计如下。优势在于设计足够简单，也基本实现了文件系统的抽象：文件和目录的层级结构，但存在比较严重的性能问题，大约只能发挥磁盘带宽 2%。其根本原因在于这种文件系统把磁盘当作了随机访问的内存，数据的存放没有考虑到磁盘的特性，所以在定位数据时开销较大。（实际表现可能为数据块离元数据块较远，所以在读取元数据之后再读取数据块的话 seek 操作开销就很大） 除此以外，因为没有对空闲空间进行高效的管理，所以可能会造成碎片化，空闲空间列表指向的空闲空间可能散布整个磁盘，在分配文件时，它们只需要获取下一个空闲块，其结果是通过在磁盘上来回访问逻辑上连续的文件，从而显著降低性能。如图所示操作将会造成 E 散布磁盘，磁盘的访问无法充分利用顺序特性。 所以现在操作系统都会有相应的磁盘碎片整理工具，它们重新组织磁盘上的数据，以连续地放置文件，并为一个或几个相邻区域腾出空闲空间，移动数据，然后重写索引节点，以反映更改。 还有一个问题就是块大小，块太小将不利于数据的传输，因为需要进行多次的定位操作，但是可以最小化碎片化的程度。快太大，碎片化就更严重。 CRUX：如何组织磁盘上的数据提升性能？ FFS: Disk Awareness Is The Solution Fast File System (FFS) 基于磁盘感知的分配策略设计了文件系统。该文件系统开启了文件系统的新纪元，即对外仍然暴露相同的 API，只是在底层提供不同的实现。 Organizing Structure: The Cylinder Group FFS 将磁盘划分成了多个 Cylinder Groups。一个 Cylinder 对应磁盘上的一组磁道，N 个 Cylinders 组成一组。 现代文件系统对外暴露的是块的逻辑地址空间，并向客户端隐藏了实际的细节。因此现代文件系统将磁盘组织成了 block groups，每一个部分就是磁盘地址空间的一部分。 不管您把它们称为柱体组还是块组，这些组都是FFS用来提高性能的核心机制。重要的是，通过在同一组中放置两个文件，FFS可以确保逐个访问不会导致跨磁盘查找时间过长。为了使用这些组来存取数据，就需要把文件和目录都放在一个组中，并且该组内只有包含了足够多的信息才能确保高效。如图所示，一个组内需要包含相应的一些信息。 Super block，每一个组中保留了超级块的副本信息，是为了防止原有的超级块损坏后无法访问文件系统，使用副本可以恢复。 inode/data bitmap 用于追踪组内 inode 和 data 的分配情况 数据块和 vsfs 一样用于存储数据，且作为一个组的最大组成部分。 Policies: How To Allocate Files and Directories 核心思想：把相关的数据放在一起 目录的放置：找一个组，已分配的目录较少，有大量空闲 inodes。 文件的放置：确保数据块和 inode 在一个组中，同一个目录下的文件尽量放在一个目录中 假设每组 10 inodes 和 10 data blocks。现在需要放置三个目录 /, /a, /b 和四个文件 /a/c，/a/d，/a/e，/b/f。假设目录占据一个块，文件占据两个块： group inodes data 0 /--------- /--------- 1 acde------ accddee--- 2 bf-------- bff------- 3 ---------- ---------- 4 ---------- ---------- 5 ---------- ---------- 6 ---------- ---------- 7 ---------- ---------- 对比散步所有 inode 到所有组的分配策略，为了防止没有组的 inode table 被很快填满：对比之下，对文件/a/c、/a/d和/a/e的访问现在跨越三个组，而不是按FFS方法的一个组。 group inodes data 0 /--------- /--------- 1 a--------- a--------- 2 b--------- b--------- 3 c--------- cc-------- 4 d--------- dd-------- 5 e--------- ee-------- 6 f--------- ff-------- 7 ---------- ---------- 目录中的文件通常是一起访问的：想象一下，编译一堆文件，然后将它们链接到一个可执行文件中，正是因为存在基于命名空间的局部性，确保了相关的文件之间的访问是又好又快的，FFS 才能提高相应的性能。 Measuring File Locality 使用 SEER 来追踪和分析文件访问的距离。比如打开了文件 f，再打开 f 对应的路径长度为 0，因为是同一个文件，但假如位于同一个目录 dir 下的 f 和 g，先打开了 f 后打开了 g，那么访问距离就变成了 1，简而言之，就是寻找两个文件的相同父目录需要多少次操作。 从下图所示的结果发现，约有 7% 的文件访问操作是针对已经打开了的文件，约有 40% 的操作是针对同一个文件或者同一个目录下的文件的访问。因此 FFS 的局部性假设是有一定根据的。 除此以外从图中看出约有 25% （看 1 和 2 之间的纵轴差值）的数据访问对应的访问距离为 2，当用户以多层次的方式结构化了一组相关目录并在它们之间一致地跳转时，就会发生这种类型的局部性。比如 proj/src/foo.c 和 proj/obj/foo.o。FFS 不会捕获这种类型的局部性，因为这样的操作会造成更多的 seek。 作为对比，还引入了 Random trace。通过 SEER Trace 中已经存在的文件进行随机的选择来模拟的，并计算对应的距离和分布。所以在随机的负载下，就很少表现出局部性。但是，因为最终每个文件都共享一个共同的祖先(例如根)，所以存在一些局部性，因此 random 作为比较点很有用 The Large-File Exception 大文件可能直接一次就填满整个数据块组，以这种方式填充块组是不可取的，因为这会防止随后的“相关”文件被放置在这个块组中，从而可能会损害文件访问位置。所以有必要设置单独的策略。 大文件的分配策略就是 在第一个组中分配了一定数量的数据块之后，找到其他组继续分配该文件对应的数据块，可能会根据每个组的空间利用情况进行选择。 假设一个组 10 inodes, 40 data blocks，文件 /a 大小为 30 个块 但是这样散布之后可能会影响该大文件的访问性能，所以选择分片大小就显得十分重要，如果足够大，那就是大部分时间都在传输，少部分时间在 seek，是一种 amortization 策略。 假设定位时间为 10ms，磁盘传输速度为 40MB/S，如果目标是一半时间用于定位，一半时间用于传输，即发挥 50% 的磁盘峰值带宽，那么每 10ms 定位之后就进行 10ms 的传输，对应的可以传输大约 409.6KB 的数据。对应如果发挥 90% 的峰值带宽，大约为 3.69MB，99% 峰值带宽大约为 40.6 MB，对应的离峰值越近，数据块大小也就越大。 FFS 没有用这种方式来计算大文件在不同组之间的分布，而是采用了一个比较简单的方法，基于本身的 inode 的结构。每一个间接块，以及它所指向的所有块，都被放在不同的组中。4KB 大小的块，32 位地址，在该策略下就意味着 4MB 文件的 1024 个块会被放置到不同的组，唯一的例外是由直接指针指向的文件的第一个 48KB。 从图示结果中不难发现磁盘传输率提升的越来越快，因为磁盘制造商擅长将更多的位元塞入同一表面，但与seek相关的驱动器的机械方面(磁盘臂速度和旋转速度)改进得相当缓慢。这意味着，随着时间的推移，机械成本会变得相对更昂贵，因此，为了摊销上述成本，您必须在各个 seek 之间转移更多的数据。 A Few Other Things About FFS sub-blocks 小文件会带来一些磁盘内部的碎片现象，比如 2KB 的文件，使用 4KB 的块则其实是浪费了一半的空间。FFS 的设计者想到了一个简单的办法来解决，引入 sub-blocks，即文件系统可以分配给文件的一些较小的数据块。因此如果创建小文件，譬如 1KB 文件，将直接使用两个 sub-blocks 而不用浪费一个 4KB 的块大小。随着文件大小的增长，文件系统将持续分配 512 字节的块直到 4KB 的大小，在这时候，文件系统将分配一个 4KB 大小的块，并把小块数据复制到该大块中，然后释放掉小块的空间。 这个过程可能不够高效，因为引入了额外的工作，譬如为了复制的一些 I/O，FFS 为了避免这种行为，修改了 libc 库，该库将缓冲写操作，然后将写操作直接写到 4KB 大小的块中，因此在大多数场景中完全避免了对小块的特殊处理。 disk layout 在 SCSI 以及其他现代设备接口之前，磁盘是比较简易的并且要求主机 CPU 亲历亲为地控制磁盘的操作。在 FFS 中，如果将文件放在磁盘的连续扇区上，就会出现问题。如下图左图所示，在顺序读的过程中，首先读数据块 0，完成之后读取数据块 1，但是读数据快 1 的时候可能发现磁盘已经转动过了，即需要再等待额外的完整的旋转时延才能读取数据块 1. FFS 采用了如下右图所示方法，通过在连续的数据块之间间隔一个其他的块的方式，在经过磁盘头之前，FFS 有足够的时间请求下一个块。事实上，FFS 能够计算出具体的需要多少个块来间隔，这个技术被称之为 parameterization，因为 FFS 会计算出磁盘的具体性能参数，并使用这些参数来决定精确的交错布局方案。 这种方法可能也有问题，即性能上可能只能达到峰值的一半，因为扫描一个磁道上的所有数据的话必须旋转两次才可以全部访问到。但现代文件系统内部读取整个磁道，并将其缓冲在一个内部磁盘缓存中(由于这个原因通常称为磁道缓冲区)。即预取。而针对顺序读操作，则直接从对应的 track buffer 中缓存相应的数据。文件系统因此不再需要担心这些难以置信的底层细节。如果设计得当，抽象和高级接口可能是一件好事。 other FFS 是首批支持较长的文件名的文件系统之一，从而在文件系统中启用更具表达性的名称，而不是传统的固定大小的方法 FFS 引入了软链接 FFS 也引入了 rename 原子操作 引入了 advisory shared/exclusive locks 以方便地构建并发程序； 引入了 Quotas 机制，让管理员能更合理地对每个用户实施配额限制，包括具体到inode 和 data block 数目的限制。 ","link":"https://blog.shunzi.tech/post/basic-of-persistence-five/"},{"title":"Series Four of Basic of Persistence -  File System Implementation","content":" 本篇为持久化技术的基础篇系列第四篇（文件系统实现），将以 vsfs 为例。 Chapter Index Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System File System Implementation CRUX：如何实现一个简单的文件系统？ The Way To Think 文件系统如何管理其数据和元数据？ 数据结构 如何将系统调用映射成对相应的数据结构的访问？ 访问方法 Overall Organization 如何组织磁盘？将磁盘划分成 blocks，假设单个块大小为 4KB。划分之后块从 0 ~ N-1 开始寻址。假设我们有一个小磁盘，只有 64 blocks 如何将数据存放到这些块中？ 因为数据大部分都是用户实际有效的数据，所以我们划分出数据区域。即后 56 个区域为数据区。 还要跟踪每个文件的信息，即一组元数据。所以划分出相应的元数据区域。如图中所示的 inode table. 还需要追踪 inode 和 data 两个区域中哪些块已经被分配或者空闲，即分配结构，通常是位图。又需要区分为 inode bitmap 和 data bitmap 还剩了一个块，通常作为超级块。通常包括文件系统的相关信息，inodes 和数据块的数目，inode table 起始块地址，magic number等 所以当挂载一个文件系统的时候，操作系统会首先读取超级块信息，从而初始化各种变量并把文件系统目录树和实际的卷关联起来。当卷中的文件被访问时，才知道从磁盘的哪个位置去读取对应的数据。 File Organization: The Inode inode 全称 index node。每一个 inode 都有对应的 i-number，即前面章节提到的 low-level name。在 vsfs 中，有了 inode-number，就能计算出 inode 所在的位置。如上图所示例子中，使用了 5 blocks 共 20KB 的容量，假设每个 inode 大小 256B，那么将能容纳 80 个 inodes。inode 的起始地址为 12KB 由于磁盘不是字节寻址的，通常是由可寻址的扇区组成，每个扇区的大小通常为 512 bytes。inode 32 对应的地址可以计算出来为 20KB，因此在磁盘上则是 20 * 1024 / 512 = 40，即扇区号为 40 的扇区 。 inode 中存放的数据信息如下，我们又称元数据信息。 inode 设计中最重要的决策之一是如何引用数据块的位置。z最简单的方式就是用 inode 内部的指针去指向数据块，每个指针指向一个数据块，但该方法有一定的局限：即文件特别大，对应的数据块数量大于 inode 中允许的数据块指针数量。 The Multi-Level Index 为了引用更大的文件，文件系统必须引入新的数据结构。一个办法就是用一个特别的指针，我们称之为 indirect pointer，这个指针不指向包含用户数据的数据块，而是指向包含更多的指向数据块的指针的数据块。因此，一个 inode 通常有确定数量（12个）的 direct pointer 和一个 indirect pointer 当文件越来越大之后，一个 indirect block 将会从 data block 区域中分配，然后将 inode 中的 indirect pointer 指向该 indirect block。假设一个块大小为 4KB，地址为 4B，那么一个块可以容纳 1024 个指针，那么这个文件最大即为 1024+12=1036 个块大小 如果为了支持更大的文件，即使用 double indirect pointer，如果更大，即 triple indirect pointer。继续加索引，也就是所谓的多级索引。double indirect pointer 即对应 （12 + 1024 + 1024*1024）个块大小的文件，此时已经超过 4G Linux ext2/ext3, NetApp’s WAFL 以及 UNIX 原始的文件系统都使用了多级索引；而 SGI XFS，Linux ext4 则使用的是 Extent 机制来代替多级索引。 一个 Extent 是指一个磁盘指针加上一个数据块的长度来表示，而不用对文件内的每一个数据块都维护指针，整个文件就只需要一个指针和一个长度从而定位到具体的地址。但是一个单独的 Extent 有很大的局限，因为可能很难找到一个足够大的且连续的空间，因此基于 Extent 的文件系统通常允许多个 Extent，从而在分配空间时就提供了更多的自由。 相比于多级索引，多级索引更加灵活，但是可能会使用更多的元数据，即元数据可能相对更大，但是 Extent 这种方式相对更加死板，但是占用空间更少。特别是，当磁盘上有足够的空闲空间并且文件可以连续布局时(这实际上是任何文件分配策略的目标)，基于 Extent 的方法显得更好。 研究人员发现大多数文件都很小，这种不平衡的设计反映了这样一个现实;如果大多数文件确实很小，那么在这种情况下进行优化是有意义的 Other 设计 inodes 的另一个办法是使用链表。inode 中不用存储多个指针，只需要存储文件的第一个块对应的指针，如果是比较大的文件，再加一个指针指向最后一个数据块。但这种链式的文件分配方法有时候可能性能较差，考虑读文件的最后一个块的时候，或者做随机的访问的时候。 为了让这种链式分配的方式更高效，很多系统会维护一个 link information 表在内存中，而不是用数据块本身存储下一个指针，该表由数据块 D 的地址索引，对应的 entry 内容则为 D 的下一个指针。，即文件中 D 之后的数据块的地址，也可能存在空值(表示文件结束)，或者其他一些标记，表示特定的块是空闲的。拥有这样一个下一个指针表使得链接分配方案可以有效地进行随机文件访问，只需首先扫描(在内存中)表中找到所需的块，然后直接访问(在磁盘上)它。 file allocation table (FAT file system) 就是这样实现的，和标准的 UNIX 文件系统也有很大的不同。它本身没有 inode 节点，而是存储关于文件的元数据并直接引用该文件的第一个块的目录条目，这使得不可能创建硬链接 Directory Organization 在许多文件系统中，目录都是一个比较简单的结构。一个目录通常包含一组数据对（entry name, inode number）。在指定目录下的每一个文件和目录，在目录对应的数据块中都存放着对应的字符串和数字。对于字符串可能还会存储对应字符串的长度。 例如 dir 中存储着 foo,bar,foobar_is_a_pretty_longname 三个文件，对应的 inode-number 为 12，13，24。dir 上的磁盘数据大致如下： inode number, record length (the total bytes for the name plus any left over space), string length (the actual length of the name), and finally the name of the entry. inum reclen strlen name 5 12 2 . 2 12 3 .. 12 12 4 foo 13 12 4 bar 24 36 28 foobar_is_a_pretty_longname 删除操作可能会造成目录所对应的数据块中间为空白，因此需要标记以下这部分空白的空间，例如使用保留的 inode number 来表示。为什么要使用记录长度 record length？就是为了在删除操作之后，新的数据对信息可以重用被删除的记录对应的空间，但是可能需要record length 来计算是否还有额外的空间剩余。 文件系统把目录当成一种特殊的文件，不是常规的文件，会对应的 inode 存取的数据中的 type 类型设置为 directory。目录有索引节点指向的数据块(可能还有间接块);这些数据块位于简单文件系统的数据块区域中。因此，我们的磁盘结构保持不变。 需要注意的是对于目录数据的存取，简单的线性表不是唯一的办法，任何其他的数据结构也是可能的。比如，XFS 使用 B-tree 存储目录，使得文件的创建操作（创建之前需要确保该文件名没有被使用）比简单的链表更快，因为链表必须遍历一次才能执行创建。 Free Space Management 文件系统必须跟踪哪些inode和数据块是空闲的，以便在分配新文件或目录时，能够为其找到空间。因此，自由空间管理对所有文件系统都很重要。在vsfs中，这个任务有两个简单的位图。 创建文件之前首先需要分配一个 inode，因此首先看 inode bitmap 中是否有空闲的 inode 可供分配，分配之后则标记 inode 被使用过了，然后更新磁盘上的位图。数据块操作同理。 Linux 文件系统 ext2/ext3 在创建文件时，将要寻找一组空闲的块序列（通常为 8 个块），找到之后分配给新创建的文件，文件系统保证文件的一部分在磁盘上是连续的，从而提高了性能。这种 pre-allocation 策略在数据块分配时是一种常用的分配方式。 空闲空间的管理方式有很多种，位图就是其中一种，早期的使用 free lists 的文件系统则是用一个在超级块中的指针指向第一个空闲块，而空闲块中又保存了下一个空闲块的指针，从而形成一个空闲块的列表。当使用空闲块的时候，相应的更新指针。 XFS 使用 B-tree 来简洁地标识磁盘的哪些块是空闲的 Access Paths: Reading and Writing 举例之前，首先假设文件系统已经挂在，且超级块信息已经读取到内存，其他数据都还在磁盘上。 Reading A File From Disk 例子：打开一个文件 /foo/bar，读取数据，然后关闭该句柄。假设文件大小为 12KB，3 个块大小。 当进行 open 操作的时候，文件系统首先需要找到文件 bar 的 inode，即获取到该文件对应的基础信息，包括权限信息和文件大小等。但是现有的参数时文件对应的绝对路径，所以需要遍历的路径才能找到对应的信息。 首先读取根目录 / ，即获取根目录对应的额 inode，获取 inode 需要知道对应 i-number，通常需要从其父目录中才能获取到对应 i-number，但是根目录没有父目录，所以根目录的 i-number 就是一个众所周知的值，因为文件系统必须知道文件系统挂载在哪里以及挂载到了什么地方。在大多数 UNIX 文件系统中，根目录对应的 inode number 是 2，因此首先读取 i-number 号为 2 的 inode 数据，即第一个 inode block。 一旦读取了根目录的 inode，文件系统就能找到该 inode 对应的数据块信息，即根目录所包含的文件条目。因此FS 将使用这些磁盘上的指针来读取目录，在本例中查找 foo 的条目，读取了多个条目之后找到了该目录，然后发现了该目录对应的 i-number，假设为 44，然后继续 下一步是递归地遍历路径名，直到找到所需的 inode。本例中继续在 foo 中找到 bar 的 inode number。open 的最后一步操作就是将 bar 的 inode 读取到内存中，然后文件系统做最后的权限检查，并为当前进程中的 open file table 分配一个文件描述符，然后返回给用户。 open 之后发起读操作，第一次读会读文件的第一个数据块，所以首先需要访问 inode 中查看第一个数据块的位置，与此同时，可能会更新 inode 中存储的最近访问时间，读取操作之后可能还会更新内存中的 open-file table 对应的额该文件描述符的信息，因为需要更新文件对应的偏移量以便读取下一个数据块。 需要注意的是，open 操作引发的 read 的数量和路径长度成比例，因为每多一个目录，我们就需要额外的访问一次该目录对应的元数据和数据，如果是大目录的话，可能会更糟。示例中我们都只需要通过读取一个块就能获取到该目录的全部数据信息，如果是大目录，可能需要读取大量数据块来才能找到对应的数据条目。 Writing A File To Disk 写文件流程和读文件类似，就是把对应的读操作换成写操作。但是和读文件不同的是，写文件需要分配对应的块，避免原有的数据被覆盖，每一次写不仅需要写数据到磁盘，还要决定分配哪个块给文件，因此还要更新其他数据结构，如 bitmap 和 inode。因此一次写操作可能会产生至少 5 次 I/O： 读 data bitmap，寻找空闲的 data block 写 data bitmap，分配之后更新对应的 bitmap 对 inode 的读写，即更新新的数据块信息到 inode 最后写实际的数据块 如果是创建文件的操作，可能更为复杂。创建操作不仅要分配 inode，还要将该文件所在的目录所包含的空间进行分配，I/O 流量将特别高： 读取 inode bitmap 寻找空闲的 inode block 写 inode bitmap，分配之后对应更新 bitmap 对应的更新自己的 inode 和所在目录的 data（父目录保存了 i-number 和文件名对应的数据对） 对自己父目录的 inode 的读取和更新 如果目录需要增长以容纳新创建的文件的话，则需要分配新的块，将会导致更多的 I/O CRUX：文件系统如何以合理的效率完成这些任务呢？如何减少文件系统的 I/O 次数？ Caching and Buffering 为了解决这一明显的巨大性能问题，大多数文件系统积极使用系统内存(DRAM)来缓存重要的块。 早期的文件系统引入了固定大小的缓存来保存流行的块，正如我们对虚拟内存的讨论一样，LRU 等策略和不同变体将决定在缓存中保留哪些块。这个固定大小的缓存通常在引导时被分配为占总内存的 10% 然而，这种内存的静态分区是很浪费的;如果文件系统在某个时间点不需要10%的内存怎么办? 使用上面描述的固定大小的方法，文件缓存中未使用的页面不能被重新用于其他用途，就是一种浪费 相比之下，现代系统采用 动态分区 方法。特别是，许多现代操作系统将虚拟内存页面和文件系统页面集成到统一的页面缓存中。通过这种方式，可以在虚拟内存和文件系统之间更灵活地分配内存，具体取决于在给定的时间内需要多少的内存。 静态分配只将固定的资源分配一次，例如，如果内存有两个可能的用户，您可以将一部分固定的内存分配给一个用户，将其余部分分配给另一个用户 动态方法更加灵活，它会随着时间的推移而给出不同数量的资源。例如，一个用户可能在一段时间内获得较高百分比的磁盘带宽，但随后，系统可能会切换并决定给另一个用户更大比例的可用磁盘带宽。 每种方法都有其优点。静态分区确保每个用户获得资源的一部分，通常提供更可预测的性能，而且通常更容易实现。动态分区可以实现更好的利用率(通过让资源紧缺的用户使用其他空闲资源)，但是实现起来更复杂，并且可能导致用户的性能下降，这些用户的空闲资源被其他人使用，然后需要很长时间才能回收。通常情况下，没有最好的方法;相反，您应该考虑手头的问题并决定哪种方法最合适。 让我们再考虑一下缓存对写操作的影响。虽然读 I/O 可以通过足够大的缓存完全避免，但写流量必须进入磁盘才能持久化。因此，缓存对写流量的作用与对读流量的作用不同。 首先，通过延迟写操作，文件系统可以将一些更新批处理到更小的 I/Os 集合中。比如 如果在创建一个文件时更新inode 位图，然后在稍后创建另一个文件时更新，则文件系统通过在第一次更新之后延迟写操作来节省一次 I/O。 其次，通过在内存中缓冲大量的写操作，系统可以调度后续的 I/Os，从而提高性能 最后，有些写操作通过延迟完全避免了。例如，如果应用程序创建了一个文件，然后删除了它，那么延迟写操作以将文件创建反映到磁盘可以完全避免这些操作。在这种情况下，惰性(将块写入磁盘)是一种优点 大多数现代文件系统都会将写操作缓存在内存中大约 5~30s，也是一种 trade-off。如果系统在写操作落盘之前挂掉的话，写操作肯定会丢失，但是如果保存在内存越久，性能就能通过批量写、延迟写、调度等策略变得更好。本质是 DURABILITY/PERFORMANCE TRADE-OFF 存储系统常常需要做 持久性和性能之间的 trade-off。如果用户希望写入的数据能够立即持久，则系统必须将新写入的数据提交到磁盘，因此写入速度较慢(但安全)。但是，如果用户可以容忍丢失少量数据，系统可以在内存中缓冲写入一段时间，然后将它们写入磁盘(在后台)。这样做可以使写入看起来很快完成，从而提高感知性能;但是，如果发生崩溃，还没有提交到磁盘的写就会丢失，因此需要进行权衡。 需要结合实际的场景中对于持久化的需求再做权衡。 数据库系统常常为了避免不必要的数据丢失就强制写操作刷入磁盘，通过使用 fsync，缓存模式时使用 diret I/O 或者直接使用裸设备接口避免文件系统的缓存。虽然大多数应用程序都需要文件系统进行权衡，但是如果默认值不理想，有足够的控制可以让系统执行您希望它执行的操作。 Homework 模拟文件系统的状态变化 vsfs.py ","link":"https://blog.shunzi.tech/post/basic-of-persistence-four/"},{"title":"Series Three of Basic of Persistence -  Files and Directories","content":" 本篇为持久化技术的基础篇系列第三篇（文件与目录）。 Chapter Index Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System Files and Directories CRUX：如何管理持久化存储设备？ 操作系统如何管理持久化存储设备 APIs 是什么样的 实现的重要方面有哪些？ Files and Directories Process 是 CPU 的虚拟化表现形式，Address Space 是内存的虚拟化表现形式，而存储的虚拟化表现形式即为 Files and Directories. 文件 只是一个线性字节数组，每个字节都可以读写。文件通常会有一个 low-level 的名称，没有显示地暴露给用户，通常是指 inode number. 操作系统不知道文件的具体结构，而文件系统的作用就是保证你读取数据时和你当初存取时的数据完全一样，实现起来看似容易实则复杂。 目录 也会有一个 low-level 的名称，如 inode number。但是相比于文件，目录的内容更为具体，即一些 low-level name 和用户可读 name 的数据对。比如，有一个文件 low-level name 为 “10”， user-readable name 为 “foo”，其所在的目录就将包含一个数据对信息 (“foo”, “10”)。目录中的每一个数据对都会有对应的文件或者子目录，从而形成一个目录树结构。 目录树起始于 root 目录（基于 UNIX 的系统的根目录通常为 /），使用某种分隔符来命名后续的子目录，直到指定所需的文件或目录为止。而文件名又由两部分组成，以 . 进行分割，前半部分为文件的名称，后半部分为文件的格式。 因此，在UNIX系统中，文件系统提供了一种统一的方式来访问磁盘、u盘、CD-ROM、许多其他设备上的文件，实际上还有许多其他东西上的文件，这些文件都位于单个目录树下 The File System Interface creating, accessing, and deleting files Creating Files 系统调用 open()，通过传递 O_CREAT 标识来创建文件。返回文件描述符。 int fd = open(&quot;foo&quot;, O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR); 参数解释： O_CREAT：如果文件不存在则创建文件 O_WRONLY：文件只能被写入 O_TRUNC：如果文件已经存在，则将其截断为零字节大小，从而删除任何现有内容 S_IRUSR|S_IWUSR：第三个参数指定权限，在本例中使文件所有者可读和可写 操作系统按进程管理文件描述符，一个文件描述符本质就是一个数字，每个进程都是私有的，在 UNIX 中用于访问文件。所以在进程对应的数据结构中，相应地保存了对应的文件描述符：最多保存 NOFILE 个描述符 struct proc { ... struct file *ofile[NOFILE]; // Open files ... }; Reading And Writing Files 博客园 - Linux strace追踪命令详解 读写命令示例： prompt&gt; echo hello &gt; foo prompt&gt; cat foo hello prompt&gt; 使用 strace 追踪命令 cat： 首先打开文件 open，只读模式 O_RDONLY，使用64位偏移量 O_LARGEFILE，返回了一个值为 3 的文件描述符。 为什么返回 3，不是返回 0 或 1？因为每个运行的进程已经打开了三个文件，分别是标准输入(进程可以读取以接收输入)、标准输出(进程可以写入以将信息转储到屏幕上)和标准错误(进程可以向其中写入错误消息)，分别对应了 0，1，2 read 读系统调用重复地从一个文件中读取一些字节：第一个参数对应文件描述符，表明读取哪一个文件，第二个参数执行了一个缓冲区，用于存放读取之后的数据，使用 strace 则显示了对应的读取结果；第三个参数对应缓冲区的大小 4KB，返回值为读取的数据的字节大写，即 hello 5 个字节加上 \\n 共 6 字节。 write 将读取到的字符串写到标准输出的文件描述符中，但可能不是直接调用 write 系统调用，可能使用了 printf 封装进行格式化输出，最终再调用了 write。 然后继续 read，但是因为文件没有额外的数据需要读取，返回 0 即告知程序数据该文件数据读取结束 close 关闭读取完了的文件描述符，至此整个文件读取完毕。 prompt&gt; strace cat foo ... open(&quot;foo&quot;, O_RDONLY|O_LARGEFILE) = 3 read(3, &quot;hello\\n&quot;, 4096) = 6 write(1, &quot;hello\\n&quot;, 6) = 6 hello read(3, &quot;&quot;, 4096) = 0 close(3) = 0 ... prompt&gt; 写操作采用了类似的步骤，首先打开相应的文件，然后调用写操作，大文件可能会重复执行写操作很多次，最后关闭该文件。可以使用 strace 相应的操作，也可以追踪自己编写的程序。 [root@ca11 shunzitest]# strace echo hello &gt; foo execve(&quot;/usr/bin/echo&quot;, [&quot;echo&quot;, &quot;hello&quot;], [/* 32 vars */]) = 0 brk(NULL) = 0x802000 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efd61a1e000 access(&quot;/etc/ld.so.preload&quot;, R_OK) = -1 ENOENT (No such file or directory) open(&quot;/etc/ld.so.cache&quot;, O_RDONLY|O_CLOEXEC) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=104858, ...}) = 0 mmap(NULL, 104858, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efd61a04000 close(3) = 0 open(&quot;/lib64/libc.so.6&quot;, O_RDONLY|O_CLOEXEC) = 3 read(3, &quot;\\177ELF\\2\\1\\1\\3\\0\\0\\0\\0\\0\\0\\0\\0\\3\\0&gt;\\0\\1\\0\\0\\0\\340$\\2\\0\\0\\0\\0\\0&quot;..., 832) = 832 fstat(3, {st_mode=S_IFREG|0755, st_size=2151672, ...}) = 0 mmap(NULL, 3981792, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7efd61431000 mprotect(0x7efd615f3000, 2097152, PROT_NONE) = 0 mmap(0x7efd617f3000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1c2000) = 0x7efd617f3000 mmap(0x7efd617f9000, 16864, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7efd617f9000 close(3) = 0 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efd61a03000 mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efd61a01000 arch_prctl(ARCH_SET_FS, 0x7efd61a01740) = 0 mprotect(0x7efd617f3000, 16384, PROT_READ) = 0 mprotect(0x606000, 4096, PROT_READ) = 0 mprotect(0x7efd61a1f000, 4096, PROT_READ) = 0 munmap(0x7efd61a04000, 104858) = 0 brk(NULL) = 0x802000 brk(0x823000) = 0x823000 brk(NULL) = 0x823000 open(&quot;/usr/lib/locale/locale-archive&quot;, O_RDONLY|O_CLOEXEC) = 3 fstat(3, {st_mode=S_IFREG|0644, st_size=106075056, ...}) = 0 mmap(NULL, 106075056, PROT_READ, MAP_PRIVATE, 3, 0) = 0x7efd5af07000 close(3) = 0 fstat(1, {st_mode=S_IFREG|0644, st_size=0, ...}) = 0 mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7efd61a1d000 write(1, &quot;hello\\n&quot;, 6) = 6 close(1) = 0 munmap(0x7efd61a1d000, 4096) = 0 close(2) = 0 exit_group(0) = ? +++ exited with 0 +++ OPEN FILE TABLE：每个进程维护一个文件描述符数组，每个描述符引用系统范围内打开的文件表中的一个条目。该表中的每个条目跟踪描述符引用的基础文件、当前偏移量和其他相关细节，比如文件是可读还是可写。 Reading And Writing, But Not Sequentially 目前讨论的读写都是顺序的，即必须从头到尾的读取或者写入文件。然后有的场景里，在指定偏移量上进行读写是很有必要的，例如索引查找，很可能是随机的。此时将使用到 lseek 参数一 fildes 文件描述符 参数二 offset 在文件中的偏移量 参数三 whence 决定 seek 如何进行 whence is SEEK_SET 参数 offset 即为新的读写位置 whence is SEEK_CUR 以目前的读写位置往后增加 offset 个位移量 whence is SEEK_END 将读写位置指向文件尾后再增加 offset 个位移量. off_t lseek(int fildes, off_t offset, int whence); 偏移量存储在进程对应的文件描述符的文件数据结构中： struct file { int ref; // 引用计数 char readable; // 可读 char writable; // 可写 struct inode *ip; // inode pointer uint off; // offset }; lseek vs seek：调用 lseek 操作不会执行磁盘的 seek 操作，lseek() 调用只是更改操作系统内存中的一个变量，该变量为特定进程跟踪下一个读或写偏移量的起始位置，磁盘的 seek 是当对磁盘发出的读或写不在与最近一次读或写在同一轨道上的时候。调用lseek()可能导致在即将进行的读或写中进行查找，但绝对不会导致任何磁盘I/O本身发生。 这些数据结构合在一起有时候又被称作 open file table，xv6 内核会以数组的形式进行维护，并有锁来控制并发访问。 struct { struct spinlock lock; struct file file[NFILE]; } ftable; 打开一个 300B 的文件，调用四次读操作，每次读 100B。不难发现 open 时，当前偏移量置为 0，每读一次后将移动指针到读取的字节大小处的偏移量，文件读取完毕之后，偏移量就不再移动了。 一个进程打开相同的文件两次，并对每一个文件描述符发起读操作，两次 open 操作分别分配了 3 和 4 的文件描述符，对应了 Open File Table （OFT） 中两个不同的项，entry 10 和 11。不难发现，每一个 entry 的偏移量都是自己独立移动的，互不干扰。 最后的例子中使用了 lseek() 在读操作之前来重新定位当前偏移量。lseek() 首先将偏移量设置为 200，然后读操作继续移动到 250。 Shared File Table Entries: fork() And dup() 在许多情况下(如上面的示例所示)，文件描述符到打开的文件表中的一个条目的映射是一对一的映射，例如，当进程运行时，它可能决定打开一个文件，读取它，然后关闭它;在这个例子中，文件在打开的文件表中有一个唯一的条目。即使其他进程同时读取相同的文件，每个进程在打开的文件表中都有自己的条目。这样的逻辑下，每个读写操作都是完全独立的，都有自己的偏移量。 但在某些情况下，Open FIle Table 中的数据信息是共享的。其中一种情况发生在父进程使用fork()创建子进程时，如下代码所示，父进程创建了子进程并等待他执行结束，然后子进程移动了偏移量之后退出，父进程等待完后直接输出当前偏移量。 int main(int argc, char *argv[]) { int fd = open(&quot;file.txt&quot;, O_RDONLY); assert(fd &gt;= 0); int rc = fork(); if (rc == 0) { rc = lseek(fd, 10, SEEK_SET); printf(&quot;child: offset %d\\n&quot;, rc); } else if (rc &gt; 0) { (void) wait(NULL); printf(&quot;parent: offset %d\\n&quot;, (int) lseek(fd, 0, SEEK_CUR)); } return 0; } 输出如下： prompt&gt; ./fork-seek child: offset 10 parent: offset 10 prompt&gt; 如图所示了连接每个进程的私有描述符数组、共享打开的文件表条目以及它对底层文件系统inode的引用的关系。此时就使用到了引用计数，当一个文件表项被共享的时候，引用计数相应地就会增加，知道所有进程都关闭了文件之后，该表项才会从 Open File Table 中移除。 在父级和子级之间共享打开的文件表条目有时是有用的。比如你创建了大量的进程来协同工作完成一个任务，不用额外的协同就可以写到同一个输出文件中。 除了 fork 以外，还有 dup() (以及 dup2(), dup3())。dup允许进程创建一个新的文件描述符，但与现有的文件描述符引用了相同的底层文件描述符。使用示例如下。dup 操作在一些输出重定向的操作中是十分有用的，比如把标准输入重定向到文件中。 int main(int argc, char *argv[]) { int fd = open(&quot;README&quot;, O_RDONLY); assert(fd &gt;= 0); int fd2 = dup(fd); // now fd and fd2 can be used interchangeably return 0; } Writing Immediately With fsync() write 调用的实质是告诉文件系统，需要将某个数据在未来的某个时间点写入持久化存储中。文件系统出于性能的考虑会把写操作缓存在内存中一段时间，5s 或者 30s，之后才会把对应的写操作给实际写入到持久化设备中。从调用者的角度来看，写操作仿佛很快就完成了，但是如果机器在写操作完成，但未实际写入到磁盘的这一段时间内发生了故障，这段数据就将丢失。 然而上层应用中会有一些如 DBMS 一样的对数据一致性要求极高的应用，DBMS 中的故障恢复协议就常常需要强制将写操作刷回到磁盘。因此大多数文件系统对此提供了额外的支持，UNIX 中即为 fsync()，强制将脏数据刷回磁盘，知道所有数据落盘之后才会返回具体的值。 使用示例如下。但是这样的执行顺序也不能完全保证所有的操作如我们所期望的执行，在一些场景下，还需要 fsync 当前文件所在的目录，从而确保不仅该文件同步到了磁盘，文件所在的目录也同步到了磁盘，因为文件很多时候可能是新创建的，也是目录的一部分。 int fd = open(&quot;foo&quot;, O_CREAT|O_WRONLY|O_TRUNC, S_IRUSR|S_IWUSR); assert(fd &gt; -1); int rc = write(fd, buffer, size); assert(rc == size); rc = fsync(fd); assert(rc == 0); Renaming Files strace mv 我们不难发现该操作调用了 rename(char*old, char *new)，该系统调用是一个原子操作，因此，rename() 对于支持某些需要对文件状态进行原子更新的应用程序非常重要。 prompt&gt; mv foo bar 使用示例：该编辑器将数据写入到了一个临时文件中，并使用 fsync 强制同步，然后应用就能确保新的文件元数据和文件数据已经落盘，再把临时文件更名为原始的文件名，最后一步原子地将新文件交换到适当的位置，同时 删除文件的旧版本，从而实现原子文件更新。（注意这里写入的时新版本的文件数据，即读取了原有的数据并合并了相应的更新） int fd = open(&quot;foo.txt.tmp&quot;, O_WRONLY|O_CREAT|O_TRUNC, S_IRUSR|S_IWUSR); write(fd, buffer, size); // write out new version of file fsync(fd); close(fd); rename(&quot;foo.txt.tmp&quot;, &quot;foo.txt&quot;); // 会覆盖原有的 foo.txt Getting Information About Files 除了文件访问之外，我们希望文件系统保存关于它所存储的每个文件的相当数量的信息，即我们常说的元数据 metadata。可以使用系统调用 stat() 或者 fstat()。 struct stat { dev_t st_dev; // ID of device containing file ino_t st_ino; // inode number mode_t st_mode; // protection nlink_t st_nlink; // number of hard links uid_t st_uid; // user ID of owner gid_t st_gid; // group ID of owner dev_t st_rdev; // device ID (if special file) off_t st_size; // total size, in bytes blksize_t st_blksize; // blocksize for filesystem I/O blkcnt_t st_blocks; // number of blocks allocated time_t st_atime; // time of last access time_t st_mtime; // time of last modification time_t st_ctime; // time of last status change }; state 输出。每个文件系统通常将这种类型的信息保存在一个称为inode的结构中，inode看作由文件系统保存的持久数据结构，其中包含我们以下看到的信息，所有inode都驻留在磁盘上；活跃的文件的 inode 的副本通常缓存在内存中以加速访问。 prompt&gt; echo hello &gt; file prompt&gt; stat file File: ‘file’ Size: 6 Blocks: 8 IO Block: 4096 regular file Device: 811h/2065d Inode: 67158084 Links: 1 Access: (0640/-rw-r-----) Uid: (30686/remzi) Gid: (30686/remzi) Access: 2011-05-03 15:50:20.157594748 -0500 Modify: 2011-05-03 15:50:20.157594748 -0500 Change: 2011-05-03 15:50:20.157594748 -0500 Removing Files unlink() 只接受要删除的文件名称作为参数，删除成功后返回 0。 prompt&gt; strace rm foo ... unlink(&quot;foo&quot;) = 0 ... Making Directories 目录也允许做上述的大量操作，但是不能直接递对目录进行操作，因为目录本质是文件系统的元数据，由文件系统完全负责。 如下所示创建了一个空目录，但是对应了两条数据，一条指向他自己 .，一条指向它的父目录 .. prompt&gt; strace mkdir foo ... mkdir(&quot;foo&quot;, 0777) = 0 ... prompt&gt; prompt&gt; ls -a ./ ../ prompt&gt; ls -al total 8 drwxr-x--- 2 remzi remzi 6 Apr 30 16:17 ./ drwxr-x--- 26 remzi remzi 4096 Apr 30 16:17 ../ Reading Directories opendir(), readdir(), closedir() 使用示例： int main(int argc, char *argv[]) { DIR *dp = opendir(&quot;.&quot;); assert(dp != NULL); struct dirent *d; while ((d = readdir(dp)) != NULL) { printf(&quot;%lu %s\\n&quot;, (unsigned long) d-&gt;d_ino, d-&gt;d_name); } closedir(dp); return 0; } 每一个目录对应的数据结构信息：目录提供的信息很少(基本上，只是将名称映射到inode号，以及一些其他细节) struct dirent { char d_name[256]; // filename ino_t d_ino; // inode number off_t d_off; // offset to the next dirent unsigned short d_reclen; // length of this record unsigned char d_type; // type of file }; Deleting Directories rmdir() 要求目录为空。 Hard Links link() 系统调用使用两个参数，一个老的路径名和一个新的，当使用 link 时就是创建了一个引用了同一个文件的指针，命令 ln 就是做这样的操作 prompt&gt; echo hello &gt; file prompt&gt; cat file hello prompt&gt; ln file file2 prompt&gt; cat file2 hello link() 的工作方式是在创建链接的目录中创建另一个名称，并将其引用到相同的 inode 编号的原始文件，没有发生任何的数据拷贝，只是给原始文件多取了一个名字，inode 号完全相同。 prompt&gt; ls -i file file2 67158084 file 67158084 file2 prompt&gt; 所以在创建文件时，相当于做了两件事情。 首先，创建了一个 inode，包含了对应的文件信息； 然后，把一个命名给链接到了这个 inode，并把这个链接放在了一个目录下。 如果使用 unlink 删除上述例子中的任何一个文件名，对于另一个文件名的访问仍然不受任何影响。因为在 unlink 的时候，文件系统会检查对应 inode number 的引用计数，来看有多少个文件引用了当前 inode，使用 unlink 则相应地删除该 inode 上对应的一个链接，并对引用计数减一，只有当引用计数为 0 的时候，文件系统才会释放这个 inode 和相应的数据块信息，从而实现真正地删除文件。 prompt&gt; rm file removed ‘file’ prompt&gt; cat file2 hello prompt&gt; echo hello &gt; file prompt&gt; stat file ... Inode: 67158084 Links: 1 ... prompt&gt; ln file file2 prompt&gt; stat file ... Inode: 67158084 Links: 2 ... prompt&gt; stat file2 ... Inode: 67158084 Links: 2 ... prompt&gt; ln file2 file3 prompt&gt; stat file ... Inode: 67158084 Links: 3 ... prompt&gt; rm file prompt&gt; stat file2 ... Inode: 67158084 Links: 2 ... prompt&gt; rm file2 prompt&gt; stat file3 ... Inode: 6715808 Symbolic Links (Soft Links) 硬链接是有一定的限制的，因此产生了软连接（符号链接） 即你不能创建对目录的硬链接，因为这样做可能会造成一个环 不能创建不同磁盘分区之间硬链接，因为 inode 只针对于某一个特定的文件系统，无法跨文件系统 ln -s 示例： prompt&gt; echo hello &gt; file prompt&gt; ln -s file file2 prompt&gt; cat file2 hello 软链接表面看着与硬链接相似，实际完全不同。 第一个区别是符号链接实际上是不同类型的文件本身。常规文件、目录和符号链接是文件系统知道的三种类型. file2 为 4 字节的原因是符号链接的形成方式是将链接到的文件的路径名作为链接文件的数据。如果文件名更长，相应的链接大小就更大 prompt&gt; stat file ... regular file ... prompt&gt; stat file2 ... symbolic link ... prompt&gt; ls -al drwxr-x--- 2 remzi remzi 29 May 3 19:10 ./ drwxr-x--- 27 remzi remzi 4096 May 3 15:14 ../ -rw-r----- 1 remzi remzi 6 May 3 19:10 file lrwxrwxrwx 1 remzi remzi 4 May 3 19:10 file2 -&gt; file 因为软链接的形成方式，所以可能出现 dangling reference 虚引用的情况，即链接了个空气。 prompt&gt; echo hello &gt; file prompt&gt; ln -s file file2 prompt&gt; cat file2 hello prompt&gt; rm file prompt&gt; cat file2 cat: file2: No such file or directory 软链接的本质就是我们常用的 win 系统中的 快捷方式 Permission Bits And Access Control Lists 底层的操作系统使用各种技术在相互竞争的实体之间以安全的方式共享有限的物理资源，文件系统提供了存储资源的虚拟化，但和 CPU 以及内存的虚拟化的不同之处在于 文件很多时候在不同的用户或者进程之间是共享的，不总是私有的。 Permission bits permission bits：-rw-r--r--. 第一个- 表示为一个常规文件， d 为目录，l 为软连接。所以 rw-r--r-- 才是权限相关的位，表示对于每一个实体，谁能访问以及如何访问。由三组组成： 文件所有者可以对文件做什么 rw- 组中的其他人可以对文件做什么 r-- 任何人(有时称为其他人)可以做什么 r-- prompt&gt; ls -l foo.txt -rw-r--r-- 1 remzi wheel 0 Aug 24 16:29 foo.txt 文件的所有者可以使用 chmod 更改权限，比如：其中 6 = 可读（4）+ 可写（2），但是另外两个组都设置为 0，因此对应的权限即为 rw-------。7 = 可读（4）+ 可写（2）+ 执行（1） # 移除所有人访问该文件的权限 prompt&gt; chmod 600 foo.txt # 给所有人添加所有权限 prompt&gt; chmod 777 hello.sh ACL 常用于分布式文件系统的访问控制，即 Access Control List。比如 AFS 中： prompt&gt; fs listacl private Access list for private is Normal rights: system:administrators rlidwka remzi rlidwka Making And Mounting A File System 如何从许多底层文件系统组装一个完整的目录树？ 格式化文件系统并挂载，从而提供访问入口。 mkfs 命令用于格式化指定的文件系统，需要指定对应的需要格式化的设备。 格式化完成之后，需要借助mount命令将文件系统挂载某一个挂载点，从而对外提供该文件系统的目录树入口。如下例子中的 /home/users 即为新创建的文件系统的根目录 prompt&gt; mount -t ext3 /dev/sda1 /home/users prompt&gt; ls /home/users/ a b Homework 实现 Stat, List Files, Tail, Recusive Search ","link":"https://blog.shunzi.tech/post/basic-of-persistence-three/"},{"title":"Series Two of Basic of Persistence - Hard Disk Drives And RAID","content":" 本篇为持久化技术的基础篇系列第二篇（硬盘设备和磁盘阵列），即一些硬件设备基础知识以及基础的 IO 处理相关知识，可以结合之前的一篇博客 Flavor of IO。 Chapter Index Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System Hard Disk Drives CRUX：如何对磁盘进行数据的存储和访问操作？ 现代硬盘驱动器如何存储数据? 接口是什么? 数据实际上是如何布局和访问的? 磁盘调度如何提高性能? The Interface 磁盘驱动器通常由大量的扇区（512 字节大小的 block）组成，每一个扇区都能被读写，对应的被编号为 0 ~ n-1，编号本身就对应了扇区的地址。 支持多扇区操作。现代文件系统将一次读写 4KB 甚至更大的数据单元，然而，当更新磁盘时，驱动器制造商制造的唯一保证是一个 512 字节的写是原子的，所以如果写入过程中发生了故障，可能出现部分写成功。即 torn write. 磁盘驱动器中有一些不成文的规定或假设：通常可以假设访问驱动器地址空间中相邻的两个块比访问相隔很远的两个块要快，假设顺序访问驱动器的块比随机访问要快得多。后来大量的实验数据表明也确实如此。 Basic Geometry 关于机械硬盘的基本结构可以参考博客 [shunzi: 存储基本概念]，此处不再赘述。 更详细的讲解也可以参考 bilibili-清华大学-陆游游-存储技术基础-P2 Disk Technologies Single-track Latency: The Rotational Delay：假设全旋转延迟为 R，那么磁盘需要承受大约 R/2 延迟，但最坏的情况即原本位于扇区 5，需要定位到扇区 6，则需要承受 R 延迟。 Multiple Tracks: Seek Time：寻道时间。例如访问扇区 11，需要找到对应的磁道，也就是所谓的 seek 操作，同旋转一样，是开销最大的磁盘操作。Seek 又分为四个步骤： acceleration 加速移动阶段 coasting 全速移动阶段 deceleration 减速移动阶段 settling 停止阶段，即到了正确的磁道，大约 0.5 - 2ms 通常旋转和寻道会同时进行，所以总的 I/O 时间： seek + rotational delay + transfer track skew 用于减少顺序跨 track 边界读写的旋转时延。如图所示, block 23 和 24 之间间隔了两个扇区，即 track skew 为两个 blocks，为什么不直接 23 对应 24在不同轨道的同一位置呢？因为磁盘一直都在旋转，在 seek 的过程中，如果对应的话，24 已经旋转了，将会等待一圈完整的旋转时延，所以一般需要间隔一定的距离。 因为物理结构的原因，外部轨道的容量一般比内部轨道更大，称之为 multi-zoned disk drives。每一个轨道被划分为一组 zones，而外轨的 zone 包含的扇区数一般大于内轨的 zone。 现代磁盘驱动器都有对应的缓存。有时候也叫 track buffer。为小容量的内存，通常为 8-16MB。用于要存放到磁盘或者从磁盘读取的数据。 例如当读一个扇区时，驱动器可能会被对应磁道上的所有扇区的数据读取到缓存中，类似于预取。 对于写操作，驱动器可以选择：是否确认写操作已完成当数据写入到了磁盘缓存中？还是等磁盘完全写入磁盘之后再确认写入完成。前者称为 write-back; 后者成为 write-through. write-back 虽然性能更好效率更高，但是可能造成数据的丢失和不一致。 具体的延迟和性能计算请参考原文，此处不再赘述。 Disk Scheduling 由于 I/O 操作开销较大，如何组织发送到磁盘的 I/O 顺序就显得十分重要。和任务调度不同的是，I/O 调度可以预测的，即一个任务大致需要花多长时间，通过测量 seek 和旋转时延，就能计算出预计的 I/O 时延，然后通常选择时间花费最短的任务执行。 SSTF: Shortest Seek Time First 即选择离当前最近的磁道的 I/O 然后执行。如图所示，首先对 21 进行操作，然后再对 2 进行操作。对于操作系统而言，具体的磁盘组件是以块为单位呈现的，所以通常操作系统寻找最近的 block. 该方法很有可能造成 starvation 现象，即磁头一直在内部磁道移动，外部磁道很少被访问。 CRUX：如何避免 starvation？ Elevator (a.k.a. SCAN or C-SCAN) 在磁盘上来回移动，按顺序在轨道上处理请求，一次从内到外或者从外到内的扫描过程我们称之为 sweep。如果要访问的块已经在这次 sweep 扫描过了，那么需要等待下一次 sweep 才能真正执行。 F-SCAN：此操作将扫描期间传入的请求放置到一个队列中，以便稍后提供服务，这样做可以延迟更靠近的请求的服务，从而避免较远的请求的 starvation。 C-SCAN：算法只从外到内扫描，然后在外轨道上重置，重新开始。这样做对内部和外部轨道更公平，因为纯粹的来回扫描有利于中间轨道 SCAN 算法又称之为 电梯算法。 CRUX：如何设计一个算法同时考虑 seek 时延和 旋转时延？ SPTF: Shortest Positioning Time First 如图所示，磁头位于 30，需要考虑先处理 8 还是先处理 16。需要判断时延主要由 seek 还是 rotation 决定。如果 seek 时间远大于 rotation，则 SSTF 更优，但如果 seek 小于 rotation，那么移动到 8 会是更好的选择，因为移动到 16 的话，由于旋转方向的原因，可能需要等待 rotation 更长的时间。 在现代磁盘驱动器中，seek 和 rotation 的时延相当，因此 SPTF 通常是更有效的，然而在系统中实现十分复杂，因为操作系统对于磁头的位置以及磁道的边界很难获取到对应的信息。所以 SPTF 通常在驱动器内部执行。 Other Scheduling issues 现代磁盘可以容纳多个未完成的请求，并且本身具有复杂的内部调度器(可以精确地实现 SPTF，在磁盘控制器内部，所有相关细节都是可见的，包括磁头的确切位置))。因此，OS调度器通常选择它认为最好的几个请求(比如16个)，并将它们全部发送到磁盘; 磁盘然后使用其内部的头部位置和详细的轨道布局信息，以最好的(SPTF)顺序服务对应的请求。 磁盘调度器执行的另一个重要的相关任务是 I/O合并。假设一系列的请求读取块 33，8，34，磁盘调度器应该合并 33 和 34 为一个 两个块大小的数据请求，调度器执行的任何重新排序都将在合并后的请求上执行。合并在操作系统级别上特别重要，因为它可以减少发送到磁盘的请求数量，从而降低开销。 现代调度器解决的最后一个问题是:在向磁盘发出I/O之前，系统应该等待多长时间? 大致分为了两种方式： work-conserving：一有请求，磁盘就立即处理。即当存在请求的时候，磁盘从不会空闲。 non-work-conserving：一些预测磁盘调度的研究表明，有时候等待一段时间可能效率更高。因为可能会有后续的更好的请求，可以通过合并等方式优化整体的请求处理效率。但是等待多长时间，什么时候开始等待都是很棘手的问题。 Homework (Simulation) 编写 disk.py 来熟悉掌握磁盘是如何工作的。 Redundant Arrays of Inexpensive Disks (RAIDs) CRUX：How can we make a large, fast, and reliable storage system? RAID：使用多个磁盘构建存储系统，实现 large/fast/reliable 的需求。对外像是一个单独的磁盘或者可以读写的数据块，内部实现复杂，有多个磁盘，内存和多个处理器组成。 Interface And RAID Internals 当文件系统向 RAID 发起读写的时候，RAID 必须计算出访问哪一个磁盘，然后再在对应的磁盘上完成实际的请求处理。 RAID 系统通常构建为一个独立的硬件盒，使用标准接口连接主机(如SCSI或SATA) RAID 相当复杂， 一个微控制器，它运行固件来指导 RAID 的操作 易失内存 DRAM，在读取和写入数据块时缓冲数据块 （可选）非易失内存，用于缓冲区安全写入，甚至可能是执行奇偶校验计算的专门逻辑 Fault Model RAID 的目的是检测和恢复某些类型的磁盘故障，因此，准确地知道预期会出现哪些错误对于实现工作设计是至关重要的： fail-stop：一个磁盘只会处于两个状态的其中一个：working/failed。该模型下我们假设 RAID 控制器能够很快检测到故障磁盘。 How To Evaluate A RAID capacity：N 个 disks, 每个 disk 有 B 个 blocks。如果没有冗余配置，那么对于客户端而言容量应该为 N * B；但是如果使用了两副本，即镜像组，容量将变为 N * B / 2 reliability：即设计中能给容忍的磁盘错误是多少 performance：性能取决于 RAID 的工作负载 Different RAIDS [1] shunzi: 存储基本概念 [2] RAID技术超详细讲解 ","link":"https://blog.shunzi.tech/post/basic-of-persistence-two/"},{"title":"Series One of Basic of Persistence - I/O Devices","content":" 本篇为持久化技术的基础篇系列第一篇（I/O 设备），即一些硬件设备基础知识以及基础的 IO 处理相关知识，可以结合之前的一篇博客 Flavor of IO。 Chapter Index Basic of Persistence Flavor of IO I/O Devices Hard Disk Drives Files and Directories File Systems Implementation Locality and The Fast File System Crash Consistency: FSCK and Journaling Log-structured File System Basic of Persistence Before Persistence 原书中的章前对话中对 Persistence 的概念举例进行了通俗的解释：假设我们摘了很多桃子，并且我们想这些桃子能够保存的更久一点，但是水果的保存总会受天气温度等环境因素的影响，我们通常可能会将桃子晒成果干，或者做成 PIE 一样的能够保存的更久一点的形式。 数据信息同理，我们想要持久地保存数据，以防出现计算机宕机/磁盘故障/掉电等其他事件破坏了我们的数据。 I/O Devices 问题1：I/O 如何集成到系统中？ 问题2：I/O 机制是什么样的？ 问题3：如何让 I/O 变得更加高效？ System Architecture 如图所示的 传统意义上的系统架构： （物理结构和成本 决定了架构） CPU 和 Memory 通过内存总线相连 部分外设（高性能 I/O 设备）通过通用的 I/O 总线（许多系统中为 PCI）和内存总线相连。eg. 显卡 其他外设则通过外围总线（SCSI, SATA, USB）相连，通常为 disk, mice, keyborads 等设备 思想：性能要求高的设备，离 CPU 更近，总线成本也更高，不适宜接入大量外设 如图所示的 现代系统结构（Intel's Z270）：现代系统越来越多地使用专用芯片组和更快的点对点互连来提高性能。 CPU 同时直连 Memory 和 Graphics，保证其对高性能的需求 CPU 通过 Intel 专有的 DMI (Direct Media Interface) 连接 I/O 芯片，其他外设再通过不同的接口连接到 I/O Chip Disk 通过 ATA / SATA / eSATA 连接；然后 USB 相关设备通过 USB 连接；网络接口（网卡）以及高性能存储设备（如 NVMe SSD）通过 PCIe 连接 A Canonical Device and The Canonical Protocol Device 一个标准化的设备主要由两部分组成：硬件接口 和 内部结构 硬件接口：暴露给系统的其他部分，相当于一层对外提供了相应的程序接口的软件，需要借助硬件接口层控制硬件的具体操作。 内部结构：设备的这一部分是特定于实现的，负责实现设备呈现给系统的抽象。非常简单的设备将有一个或几个硬件芯片来实现其功能;更复杂的设备将包括一个简单的CPU、一些通用内存和其他特定于设备的芯片来完成它们的工作。如 RAID 控制器就由大量的 firmware 组成来实现相应的功能。 Protocol Interface 由三个寄存器组成： status：查看当前设备的状态 command：告诉设备执行某项任务 data：传递数据给设备，或者从设备中获取数据 交互过程示例如下： step 1: 通过重复地读取 STATUS 寄存器判断是否 Device 空闲，即轮询 Device step 2: 将此次需要处理的数据发送 DATA 寄存器，若当前设备为磁盘设备，大量的写操作需要传输数据块到数据寄存器中，倘若 CPU 参与了此次数据移动过程，此次过程被称为 PIO（Programmed I/O） step 3：将此次需要进行的指令发送到指令寄存器中，显示地告诉设备数据也已经准备就绪，可以开始执行对应的操作 step 4：等待设备完成操作，再次对设备进行轮询，重复 step 1 While (STATUS == BUSY) ; // wait until device is not busy Write data to DATA register Write command to COMMAND register (starts the device and executes the command) While (STATUS == BUSY) ; // wait until device is done with your request 该协议的优点在于可以正常工作以及足够简单，但是不够高效。因为轮询机制的存在，需要花费大量的 CPU 时钟来等待设备完成前一个任务。 CRUX：有什么比较好的方式可以检查设备状态，不用通过轮询，从而减小 CPU 开销？ Lowering CPU Overhead With Interrupts 使用中断机制代替轮询，当设备完成了相应任务之后，触发一个硬中断，CPU 再去对应地执行中断处理任务，等待任务完成的过程中，CPU 可以切换去处理其他任务。 中断机制允许计算和 I/O 操作同时进行，从而提升了利用率。如图所示，上图使用 polling 机制等待读取到存储在磁盘上的数据，下图所示则使用了中断机制，在等待上一个任务完成的过程中可以执行别的任务，上一个任务完成后再使用中断通知CPU，从而唤醒 Process 1 继续执行 但需要注意的是，中断机制不一定总是比轮询机制高效，因为中断机制引入了不同进程之间的切换操作，该操作也是有一定的开销的，针对通过比较短时间的轮询就能实现的操作往往会比中断机制更为高效。同时大量的中断操作可能导致操作系统出现活锁。 因此， 如果一个设备的处理速度比较快，通常轮询机制更为高效 如果处理速度较慢，通常中断机制更为高效 如果处理速度未知，时快时慢，通常使用混合机制来处理，即两阶段方式：轮询一段时间之后执行中断。 在网络传输过程中，也尽量不使用中断，因为针对那种大流量的数据传输，每一个数据包就进行一次中断，会出现活锁（即只处理中断，无法处理实际的操作和请求）。 另一个基于中断的优化是合并操作，在这种情况下，一个需要首先引起一个中断的设备在将中断发送给CPU之前会等待一点时间。在等待期间，其他请求可能很快就会完成，因此可以将多个中断合并为单个中断交付，从而降低中断处理的开销。当然，等待太久会增加请求的延迟。 More Efficient Data Movement With DMA 除了轮询机制占用 CPU 的资源以外，针对 PIO 过程，将大块数据传输到设备时，CPU又一次因一个相当琐碎的任务而负担过重。如图所示，Process 1 将要写部分数据到磁盘，发起 I/O 操作，首先将数据从内存拷贝到设备，如图中 c 所示，拷贝完成后开始向磁盘发起写。 CRUX：如何降低 PIO 过程中设备上的数据寄存器的传输操作给 CPU 带来的开销？ Solution DMA Direct Memory Access (DMA)：直接内存访问。DMA 引擎本质上是系统中的一个具体的设备，它可以在没有太多 CPU 干预的情况下协调设备和主内存之间的传输。 DMA 的工作原理如下：为了将数据传输到设备，操作系统需要通过程序的方式告诉 DMA 引擎数据在内存中的位置、要复制多少数据以及要将数据发送到哪个设备。告诉 DMA 之后，操作系统的任务就完成了，就可以切换去执行其他任务。DMA 开始工作，当完成相应的数据传输以后，发起一个中断，此时 OS 就知道传输已经完成了，将会继续执行之前挂起的任务。 如图所示，相比于无 DMA 的方案，数据拷贝的过程主要由 DMA 来完成，此时 CPU 可以去执行别的任务来提高利用率。 Methods Of Device Interaction CRUX: 硬件与设备之间如何进行交互的？ I/O instructions 最古老的方法（被IBM大型机使用多年），显示的 I/O instructions。这些指令为操作系统指定了将数据发送到特定设备寄存器的方法，因此允许构建上述协议。 例如 x86 机器上就会用 in/out 指令和设备进行通信，比如发送数据到指定设备，caller 需要指定一个存储该数据的寄存器，以及具体的端口来表明是哪个设备，才能真正地执行对应的操作。 I/O instructions 通常是高优先级的，因为通常是由 OS 发出相应的指令，因此 OS 也是唯一一个允许直接和设备通信的实体。假设所有程序都能读写磁盘，将会造成十分混乱的局面，每个程序都将能根据这个漏洞操纵整个机器。 Memory-mapped I/O 该方法让硬件寄存器能够如在内存中一样被直接访问。为了访问某一个寄存器，操作系统发起一个对相应地址的 load (to read) 或者 store (to write) 指令，然后硬件将相应的指令发送到对应的设备。 这两种方法都没有太大的优势。内存映射方法很好，因为不需要新的指令来支持它，但是这两种方法现在仍然在使用。 Fitting Into The OS: The Device Driver CRUX: 不同的设备可能使用了不同的接口，操作系统如何适配对应的设备，如何屏蔽设备指令的实现细节？ 解决该问题的核心思想为 抽象。在操作系统的最底层，必须要有一层软件，知道和硬件交互的具体细节，我们称之为设备驱动，设备交互的任何细节都被封装在其中。 以文件系统软件栈为例，应文件系统完全不知道自己使用了哪种类型的磁盘，因为文件系统通常只对通用块层发起对块的读写，由通用块层将读写请求路由到合适的设备驱动，再由设备驱动处理具体请求。 图中还表示了裸设备接口，专门提供给一些特殊的应用，如 fsck 或者磁盘分区工具，从而不使用文件抽象直接读写块。大多数系统都提供了这种接口以供一些低级别的存储管理应用使用。 这种抽象/封装的方式也有一定的坏处。即当底层设备拥有大量的特性的时候，为了提供通用的接口，可能导致无法充分利用其特性。例如 SCSI 设备本身是支持了大量的错误处理的，相比于 ATA/IDE，但是在通用块层的实现只能报告为 EIO (generic IO error) 随着时间的的推移，设备驱动的代码占据了整个 Linux 代码的 70%，由于设备驱动的复杂性和多样性，很多设备驱动代码是由业余人士贡献的，往往可能导致 Linux 内核 crash。 Case Study: A Simple IDE Disk Driver 如图所示，IDE 磁盘暴露的接口主要由四个寄存器组成：(这些寄存器通过读写具体的 I/O 地址从而变得可用，使用 in/out 指令) 控制寄存器 control 命令块寄存器 command block 状态寄存器 status 错误寄存器 error 假设设备已经初始化，与设备交互的基本协议如下： 等待驱动器准备好：读取状态寄存器 0x1F7 (轮询直到 READY 或者 NOT BUSY) 向命令寄存器写入参数 (0x1F2-0x1F6)：写入扇区计数（Sector Count），要访问的扇区对应的逻辑块地址（LBA of sectors to be accessed），驱动器编号（IDE 只允许两个设备，master = 0x00, slave = 0x10） 开始 I/O 操作 (0x1F7)：向命令寄存器发起读写，直接将 READ/WRITE 命令写入到命令寄存器。 数据传输（写操作）：直到驱动器 Status Ready 且驱动器请求数据，将数据写入写入到 Data Port. 处理中断：最简单的例子就是为每一个传输了的扇区处理一次中断，更为复杂的实现则是等整个传输过程结束之后处理中断。 错误处理：每一次操作之后，读取状态寄存器，如果为 ERROR 状态，对应地读取错误寄存器查看具体的错误原因。 该协议的大部分都可以 xv6 IDE Driver 的实现中找到。如下所示的代码中主要实现了四个功能： ide_rw：如果有任务挂起，则将请求排队，或者 ide_start_request 直接发送到磁盘。都需要等待请求完成，调用线程进入休眠状态。 ide start request: 发送请求到磁盘，in/out 指令被调用来对磁盘进行读写，一开始需要调用 ide_wait_ready() ide_wait_ready：确保设备在接受请求之前处于就绪状态。 ide_intr: 当中断发生时被调用，如果是读请求，则从设备中读取数据，并唤醒等待该 I/O 操作的进程继续执行。如果 I/O 队列中还有别的请求，则开始执行下一个 ide_start_request() static int ide_wait_ready() { while (((int r = inb(0x1f7)) &amp; IDE_BSY) || !(r &amp; IDE_DRDY)); // loop until drive isn’t busy } static void ide_start_request(struct buf *b) { ide_wait_ready(); outb(0x3f6, 0); // generate interrupt outb(0x1f2, 1); // how many sectors? outb(0x1f3, b-&gt;sector &amp; 0xff); // LBA goes here ... outb(0x1f4, (b-&gt;sector &gt;&gt; 8) &amp; 0xff); // ... and here outb(0x1f5, (b-&gt;sector &gt;&gt; 16) &amp; 0xff); // ... and here! outb(0x1f6, 0xe0 | ((b-&gt;dev&amp;1)&lt;&lt;4) | ((b-&gt;sector&gt;&gt;24)&amp;0x0f)); if(b-&gt;flags &amp; B_DIRTY){ outb(0x1f7, IDE_CMD_WRITE); // this is a WRITE outsl(0x1f0, b-&gt;data, 512/4); // transfer data too! } else { outb(0x1f7, IDE_CMD_READ); // this is a READ (no data) } } void ide_rw(struct buf *b) { acquire(&amp;ide_lock); for (struct buf **pp = &amp;ide_queue; *pp; pp=&amp;(*pp)-&gt;qnext); // walk queue *pp = b; // add request to end if (ide_queue == b) // if q is empty ide_start_request(b); // send req to disk while ((b-&gt;flags &amp; (B_VALID|B_DIRTY)) != B_VALID) sleep(b, &amp;ide_lock); // wait for completion release(&amp;ide_lock); } void ide_intr() { struct buf *b; acquire(&amp;ide_lock); if (!(b-&gt;flags &amp; B_DIRTY) &amp;&amp; ide_wait_ready() &gt;= 0) insl(0x1f0, b-&gt;data, 512/4); // if READ: get data b-&gt;flags |= B_VALID; b-&gt;flags &amp;= ˜B_DIRTY; wakeup(b); // wake waiting process if ((ide_queue = b-&gt;qnext) != 0) // start next request ide_start_request(ide_queue); // (if one exists) release(&amp;ide_lock); } ","link":"https://blog.shunzi.tech/post/basic-of-persistence-one/"},{"title":"存储系统中的一致性","content":" 源于最近的项目里涉及到的一致性问题，此处是指存储系统内部的数据一致性。 注意和 分布式系统中的一致性以及协同算法 进行区分，此处更多的是多层存储之间的一致性。 主要会先了解记录常见的存储系统中的数据一致性保证，如文件系统、数据库等；再分析项目中是否真正数据存在一致性问题。 文件系统中的一致性 前言 预备知识 知乎：计算机存储术语: 扇区，磁盘块，页 ByteLiu：The differences between a chunk, page, block, sector and bytes/bits within storage systems? 问题 Linux 系统中的 Page Cache 缓存文件数据，可能造成缓存数据和磁盘数据的不一致性。 一致性概念 操作系统中的文件 = 数据 + 元数据 文件一致性其实包含了两个方面：数据一致+元数据一致。 影响 不同的用户或者应用程序有不同的一致性需求，因为一致性往往是会对性能产生影响的。 例如，如果要求强一致性，一个IO操作处理之后，数据能够立马落盘，此时就需要跳过缓存直接同步写入磁盘。此时性能相比于有缓存的情况，将会有一定程度的下降。 需求 需要提供可由用户自定义的一致性方案，即向用户提供可控制一致性的接口。 系统内部自动保证一致性，某些用户或者应用程序对文件数据的一致性要求可能没有那么高，无需每次写入都实时调用相应的接口去保证文件缓存数据和磁盘上数据的一致性。 Linux 下的方案 提供特定接口，可调用接口来主动地保证文件一致性 系统中存在定期任务（表现形式为内核线程），周期性地同步文件系统中文件脏数据块 一致性实现 文件一致性接口 fsync(int fd)： 将fd代表的文件的脏数据和脏元数据全部刷新至磁盘中 fdatasync(int fd)： 将fd代表的文件的脏数据刷新至磁盘，同时对必要的元数据刷新至磁盘中，这里所说的必要的概念是指：对接下来访问文件有关键作用的信息，如文件大小，而文件修改时间等不属于必要信息 sync()： 则是对系统中所有的脏的文件数据元数据刷新至磁盘中 fsync 函数流程： fsync(int fd)(位于fs/sync.c中) ---&gt;do_fsync(fd, 0)（位于fs/sync.c中） ---&gt;vfs_fsync(file, datasync)（位于fs/sync.c中） ---&gt;vfs_fsync_range(file, 0, LLONG_MAX, datasync)（位于fs/sync.c中） ---&gt;filemap_write_and_wait_range(mapping,start, end)（位于mm/filemap.c中） ---&gt;__filemap_fdatawrite_range(mapping,lstart, lend,WB_SYNC_ALL)（位于mm/filemap.c中） ---&gt;filemap_fdatawait_range(mapping,lstart,lend)（位于mm/filemap.c中） ---&gt;ext2_fsync(struct file*file, int datasync)（针对ext2文件系统，位于fs/ext2/file.c） ---&gt;generic_file_fsync(file, datasync)（位于fs/libfs.c中） 大致流程：同步文件脏数据 -&gt; inode 加锁 -&gt; 同步文件元数据 -&gt; 释放 inode 锁 fdatasync 函数流程： fdatasync(intfd)(位于fs/sync.c中) ---&gt;do_fsync(fd, 1)（位于fs/sync.c中） ---&gt;vfs_fsync(file, datasync)（位于fs/sync.c中） ---&gt;vfs_fsync_range(file, 0, LLONG_MAX, datasync)（位于fs/sync.c中） ---&gt;filemap_write_and_wait_range(mapping, start, end)（位于mm/filemap.c中） ---&gt;__filemap_fdatawrite_range(mapping,lstart, lend,WB_SYNC_ALL)（位于mm/filemap.c中） ---&gt;filemap_fdatawait_range(mapping,lstart,lend)（位于mm/filemap.c中） ---&gt;ext2_fsync(struct file*file, int datasync)（针对ext2文件系统，位于fs/ext2/file.c） ---&gt;generic_file_fsync(file, datasync)（位于fs/libfs.c中） 与 fsync 的区别在于 参数datasync. fsync将其设置为0，而fdatasync的实现中将其设置为1。在函数generic_file_fsync(file, datasync)中会对该参数的设置做出判断。 若 inode 并没有被修改，即 I_DIRTY 没有被修改，此时无需同步inode； 若 inode 未被设置 I_DIRTY_DATASYNC 且 datasync 被设置，这意味着调用者只想在 I_DIRTY_DATASYNC 被设置时（inode 关键部分被修改，如文件大小信息）才去同步 inode，如果 I_DIRTY_DATASYNC 未被设置，也就意味着 inode 关键成员未作修改，此时无需同步，直接返回。 int generic_file_fsync(struct file *file,int datasync) { structwriteback_control wbc = { .sync_mode= WB_SYNC_ALL, .nr_to_write= 0, /* metadata-only; caller takes care of data */ }; struct inode*inode = file-&gt;f_mapping-&gt;host; int err; int ret; ret = sync_mapping_buffers(inode-&gt;i_mapping); if(!(inode-&gt;i_state &amp; I_DIRTY)) return ret; if (datasync&amp;&amp; !(inode-&gt;i_state &amp; I_DIRTY_DATASYNC)) return ret; err = sync_inode(inode, &amp;wbc); if (ret == 0) ret = err; return ret; } sync 被动一致性 被动一致性是指系统后台存在定期的任务刷新某些文件的脏数据以及元数据，且定期任务应该以内核线程的形式出现。 系统中存在一个管理线程和多个刷新线程（每个持久存储设备对应一个刷新线程）。 管理线程监控设备上的脏页面情况，若设备一段时间内没有产生脏页面，就销毁设备上的刷新线程；若监测到设备上有脏页面需要回写且尚未为该设备创建刷新线程，那么创建刷新线程处理脏页面回写 刷新线程：负责将设备中的脏页面回写至持久存储设备中。 管理线程 主要工作是监视工作线程的运行状况，根据设备上的脏页面状况调整工作线程的运行，如设备上无脏页面且设备的工作线程已经有一段时间未被激活那么就kill该设备的回写线程，如果设备上有回写页面但尚未创建回写线程，那么为设备创建回写线程并启动线程运行。 主要流程： 遍历系统中所有的设备，判断设备当前状态，如果设备脏inode链表不为空或者设备任务队列不为空且该设备当前尚未创建回写线程，那么为设备创建回写线程； 如果设备当前脏inode链表为空且设备的回写线程已经有较长一段时间未活跃，那么就需要kill该设备的回写线程 static int bdi_forker_thread(void *ptr) { struct bdi_writeback *me = ptr; current-&gt;flags |= PF_FLUSHER | PF_SWAPWRITE; set_freezable(); /* * Our parent may run at a different priority, just set us to normal */ set_user_nice(current, 0); //线程运行在一个大的循环之中 for (;;) { struct task_struct *task = NULL; struct backing_dev_info *bdi; enum { NO_ACTION, /* Nothing to do */ FORK_THREAD, /* Fork bdi thread */ KILL_THREAD, /* Kill inactive bdi thread */ } action = NO_ACTION; /* 如果当前设备上也有脏的inode或者有回写任务，那么处理，但一般来说，控制线程对应的设备上并不会产生脏inode或者回写任务 */ if (wb_has_dirty_io(me) || !list_empty(&amp;me-&gt;bdi-&gt;work_list)) { del_timer(&amp;me-&gt;wakeup_timer); wb_do_writeback(me, 0); } spin_lock_bh(&amp;bdi_lock); set_current_state(TASK_INTERRUPTIBLE); list_for_each_entry(bdi, &amp;bdi_list, bdi_list) { bool have_dirty_io; if (!bdi_cap_writeback_dirty(bdi) || bdi_cap_flush_forker(bdi)) continue; WARN(!test_bit(BDI_registered, &amp;bdi-&gt;state),&quot;bdi %p/%s is not registered!\\n&quot;, bdi, bdi-&gt;name); have_dirty_io = !list_empty(&amp;bdi-&gt;work_list) || wb_has_dirty_io(&amp;bdi-&gt;wb); /* 若设备上有任务需要回写并且尚未创建回写线程 */ if (!bdi-&gt;wb.task &amp;&amp; have_dirty_io) { /* 为设备设置Pending标志位，这样其他线程如果想要移除该设备，必须等在该标志位上 */ set_bit(BDI_pending, &amp;bdi-&gt;state); action = FORK_THREAD; break; } spin_lock(&amp;bdi-&gt;wb_lock); /* 如果设备没有任务且长时间尚未处于活跃状态，那么Kill设备的回写线程，如果它存在的话 * 这里对设备加wb_lock是为了保证在此过程中没有其他的线程对向该设备发送回写任务并且唤醒该回写线程 */ if (bdi-&gt;wb.task &amp;&amp; !have_dirty_io &amp;&amp; time_after(jiffies, bdi-&gt;wb.last_active + bdi_longest_inactive())) { task = bdi-&gt;wb.task; bdi-&gt;wb.task = NULL; spin_unlock(&amp;bdi-&gt;wb_lock); set_bit(BDI_pending, &amp;bdi-&gt;state); action = KILL_THREAD; break; } spin_unlock(&amp;bdi-&gt;wb_lock); } spin_unlock_bh(&amp;bdi_lock); /* Keep working if default bdi still has things to do */ if (!list_empty(&amp;me-&gt;bdi-&gt;work_list)) __set_current_state(TASK_RUNNING); switch (action) { case FORK_THREAD: __set_current_state(TASK_RUNNING); task = kthread_create(bdi_writeback_thread, &amp;bdi-&gt;wb, &quot;flush-%s&quot;, dev_name(bdi-&gt;dev)); if (IS_ERR(task)) { /* 如果为设备创建回写线程失败，那么管理线程亲自操刀，回写设备上的任务 */ bdi_flush_io(bdi); } else { spin_lock_bh(&amp;bdi-&gt;wb_lock); bdi-&gt;wb.task = task; spin_unlock_bh(&amp;bdi-&gt;wb_lock); wake_up_process(task); } break; case KILL_THREAD: __set_current_state(TASK_RUNNING); kthread_stop(task); break; case NO_ACTION: if (!wb_has_dirty_io(me) || !dirty_writeback_interval) /* 如果对设备遍历了一圈发现没有设备上需要进行任何的处理，那么好吧，我们尽量睡眠更长的时间 * 这样可以更省电 */ schedule_timeout(bdi_longest_inactive()); else schedule_timeout(msecs_to_jiffies(dirty_writeback_interval * 10)); try_to_freeze(); /* Back to the main loop */ continue; } /* 任务做完以后，清除Pending标志位，这样其它想要移除该设备的线程便可以继续处理了 */ clear_bit(BDI_pending, &amp;bdi-&gt;state); smp_mb__after_clear_bit(); wake_up_bit(&amp;bdi-&gt;state, BDI_pending); } return 0; } 工作线程 每个设备保存脏文件链表，保存的是该设备上存储的脏文件的inode节点。所谓的回写文件脏页面即回写该inode链表上的某些文件的脏页面。 系统中存在多个回写时机： 应用程序主动调用回写接口（fsync，fdatasync以及sync等） 管理线程周期性地唤醒设备上的回写线程进行回写 某些应用程序/内核任务发现内存不足时要回收部分缓存页面而事先进行脏页面回写 系统为每个设备创建一个回写线程，而不是每个磁盘分区创建一个回写线程。 崩溃一致性 问题产生的本质原因是：文件系统对磁盘发出的更新操作通常是一个复合操作，即磁盘无法通过一次写入操作就能完成整个更新请求，相反，它需要依次更新多个相关数据结构，即多次写入磁盘才能顺利完成请求。在这多个依次写入的操作中间，系统发生了某种软件或硬件方面的错误（如突然断电power loss 或者系统崩溃 system crash），使得此次更新请求未能顺利完成，即只更新了部分数据结构，这就导致被更新的数据结构处于一个不一致的状态 可能导致很多问题，典型包括 space leak、garbage data 和 inconsistent data structures 三种 。 Example 以一个简单的标准文件系统为例，假设在如图所示的磁盘数据块布局中，已经存在如下的数据。 inode bitmap (8b). eg. 对应 inode 的 bit 00100000 data bitmap (8b) eg. 对应 data 的 bit. 00001000 inode (total 8 nodes, point to 4 blocks). eg. inode 号为 2 的 I[V1]，V1 意为版本号为 1 data block (total 8 blocks) eg. block 号为 4 的 Da I[V1] 中存储的元数据如下： owner : remzi permissions : read-write size : 1 // 指向的 block 数量 pointer : 4 // 指向的第一个数据块 pointer : null pointer : null pointer : null 即将进行的操作为：追加写文件。（即向已存在的文件追加新的数据块）对应的会修改三个磁盘上的数据结构： inode：指向新的数据块，并重新统计大小 size data block：写入新的数据块，eg. Db data block bitmap：写入新的数据块对应的位图 eg. B[V2] = 00001100 更新后的元数据信息 I[V2] 为： owner : remzi permissions : read-write size : 2 pointer : 4 pointer : 5 pointer : null pointer : null 磁盘上的整体数据布局如图： 对应的三个数据结构的修改也就意味着文件系统需要和磁盘之间至少进行三次数据传输。由于页缓存的存在，数据结构对应的修改也不会立马持久化到磁盘，而是缓存在页缓存中，直到缓存刷新线程将数据刷回磁盘。在刷回磁盘之前，CRASH 导致这三个写操作中的其中一个或者两个操作成功，都将导致不一致的现象。 Crash Scenarios 主要分为一个操作失败和两个操作失败两种情况。 Single Write Success data block 写成功：没有对应 bitmap 和 inode，即该数据块会被文件系统当作是空闲的未使用的数据块，不会导致其他问题，只是下次写入会将该数据块原有的内容覆盖。 inode I[V2] 写成功：inode 对应的指向了上图中的 data block 5，但是由于数据块未能成功写入，会导致读取该文件对应的第二个数据块时，读取到垃圾数据（即原本存在于 data block 5 上的数据，空数据或者旧数据） 除此以外，由于对应的 bitmap 也未写入，将出现 inode 指向了未分配的 data block 的情况，在文件系统的实际使用中，需要处理这种不一致的情况，来避免无效数据的访问。 bitmap B[V2] 写成功：即分配了 data block 5，但无 inode 指向该数据块，因此又出现了上面提到的文件系统内部的不一致情况。如果不解决该问题，将会导致 data block 5 不再能被分配，即 space leak。 Two writes Success (Single failure) data block 写失败：即文件系统元数据是完整一致的，但是对应的数据块 5 仍为垃圾数据，即会导致读取到垃圾数据。 bitmap B[V2] 写失败：即 inode 指向了未分配的数据区域，即便该数据区域里数据有效，但是都将导致未来不再能对该数据区域进行修改。 inode I[V2] 写失败：仍为 inode 和 bitmap 之间的不一致，即便该数据块是真实有效且被分配管理，但是无法得知该数据块属于哪一个文件，因为没有对应的 inode 指向该数据块。 Summary 理想的效果是实现每一次写操作都是从一致的状态转变成另外一个一致的状态，且状态的转变过程是原子性的。但是往往写操作会涉及到很多个数据块（block）的操作，而磁盘只能保证一个扇区（sector - 512Bytes）的原子性，所以一次写操作过程中很容易产生崩溃一致性问题。crash-consistency problem / consistent-update problem Solutions Solution#1 The File System Checker (FSCK) 作为早期文件系统一种简单的解决崩溃一致性的方法：即允许产生不一致的情况，但是会在之后进行修复，比如 reboot 的时候。 fsck 就是一种 UNIX 上的用于检测数据不一致并修复的工具，其他操作系统也有类似的工具。但这类工具不能解决所有问题，只能解决元数据内部的数据不一致情况，无法处理垃圾数据的情况。 fsck 主要检测以下元数据信息： Superblock：主要是进行完整性检查，如果发现了已经损坏的超级块，则相应地使用超级块的副本 Free blocks：通过遍历所有数据块临时性重构 d-bmap 以检验其同 inode 之间是否保持一致 Inode State：检查每个 inode 是否损坏或其他问题。如确保每一个 inode 都有对应的有效类型（文件/目录/符号链接等） Inode links：检查 inode 引用计数，比较实际的引用数和 inode 中存储的计数，通常会修正 inode 中存储的计数。如果发现了一个分配了的 inode，但是没有目录引用该 inode，那么该 inode 将被移动到 lost+found 目录. Duplicates：检查重复指针。即两个不同的inode引用同一块的情况。如果一个 inode 明显是坏的，它可能被清除。另外，也可以复制指向的块，从而按需要为每个inode 提供自己的副本 Bad Blocks：在扫描所有指针列表时，还会检查错误的块指针。错误的块指针往往可能执行超过有效范围的块，fsck 将删除 inode 中对应的块指针。 Directory Checks：检查对应文件系统格式的目录，对每个目录的内容执行附加的完整性检查，以确保 . 和 .. 是第一个条目，每个 inode 在一个目录条目中引用的都被分配，确保在整个层次结构中没有哪个目录被链接超过一次。 fsck 存在的问题很明显，除了不能解决 garbage data 的问题外，更让人不能接受的是，修复所耗费时间过长，这在文件系统日渐增长的情况下显得尤为突出。修复的时间会随着容量的增加变得更长。fsck 最为不科学的一点就是：部分数据的不一致现象可能需要整个磁盘执行故障恢复的操作。 Solution#2 Journaling (or WAL) 借鉴数据库管理系统（DBS）的一致性解决方案：WAL（写前日志）。在文件系统中通常将 WAL 称呼为 Journaling。大量的文件系统使用了该方法来保证崩溃一致性。如 ext3, ext4, reiserfs, IBM's JFS, SGI's XFS, Windows NTFS。 Journaling 即在向磁盘写入实际的数据之前，先额外写一些数据（也被称为日志记录log）到磁盘指定区域，然后再更新磁盘写入实际数据。如此一来，若在磁盘更新的过程中发生了系统崩溃，则可读取之前写入到磁盘指定区域的log，以推断出被中断的更新操作的详细内容，然后针对性地重新执行更新操作。此种方案避免了对整个文件系统的全盘扫描，因此理论上提高了崩溃恢复效率。 Ext3 就是在 Ext2 的基础上引入了 Journaling 机制 Data Journaling 即将完整的数据块信息和元数据信息均写入到 WAL 中。（即 Ext3 的 WAL 机制） 如图所示，以追加写 Db 为例。 TxB 标志一条新的日志记录的开始 （包含一个事务号 transaction id 和 后续要写入的数据对应的地址信息） I[V2] 要写入的 Inode 数据 B[V2] 要写入的数据块 bitmap Db 要写入的数据块 TxE 标志日志记录的结束 （也包含对应的事务号 transaction id） 此种方式又被称之为物理日志 physical logging；对应地也有 logical logging，即不将数据放入 Journal 中。 故 data journaling 划分为两个阶段： Journal Write：写入事务开启标志、数据和元数据、事务结束标志到日志中，等待所有写操作完成 Checkpoint：将数据与元数据写入到实际需要写入的地址上 存在问题：写入日志的过程中发生了崩溃将会造成不一致的情况。因为文件系统会缓存 I/O 请求以提升写性能（串行执行五个操作效率太低），即五部分数据可能会以任意顺序写入磁盘，则可能出现一种如图所示的情况，除开 Db 以外的数据均写入成功，在写 Db 的过程中发生了 Crash 在崩溃恢复阶段，当读入这条日志记录时，可能引起严重错误，因为 Db 的数据块内容是任意 garbage data。Ext4 引入了日志校验和机制，通过在日志记录的开始和末尾处增加数据块的校验和，来帮助故障恢复时对日志完整有效性的判断，从而提高恢复效率。 为了应对该问题，文件系统采用一种类似于“二阶段提交”的方式来写入日志：它先写入除TxE结构之外的日志部分，等到它们真正被写入磁盘后，再接着写入TxE结构到磁盘。 在写入第一部分的日志记录时发生了系统崩溃，那么此条日志记录是不完整的，在文件系统重启执行崩溃恢复时，会将此条日志记录视为非法，因此不会导致任何不一致的状态 考虑到 TxE 结构一般较小，不足一个扇区大小 sector (&lt; 512 Bytes)，因此其写入操作不会发生 torn write，换言之，日志记录的第二阶段的写入也是原子性的。 步骤： Journal write，同样先写日志，只不过先要确保除TxE结构之外的日志部分先写入到磁盘； Journal commit，进一步写入TxE，以确保整条日志记录写入的原子性； Checkpoint，最后才将需要追加或更新的文件数据内容真正地更新到磁盘对应的数据块中。 恢复 在顺利为每个更新操作写入data journaling日志记录后，一旦发生任何系统崩溃情况，则在文件系统恢复过程中： 一方面，若是在日志记录本身的写入过程中发生了系统崩溃，此时日志记录并不完整，因此恢复程序应当直接跳过，不会造成系统任何不一致现象 另一方面，若是在 checkpoint 阶段发生了系统崩溃，则只需要读取并解析对应日志内容，然后实施日志 replay 即可重新尝试将更新持久化到磁盘，这种日志类型在数据库管理系统中被称为 redo log。 data journaling 问题 data jounaling 模式能基本解决 crash consistency 问题，其不足之处在于日志记录包含了实际数据块，因此占用了较多额外空间。另外，恢复过程还会引入部分 redundant write。 redundant write （Batching Log Updates） 问题描述: 同一个目录下的两个文件 foo1 和 foo2 依次进行更新，更新目录下的一个文件，至少需要更新的磁盘数据结构包括 i-bmap、inode、目录所关联的 data block 以及目录的 inode，因此，这些信息全部需要作为日志内容写入到磁盘，那么 foo1 和 foo2 的更新则需要重复写入目录的 inode 及其关联的 data block，因此会导致较多写操作开销（考虑当更新同一个目录下的多个文件或目录的情况） ext3 文件系统采用 batch update 策略来解决。具体而言，它会先将文件的更新进行缓存，并标记对应的日志记录需要存盘，当发现同一目录中其它文件也需要更新时，会将对应的日志记录合并到前一文件所对应的日志记录的数据块中，最后当达到刷盘周期时，将包含多个文件更新的日志记录一次性写入磁盘。 Space 日志记录包含了数据块，占用了大量的空间，而且也会增加崩溃恢复过程的耗时。考虑优化日志记录来减小存储空间的使用。 Making The Log Finite：回收已经持久化的数据对应的存储空间 Metadata Journaling：日志记录中仅保留元数据减小日志 Making The Log Finite 通过日志循环写入(circular log)配合日志释放来解决。具体而言，将存储日志的磁盘区域作为一个环形数组即可，环形数组的首尾指针即为没有被释放的日志记录边界，为了方便，可以将这两个指针存储在journal superblock中。如下图所示。 为了实现circular log，需在每次成功文件更新的操作后，即时将日志区域中对应条目释放掉（即更新circular log的首尾指针）。通过引入circular log后，为了保证crash consistency，更新文件的整个过程可扩展为如下四个阶段： Journal write，写入除 TxB 结构之外的其它日志记录内容； Journal commit，进一步写入日志的 TxB 结构，至此，完成了日志记录的原子性写入； Checkpoint，将文件更新或追加的数据真正写入到磁盘； Free，释放掉步骤 1 和 2 中写入的日志记录，以备后续空间复用。 仍然存在的问题：每一个被更新的文件，其更新数据块需要被写入两次。 Metadata Journaling 数据双写（日志和Disk）问题严峻，严重影响磁盘实际使用带宽，在写入元数据和实际数据之间的寻道操作开销也较大，故考虑使用 metadata journaling (ordered journaling)，如下所示： 如果仍保留原有的 data journaling 流程，只是去除日志中的数据块部分，将会产生垃圾数据，即日志记录写入完成，但实际数据写入过程可能发生错误。故需要调整此时的写入流程：（步骤1 和 2 无顺序依赖，可并发执行） Data write，将被更新的文件的数据块真正写入到磁盘； Journal metadata write，只将被更新的文件的元信息相关的数据块以及 TxB 构成的日志记录写入到磁盘； Journal commit，进一步将此日志记录的 TxE 结构写入磁盘； Checkpoint metadata，将被更新文件的元信息相关的数据块真正写入到磁盘； Free，释放掉步骤 2 和 3 中写入的日志记录，以备后续空间复用。 通过强制数据先写，文件系统可以保证指针永远不会指向垃圾数据。事实上，实现崩溃一致性的核心就是：“write the pointed-to object before the object that points to it。 即 在写入指向目标数据的访问信息（指针）之前，首先写入相应的数据 （取钱之前得先存钱） 元数据日志比全数据日志更为广泛被使用。Ext3 提供了多种日志模式，其中 ordered 即为元数据日志。NTFS，XFS 都有对应的元数据日志实现。 Tricky Case: Block Reuse 最为繁琐的操作——删除操作可能导致数据块重用的问题。如下图所示，foo 表示一个目录，当我们在目录 foo 下创建一个文件时，若采用metadata journaling模式，其磁盘数据块布局可简化如下。注意，因为目录所包含内容也被视为元数据，因此会被记录到日志中。 此时，用户删除目录中的所有内容和目录本身，释放出 Block 1000 以供重用，最后，用户创建一个新文件(比方说 bar)，它重用了原来属于 foo 的相同 Block 1000。 现在假设发生了崩溃，所有这些信息仍然在日志中。在回放期间，恢复过程简单地回放日志中的所有内容，包括写块 1000 中的目录数据; 这时候恢复过程中的旧数据将会覆盖已经写入的 bar 数据，当用户读取 bar 对应的数据时，将出现不一致的情况。 一种比较简单的处理方式是在 checkpoint 日志删除释放 之前都不重用对应的 datablock ext3 文件系统的处理方式比较简单，它会将那些被删除文件或目录的日志记录标记成 revoke，且在崩溃恢复过程中，当扫描到包含 revoke 的日志记录时，不会对此日志记录执行 replay，因此避免了数据块覆盖的情况。 Solution#3 Other Approaches Soft Updates 基本原理是将所有文件的更新操作请求进行严格排序，并且保证磁盘对应的数据结构不会处于不一致的状态，比如先写文件数据内容，再写文件元信息，以保证inode不会关联一个无效的数据块。 但是Soft Updates实现起来比较复杂，需要充分了解文件系统内部相关数据结构知识，因此增加了文件系统实现的复杂性 CoW Copy-On-Write 不直接更新文件包含的数据块，相反，它会创建一个完整的更新后的副本，当完成了若干个更新操作后，再一次性将更新后的数据块关联到被对应的被更新文件。 ZFS 就同时使用 cow 和 journaling两种技术，LFS 也是 CoW 的一个早期例子 backpointer-based consistency FAST12 - Consistency Without Ordering 在写操作之间不强制排序，为了达到一致性，一个额外的反向指针被添加到系统中的每个块中，即每个数据块都有一个对它所属的inode的引用。 当访问一个文件时，文件系统可以通过检查正向指针(例如inode中的地址或直接块)是否指向指向它的块来确定该文件是否一致 通过向文件系统添加反向指针，可以获得一种新的延迟崩溃一致性 optimistic crash consistency SOSP13 - Optimistic crash consistency 这种新方法通过使用事务校验和的一般化形式，向磁盘发出尽可能多的写操作，还包括一些其他技术，可以在出现不一致时检测。对于某些工作负载，这些乐观技术可以将性能提高一个数量级。但是，要想真正正常工作，需要一个稍微不同的磁盘接口 主要用于优化磁盘写入日志记录的过程，以减少等待数据刷盘所导致的时间开销。 文件系统崩溃一致性测试 OSDI18 - Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing Github Repo crashmonkey 参考文献 [1] 知乎 - 丁凯：Linux文件一致性（一） [2] 知乎 - 丁凯：Linux文件一致性（二）：脏文件回写之管理线程 [3] 知乎 - 丁凯：Linux文件一致性（二）：脏文件回写之工作线程 [4] 一抹光辉油彩：简单文件系统的崩溃一致性和日志 [5] 简书：文件系统天生就是不平等的 - 实现崩溃一致性应用的复杂性 [6] 知乎 - 蒋炎岩：崩溃一致性：你的程序真的正确保存了数据吗？ [7] JciX：存储系统的崩溃一致性问题 (Crash Consistency) [8] Arpaci-Dusseau R H, Arpaci-Dusseau A C. Operating systems: Three easy pieces[M]. Arpaci-Dusseau Books LLC, 2018 [9] OSDI18 - Finding Crash-Consistency Bugs with Bounded Black-Box Crash Testing ","link":"https://blog.shunzi.tech/post/dist-block-consistency/"},{"title":"AI For System Papers Index","content":" Index for AI-4-Systems researches. Some aspects in intelligent storage field. Continuous update. AI For Systems Disk Failure Prediction FAST20 - Making Disk Failure Predictions SMARTer! Slides We present analysis and findings from one of the largest disk failure prediction studies covering a total of 380,000 hard drives over a period of two months across 64 sites of a large leading data center operator. Our proposed machine learning based models predict disk failures with 0.95 F-measure and 0.95 Matthews correlation coefficient (MCC) for 10-days prediction horizon on average. Findings: SMART attributes do not always have the strong predictive capability at long prediction horizon windows for all disks The value of performance metrics (related to capacity, throughput, etc.) Exhibit more variations before the actual drive failure Show distinguishable behavior from healthy disks Prediction can be further improved by incorporating the location information. (site, room, rack, and server) Disks in close spatial neighborhood Affected by the same environmental factors (such as humidity and temperature) Experience similar vibration level (known to affect the reliability of disks) Data: ML Models: Bayes classifier (Bayes) Random forest (RF) Gradient boosted decision trees (GBDT) Long short-term memory network (LSTM) Convolutional neural network with long short-term memory (CNN-LSTM) Conclusion: SPL group performs the best across all ML models (performance and location features improve the effectiveness of prediction) The improvement of adding location info is limited and pronounced only in the presence of performance features CNN-LSTM performs close to the best in all situations Trade-off between models with respect to different availability of feature sets Prediction Horizon: Storage System Tuning SIGMOD19 - An End-to-End Automatic Cloud Database Tuning System Using Deep Reinforcement Learning Optimize IO Behavior DATE20 - A Machine Learning Based Write Policy for SSD Cache in Cloud Block Storage Source Code Based on our analysis on a typical cloud block storage system, approximately 47.09% writes are write-only, i.e., writes to the blocks which have not been read during a certain time window We propose an ML-WP, Machine Learning Based Write Policy, which reduces write traffic to SSDs by avoiding writing write-only data. Main challenge in this approach is to identify write-only data in a real-time manner. Last choost Naive Bayes algorithm. Appropriate features: last access timestamp last address information average write size Big request ratio (&gt; 64KB) Small request ratio (&lt; 8KB&gt;) Experimental results show that, compared with the industry widely deployed writeback policy, ML-WP decreases write traffic to SSD cache by 41.52%, while improving the hit ratio by 2.61% and reducing the average read latency by 37.52%. ","link":"https://blog.shunzi.tech/post/AI-for-Systems-index/"},{"title":"Database Internal - LSM-tree Summary","content":" The LSM-tree basic in database system. The LSM-tree basic in database system. Here is the basic of LSM-tree, including the knowledge of data structure and some optimization. The content is based on the book Database Internal. I will present my understanding. Only summary, not details. Before LSM-tree Mutable Structures VS Immutable Structures Mutable: In-place Update. Immutable: Out-of-place Update In-place Update VS Out-of-place Update: In-place update storage structures are optimized for read performance: after locating data on disk, the record can be returned to the client. This comes at the expense of write performance: to update the data record in place, it first has to be located on disk Append-only storage is optimized for write performance. Writes do not have to locate records on disk to overwrite them. However, this is done at the expense of reads, which have to retrieve multiple data record versions and reconcile them. Immutable Structures Safety characteristics: once created, an immutable structure doesn’t change, all of its references can be accessed concurrently, and its integrity is guaranteed by the fact that it cannot be modified. Immutable files can hold multiple copies, more recent ones overwriting the older ones, while mutable files generally hold only the most recent value instead. Immutable LSM Trees use append-only storage and merge reconciliationn, and B-Trees locate data records on disk and update pages at their original offsets in the file. LSM-Tree We concluded that space overhead and write amplification can be improved by using buffering, and there are two ways buffering: To postpone propagating writes to diskresident pages To make write operations sequential. LSM Tree uses buffering and append-only storage to achieve sequential writes. LSM Trees defer data file writes and buffer changes in a memory-resident table. LSM Structure Two components Disk Components: Comprised of immutable segments and the disk component is organised as a B-tree, with 100% occupancy and read-only pages. Memory-resident tree contents are flushed on disk in parts. During a flush, for each flushed in-memory subtree, we find a corresponding subtree on disk and write out the merged contents of a memory-resident segment and disk-resident subtree into the new segment on disk. After the subtree is flushed, superseded memory-resident and disk-resident subtrees are discarded and replaced with the result of their merge, which becomes addressable from the preexisting sections of the disk-resident tree. A merge can be implemented by advancing iterators reading the disk-resident leaf nodes and contents of the in-memory tree in lockstep. Since both sources are sorted, to produce a sorted merged result, we only need to know the current values of both +iterators during each step of the merge process. This approach is a logical extension and continuation of our conversation on immutable B-Trees. Copy-on-write B-Trees use B-Tree structure, but their nodes are not fully occupied, and they require copying pages on the root-leaf path and creating a parallel tree structure. Here, we do something similar, but since we buffer writes in memory, we amortize the costs of the disk-resident tree update. As for merges and flushes: As soon as the flush process starts, all new writes have to go to the new memtable. (Immutable Memtable and Mutable Memtable) During the subtree flush, both the disk-resident and flushing memory-resident subtree have to remain accessible for reads. Afer the flush, publishing merged contents, and discarding unmerged disk- and memory-resident contents have to be performed atomically. Multicomponent LSM Trees Let’s consider an alternative design, multicomponent LSM Trees that have more than just one disk-resident table. It quickly becomes evident that after multiple flushes we’ll end up with multiple disk resident tables, and their number will only grow over time. Since we do not always know exactly which tables are holding required data records, we might have to access multiple files to locate the searched data. Having to read from multiple sources instead of just one might get expensive. To mitigate this problem and keep the number of tables to minimum, a periodic merge process called compaction is triggered. Compaction picks several tables, reads their contents, merges them, and writes the merged result out to the new combined file. Old tables are discarded simultaneously with the appearance of the new merged table. In-memory Tables Flush Strategy: Memtable flushes can be triggered periodically, or by using a size threshold. Before memtable can be flushed, the memtable has to be switched: a new memtable is allocated, and it becomes a target for all new writes, while the old one moves to the flushing state. These two steps have to be performed atomically. The flushing memtable remains available for reads until its contents are fully flushed. After this, the old memtable is discarded in favor of a newly written disk-resident table, which becomes available for reads. Components Current memtable: Receives writes and serves reads. Flushing memtable: Available for reads. On-disk ﬂush target: Does not participate in reads, as its contents are incomplete. Flushed tables: Available for reads as soon as the flushed memtable is discarded. Compacting tables: Currently merging disk-resident tables. Compacted tables: Created from flushed or other compacted tables. Crash Consistency Until the memtable is fully flushed, the only disk-resident version of its contents is stored in the write-ahead log. When memtable contents are fully flushed on disk, the log can be trimmed, and the log section, holding operations applied to the flushed memtable, can be discarded. Updates And Deletes Core: Out-of-Place Updates Delete Single Key-Value Example: Delete k1 Deletes need to be done by inserting a special delete entry (called tombstone or a dormant certificate) instead of removing directly. The reconciliation process picks up tombstones, and filters out the shadowed values. Range Key-Value Sometimes it might be useful to remove a consecutive range of keys rather than just a single key. This can be done using predicate deletes, which work by appending a delete entry with a predicate that sorts according to regular record-sorting rules. During reconciliation, data records matching the predicate are skipped and not returned to the client. Such as DELETE FROM table WHERE key ≥ &quot;k2&quot; AND key &lt; &quot;k4&quot;. Apache Cassandra implements this approach and calls it range tombstones. A range tombstone covers a range of keys rather than just a single key. When using range tombstones, resolution rules have to be carefully considered because of overlapping ranges and disk-resident table boundaries.For example, the following combination will hide data records associated with k2 and k3 from the final result: LSM Tree Lookups LSM Trees consist of multiple components. During lookups, more than one component is usually accessed, so their contents have to be merged and reconciled before they can be returned to the client. Merge-Iteration Since contents of disk-resident tables are sorted, we can use a multiway merge-sort algorithm. A multiway merge-sort uses a priority queue, such as min-heap that holds up to N elements (where N is the number of iterators), which sorts its contents and prepares the next-in-line smallest element to be returned. The head of each iterator is placed into the queue. An element in the head of the queue is then the minimum of all iterators. When the smallest element is removed from the queue, the iterator associated with it is checked for the next value, which is then placed into the queue, which is re-sorted to preserve the order. Since all iterator contents are sorted, reinserting a value from the iterator that held the previous smallest value of all iterator heads also preserves an invariant that the queue still holds the smallest elements from all iterators. Whenever one of the iterators is exhausted, the algorithm proceeds without reinserting the next iterator head. The algorithm continues until either query conditions are satisfied or all iterators are exhausted. It may happen that we encounter more than one data record for the same key during merge-iteration. From the priority queue and iterator invariants, we know that if each iterator only holds a single data record per key, and we end up with multiple records for the same key in the queue, these data records must have come from the different iterators. For example: two disk-resident tables: The priority queue is filled from the iterator heads: Key k1 is the smallest key in the queue and is appended to the result. Since it came from Iterator 2, we refill the queue from it: Now, we have two records for the k2 key in the queue. We can be sure there are no other records with the same key in any iterator because of the aforementioned invariants. Same-key records are merged and appended to the merged result. The queue is refilled with data from both iterators: Since all iterators are now empty, we append the remaining queue contents to the output: Steps: Initially, fill the queue with the first items from each iterator. Take the smallest element (head) from the queue. Refill the queue from the corresponding iterator, unless this iterator is exhausted. Overhead In terms of complexity, merging iterators is the same as merging sorted collections. It has O(N) memory overhead, where N is the number of iterators. A sorted collection of iterator heads is maintained with O(log N) (average case) Reconciliation Merge-iteration is just a single aspect of what has to be done to merge data from multiple sources. Another important aspect is reconciliation and conflict resolution of the data records associated with the same key. To reconcile data records, we need to understand which one of them takes precedence. Data records hold metadata necessary for this, such as timestamps. To establish the order between the items coming from multiple sources and find out which one is more recent, we can compare their timestamps. Records shadowed by the records with higher timestamps are not returned to the client or written during compaction. Maintenance in LSM Trees Compaction picks multiple disk-resident tables, iterates over their entire contents using the aforementioned merge and reconciliation algorithms, and writes out the results into the newly created table. Since disk-resident table contents are sorted, and because of the way merge-sort works, compaction has a theoretical memory usage upper bound, since it should only hold iterator heads in memory. All table contents are consumed sequentially, and the resulting merged data is also written out sequentially. These details may vary between implementations due to additional optimizations. Compacting tables remain available for reads until the compaction process finishes, which means that for the duration of compaction, it is required to have enough free space available on disk for a compacted table to be written. At any given time, multiple compactions can be executed in the system. However, these concurrent compactions usually work on nonintersecting sets of tables. A compaction writer can both merge several tables into one and partition one table into multiple tables. Leveled Compaction One of the frequently implemented compaction strategies is called leveled compaction. For example, it is used by RocksDB. Leveled compaction separates disk-resident tables into levels. Tables on each level have target sizes, and each level has a corresponding index number (identifier). Level-0 tables are created by flushing memtable contents. Tables in level 0 may contain overlapping key ranges. As soon as the number of tables on level 0 reaches a threshold, their contents are merged, creating new tables for level 1. Key ranges for the tables on level 1 and all levels with a higher index do not overlap, so level-0 tables have to be partitioned during compaction, split into ranges, and merged with tables holding corresponding key ranges. Alternatively, compaction can include all level-0 and level-1 tables, and output partitioned level-1 tables. Compactions on the levels with the higher indexes pick tables from two consecutive levels with overlapping ranges and produce a new table on a higher level. Keeping different key ranges in the distinct tables reduces the number of tables accessed during the read. This is done by inspecting the table metadata and filtering out the tables whose ranges do not contain a searched key. Each level has a limit on the table size and the maximum number of tables. As soon as the number of tables on level 1 or any level with a higher index reaches a threshold, tables from the current level are merged with tables on the next level holding the overlapping key range. Size-tiered compaction In size-tiered compaction, rather than grouping disk-resident tables based on their level, they’re grouped by size: smaller tables are grouped with smaller ones, and bigger tables are grouped with bigger ones. Level 0 holds the smallest tables that were either flushed from memtables or created by the compaction process. When the tables are compacted, the resulting merged table is written to the level holding tables with corresponding sizes. The process continues recursively incrementing levels, compacting and promoting larger tables to higher levels, and demoting smaller tables to lower levels. Read, Write, and Space Amplification When implementing an optimal compaction strategy, we have to take multiple factors into consideration. One approach is to reclaim space occupied by duplicate records and reduce space overhead, which results in higher write amplification caused by rewriting tables continuously. The alternative is to avoid rewriting the data continuously, which increases read amplification (overhead from reconciling data records associated with the same key during the read), and space amplification (since redundant records are preserved for a longer time). Read amplification：Resulting from a need to address multiple tables to retrieve data. Write amplification：Caused by continuous rewrites by the compaction process. Space amplification：Arising from storing multiple records associated with the same key. Implementation Details how memory- and disk-resident tables are implemented how secondary indexes work how to reduce the number of diskresident tables accessed during read and new ideas related to log-structured storage. Sorted String Tables Disk-resident tables are often implemented using Sorted String Tables (SSTables). As the name suggests, data records in SSTables are sorted and laid out in key order. SSTables usually consist of two components: index files and data files. Index files Index files are implemented using some structure allowing logarithmic lookups, such as B-Trees, or constant-time lookups, such as hashtables. The index component holds keys and data entries (offsets in the data file where the actual data records are located). Data files The data component consists of concatenated key-value pairs. The cell design and data record formats we discussed in Chapter 3 are largely applicable to SSTables. The main difference here is that cells are written sequentially and are not modified during the life cycle of the SSTable. Since the index files hold pointers to the data records stored in the data file, their offsets have to be known by the time the index is created. During compaction, data files can be read sequentially without addressing the index component, as data records in them are already ordered. Since tables merged during compaction have the same order, and merge-iteration is order-preserving, the resulting merged table is also created by writing data records sequentially in a single run. As soon as the file is fully written, it is considered immutable, and its disk-resident contents are not modified. SSTable-Attached Secondary Indexes Apache Cassandra： SSTableAttached Secondary Indexes (SASI) To allow indexing table contents not just by the primary key, but also by any other field, index structures and their life cycles are coupled with the SSTable life cycle, and an index is created per SSTable. When the memtable is flushed, its contents are written to disk, and secondary index files are created along with the SSTable primary key index. Since LSM Trees buffer data in memory and indexes have to work for memory-resident contents as well as the disk-resident ones, SASI maintains a separate in-memory structure, indexing memtable contents. Bloom Filters One of the ways to prevent table lookup is to store its key range (smallest and largest keys stored in the given table) in metadata, and check if the searched key belongs to the range of that table. This information is imprecise and can only tell us if the data record can be present in the table. To improve this situation, many implementations, including Apache Cassandra and RocksDB, use a data structure called a Bloom filter. Bloom filter can be used to tell if the key might be in the table or is definitely not in the table. Using Bloom filters associated with disk-resident tables helps to significantly reduce the number of tables accessed during a read. More details please visit Bloom Filters; Skiplist There are many different data structures for keeping sorted data in memory, and one that has been getting more popular recently because of its simplicity is called a skiplist Skiplists do not require rotation or relocation for inserts and updates, and use probabilistic balancing instead. Skiplists are generally less cache-friendly than in-memory B-Trees, since skiplist nodes are small and randomly allocated in memory. Some implementations improve the situation by using unrolled linked lists. A skiplist consists of a series of nodes of a different height, building linked hierarchies allowing to skip ranges of items. Each node holds a key, and, unlike the nodes in a linked list, some nodes have more than just one successor. A node of height h is linked from one or more predecessor nodes of a height up to h. Nodes on the lowest level can be linked from nodes of any height. Node height is determined by a random function and is computed during insert. Nodes that have the same height form a level. The number of levels is capped to avoid infinite growth, and a maximum height is chosen based on how many items can be held by the structure. There are exponentially fewer nodes on each next level. Lookups work by following the node pointers on the highest level. As soon as the search encounters the node that holds a key that is greater than the searched one, its predecessor’s link to the node on the next level is followed. In other words, if the searched key is greater than the current node key, the search continues forward. If the searched key is smaller than the current node key, the search continues from the predecessor node on the next level. This process is repeated recursively until the searched key or its predecessor is located. Lookup Example Searching for key 7 in the skiplist Follow the pointer on the highest level, to the node that holds key 10. 7 &lt; 10 Since the searched key 7 is smaller than 10, the next-level pointer from the head node is followed, locating a node holding key 5. 7 &gt; 5 The highest-level pointer on this node is followed, locating the node holding key 10 again. 7 &lt; 10 The searched key 7 is smaller than 10, and the next-level pointer from the node holding key 5 is followed, locating a node holding the searched key 7. Insert During insert, an insertion point (node holding a key or its predecessor) is found using the aforementioned algorithm, and a new node is created. To build a tree-like hierarchy and keep balance, the height of the node is determined using a random number, generated based on a probability distribution. Pointers in predecessor nodes holding keys smaller than the key in a newly created node are linked to point to that node. Their higher-level pointers remain intact. Pointers in the newly created node are linked to corresponding successors on each level. Delete orward pointers of the removed node are placed to predecessor nodes on corresponding levels. Other We can create a concurrent version of a skiplist by implementing a linearizability scheme that uses an additional fully_linked flag that determines whether or not the node pointers are fully updated. This flag can be set using compare-and-swap. This is required because the node pointers have to be updated on multiple levels to fully restore the skiplist structure In languages with an unmanaged memory model, reference counting or hazard pointers can be used to ensure that currently referenced nodes are not freed while they are accessed concurrently. This algorithm is deadlock-free, since nodes are always accessed from higher levels. Apache Cassandra uses skiplists for the secondary index memtable implementation. WiredTiger uses skiplists for some in-memory operations. SkipList Details Disk Access Since most of the table contents are disk-resident, and storage devices generally allow accessing data blockwise, many LSM Tree implementations rely on the page cache for disk accesses and intermediate caching. Many techniques described in “Buffer Management” on page 81, such as page eviction and pinning, still apply to log-structured storage. The most notable difference is that in-memory contents are immutable and therefore require no additional locks or latches for concurrent access. Reference counting is applied to make sure that currently accessed pages are not evicted from memory, and in-flight requests complete before underlying files are removed during compaction. Another difference is that data records in LSM Trees are not necessarily page aligned, and pointers can be implemented using absolute offsets rather than page IDs for addressing. In Figure 7-9, you can see records with contents that are not aligned with disk blocks. Some records cross the page boundaries and require loading several pages in memory. Compression We’ve discussed compression already in context of B-Trees. Similar ideas are also applicable to LSM Trees. The main difference here is that LSM Tree tables are immutable, and are generally written in a single pass. When compressing data page-wise, compressed pages are not page aligned, as their sizes are smaller than that of uncompressed ones. To be able to address compressed pages, we need to keep track of the address boundaries when writing their contents. We could fill compressed pages with zeros, aligning them to the page size, but then we’d lose the benefits of compression. To make compressed pages addressable, we need an indirection layer which stores offsets and sizes of compressed pages. Figure 7-10 shows the mapping between compressed and uncompressed blocks. Compressed pages are always smaller than the originals, since otherwise there’s no point in compressing them. During compaction and flush, compressed pages are appended sequentially, and compression information (the original uncompressed page offset and the actual compressed page offset) is stored in a separate file segment. During the read, the compressed page offset and its size are looked up, and the page can be uncompressed and materialized in memory. Unordered LSM Storage Unordered stores generally do not require a separate log and allow us to reduce the cost of writes by storing data records in insertion order. Bitcask Bitcask, one of the storage engines used in Riak, is an unordered log-structured storage engine. Unlike the log-structured storage implementations discussed so far, it does not use memtables for buffering, and stores data records directly in logfiles. To make values searchable, Bitcask uses a data structure called keydir, which holds references to the latest data records for the corresponding keys. Old data records may still be present on disk, but are not referenced from keydir, and are garbage-collected during compaction. Keydir is implemented as an in-memory hashmap and has to be rebuilt from the logfiles during startup. Write: During a write, a key and a data record are appended to the logfile sequentially, and the pointer to the newly written data record location is placed in keydir. Read: Reads check the keydir to locate the searched key and follow the associated pointer to the logfile, locating the data record. Since at any given moment there can be only one value associated with the key in the keydir, point queries do not have to merge data from multiple sources. Figure 7-11 shows mapping between the keys and records in data files in Bitcask. Logfiles hold data records, and keydir points to the latest live data record associated with each key. Shadowed records in data files (ones that were superseded by later writes or deletes) are shown in gray. Compaction: During compaction, contents of all logfiles are read sequentially, merged, and written to a new location, preserving only live data records and discarding the shadowed ones. Keydir is updated with new pointers to relocated data records. Data records are stored directly in logfiles, so a separate write-ahead log doesn’t have to be maintained, which reduces both space overhead and write amplification. A downside of this approach is that it offers only point queries and doesn’t allow range scans, since items are unordered both in keydir and in data files. Benefits and Disadvantages Advantages of this approach are simplicity and great point query performance. Even though multiple versions of data records exist, only the latest one is addressed by keydir. However, having to keep all keys in memory and rebuilding keydir on startup are limitations that might be a deal breaker for some use cases. While this approach is great for point queries, it does not offer any support for range queries. WiscKey WiscKey decouples sorting from garbage collection by keeping the keys sorted in LSM Trees, and keeping data records in unordered append-only files called vLogs (value logs). This approach can solve two problems mentioned while discussing Bitcask: a need to keep all keys in memory and to rebuild a hashtable on startup. Figure 7-12 shows key components of WiscKey, and mapping between keys and log files. vLog files hold unordered data records. Keys are stored in sorted LSM Trees, pointing to the latest data records in the logfiles. Since keys are typically much smaller than the data records associated with them, compacting them is significantly more efficient. This approach can be particularly useful for use cases with a low rate of updates and deletes, where garbage collection won’t free up as much disk space. The main challenge here is that because vLog data is unsorted, range scans require random I/O. WiscKey uses internal SSD parallelism to prefetch blocks in parallel during range scans and reduce random I/O costs. In terms of block transfers, the costs are still high: to fetch a single data record during the range scan, the entire page where it is located has to be read. During compaction, vLog file contents are read sequentially, merged, and written to a new location. Pointers (values in a key LSM Tree) are updated to point to these new locations. To avoid scanning entire vLog contents, WiscKey uses head and tail pointers, holding information about vLog segments that hold live keys. Since data in vLog is unsorted and contains no liveness information, the key tree has to be scanned to find which values are still live. Performing these checks during garbage collection introduces additional complexity: traditional LSM Trees can resolve file contents during compaction without addressing the key index. More details please visist WiscKey in my Blog Concurrency in LSM Trees The main concurrency challenges in LSM Trees are related to switching table views (collections of memory- and disk-resident tables that change during flush and compaction) and log synchronization. Memtables are also generally accessed concurrently (except core-partitioned stores such as ScyllaDB), but concurrent in-memory data structures are out of the scope of this book. Flush Rules The new memtable has to become available for reads and writes. The old (flushing) memtable has to remain visible for reads. The flushing memtable has to be written on disk. Discarding a flushed memtable and making a flushed disk-resident table have to be performed as an atomic operation. The write-ahead log segment, holding log entries of operations applied to the flushed memtable, has to be discarded. Approaches Apache Cassandra solves these problems by using operation order barriers: all operations that were accepted for write will be waited upon prior to the memtable flush. This way the flush process (serving as a consumer) knows which other processes (acting as producers) depend on it. Synchronization points Memtable switch: After this, all writes go only to the new memtable, making it primary, while the old one is still available for reads. Flush finalization: Replaces the old memtable with a flushed disk-resident table in the table view. Write-ahead log truncation: Discards a log segment holding records associated with a flushed memtable. Log Stacking Many modern filesystems are log structured: they buffer writes in a memory segment and flush its contents on disk when it becomes full in an append-only manner SSDs use log-structured storage, too, to deal with small random writes, minimize write overhead, improve wear leveling, and increase device lifetime. LSM Trees and SSDs are a good match, since sequential workloads and append-only writes help to reduce amplification from inplace updates, which negatively affect performance on SSDs. If we stack multiple log-structured systems on top each other, we can run into several problems that we were trying to solve using LSS, including write amplification, fragmentation, and poor performance. At the very least, we need to keep the SSD flash translation layer and the filesystem in mind when developing our applications FTL Flash Translation Layer Using a log-structuring mapping layer in SSDs is motivated by two factors: small random writes have to be batched together in a physical page the fact that SSDs work by using program/erase cycles. Writes can be done only into previously erased pages. This means that a page cannot be programmed (in other words, written) unless it is empty (in other words, was erased). A single page cannot be erased, and only groups of pages in a block (typically holding 64 to 512 pages) can be erased together. Figure 7-13 shows a schematic representation of pages, grouped into blocks. The flash translation layer (FTL) translates logical page addresses to their physical locations and keeps track of page states (live, discarded, or empty). When FTL runs out of free pages, it has to perform garbage collection and erase discarded pages. There are no guarantees that all pages in the block that is about to be erased are discarded. Before the block can be erased, FTL has to relocate its live pages to one of the blocks containing empty pages. Figure 7-14 shows the process of moving live pages from one block to new locations. When all live pages are relocated, the block can be safely erased, and its empty pages become available for writes. Since FTL is aware of page states and state transitions and has all the necessary information, it is also responsible for SSD wear leveling. Wear leveling distributes the load evenly across the medium, avoiding hotspots, where blocks fail prematurely because of a high number of program-erase cycles. It is required, since flash memory cells can go through only a limited number of program-erase cycles, and using memory cells evenly helps to extend the lifetime of the device. Summary In summary, the motivation for using log-structured storage on SSDs is to amortize I/O costs by batching small random writes together, which generally results in a smaller number of operations and, subsequently, reduces the number of times the garbage collection is triggered. Filesystem Logging On top of that, we get filesystems, many of which also use logging techniques for write buffering to reduce write amplification and use the underlying hardware optimally. Log stacking manifests in a few different ways. First, each layer has to perform its own bookkeeping, and most often the underlying log does not expose the information necessary to avoid duplicating the efforts. Figure 7-15 shows a mapping between a higher-level log (for example, the application) and a lower-level log (for example, the filesystem) resulting in redundant logging and different garbage collection patterns. Misaligned segment writes can make the situation even worse, since discarding a higher-level log segment may cause fragmentation and relocation of the neighboring segments’ parts. Because layers do not communicate LSS-related scheduling (for example, discarding or relocating segments), lower-level subsystems might perform redundant operations on discarded data or the data that is about to be discarded. Similarly, because there’s no single, standard segment size, it may happen that unaligned higher-level segments occupy multiple lower-level segments. All these overheads can be reduced or completely avoided. Even though we say that log-structured storage is all about sequential I/O, we have to keep in mind that database systems may have multiple write streams (for example, log writes parallel to data record writes). When considered on a hardware level, interleaved sequential write streams may not translate into the same sequential pattern: blocks are not necessarily going to be placed in write order. Figure 7-16 shows multiple streams overlapping in time, writing records that have sizes not aligned with the underlying hardware page size. This results in fragmentation that we tried to avoid. To reduce interleaving, some database vendors recommend keeping the log on a separate device to isolate workloads and be able to reason about their performance and access patterns independently. However, it is more important to keep partitions aligned to the underlying hardware and keep writes aligned to page size. LLAMA and Mindful Stacking Bw-Tree is layered on top of a latch-free, log-structured, access-method aware (LLAMA) storage subsystem. This layering allows Bw-Trees to grow and shrink dynamically, while leaving garbage collection and page management transparent for the tree. Here, we’re most interested in the access-method aware part, demonstrating the benefits of coordination between the software layers. Without access-method awareness, interleaved delta nodes that belong to different logical nodes will be written in their insertion order. Bw-Tree awareness in LLAMA allows for the consolidation of several delta nodes into a single contiguous physical location. If two updates in delta nodes cancel each other (for example, an insert followed by delete), their logical consolidation can be performed as well, and only the latter delete can be persisted. LSS garbage collection can also take care of consolidating the logical Bw-Tree node contents. This means that garbage collection will not only reclaim the free space, but also significantly reduce the physical node fragmentation. If garbage collection only rewrote several delta nodes contiguously, they would still take the same amount of space, and readers would need to perform the work of applying the delta updates to the base node. At the same time, if a higher-level system consolidated the nodes and wrote them contiguously to the new locations, LSS would still have to garbage-collect the old versions. By being aware of Bw-Tree semantics, several deltas may be rewritten as a single base node with all deltas already applied during garbage collection. This reduces the total space used to represent this Bw-Tree node and the latency required to read the page while reclaiming the space occupied by discarded pages. You can see that, when considered carefully, stacking can yield many benefits. It is not necessary to always build tightly coupled single-level structures. Good APIs and exposing the right information can significantly improve efficiency. Open-Channel SSDs An alternative to stacking software layers is to skip all indirection layers and use the hardware directly. For example, it is possible to avoid using a filesystem and flash translation layer by developing for Open-Channel SSDs. This way, we can avoid at least two layers of logs and have more control over wear-leveling, garbage collection, data placement, and scheduling. One of the implementations that uses this approach is LOCS (LSM Tree-based KV Store on Open-Channel SSD) Wang, Peng, Guangyu Sun, Song Jiang, Jian Ouyang, Shiding Lin, Chen Zhang, and Jason Cong. 2014. “An Efficient Design and Implementation of LSM-tree based Key-Value Store on Open-Channel SSD.” EuroSys ’14 Proceedings of the Ninth European Conference on Computer Systems (April): Article 16. https://doi.org/10.1145/2592798.2592804. Another example using Open-Channel SSDs is LightNVM, implemented in the Linux kernel Bjørling, Matias, Javier González, and Philippe Bonnet. 2017. “LightNVM: the Linux open-channel SSD subsystem.” In Proceedings of the 15th Usenix Conference on File and Storage Technologies (FAST’17), 359-373. USENIX. You can draw a parallel with using the O_DIRECT flag to bypass the kernel page cache, which gives better control, but requires manual page management. Software Defined Flash (SDF) [OUYANG14 SDF: software-defined flash for web-scale internet storage systems.”], a hardware/software codesigned OpenChannel SSDs system, exposes an asymmetric I/O interface that takes SSD specifics into consideration. Sizes of read and write units are different, and write unit size corresponds to erase unit size (block), which greatly reduces write amplification. This setting is ideal for log-structured storage, since there’s only one software layer that performs garbage collection and relocates pages. Additionally, developers have access to internal SSD parallelism, since every channel in SDF is exposed as a separate block device, which can be used to further improve performance. Hiding complexity behind a simple API might sound compelling, but can cause complications in cases in which software layers have different semantics. Exposing some underlying system internals may be beneficial for better integration. Summary Log-structured storage is used everywhere: from the flash translation layer, to filesystems and database systems. It helps to reduce write amplification by batching small random writes together in memory. To reclaim space occupied by removed segments, LSS periodically triggers garbage collection. LSM Trees take some ideas from LSS and help to build index structures managed in a log-structured manner: writes are batched in memory and flushed on disk; shadowed data records are cleaned up during compaction. It is important to remember that many software layers use LSS, and make sure that layers are stacked optimally. Alternatively, we can skip the filesystem level altogether and access hardware directly ","link":"https://blog.shunzi.tech/post/database-internal-lsm-tree-summary/"},{"title":"Database Internal - B-tree Summary","content":" The B-tree basic in database system. The B-tree basic in database system. Here is the basic of B-tree, including the knowledge of data structure and some optimization. The content is based on the book Database Internal and Algorithms Visualization.I will present my understanding. Only summary, not details. Before B-Tree The B-tree is usually used for index of data. I want to introduce a simple data structure which is used in simple query operation. Sorted List: If data is sorted, we can look up the data quickly. Two major methods: (Eg. Look up 12) Linear Search: From the start/end of the list, compare the data with target data 12 one by one. Time complexity: O(N)O(N)O(N) Binray Search: Get the max, min, and middle data of this sorted list by reading the data with given index (0, list.length and list.length / 2). Compare these data with target data to find the target data located area. In small area, recurse until find the target data or mark it not found. Time complexity: O(log⁡2Nx)O(\\log_2{Nx })O(log2​Nx) B-Tree B-Tree was introduced by Rudolph Bayer and Edward M. McCreight back in 1971. Binary Search Tree (BST) Sorted, In-Memory data structure. Used for efficient key-value lookups. Represented by a key, a value associated with this key. Each node splits the search space into left and right subtrees, as figure shows: a node key is greater than any key stored in its left subtree and less than any key stored in its right subtree. Look up from root node, if greater than root node, try to look up in the right child tree. If less, continue to look up in left child tree. Recurse until find it or compared all leaf nodes. Tree Balancing Why need balancing ? Insert operations do not follow any specific pattern, and element insertion might lead to the situation where the tree is unbalanced (i.e., one of its branches is longer than the other one). The worst-case scenario is shown in Figure (b), where we end up with a pathological tree, which looks more like a linked list, and instead of desired logarithmic complexity, we get linear, as illustrated in Figure (a). In the balanced tree, following the left or right node pointer reduces the search space in half on average, so lookup complexity is logarithmic: O(log⁡2N)O(\\log_2{N})O(log2​N). If the tree is not balanced, worst-case complexity goes up to O(N)O(N)O(N), since we might end up in the situation where all elements end up on one side of the tree. How to balance? Balance Status: Balancing is done by reorganizing nodes in a way that minimizes tree height and keeps the number of nodes on each side within bounds. One of the ways to keep the tree balanced is to perform a rotation step after nodes are added or removed. If the insert operation leaves a branch unbalanced (two consecutive nodes in the branch have only one child), we can rotate nodes around the middle one. In the example shown in Figure 2-4, during rotation the middle node (3), known as a rotation pivot, is promoted one level higher, and its parent becomes its right child. Trees for Disk-Based Storage As previously mentioned, unbalanced trees have a worst-case complexity of O(N)O(N)O(N). Balanced trees give us an average O(log⁡2N)O(\\log_2{N})O(log2​N). At the same time, due to low fanout (fanout is the maximum allowed number of children per node), we have to perform balancing, relocate nodes, and update pointers rather frequently. Increased maintenance costs make BSTs impractical as on-disk data structures. Problems Locality: since elements are added in random order, there’s no guarantee that a newly created node is written close to its parent, which means that node child pointers may span across several disk pages. We can improve the situation to a certain extent by modifying the tree layout and using paged binary trees. Tree height: Since binary trees have a fanout of just two, height is a binary logarithm of the number of the elements in the tree, and we have to perform O(log⁡2N)O(\\log_2{N})O(log2​N) seeks to locate the searched element and, subsequently, perform the same number of disk transfers. 2-3 Trees and other low-fanout trees have a similar limitation: while they are useful as in-memory data structures, small node size makes them impractical for external storage. A naive on-disk BST implementation would require as many disk seeks as comparisons, since there’s no built-in concept of locality. This sets us on a course to look for a data structure that would exhibit this property. Requirements High fanout to improve locality of the neighboring keys. Low height to reduce the number of seeks during traversal. Fanout and height are inversely correlated. Disk-Based Structures On-disk data structures are often used when the amounts of data are so large that keeping an entire dataset in memory is impossible or not feasible. Only a fraction of the data can be cached in memory at any time, and the rest has to be stored on disk in a manner that allows efficiently accessing it. Hard Disk Drives On spinning disks, seeks increase costs of random reads because they require disk rotation and mechanical head movements to position the read/write head to the desired location. However, once the expensive part is done, reading or writing contiguous bytes (i.e., sequential operations) is relatively cheap. The smallest transfer unit of a spinning drive is a sector, so when some operation is performed, at least an entire sector can be read or written. Sector sizes typically range from 512 bytes to 4 Kb. Head positioning is the most expensive part of an operation on the HDD. This is one of the reasons we often hear about the positive effects of sequential I/O: reading and writing contiguous memory segments from disk. Benefit a lot from the performance gap between sequential I/O and random I/O. Solid State Drives A typical SSD is built of memory cells, connected into strings (typically 32 to 64 cells per string), strings are combined into arrays, arrays are combined into pages, and pages are combined into blocks. Depending on the exact technology used, a cell can hold one or multiple bits of data. Pages vary in size between devices, but typically their sizes range from 2 to 16 Kb. Blocks typically contain 64 to 512 pages. Blocks are organized into planes and, finally, planes are placed on a die. SSDs can have one or more dies. You can view the details of SSD structure from this post Virtual SSD Since in both device types (HDDs and SSDs) we are addressing chunks of memory rather than individual bytes (i.e., accessing data block-wise), most operating systems have a block device abstraction. It hides an internal disk structure and buffers I/O operations internally, so when we’re reading a single word from a block device, the whole block containing it is read. This is a constraint we cannot ignore and should always take into account when working with disk-resident data structures. In SSDs, we don’t have a strong emphasis on random versus sequential I/O, as in HDDs, because the difference in latencies between random and sequential reads is not as large. There is still some difference caused by prefetching, reading contiguous pages, and internal parallelism. Even though garbage collection is usually a background operation, its effects may negatively impact write performance, especially in cases of random and unaligned write workloads. Writing only full blocks, and combining subsequent writes to the same block, can help to reduce the number of required I/O operations. On-Disk Structures Limitations The cost of disk access itself. Major: The smallest unit of disk operation is a block. Ideas On-disk structures are designed with their target storage specifics in mind and generally optimize for fewer disk accesses. We can do this by improving locality, optimizing the internal representation of the structure, and reducing the number of out-of-page pointers. From above chapters, we came to the conclusion that high fanout and low height are desired properties for an optimal on-disk data structure. We’ve also just discussed additional space overhead coming from pointers, and maintenance overhead from remapping these pointers as a result of balancing. B-Trees combine these ideas: increase node fanout reduce tree height the number of node pointers the frequency of balancing operations. Paged Binary Trees Laying out a binary tree by grouping nodes into pages Improves the situation with locality. To find the next node, it’s only necessary to follow a pointer in an already fetched page. However, there’s still some overhead incurred by the nodes and pointers between them. Laying the structure out on disk and its further maintenance are nontrivial endeavors, especially if keys and values are not presorted and added in random order. Balancing requires page reorganization, which in turn causes pointer updates. Ubiquitous B-Trees B-Tree is not Binary Tree. It means Balanced Tree. B-Tree builds a hierarchy that helps to navigate and locate the searched items quickly. B-Trees build upon the foundation of balanced search trees and are different in that they have higher fanout (have more child nodes) and smaller height. Differences with binary trees: binary tree nodes are drawn as circles. Since each node is responsible just for one key and splits the range into two parts B-Tree nodes are often drawn as rectangles, and pointer blocks are also shown explicitly to highlight the relationship between child nodes and separator keys. If we use same way to depicte binary tree, and it will look like as follows: B-Trees are sorted: keys inside the B-Tree nodes are stored in order. Because of that, to locate a searched key, we can use an algorithm like binary search. This also implies that lookups in B-Trees have logarithmic complexity. Since B-Tree nodes store dozens or even hundreds of items, we only have to make one disk seek per level jump. Using B-Trees, we can efficiently execute both point and range queries. B-Tree Hierarchy Components B-Trees consist of multiple nodes. Each node holds up to N keys and N + 1 pointers to the child nodes. These nodes are logically grouped into three groups: Root node Leaf nodes Internal nodes Concepts Since B-Trees are a page organization technique (i.e., they are used to organize and navigate fixed-size pages), we often use terms node and page interchangeably. The relation between the node capacity and the number of keys it actually holds is called occupancy. fanout: the number of keys stored in each node. Higher fanout helps to amortize the cost of structural changes required to keep the tree balanced and to reduce the number of seeks by storing keys and pointers to child nodes in a single block or multiple consecutive blocks. Balancing operations (namely, splits and merges) are triggered when the nodes are full or nearly empty. B+ Trees B-Trees allow storing values on any level: in root, internal, and leaf nodes. B+-Trees store values only in leaf nodes. Internal nodes store only separator keys used to guide the search algorithm to the associated value stored on the leaf level. Since values in B+-Trees are stored only on the leaf level, all operations (inserting, updating, removing, and retrieving data records) affect only leaf nodes and propagate to higher levels only during splits and merges. B+B^+B+ Tree is used in MySQL index. You can view the details about differences between B+ tree and B-tree by clicking https://www.cnblogs.com/liqiangchn/p/9060521.html. Separator Keys Keys stored in B-Tree nodes are called index entries, separator keys, or divider cells. They split the tree into subtrees (also called branches or subranges), holding corresponding key ranges. Keys are stored in sorted order to allow binary search. A subtree is found by locating a key and following a corresponding pointer from the higher to the lower level. Some B-Tree variants also have sibling node pointers, most often on the leaf level, to simplify range scans。(Such as B+ tree) These pointers help avoid going back to the parent to find the next sibling. Some implementations have pointers in both directions, forming a double-linked list on the leaf level, which makes the reverse iteration possible. Storage Utilization: Since B-Trees reserve extra space inside nodes for future insertions and updates, tree storage utilization can get as low as 50%, but is usually considerably higher. Higher occupancy does not influence B-Tree performance negatively. How to build B-Tree? What sets B-Trees apart is that, rather than being built from top to bottom (as binary search trees), they’re constructed the other way around—from bottom to top. The number of leaf nodes grows, which increases the number of internal nodes and tree height. B-Tree Lookup Complexity 2 standpoints: the number of block transfers the number of comparisons done during the lookup Block transfers Assume number of keys per node is N; K times more nodes on each new level; M is a total number of items in the B-Tree. To find a searched key, at most log⁡KM\\log_K{M}logK​M pages are addressed. Comparisons done during the lookup From the perspective of number of comparisons, the logarithm base is 2, since searching a key inside each node is done using binary search. Every comparison halves the search space, so complexity is log⁡2M\\log_2 Mlog2​M. B-Tree Lookup Algorithm To find an item in a B-Tree, we have to perform a single traversal from root to leaf. The objective of this search is to find a searched key or its predecessor. Finding an exact match is used for point queries, updates, and deletions Finding its predecessor is useful for range scans and inserts. The algorithm starts from the root and performs a binary search, comparing the searched key with the keys stored in the root node until it finds the first separator key that is greater than the searched value. This locates a searched subtree. As soon as we find the subtree, we follow the pointer that corresponds to it and continue the same search process (locate the separator key, follow the pointer) until we reach a target leaf node, where we either find the searched key or conclude it is not present by locating its predecessor. During the point query, the search is done after finding or failing to find the searched key. During the range scan, iteration starts from the closest found key-value pair and continues by following sibling pointers until the end of the range is reached or the range predicate is exhausted. Couting Keys B-Tree Node Splits To insert the value into a B-Tree, we first have to locate the target leaf and find the insertion point. For that, we use the algorithm described in the previous section. After the leaf is located, the key and value are appended to it. Updates in B-Trees work by locating a target leaf node using a lookup algorithm and associating a new value with an existing key. If the target node doesn’t have enough room available, we say that the node has overflowed and has to be split in two to fit the new data. Split conditions: For leaf nodes: if the node can hold up to N key-value pairs, and inserting one more key-value pair brings it over its maximum capacity N. For nonleaf nodes: if the node can hold up to N + 1 pointers, and inserting one more pointer brings it over its maximum capacity N + 1. Splits are done by allocating the new node, transferring half the elements from the splitting node to it, and adding its first key and pointer to the parent node. In this case, we say that the key is promoted. The index at which the split is performed is called the split point (also called the midpoint). All elements after the split point (including split point in the case of nonleaf node split) are transferred to the newly created sibling node. The rest of the elements remain in the splitting node. If the parent node is full and does not have space available for the promoted key and pointer to the newly created node, it has to be split as well. This operation might propagate recursively all the way to the root. As soon as the tree reaches its capacity (i.e., split propagates all the way up to the root), we have to split the root node. When the root node is split, a new root, holding a split point key, is allocated. The old root (now holding only half the entries) is demoted to the next level along with its newly created sibling, increasing the tree height by one. The tree height change conditions: the root node is split and the new root is allocated two nodes are merged to form a new root On the leaf and internal node levels, the tree only grows horizontally. i.e. Leaf node split during the insertion of 11. New element and promoted key are shown in gray. i.e. Nonleaf node split during the insertion of 11. New element and promoted key are shown in gray. Steps Allocate a new node. Copy half the elements from the splitting node to the new one. Place the new element into the corresponding node. At the parent of the split node, add a separator key and a pointer to the new node. B-Tree Node Merges If neighboring nodes have too few values (i.e., their occupancy falls under a threshold), the sibling nodes are merged. This situation is called underflow. It may need to merge nodes when delete elements. Merge Conditions For leaf nodes: if a node can hold up to N key-value pairs, and a combined number of key-value pairs in two neighboring nodes is less than or equal to N. For nonleaf nodes: if a node can hold up to N + 1 pointers, and a combined number of pointers in two neighboring nodes is less than or equal to N + 1. How to merge nodes i.e. DELETE 16 - Leaf node merge: Generally, elements from the right sibling are moved to the left one, but it can be done the other way around as long as the key order is preserved. i.e. DELETE 10 - Nonleaf node merge: During the merge of nonleaf nodes, we have to pull the corresponding separator key from the parent (i.e., demote it). The number of pointers is reduced by one because the merge is a result of the propagation of the pointer deletion from the lower level, caused by the page removal. Just as with splits, merges can propagate all the way to the root level. Steps Copy all elements from the right node to the left one. Remove the right node pointer from the parent (or demote it in the case of a nonleaf merge). Remove the right node. Summary Binary search trees might have similar complexity characteristics, but still fall short of being suitable for disk because of low fanout and a large number of relocations and pointer updates caused by balancing. B-Trees solve both problems by increasing the number of items stored in each node (high fanout) and less frequent balancing operations. We can use this knowledge to create in-memory B-Trees. To create a disk-based implementation, we need to go into details of how to lay out B-Tree nodes on disk and compose on-disk layout using data-encoding formats. File Formats Explore how exactly B-Trees and other structures are implemented on disk. We access the disk in a way that is different from how we access main memory. From an application developer’s perspective, memory accesses are mostly transparent. Because we do not have to manage virtual memory offsets manually. Disks are accessed using system calls. We usually have to specify the offset inside the target file, and then interpret on-disk representation into a form suitable for main memory. The semantics of pointer management in on-disk structures are somewhat different from in-memory ones. It is useful to think of on-disk B-Trees as a page management mechanism: algorithms have to compose and navigate pages. Pages and pointers to them have to be calculated and placed accordingly. Motivation The differences between memory and disks when we program with: Memory: Unmanaged memory model. We allocate a block of data and slice it any way we like, using fixed-size primitives and structures. If we want to reference a larger chunk of memory or a structure with variable size, we use pointers. We can allocate more memory any time we need (within reasonable bounds) without us having to think or worry about whether or not there’s a contiguous memory segment available, whether or not it is fragmented, or what happens after we free it. Disk: We have to take care of garbage collection and fragmentation ourselves. For a disk-resident data structure to be efficient, we need to lay out data on disk in ways that allow quick access to it, and consider the specifics of a persistent storage medium, come up with binary data formats, and find a means to serialize and deserialize data efficiently. Even though the operating system and filesystem take over some of the responsibilities, implementing on-disk structures requires attention to more details and has more pitfalls. Binary Encoding To store data on disk efficiently, it needs to be encoded using a format that is compact and easy to serialize and deserialize. Since we do not have primitives such as malloc and free, but only read and write, we have to think of accesses differently and prepare data accordingly. Before we can organize records into pages, we need to understand how to represent keys and data records in binary form, how to combine multiple values into more complex structures, and how to implement variable-size types and arrays Primitive Types Most numeric data types are represented as fixed-size values. When working with multibyte numeric values, it is important to use the same byte-order (endianness) for both encoding and decoding. Endianness determines the sequential order of bytes: Big-endian: The order starts from the most-significant byte (MSB), followed by the bytes in decreasing significance order. In other words, MSB has the lowest address. Little-endian: The order starts from the least-significant byte (LSB), followed by the bytes in increasing significance order. Records consist of primitives like numbers, strings, booleans, and their combinations. However, when transferring data over the network or storing it on disk, we can only use byte sequences. In order to send or write the record, we have to serialize it (convert it to an interpretable sequence of bytes) Before we can use it after receiving or reading, we have to deserialize it (translate the sequence of bytes back to the original record). Number Different numeric types may vary in size. byte value is 8 bits short is 2 bytes (16 bits) int is 4 bytes (32 bits) long is 8 bytes (64 bits) Floating-point numbers (such as float and double) are represented by their sign, fraction, and exponent. IEEE 754 Standard: A 32-bit float represents a single-precision value. For example, a floating-point number 0.15652 has a binary representation, as shown in Figure 3-2. 计算机基础——IEEE754标准的浮点数的转化 The first 23 bits represent a fraction The following 8 bits represent an exponent 1 bit represents a sign (whether or not the number is negative). The double represents a double-precision floating-point value. Strings and Variable-Size Data Composing more complex values together is much like struct in C. You can combine primitive values into structures and use fixed-size arrays or pointers to other memory regions. Strings and other variable-size data types (such as arrays of fixed-size data) can be serialized as a number, representing the length of the array or string, followed by size bytes: the actual data. For strings, this representation is often called UCSD String or Pascal String, named after the popular implementation of the Pascal programming language. String { size uint_16; data byte[size]; } Bit-Packed Data: Booleans, Enums, and Flags Booleans can be represented either by using a single byte, or encoding true and false as 1 and 0 values. Enums, short for enumerated types, can be represented as integers and are often used in binary formats and communication protocols. Enums are used to represent oftenrepeated low-cardinality values. enum NodeType { ROOT, // 0x00h INTERNAL, // 0x01h LEAF // 0x02h }; Flags, kind of a combination of packed booleans and enums. Flags can represent nonmutually exclusive named boolean parameters. int IS_LEAF_MASK = 0x01h; // bit #1 int VARIABLE_SIZE_VALUES = 0x02h; // bit #2 int HAS_OVERFLOW_PAGES = 0x04h; // bit #3 Just like packed booleans, flag values can be read and written from the packed value using bitmasks and bitwise operators. // Set the bit flags |= HAS_OVERFLOW_PAGES; flags |= (1 &lt;&lt; 2); // Unset the bit flags &amp;= ~HAS_OVERFLOW_PAGES; flags &amp;= ~(1 &lt;&lt; 2); // Test whether or not the bit is set is_set = (flags &amp; HAS_OVERFLOW_PAGES) != 0; is_set = (flags &amp; (1 &lt;&lt; 2)) != 0; General Principles How the addressing is going to be done Whether the file is going to be split into same-sized pages, which are represented by a single block or multiple contiguous blocks. Most in-place update storage structures use pages of the same size, since it significantly simplifies read and write access. Append-only storage structures often write data page-wise, too: records are appended one after the other and, as soon as the page fills up in memory, it is flushed on disk. The file usually starts with a fixed-size header and may end with a fixed-size trailer, which hold auxiliary information that should be accessed quickly or is required for decoding the rest of the file. Many data stores have a fixed schema, specifying the number, order, and type of fields the table can hold. Having a fixed schema helps to reduce the amount of data stored on disk: instead of repeatedly writing field names, we can use their positional identifiers. It is similar to relation database. ex. Company Employee. Now, to access first_name, we can slice first_name_length bytes after the fixed-size area. To access last_name, we can locate its starting position by checking the sizes of the variable-size fields that precede it. To avoid calculations involving multiple fields, we can encode both offset and length to the fixed-size area. In this case, we can locate any variable-size field separately. Database files often consist of multiple parts, with a lookup table aiding navigation and pointing to the start offsets of these parts written either in the file header, trailer, or in the separate file. Page Structure Database systems store data records in data and index files. These files are partitioned into fixed-size units called pages, which often have a size of multiple filesystem blocks. Page sizes usually range from 4 to 16 Kb. The original B-Tree paper [BAYER72] describes a simple page organization for fixedsize data records, where each page is just a concatenation of triplets, as shown in Figure 3-4: keys are denoted by k, associated values are denoted by v, and pointers to child pages are denoted by p. Some downsides: Appending a key anywhere but the right side requires relocating elements It doesn’t allow managing or accessing variable-size records efficiently and works only for fixed-size data. Slotted Pages When storing variable-size records, the main problem is free space management: reclaiming the space occupied by removed records.\\ To simplify space management for variable-size records, we can split the page into fixed-size segments. However, we end up wasting space if we do that, too. For example, if we use a segment size of 64 bytes, unless the record size is a multiple of 64, we waste 64 - (n modulo 64) bytes, where n is the size of the inserted record. In other words, unless the record is a multiple of 64, one of the blocks will be only partially filled. Space reclamation can be done by simply rewriting the page and moving the records around, but we need to preserve record offsets, since out-of-page pointers might be using these offsets. Goal Store variable-size records with a minimal overhead. Reclaim space occupied by the removed records. Reference records in the page without regard to their exact locations. Slotted Page To efficiently store variable-size records such as strings, binary large objects (BLOBs), etc., we can use an organization technique called slotted page. Used in PostgreSQL. We organize the page into a collection of slots or cells and split out pointers and cells in two independent memory regions residing on different sides of the page. A slotted page has a fixed-size header that holds important information about the page and cells. Cells may differ in size and can hold arbitrary data: keys, pointers, data records, etc. Figure 3-5 shows a slotted page organization, where every page has a maintenance region (header), cells, and pointers to them. How to achieve the goal? Minimal overhead: the only overhead incurred by slotted pages is a pointer array holding offsets to the exact positions where the records are stored. Space reclamation: space can be reclaimed by defragmenting and rewriting the page. Dynamic layout: from outside the page, slots are referenced only by their IDs, so the exact location is internal to the page. Cell Layout On a cell level, we have a distinction between key and key-value cells. Key cells hold a separator key and a pointer to the page between two neighboring pointers. Key-value cells hold keys and data records associated with them. We assume that all cells within the page are uniform (for example, all cells can hold either just keys or both keys and values; similarly, all cells hold either fixed-size or variable-size data, but not a mix of both). This means we can store metadata describing cells once on the page level, instead of duplicating it in every cell. Key Cell To compose a key cell, we need to know: Cell type (can be inferred from the page metadata) Key size ID of the child page this cell is pointing to Key bytes A variable-size key cell layout might look something like this (a fixed-size one would have no size specifier on the cell level: We have grouped fixed-size data fields together, followed by key_size bytes. With Fixed Size key, the offset is easy to be calculated by using static and precomputed offsets. For Variable size key, we need to use the key_size for every key offset calculation. Key-Value Cells The key-value cells hold data records instead of the child page IDs. Cell type (can be inferred from page metadata) Key size Value size Key bytes Data record bytes * Combining Cells into Slotted Pages We append cells to the right side of the page (toward its end) and keep cell offsets/pointers in the left side of the page, as shown in Figure 3-6. Keys can be inserted out of order and their logical sorted order is kept by sorting cell offset pointers in key order. This design allows appending cells to the page with minimal effort, since cells don’t have to be relocated during insert, update, or delete operations. ex. Two names are added to thepage, and their insertion order is: Tom and Leslie. As you can see in Figure 3-7, their logical order (in this case, alphabetical), does not match insertion order (order in which they were appended to the page). Cells are laid out in insertion order, but offsets are re-sorted to allow using binary search. Now, we’d like to add one more name to this page: Ron. New data is appended at the upper boundary of the free space of the page, but cell offsets have to preserve the lexi‐cographical key order: Leslie, Ron, Tom. To do that, we have to reorder cell offsets: pointers after the insertion point are shifted to the right to make space for the new pointer to the Ron cell, as you can see in Figure 3-8. Managing Variable-Size Data Removing an item from the page does not have to remove the actual cell and shift other cells to reoccupy the freed space. Instead the cell can be marked as deleted an in-memory availability list can be updated with the amount of freed memory and a pointer to the freed value. The availability list stores offsets of freed segments and their sizes. When inserting a new cell, we first check the availability list to find if there’s a segment where it may fit. Some database(Ex.SQLite) may store the the first available segments and total number of available bytes within the page to quickly check whether or not we can fit a new element into the page after defragmenting it. Strategy to calculate fit First Fit: This might cause a larger overhead, since the space remaining after reusing the first suitable segment might be too small to fit any other cell, so it will be effectively wasted. Best Fit: For best fit, we try to find a segment for which insertion leaves the smallest remainder. Others If we cannot find enough consecutive bytes to fit the new cell but there are enough fragmented bytes available, live cells are read and rewritten, defragmenting the page and reclaiming space for new writes. If there’s not enough free space even after defragmentation, we have to create an overflow page. To improve locality (especially when keys are small in size), some implementations store keys and values separately on the leaf level.Keeping keys together can improve the locality during the search. After the searched key is located, its value can be found in a value cell with a corresponding index. (Sorted) With variable-size keys, this requires us to calculate and store an additional value cell pointer Summary In summary, to simplify B-Tree layout, we assume that each node occupies a single page. And a page consists of fixed-size header cell pointer block cells Cells hold keys and pointers to the pages representing child nodes or associated data records. BTrees use simple pointer hierarchies: page identifiers to locate the child nodes in the tree file, and cell offsets to locate cells within the page. Versioning The binary file format can change. Most of the time, any storage engine version has to support more than one serialization format. (e.g., current and one or more legacy formats for backward compatibility). To support that, we have to be able to find out which version of the file we’re up against. Apache Cassandra is using version prefixes in filenames. This way, you can tell which version the file has without even opening it. As of version 4.0, a data file name has the na prefix, such as na-1-bigData.db. Older files have different prefixes: files written in version 3.0 have the ma prefix. Alternatively, the version can be stored in a separate file. For example, PostgreSQL stores the version in the PG_VERSION file. The version can also be stored directly in the index file header. In this case, a part of the header (or an entire header) has to be encoded in a format that does not change between versions. After finding out which version the file is encoded with, we can create a version-specific reader to interpret the contents. Checksumming Files on disk may get damaged or corrupted by software bugs and hardware failures. To identify these problems preemptively and avoid propagating corrupt data to other subsystems or even nodes, we can use checksums and cyclic redundancy checks (CRCs). Checksums provide the weakest form of guarantee and aren’t able to detect corruption in multiple bits. They’re usually computed by using XOR with parity checks or summation. CRCs can help detect burst errors (e.g., when multiple consecutive bits got corrupted) and their implementations usually use lookup tables and polynomial division. Multibit errors are crucial to detect, since a significant percentage of failures in communication networks and storage devices manifest this way. Noncryptographic hashes and CRCs should not be used to verify whether or not the data has been tampered with. For this, you should always use strong cryptographic hashes designed for security. The main goal of CRC is to make sure that there were no unintended and accidental changes in data. These algorithms are not designed to resist attacks and intentional changes in data Since computing a checksum over the whole file is often impractical and it is unlikely we’re going to read the entire content every time we access it, page checksums are usually computed on pages and placed in the page header. This way, checksums can be more robust (since they are performed on a small subset of the data), and the whole file doesn’t have to be discarded if corruption is contained in a single page Implementing B-Trees Page Header The page header holds information about the page that can be used for navigation, maintenance, and optimizations. Flags that describe page contents and layout. Number of cells in the page. Lower and upper offsets marking the empty space (used to append cell offsets and data) Other useful metadata PostgreSQL stores the page size and layout version in the header. MySQL InnoDB, page header holds the number of heap records, level, and some other implementation-specific values. In SQLite, page header stores the number of cells and a rightmost pointer. Magic Numbers One of the values often placed in the file or page header is a magic number. Usually, it’s a multibyte block, containing a constant value that can be used to signal that the block represents a page, specify its kind, or identify its version. Magic numbers are often used for validation and sanity checks. It’s very improbable that the byte sequence at a random offset would exactly match the magic number. If it did match, there’s a good chance the offset is correct. Ex. during write we can place the magic number 50 41 47 45 (hex for PAGE) into the header. During the read, we validate the page by comparing the four bytes from the read header with the expected byte sequence. Sibling Links Some implementations store forward and backward links, pointing to the left and right sibling pages. These links help to locate neighboring nodes without having to ascend back to the parent. (One of differences between B-tree and B+Tree). Benefits: Range Query Performance Loss: Add complexity to split and merge operations as may need to update sibling offsets. Rightmost Pointers B-Tree separator keys have strict invariants: they’re used to split the tree into subtrees and navigate them, so there is always one more pointer to child pages than there are keys. In many implementations, nodes look more like the ones displayed in Figure 4-2: each separator key has a child pointer, while the last pointer is stored separately, since it’s not paired with any key. SQLite stored the extra pointer in the header. If the rightmost child is split and the new cell is appended to its parent, the rightmost child pointer has to be reassigned. After the split, the cell appended to the parent (shown in gray) holds the promoted key and points to the split node. The pointer to the new node is assigned instead of the previous rightmost pointer. Node High Keys ","link":"https://blog.shunzi.tech/post/database-internal-b-tree-summary/"},{"title":"Amplification and RUM","content":" Distributed Storage System Basic Serie 2 - Amplification and RUM. Here is the concept of Amplification and RUM in storage system. The content is based on the Read, write &amp; space amplification - pick 2, The RUM Conjecture and CRUM conjecture.I will present my understanding. Only summary, not details. Read, write &amp; space amplification - pick 2 Good things come in threes, then reality bites and you must choose at most two. This choice is well known in distributed systems with CAP, PACELC and FIT. There is a similar choice for database engines. An algorithm can optimize for at most two from read, write and space amplification. This means one algorithm is unlikely to be better than another at all three. Such as B-tree has less read amplification, LSM-tree has less write amplification. Build a framework to describe the storage system. Read, write &amp; space amplification. CAP: Consistency, Availability, Partition (Dsitributed System) PACELC: one has to choose between availability (A) and consistency (C) (as per the CAP theorem), but else (E), even when the system is running normally in the absence of partitions, one has to choose between latency (L) and consistency (C). (Dsitributed System) FIT: A three-way tradeoff between fairness, isolation, and throughput (FIT) (Scalable database system which supports atomic distributed transactions) Purpose Read, write and space amplification explain performance and efficiency when evaluating algorithms for real and potential workloads. They aren't a replacement for Big O notation. They usually assume a specific workload and configuration including RAM size, database size and type of storage. For database system, the amplification is more important than other evaluationg algorithms, since the system contains a lot of software and hardware components. And the test benachmark is also complicated. Read Amp Read-amp is the amount of work done per logical read operation. This can be defined for in-memory databases, persistent databases assuming no cache (worst-case behavior) and persistent databases assuming some cache (average-case behavior). The work done in-memory can be the number of key comparisons and traditional algorithm analysis can be used. The work done on-disk includes the number of bytes transferred and seeks (seeks matter on disks, not on NVM). The work done can also include the cost of decompressing data read from storage which is a function of the read block size and compression algorithm. Read-amp is defined separately for point and range queries. For range queries the range length matters (the number of rows to be fetched). In Linkbench the average range query fetches about 20 rows. Read-amp can also be defined for point queries on keys that don't exist. Some algorithms use a bloom filter to avoid disk IO for keys that don't exist. Queries for non-existent keys is common in some workloads. Bloom filters can't be used for a range query. The most frequent query in Linkbench is a range query that includes an equality predicate on the first two columns of the range query index. With RocksDB we define a prefix bloom filter to benefit from that. Write Amp Write-amp is the amount of work done per write operation. This can include the number of bytes written to storage and disk seeks per logical write operation. This can be split into in-memory and on-disk write-amp but I frequently ignore in-memory write-amp. There is usually a cost to pay in storage reads and writes following a logical write. With write-amp we are ignoring the read cost. The read cost is immediate for an update-in-place algorithm like a B-Tree as a page must be read to modify it. The read cost is deferred for a write-optimized algorithm like an LSM as compaction is done in the background and decoupled from the logical write. There is usually some write cost that is not deferred - updating in-memory structures and writing a redo log. With flash storage there is usually additional write-amp from the garbage collection done by the FTL to provide flash blocks that can be rewritten. Be careful about assuming too much about the benefit of sequential and large writes from a write-optimized database engine. While the physical erase block size on a NAND chip is not huge, many storage devices have something that spans physical erase blocks when doing GC that I will call a logical erase block. When data with different lifetimes ends up in the same logical erase block then the long-lived data will be copied out and increase flash GC write-amp (WAF greater than 1). I look forward to the arrival of multi-stream to reduce flash GC WAF. Space Amp Space-amp is the ratio of the size of the database to the size of the data in the database. Compression decreases space-amp. It is increased by fragmentation with a B-Tree and old versions of rows with an LSM. A low value for space-amp is more important with flash storage than disk because of the price per GB for storage capacity. The RUM Conjecture The ubiquitous fight between the Read, the Update, and the Memory overhead of access methods for modern data systems The fundamental challenges that every researcher, systems architect, or developer faces when designing a new access method are how to minimize, i) read times (R), ii) update cost (U), iii) memory (or storage) overhead (M). In this project we first conjecture that when optimizing the read-update-memory overheads, optimizing in any two areas negatively impacts the third. Based on the RUM Conjecture, at DASlab, we study the manifestation of the balance of the RUM overheads in state-of-the-art access methods, and we pursue a path toward RUM-aware access methods for future data systems. Rum Space When building access methods of modern systems, one is confronted with the same fundamental challenges, and design decisions. In particular, there are three quantities that researchers always try to minimize: (1) the read overhead (R), (2) the update overhead (U), (3) the memory (or storage) overhead (M) Deciding which overhead(s) to optimize for and to what extent, remains a prominent part of the process of designing a new access method, especially as hardware and workloads change over time. The design space of the three RUM overheads can be seen as a three dimensional space or, if projected on a two-dimensional plane, as the triangle shown in the left hand-side, where each access method is mapped to a point or -- if it can be tuned to have varying RUM behavior -- to an area. Notice: This is a triangle, not a triangular pyramid. And the center of this triangle is Incentre. Rum Conjecture An ideal solution is an access method that always provides the lowest read cost, the lowest update cost, and requires no extra memory or storage space over the base data. In practice, data structures are designed to compromise between the three RUM overheads, while the optimal design depends on a multitude of factors such as hardware, workload, and user expectations. Rum Conjecture: When designing access methods we set an upper bound for two of the RUM overheads, this implies a hard lower bound for the third overhead which cannot be further reduced. Rum Overheads in Access Methods Today, when building access methods, there is a wealth of approaches tailored for specific use cases. For example, in order to minimize the cost of updating data, one can use a design based on differential structures, allowing many queries to consolidate updates and avoid the cost of reorganizing data. Such an approach, however, increases the space overhead and hinders read cost as now queries need to merge any relevant pending updates during processing. Another example is that the read cost can be minimized by storing data in multiple different physical layouts, each layout being appropriate for minimizing the read cost for a particular workload. Update and space costs, however, increase because now there are multiple data copies. We further study how existing access methods balance the RUM overheads. Toward Rum-Aware Access Methods The RUM Conjecture opens the path for exciting research challenges towards the goal of creating RUM-adaptive access methods. Future data systems should include versatile tools to interact with the data the way the workload, the application, and the hardware need and not vice versa. In other words, the application, the workload, and the hardware should dictate how we access our data, and not the constraints of our systems. Tuning access methods becomes increasingly important if, on top of big data and hardware, we consider the development of specialized systems and tools to cater data, aiming at servicing a narrow set of applications each. As more systems are built, the complexity of finding the right access method increases as well. As part of this project we design access methods with dynamic and tunable RUM behavior. CRUM conjecture - read, write, space and cache amplification The C in CRUM is the amount of memory per key-value pair (or row) the DBMS needs so that either a point query or the first row from a range query can be retrieved with at most X storage reads. The C can also be reported as the minimal database : memory ratio to achieve at most X storage reads per point query. Cache Amplification The cache-amp describes memory efficiency. It represents the minimal database : memory ratio such that a point query requires at most X storage reads. A DBMS with cache-amp=10 (C=10) needs 10 times more memory than one with C=100 to satisfy the at most X reads constraint. It can be more complicated to consider cache-amp for range seek and range next because processing them is more complicated for an LSM or index+log algorithm. Therefore I usually limit this to point queries. For a few years I limited this to X=1 (at most 1 storage read). But it will be interesting to consider X=2 or 3. With X=1: For a b-tree all but the leaf level must be in cache For an LSM the cache must include all bloom filter and index blocks, all data blocks but the max level For an index+log approach it depends (wait for another blog post) ","link":"https://blog.shunzi.tech/post/amplification-and-rum/"},{"title":"Flavor of IO","content":" Distributed Storage System Basic Serie 1 - Flavor of IO. Distributed Storage System Basic Serie 1 - Flavor of IO. Here is the basic of linux io and try to compare io operations. The content is based on the On Disk IO - Flavors of IO and I will present my understanding. Only summary, not details. Disk I/O Flavors of I/O Syscalls: open, write, read, fsync, sync, close System I/O can be defined as any operation that writes data into the storage layers accessible only to the kernel's address space via the kernel's system call interface. Standard IO: fopen, fwrite, fread, fflush, fclose Usually provided by library Writes using these functions may not result in system calls, meaning that the data still lives in buffers in the application's address space after making such a function call. Vectored IO: writev, readv Memory mapped IO: open, mmap, msync, munmap similar to the system I/O Files are still opened and closed using the same interfaces, but access to the file data is performed by mapping that data into the process' address space, and then performing memory read and write operations as you would with any other application buffer. I/O API System I/O Open: open(), creat() Write: write(), aio_write(), pwrite(), pwritev() Sync: fsync(), sync() Close: close() Stream I/O Open: fopen(), fdopen(), freopen() Write: fwrite(), fputc(), fputs(), putc(), putchar(), puts() Sync: fflush(), followed by fsync() or sync() fflush() flush the data in library cache to the disk (actual page cache) fsync() flush the data in page cache to disks and metedata to disks. (Usually contains 2 operations: data and metedata) fdatasync() flush the data definitely and flush the metedata if necessary.(Such as the file size changed.) Close: fclose() Memory mapped IO Open: open(), creat() Map: mmap() Write: memcpy(), memmove(), read(), or any other routine that writes to application memory Sync: msync() Unmap: munmap() Close: close() Sector/Block/Page Sector is the smallest unit of data transfer for block device. And size is always 512 bytes in most disk devices. The smallest addressable unit of File System is block. Block is a group of multiple adjacent sectors requested by a device driver. 512/1024/2048/4096 bytes. Virtual Memory works with pages, which map to filesystem blocks. Typical page size is 4096 bytes. (Usually IO is done through the Virtual Memory, which caches requested filesystem blocks in memory and serves as a buffer for intermediate operations) Virtual Memory pages map to Filesystem blocks, which map to Block Device sectors. Standard IO When reading the data, Page Cache is addressed first. If the data is absent, the Page Fault is triggered and contents are paged in. (This means that reads, performed against the currently unmapped area will take longer) During writes, buffer contents are first written to Page Cache. ( This means that data does not reach the disk right away. The actual hardware write is done when Kernel decides it’s time to perform a writeback of the dirty page) Standard IO takes a user space buffer (Byte Buffer) and then copies it’s content to the page cache. When the O_DIRECT flag is used, the buffer is written directly to the block device. (It means O_DIRECT will not use page cache) Byte Buffer in Application is implemented by application to try to align with single page(4K) in Page Cache. (For example, in program, we usually create a buffer array to send/receive data) Page Cache Page Cache stores the recently accessed fragments of files that are more likely to be accessed in the nearest time. How Buffered IO works: Applications perform reads and writes through the Kernel Page Cache, which allows sharing pages processes, serving reads from cache and throttling writes to reduce IO. Read OP: Cache Miss: block device -&gt; kernel page cache -&gt; application Cache Hit: kernel page cache -&gt; application Cache evict/flush: LRU Pages (Least Recently Used) Write OP: application -&gt; kernel page cache (Mark Page Dirty) -&gt; (Wait Flush/Writeback) disk Flush/Writeback Strategy: thresholds of dirty page; period flush. https://www.kernel.org/doc/Documentation/sysctl/vm.txt Temporal locality principle (时间局部性原理): Recently accessed pages will be accessed again at some point in nearest future. Spatial Locality (空间局部性): implies that the elements physically located nearby have a good chance of being located close to each other. This principle is used in a process called “prefetch” that loads file contents ahead of time anticipating their access and amortizing some of the IO costs. Page Cache also improves IO performance by delaying writes and coalescing adjacent reads. Buffer cache is same as page cache in recent years. Direct IO Direct IO: Bypass the Page Cache, and offer a fine-grained control over IO operations. Most used in systems which implement cache by self. PostgreSQL use direct io in WAL, since they are sure this log data will not be reused immediately. Notice: fsync() is still required for files opened with O_DIRECT in order to save the data to stable storage. It is decided by specific file system and operations. Some file systems contain metadata cache, if you create a new file, you should write all data and metadata to disks, but O_DIRECT option only can make sure the data be stored on the disk. The metadata stored in file system cache need other sync operations to be flushed to disks. Some operations may not change the metadata, so O_DIRECT option can make sure data be flushed to disk right although without fsync(). So in MySQL configuration, you can set different options for innodb_flush_method, such as O_DIRECT, O_DIRECT_NO_FSYNC. You can view details by clicking https://dev.mysql.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_flush_method You can refer Zhihu：使用O_DIRECT_NO_FSYNC来提升MySQL性能 Block Alignment Unaligned writes. Aligned writes. Whether or not O_DIRECT flag is used, it is always a good idea to make sure your reads and writes are block aligned. Although page cache will help to make sure write is aligned. Nonblocking FileSystem IO There’s no true “nonblocking” Filesystem IO. Compared with sockets i/o, block device operations are considered non-blocking. Filesystem IO delays are not taken into account by the system. Possibly this decision was made because there’s a more or less hard time bound on operation completion. The NIO concept is always used in network. Memory Mapping Memory mapping (mmap) allows you to access a file as if it was loaded in memory entirely. Different mode: private mmap: write would trigger copy-on-write of the page in question in order to leave the original page intact and keep the changes private. shared mmap: the file mapping is shared with other processes so they can see updates to the mapped memory segment. Additionally, changes are carried through to the underlying file (precise control over which requires the use of msync). Memory Space Allocate: Lazy Manner. First I/O operation trigger a page fault and allocate the appropriate page. Adavantages and Disadvantages: Advantages: Avoid creating an extraneous copy of the buffer in memory Avoid system call (less subsequent context switch) Disadvatages: imposes overhead of the kernel data structures required for managing the memory mappings Memory-mapped file size limit: Most of the time, the kernel code is much more memory-friendly anyways and 64 bit architectures allow mapping larger files. Usage: MongoDB default storage engine was mmap-backed. Page cache optimization Page case is at the cost of control loss. One of the ways of informing the kernel about your intentions is using fadvise. FADV_SEQUENTIAL specifies that the file is read sequentially, from lower offsets to higher ones, so the kernel can make sure to fetch the pages in advance, before the actual read occurs. FADV_RANDOM disables read-ahead, evicting pages that are unlikely to be accessed any time soon from the page cache. FADV_WILLNEED notifies the OS that the page will be needed by the process in the near future. This gives the kernel an opportunity to cache the page ahead of time and, when the read operation occurs, to serve it from the page cache instead of page-faulting. FADV_DONTNEED advises the kernel that it can free the cache for the corresponding pages (making sure that the data is synchronised with the disk beforehand). There’s one more flag (FADV_NOREUSE), but on Linux it has no effect. Just as the name suggests, fadvise is only acting advisory. The kernel is not obligated to do exactly as fadvise suggests. Usage: RocksDB call mlock: It allows you to force pages to be held in memory. This means that once the page is loaded into memory, all subsequent operations will be served from the page cache. AIO Asynchronous IO (AIO). AIO is an interface allowing to initiate multiple IO operations and register callbacks that will be triggered on their completion. io_submit allows passing one or multiple commands, holding a buffer, offset and an operation that has to be performed. io_getevents Completions can be queried by using io_getevents, a call that allows to collect result events for corresponding commands. Others Linux AIO has several shortcomings: the syscalls API isn’t exposed by the glibc and requires a library for wiring them up (libaio seems to be the most popular) Despite several attempts to fix that, only file descriptors with O_DIRECT flag are supported, so buffered asynchronous operations won’t work. Besides, some operations, such as stat, fsync, open and some others aren’t fully asynchronou Differences between Linux AIO and POSIX AIO The Posix AIO implementation on Linux is implemented completely in user space and does not use this Linux-specific AIO subsystem at all. Vectored IO One, possibly less popular, method of performing IO operations is Vectored IO (also known as Scatter/Gather). It is called this way because it operates on a vector of buffers and allows reading and writing data to/from disk using multiple buffers per system call. Advantages Such an approach can help by allowing reading smaller chunks (therefore avoiding allocation of large memory areas for contiguous blocks). Reduce the amount of system calls required to fill up all these buffers with data from disk Reads and writes are atomic: The kernel prevents other processes from performing IO on the same descriptor during read and write operations, guaranteeing data integrity. Usage A few databases use the Vectored IO. This might be because general purpose databases work with a bunch of files simultaneously, trying to guarantee liveness for each running operation and reduce their latencies, so and data is accessed and cached block-wise. Vectored IO is more useful for analytics workloads and/or columnar databases, where the data is stored on disk contiguously, and its processing can be done in parallel in sparse blocks. One of the examples is Apache Arrow. Reference Ensuring data reaches disk: https://lwn.net/Articles/457667/ ","link":"https://blog.shunzi.tech/post/flavor-of-io/"},{"title":"Distributed Storage System Basic","content":" Distributed Storage System Basic. Distributed Storage System Basic. Here is the basic of storage and distributed system, including the knowledge of data structure and some benchmarks. The content is based on the reading list recommended by TinyKV and I will present my understanding. Only summary, not details. Basic TinyKV Reading List Index of Reading List It may not be newst version, so you can view the newst index by clicking the above link. And here is following the commit a5c29fcf0702f90e969af5b7bcdc8797ce1898a0 Storage Engine Disk I/O Flavors of I/O https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017 https://medium.com/databasss/on-disk-io-part-2-more-flavours-of-io-c945db3edb13? Amplification and RUM http://smalldatum.blogspot.com/2015/11/read-write-space-amplification-pick-2_23.html http://daslab.seas.harvard.edu/rum-conjecture/ http://smalldatum.blogspot.com/2019/05/crum-conjecture-read-write-space-and.html B-Tree (Optional) Reading Materials Database Internals: 2. B-Tree Basics Database Internals: 3. File Formats Database Internals: 4. Implementing B-Trees Reference https://www.cs.usfca.edu/~galles/visualization/Algorithms.html https://github.com/etcd-io/bbolt LSM-Tree Reading Materials Database Internals: 7. Log-Structured Storage Reference https://github.com/google/leveldb https://github.com/facebook/rocksdb https://github.com/syndtr/goleveldb B-Tree vs LSM-tree https://tikv.org/docs/deep-dive/key-value-engine/b-tree-vs-lsm/ LSM-Tree Evolution Wisckey https://www.usenix.org/system/files/conference/fast16/fast16-papers-lu.pdf HashKV (Optional) https://www.usenix.org/system/files/conference/atc18/atc18-chan.pdf Monkey (Optional) https://stratos.seas.harvard.edu/files/stratos/files/monkeykeyvaluestore.pdf Reference https://github.com/dgraph-io/badger https://github.com/tikv/titan Serializing &amp; RPC Protocol Buffer Reading Materials https://developers.google.com/protocol-buffers/docs/overview Reference https://github.com/dgraph-io/badger https://github.com/protocolbuffers/protobuf gRPC Reading Materials https://www.grpc.io/docs/quickstart/go/ Reference https://github.com/grpc/grpc-go Data Partitioning Range vs Hash https://tikv.org/docs/deep-dive/scalability/data-sharding/ Partitioning of DynamoDB (Optional) https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf https://dzone.com/articles/partitioning-behavior-of-dynamodb Partitioning of TiKV https://pingcap.com/blog/2017-07-11-tidbinternal1/ Replication &amp; Consistency Reading Materials Database Internals: 11. Replication and Consistency CAP vs PACELE http://www.cs.umd.edu/~abadi/papers/abadi-pacelc.pdf Clock and Time https://dzone.com/articles/tick-or-tock-keeping-time-and-order-in-distributed-1 Consistency https://arxiv.org/pdf/1902.03305.pdf Consensus Quorum https://en.wikipedia.org/wiki/Quorum_(distributed_computing) Paxos (Optional) Reading Materials Database Internals: 14. Consensus @ Paxos Reference https://lamport.azurewebsites.net/pubs/paxos-simple.pdf Raft Reading Materials Database Internals: 14. Consensus @ Raft Reference https://raft.github.io/ https://pdos.csail.mit.edu/6.824/labs/lab-raft.html Scale &amp; Balance Multi-Raft https://tikv.org/deep-dive/scalability/multi-raft/ Split &amp; Merge https://pingcap.com/blog/2017-07-20-tidbinternal3/ Balance https://pingcap.com/blog/2017-07-20-tidbinternal3/ Distributed Transactions Reading Materials Database Internals: 5. Transaction Processing and Recovery Database Internals: 13. Distributed Transactions ACID https://en.wikipedia.org/wiki/ACID Isolation (Optional) https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf https://www.jianshu.com/p/3673e612cce2 (Chinese) Spanner (Optional) https://storage.googleapis.com/pub-tools-public-publication-data/pdf/45855.pdf https://www.jianshu.com/p/f307bd2023f5 (Chinese) Percolator https://tikv.org/docs/deep-dive/distributed-transaction/percolator/ https://tikv.org/docs/deep-dive/distributed-transaction/optimized-percolator/ Coprocessor (Optional) https://blogs.apache.org/hbase/entry/coprocessor_introduction https://zhuanlan.zhihu.com/p/89518391 (Chinese) Benchmark &amp; Testing sysbench (Optional) https://github.com/akopytov/sysbench https://www.percona.com/blog/2019/04/25/creating-custom-sysbench-scripts/ https://www.jianshu.com/p/30933e0bebe7 (Chinese) go-tpc (Optional) https://github.com/pingcap/go-tpc go-ycsb https://github.com/pingcap/go-ycsb https://medium.com/@siddontang/use-go-ycsb-to-benchmarkdifferent-databases-8850f6edb3a7 https://www.jianshu.com/p/73d22befe47d (Chinese) Chaos https://principlesofchaos.org/ https://pingcap.com/blog/chaos-practice-in-tidb/ https://medium.com/@siddontang/use-chaos-to-test-the-distributed-system-linearizability-4e0e778dfc7d Summary Index Disk I/O Flavors of I/O Amplification and RUM B-Tree Database Internal B-tree Summary ","link":"https://blog.shunzi.tech/post/distributed-storage-system-basic/"},{"title":"Head First Golang","content":" Head first Golang. Here is the basic of go-language, including grammer basic and some features. The content is based on the tutorial A tour of Go and I will present my understanding. Furthermore, I will try to anaylyze the priciple Basic A tour of Go (Online) Index of A tour of Go Basic [1] Hello world package main import &quot;fmt&quot; func main() { fmt.Println(&quot;Hello, Golang World!&quot;) } [2] Package [3] Function [4] Variables [5] Basic Type [6] Flow Control Exercise: Loops and Functions Description Exercise: Loops and Functions Solution [7] Poiner, Struct, Slices Exercise: Slices Description Exercise: Slices Solution [8] Map Exercise: Word Count Description Exercies: Word Count Solution package main import ( &quot;strings&quot; &quot;golang.org/x/tour/wc&quot; ) func WordCount(s string) map[string]int { m := make(map[string]int) words := strings.Fields(s) for _, v := range words { m[v]++ } return m } func main() { wc.Test(WordCount) } [9] Function As Value Exercise: Fibonacci closure Description Exercise: Fibonacci closure Solution package main import &quot;fmt&quot; // fibonacci is a function that returns // a function that returns an int. func fibonacci() func() int { fib1 := 0 fib2 := 1 return func() int { fib2 = fib1 + fib2 fib1 = fib2 - fib1 return fib2 - fib1 } } func main() { f := fibonacci() for i := 0; i &lt; 10; i++ { fmt.Println(f()) } } Methods and Interfaces [1] Methods 注意值传递 还是 引用传递 package main import &quot;fmt&quot; func main() { /* 定义局部变量 */ var a int = 100 var b int = 200 var c int = 100 var d int = 200 fmt.Printf(&quot;交换前 a 的值为 : %d\\n&quot;, a) fmt.Printf(&quot;交换前 b 的值为 : %d\\n&quot;, b) /* 通过调用函数来交换值 */ valueSwap(a, b) fmt.Printf(&quot;value_swap 交换后 a 的值 : %d\\n&quot;, a) fmt.Printf(&quot;value_swap 交换后 b 的值 : %d\\n&quot;, b) fmt.Printf(&quot;交换前 c 的值为 : %d\\n&quot;, c) fmt.Printf(&quot;交换前 d 的值为 : %d\\n&quot;, d) refSwap(&amp;c, &amp;d) fmt.Printf(&quot;pref_swap 交换后 c 的值为 : %d\\n&quot;, c) fmt.Printf(&quot;pref_swap 交换后 d 的值为 : %d\\n&quot;, d) } /* 定义相互交换值的函数 */ func valueSwap(x, y int) int { var temp int temp = x /* 保存 x 的值 */ x = y /* 将 y 值赋给 x */ y = temp /* 将 temp 值赋给 y*/ return temp } /* 定义交换值函数*/ func refSwap(x *int, y *int) { var temp int temp = *x /* 保持 x 地址上的值 */ *x = *y /* 将 y 值赋给 x */ *y = temp /* 将 temp 值赋给 y */ } [2] Interfaces 根据函数名来确定接口实现关系 package main import &quot;fmt&quot; type Phone interface { call(param int) string takephoto() } type Huawei struct { } func (huawei Huawei) call(param int) string{ fmt.Println(&quot;i am Huawei, i can call you!&quot;, param) return &quot;damon&quot; } func (huawei Huawei) takephoto() { fmt.Println(&quot;i can take a photo for you&quot;) } func main(){ var phone Phone phone = new(Huawei) phone.takephoto() r := phone.call(50) fmt.Println(r) } [3] Type Assertions package main import &quot;fmt&quot; // type switches func do(i interface{}) { switch v := i.(type) { case int: fmt.Printf(&quot;Twice %v is %v\\n&quot;, v, v*2) case string: fmt.Printf(&quot;%q is %v bytes long\\n&quot;, v, len(v)) default: fmt.Printf(&quot;I don't know about type %T!\\n&quot;, v) } } func testTypeSwitchs() { do(21) do(&quot;hello&quot;) do(true) } W func testTypeAssert() { var i interface{} = &quot;hello&quot; // t := i.(T) s := i.(string) fmt.Println(s) // f = i.(float64) // panic // fmt.Println(f) // t, ok := i.(T) s, ok := i.(string) fmt.Println(s, ok) f, ok := i.(float64) fmt.Println(f, ok) } func main() { testTypeAssert() testTypeSwitchs() } [4] Stringers package main import &quot;fmt&quot; // Person struct type Person struct { Name string Age int } func (p Person) String() string { return fmt.Sprintf(&quot;%v (%v years)&quot;, p.Name, p.Age) } func testStringers() { a := Person{&quot;Arthur Dent&quot;, 42} z := Person{&quot;Zaphod Beeblebrox&quot;, 9001} fmt.Println(a, z) } // IPAddr type type IPAddr [4]byte func (ip IPAddr) String() string { return fmt.Sprintf(&quot;%v.%v.%v.%v&quot;, ip[0], ip[1], ip[2], ip[3]) } func exerciseTest() { hosts := map[string]IPAddr{ &quot;loopback&quot;: {127, 0, 0, 1}, &quot;googleDNS&quot;: {8, 8, 8, 8}, } for name, ip := range hosts { fmt.Printf(&quot;%v: %v\\n&quot;, name, ip) } } func main() { testStringers() exerciseTest() } [5] Errors package main import &quot;fmt&quot; func main() { fmt.Println(&quot;外层开始&quot;) defer func() { fmt.Println(&quot;外层准备recover&quot;) if err := recover(); err != nil { fmt.Printf(&quot;%#v-%#v\\n&quot;, &quot;外层&quot;, err) // err已经在上一级的函数中捕获了，这里没有异常，只是例行先执行defer，然后执行后面的代码 } else { fmt.Println(&quot;外层没做啥事&quot;) } fmt.Println(&quot;外层完成recover&quot;) }() fmt.Println(&quot;外层即将异常&quot;) f() fmt.Println(&quot;外层异常后&quot;) defer func() { fmt.Println(&quot;外层异常后defer&quot;) }() } func f() { fmt.Println(&quot;内层开始&quot;) defer func() { fmt.Println(&quot;内层recover前的defer&quot;) }() defer func() { fmt.Println(&quot;内层准备recover&quot;) if err := recover(); err != nil { fmt.Printf(&quot;%#v-%#v\\n&quot;, &quot;内层&quot;, err) // 这里err就是panic传入的内容 } fmt.Println(&quot;内层完成recover&quot;) }() defer func() { fmt.Println(&quot;内层异常前recover后的defer&quot;) }() panic(&quot;异常信息&quot;) defer func() { fmt.Println(&quot;内层异常后的defer&quot;) }() fmt.Println(&quot;内层异常后语句&quot;) //recover捕获的一级或者完全不捕获这里开始下面代码不会再执行 } [6] Readers package main import ( &quot;fmt&quot; &quot;io&quot; &quot;os&quot; &quot;strings&quot; ) func basicReaderTest() { r := strings.NewReader(&quot;Hello, Reader!&quot;) b := make([]byte, 8) for { n, err := r.Read(b) fmt.Printf(&quot;n = %v err = %v b = %v\\n&quot;, n, err, b) fmt.Printf(&quot;b[:n] = %q\\n&quot;, b[:n]) if err == io.EOF { break } } } // Implement a Reader type that emits an infinite stream of the ASCII character 'A'. // import &quot;golang.org/x/tour/reader&quot; // MyReader struct type MyReader struct{} func (r MyReader) Read(b []byte) (int, error) { for i := 0; i &lt; len(b); i++ { b[i] = 'A' } return len(b), nil } func readerExercise() { // reader.Validate(MyReader{}) } type rot13Reader struct { r io.Reader } func (rot *rot13Reader) Read(b []byte) (int, error) { n, e := rot.r.Read(b) for i := 0; i &lt; n; i++ { if (b[i] &gt;= 'A' &amp;&amp; b[i] &lt; 'N') || (b[i] &gt;= 'a' &amp;&amp; b[i] &lt; 'n') { b[i] += 13 } else if (b[i] &gt; 'M' &amp;&amp; b[i] &lt;= 'Z') || (b[i] &gt; 'm' &amp;&amp; b[i] &lt;= 'z') { b[i] -= 13 } } return n, e } func rot13ReaderTest() { // Build string reader s := strings.NewReader(&quot;Lbh penpxrq gur pbqr!&quot;) // Build rot13 reader r := rot13Reader{s} // Call reader.read io.Copy(os.Stdout, &amp;r) } func main() { basicReaderTest() rot13ReaderTest() } [7] Images package main import ( &quot;fmt&quot; &quot;image&quot; &quot;image/color&quot; //&quot;golang.org/x/tour/pic&quot; ) // package image // type Image interface { // ColorModel() color.Model // Bounds() Rectangle // At(x, y int) color.Color // } func testImage() { m := image.NewRGBA(image.Rect(0, 0, 100, 100)) fmt.Println(m.Bounds()) fmt.Println(m.At(0, 0).RGBA()) } // Image Struct type Image struct{ w, h int } // ColorModel method func (i Image) ColorModel() color.Model { return color.RGBAModel } // Bounds method func (i Image) Bounds() image.Rectangle { return image.Rect(0, 0, i.w, i.h) } // At method func (i Image) At(x, y int) color.Color { r, g, b := uint8(x*y), uint8(x^y), uint8((x+y)/2) return color.RGBA{r, g, b, 255} } func imageExercise() { // m := Image{1000, 1000} // pic.ShowImage(m) } func main() { testImage() } Concurrency [1] Goroutines 关于 Go 的并发相关可以单独再开一篇介绍了，此处不表。 package main import ( &quot;fmt&quot; &quot;time&quot; ) func say(s string) { for i := 0; i &lt; 5; i++ { time.Sleep(100 * time.Millisecond) fmt.Println(s) } } func goroutinesTest() { go say(&quot;world&quot;) say(&quot;hello&quot;) } func main() { goroutinesTest() } [2] Channels Exercise Binary Tree Desc Exercise Binary Tree Solution package main import ( &quot;fmt&quot; &quot;golang.org/x/tour/tree&quot; ) // Walk walks the tree t sending all values // from the tree to the channel ch. func Walk(t *tree.Tree, ch chan int) { if t != nil { Walk(t.Left, ch) ch &lt;- t.Value Walk(t.Right, ch) } } func goWalk(t *tree.Tree, ch chan int) { Walk(t, ch) close(ch) } // Same determines whether the trees // t1 and t2 contain the same values. func Same(t1, t2 *tree.Tree) bool { ch1, ch2 := make(chan int, 10), make(chan int, 10) go goWalk(t1, ch1) go goWalk(t2, ch2) for { v1, ok1 := &lt;-ch1 v2, ok2 := &lt;-ch2 if ok1 != ok2 || v1 != v2 { return false } if ok1 == false { break } } return true } func main() { ch := make(chan int) go Walk(tree.New(1), ch) fmt.Println(Same(tree.New(1), tree.New(2))) fmt.Println(Same(tree.New(1), tree.New(1))) fmt.Println(Same(tree.New(2), tree.New(1))) } [3] Mutex Exercise Web Crawler Desc Exercise Web Crawler Solution package main import ( &quot;fmt&quot; // No longer needed for sleeping. //&quot;time&quot; &quot;sync&quot; ) type Fetcher interface { // Fetch returns the body of URL and // a slice of URLs found on that page. Fetch(url string) (body string, urls []string, err error) } type SafeCounter struct { v map[string]bool mux sync.Mutex wg sync.WaitGroup } var c SafeCounter = SafeCounter{v: make(map[string]bool)} func (s SafeCounter) checkvisited(url string) bool { s.mux.Lock() defer s.mux.Unlock() _, ok := s.v[url] if ok == false { s.v[url] = true return false } return true } // Crawl uses fetcher to recursively crawl // pages starting with url, to a maximum of depth. func Crawl(url string, depth int, fetcher Fetcher) { // TODO: Fetch URLs in parallel. // TODO: Don't fetch the same URL twice. // This implementation doesn't do either: defer c.wg.Done() if depth &lt;= 0 { return } if c.checkvisited(url) { return } body, urls, err := fetcher.Fetch(url) if err != nil { fmt.Println(err) return } fmt.Printf(&quot;found: %s %q\\n&quot;, url, body) for _, u := range urls { c.wg.Add(1) go Crawl(u, depth-1, fetcher) } return } func main() { c.wg.Add(1) Crawl(&quot;http://golang.org/&quot;, 4, fetcher) c.wg.Wait() // Not ideal to sleep on the main thread. //time.Sleep(5*time.Second) } // fakeFetcher is Fetcher that returns canned results. type fakeFetcher map[string]*fakeResult type fakeResult struct { body string urls []string } func (f fakeFetcher) Fetch(url string) (string, []string, error) { if res, ok := f[url]; ok { return res.body, res.urls, nil } return &quot;&quot;, nil, fmt.Errorf(&quot;not found: %s&quot;, url) } // fetcher is a populated fakeFetcher. var fetcher = fakeFetcher{ &quot;http://golang.org/&quot;: &amp;fakeResult{ &quot;The Go Programming Language&quot;, []string{ &quot;http://golang.org/pkg/&quot;, &quot;http://golang.org/cmd/&quot;, }, }, &quot;http://golang.org/pkg/&quot;: &amp;fakeResult{ &quot;Packages&quot;, []string{ &quot;http://golang.org/&quot;, &quot;http://golang.org/cmd/&quot;, &quot;http://golang.org/pkg/fmt/&quot;, &quot;http://golang.org/pkg/os/&quot;, }, }, &quot;http://golang.org/pkg/fmt/&quot;: &amp;fakeResult{ &quot;Package fmt&quot;, []string{ &quot;http://golang.org/&quot;, &quot;http://golang.org/pkg/&quot;, }, }, &quot;http://golang.org/pkg/os/&quot;: &amp;fakeResult{ &quot;Package os&quot;, []string{ &quot;http://golang.org/&quot;, &quot;http://golang.org/pkg/&quot;, }, }, } ","link":"https://blog.shunzi.tech/post/head-first-go-lang/"},{"title":"S3FS","content":" FUSE-based file system backed by Amazon S3 基于对象存储的文件系统，国内有 aliyun/ossfs, huaweicloud/obsfs 本质是实现了将对象存储的 Bucket 挂载到了本地文件系统中，从而通过本地文件系统操作 OSS 上的对象 S3FS S3FS Github Repo Ubuntu Man Page S3FS Overfiew S3FS 是一个 FUSE 文件系统，它允许您将 Amazon S3 bucket 挂载为本地文件系统。它在 S3 中以自然和透明的方式存储文件。您可以使用其他程序来访问相同的文件)。s3fs 可以处理的对象的最大大小取决于 Amazon S3。例如，当使用单一 PUT API 时，可以达到 5GB。当使用多部分上传 API 时，最多支持 5TB。 s3fs 是稳定的，并在许多生产环境中使用，例如，rsync 备份到 s3。 Dependency FUSE FUSE: File System In User Space. 用户态文件系统，实现逻辑可以在用户态层面代码高度定制，相比于传统的内核文件系统如 Ext4 等，易用性更高，效率因为用户态和内核态的频繁切换相对较低。 内核必须支持 FUSE, 2.6.18-164 之前的内核可能不支持 FUSE。虚拟专用服务器(VPS)可能没有在其内核中编译 FUSE 支持。 黑客画家：什么是 FUSE？ S3 Standard Object Storage 由于是基于对象存储构建的存储系统，所以需要一个对象存储系统，S3FS 则是针对实现了 S3 标准的对象存储构建的本地文件系统。 Features 支持POSIX 文件系统的大部分功能，包括文件读写，目录，链接操作，权限， uid/gid，以及扩展属性（extended attributes） 支持随机写和追加写 大文件使用分片上传 通过在服务端拷贝实现 rename MD5 数据完整性校验 内存缓存元数据 本地磁盘数据缓存 ... Limitations 不能完全等价于内核态的文件系统，无论是性能还是语义支持方面都存在一定差距。 随机写和追加写可能导致整个文件的重写，可以使用分片上传进行优化 元数据的相关操作，例如枚举，性能较差，因为要访问对象存储 底层对象存储系统的最终一致性可能会使得 S3FS 暂时产生一些过时的数据 文件/文件夹的rename操作不是原子的。 多客户端挂载同一个 Bucket 时需要协调各个客户端的行为，避免对一个文件的不同客户端的并发写 不支持 hard link 不适合用在高并发读/写的场景，这样会让系统的 load 升高 原理 机制 1.读取文件 S3FS 对于文件的存储分为临时文件和缓存两种方式，用户可以在命令行中通过 use_cache 参数指定缓存目录来启动缓存方式。 用户通过 offset 和 size 来读取指定文件中的特定区域，如果本地没有相应的内容 S3FS 会通过网络请求 S3 上的相应内容，并且将对应的内容存储到本地的临时文件或者缓存中。 2.文件逻辑架构 不管是临时文件还是缓存文件，S3FS 都用同一个逻辑架构组织这个文件，S3FS 使用一个页的列表来代表一个文件，每页都是这个文件中的一部分： 组成： FDPage: 代表了一个文件中的一页，即也是文件中的部分内容。 //------------------------------------------------ // fdpage &amp; PageList //------------------------------------------------ // page block information struct fdpage { off_t offset; // 偏移量 off_t bytes; // 页的大小 bool loaded; // 是否加载到了本地 bool modified; // 是否修改 // 构造函数 fdpage(off_t start = 0, off_t size = 0, bool is_loaded = false, bool is_modified = false) : offset(start), bytes(size), loaded(is_loaded), modified(is_modified) {} // 获取下一页对应的偏移量 off_t next(void) const { return (offset + bytes); } // 获取该页的末尾对应的偏移量 off_t end(void) const { return (0 &lt; bytes ? offset + bytes - 1 : 0); } }; // Page List typedef std::list&lt;struct fdpage&gt; fdpage_list_t; PageList：代表一个本地文件，它是 FDPage 的一个链表。 class PageList { friend class FdEntity; // only one method access directly pages. private: fdpage_list_t pages; public: // 页状态枚举 enum page_status{ PAGE_NOT_LOAD_MODIFIED = 0, PAGE_LOADED, PAGE_MODIFIED, PAGE_LOAD_MODIFIED }; private: void Clear(void); bool Compress(bool force_modified = false); bool Parse(off_t new_pos); bool RawGetUnloadPageList(fdpage_list_t&amp; dlpages, off_t offset, off_t size); public: static void FreeList(fdpage_list_t&amp; list); explicit PageList(off_t size = 0, bool is_loaded = false, bool is_modified = false); explicit PageList(const PageList&amp; other); ~PageList(); bool Init(off_t size, bool is_loaded, bool is_modified); off_t Size(void) const; bool Resize(off_t size, bool is_loaded, bool is_modified); bool IsPageLoaded(off_t start = 0, off_t size = 0) const; // size=0 is checking to end of list bool SetPageLoadedStatus(off_t start, off_t size, PageList::page_status pstatus = PAGE_LOADED, bool is_compress = true); bool FindUnloadedPage(off_t start, off_t&amp; resstart, off_t&amp; ressize) const; off_t GetTotalUnloadedPageSize(off_t start = 0, off_t size = 0) const; // size=0 is checking to end of list int GetUnloadedPages(fdpage_list_t&amp; unloaded_list, off_t start = 0, off_t size = 0) const; // size=0 is checking to end of list bool GetLoadPageListForMultipartUpload(fdpage_list_t&amp; dlpages); bool GetMultipartSizeList(fdpage_list_t&amp; mplist, off_t partsize) const; bool IsModified(void) const; bool ClearAllModified(void); bool Serialize(CacheFileStat&amp; file, bool is_output, ino_t inode); void Dump(void); }; FdEntity：对一个文件的全面描述，包括页链表、本地文件描述符、文件路径等。 class FdEntity { private: static bool mixmultipart; // whether multipart uploading can use copy api. pthread_mutex_t fdent_lock; bool is_lock_init; int refcnt; // reference count 用于刷回 std::string path; // object path int fd; // file descriptor(tmp file or cache file) FILE* pfile; // file pointer(tmp file or cache file) ino_t inode; // inode number for cache file headers_t orgmeta; // original headers at opening off_t size_orgmeta; // original file size in original headers pthread_mutex_t fdent_data_lock;// protects the following members PageList pagelist; std::string upload_id; // for no cached multipart uploading when no disk space etaglist_t etaglist; // for no cached multipart uploading when no disk space off_t mp_start; // start position for no cached multipart(write method only) off_t mp_size; // size for no cached multipart(write method only) std::string cachepath; // local cache file path // (if this is empty, does not load/save pagelist.) std::string mirrorpath; // mirror file path to local cache file path private: static int FillFile(int fd, unsigned char byte, off_t size, off_t start); static ino_t GetInode(int fd); void Clear(void); ino_t GetInode(void); int OpenMirrorFile(void); bool SetAllStatus(bool is_loaded); // [NOTE] not locking //bool SetAllStatusLoaded(void) { return SetAllStatus(true); } bool SetAllStatusUnloaded(void) { return SetAllStatus(false); } public: static bool SetNoMixMultipart(void); explicit FdEntity(const char* tpath = NULL, const char* cpath = NULL); ~FdEntity(); void Close(void); bool IsOpen(void) const { return (-1 != fd); } int Open(headers_t* pmeta = NULL, off_t size = -1, time_t time = -1, bool no_fd_lock_wait = false); bool OpenAndLoadAll(headers_t* pmeta = NULL, off_t* size = NULL, bool force_load = false); int Dup(bool lock_already_held = false); const char* GetPath(void) const { return path.c_str(); } bool RenamePath(const std::string&amp; newpath, std::string&amp; fentmapkey); int GetFd(void) const { return fd; } bool IsModified(void) const { return pagelist.IsModified(); } bool GetStats(struct stat&amp; st, bool lock_already_held = false); int SetCtime(time_t time); int SetMtime(time_t time, bool lock_already_held = false); bool UpdateCtime(void); bool UpdateMtime(void); bool GetSize(off_t&amp; size); bool SetMode(mode_t mode); bool SetUId(uid_t uid); bool SetGId(gid_t gid); bool SetContentType(const char* path); int Load(off_t start = 0, off_t size = 0, bool lock_already_held = false, bool is_modified_flag = false); // size=0 means loading to end int NoCacheLoadAndPost(off_t start = 0, off_t size = 0); // size=0 means loading to end int NoCachePreMultipartPost(void); int NoCacheMultipartPost(int tgfd, off_t start, off_t size); int NoCacheCompleteMultipartPost(void); int RowFlush(const char* tpath, bool force_sync = false); int Flush(bool force_sync = false) { return RowFlush(NULL, force_sync); } ssize_t Read(char* bytes, off_t start, size_t size, bool force_load = false); ssize_t Write(const char* bytes, off_t start, size_t size); bool ReserveDiskSpace(off_t size); }; typedef std::map&lt;std::string, class FdEntity*&gt; fdent_map_t; // key=path, value=FdEntity* IO 流程 读取文件流程 不启用缓存模式：不启用缓存模式下，S3FS 会在本地新建一个临时文件来存储网络传送过来的数据，读取结束后关闭相应的句柄，这样做得好处是如果多个进程同时读取同一个文件就不需要频繁的发起网络请求，当这个临时文件的所有句柄都关闭后这个临时文件也会删除。 if(0 == refcnt){ AutoLock auto_data_lock(&amp;fdent_data_lock); if(!cachepath.empty()){ // [NOTE] // Compare the inode of the existing cache file with the inode of // the cache file output by this object, and if they are the same, // serialize the pagelist. // ino_t cur_inode = GetInode(); if(0 != cur_inode &amp;&amp; cur_inode == inode){ CacheFileStat cfstat(path.c_str()); if(!pagelist.Serialize(cfstat, true, inode)){ S3FS_PRN_WARN(&quot;failed to save cache stat file(%s).&quot;, path.c_str()); } } } if(pfile){ fclose(pfile); pfile = NULL; } fd = -1; inode = 0; if(!mirrorpath.empty()){ if(-1 == unlink(mirrorpath.c_str())){ S3FS_PRN_WARN(&quot;failed to remove mirror cache file(%s) by errno(%d).&quot;, mirrorpath.c_str(), errno); } mirrorpath.erase(); } 启用缓存模式：启用缓存模式下，S3FS 会将 S3 的数据在本地缓存一份，如果磁盘空间不够，S3FS 会删除部分没有连接的文件来预留出磁盘空间。对于需要经常访问的文件，有一份在本地的缓存非常有必要。 Read OP ssize_t FdEntity::Read(char* bytes, off_t start, size_t size, bool force_load) { S3FS_PRN_DBG(&quot;[path=%s][fd=%d][offset=%lld][size=%zu]&quot;, path.c_str(), fd, static_cast&lt;long long int&gt;(start), size); if(-1 == fd){ return -EBADF; } AutoLock auto_lock(&amp;fdent_data_lock); if(force_load){ pagelist.SetPageLoadedStatus(start, size, PageList::PAGE_NOT_LOAD_MODIFIED); } ssize_t rsize; // check disk space if(0 &lt; pagelist.GetTotalUnloadedPageSize(start, size)){ // load size(for prefetch) size_t load_size = size; if(start + static_cast&lt;ssize_t&gt;(size) &lt; pagelist.Size()){ ssize_t prefetch_max_size = max(static_cast&lt;off_t&gt;(size), S3fsCurl::GetMultipartSize() * S3fsCurl::GetMaxParallelCount()); if(start + prefetch_max_size &lt; pagelist.Size()){ load_size = prefetch_max_size; }else{ load_size = pagelist.Size() - start; } } if(!ReserveDiskSpace(load_size)){ S3FS_PRN_WARN(&quot;could not reserve disk space for pre-fetch download&quot;); load_size = size; if(!ReserveDiskSpace(load_size)){ S3FS_PRN_ERR(&quot;could not reserve disk space for pre-fetch download&quot;); return -ENOSPC; } } // Loading int result = 0; if(0 &lt; size){ result = Load(start, load_size, /*lock_already_held=*/ true); } FdManager::FreeReservedDiskSpace(load_size); if(0 != result){ S3FS_PRN_ERR(&quot;could not download. start(%lld), size(%zu), errno(%d)&quot;, static_cast&lt;long long int&gt;(start), size, result); return -EIO; } } // Reading if(-1 == (rsize = pread(fd, bytes, size, start))){ S3FS_PRN_ERR(&quot;pread failed. errno(%d)&quot;, errno); return -errno; } return rsize; } Write OP ssize_t FdEntity::Write(const char* bytes, off_t start, size_t size) { S3FS_PRN_DBG(&quot;[path=%s][fd=%d][offset=%lld][size=%zu]&quot;, path.c_str(), fd, static_cast&lt;long long int&gt;(start), size); if(-1 == fd){ return -EBADF; } // check if not enough disk space left BEFORE locking fd if(FdManager::IsCacheDir() &amp;&amp; !FdManager::IsSafeDiskSpace(NULL, size)){ FdManager::get()-&gt;CleanupCacheDir(); } AutoLock auto_lock(&amp;fdent_data_lock); // check file size if(pagelist.Size() &lt; start){ // grow file size if(-1 == ftruncate(fd, start)){ S3FS_PRN_ERR(&quot;failed to truncate temporary file(%d).&quot;, fd); return -EIO; } // add new area pagelist.SetPageLoadedStatus(pagelist.Size(), start - pagelist.Size(), PageList::PAGE_MODIFIED); } int result = 0; ssize_t wsize; if(0 == upload_id.length()){ // check disk space off_t restsize = pagelist.GetTotalUnloadedPageSize(0, start) + size; if(ReserveDiskSpace(restsize)){ // enough disk space // Load uninitialized area which starts from 0 to (start + size) before writing. if(!FdEntity::mixmultipart){ if(0 &lt; start){ result = Load(0, start, /*lock_already_held=*/ true); } } FdManager::FreeReservedDiskSpace(restsize); if(0 != result){ S3FS_PRN_ERR(&quot;failed to load uninitialized area before writing(errno=%d)&quot;, result); return static_cast&lt;ssize_t&gt;(result); } }else{ // no enough disk space if(0 != (result = NoCachePreMultipartPost())){ S3FS_PRN_ERR(&quot;failed to switch multipart uploading with no cache(errno=%d)&quot;, result); return static_cast&lt;ssize_t&gt;(result); } // start multipart uploading if(0 != (result = NoCacheLoadAndPost(0, start))){ S3FS_PRN_ERR(&quot;failed to load uninitialized area and multipart uploading it(errno=%d)&quot;, result); return static_cast&lt;ssize_t&gt;(result); } mp_start = start; mp_size = 0; } }else{ // already start multipart uploading } // Writing if(-1 == (wsize = pwrite(fd, bytes, size, start))){ S3FS_PRN_ERR(&quot;pwrite failed. errno(%d)&quot;, errno); return -errno; } if(0 &lt; wsize){ pagelist.SetPageLoadedStatus(start, wsize, PageList::PAGE_LOAD_MODIFIED); } // Load uninitialized area which starts from (start + size) to EOF after writing. if(!FdEntity::mixmultipart){ if(pagelist.Size() &gt; start + static_cast&lt;off_t&gt;(size)){ result = Load(start + size, pagelist.Size(), /*lock_already_held=*/ true); if(0 != result){ S3FS_PRN_ERR(&quot;failed to load uninitialized area after writing(errno=%d)&quot;, result); return static_cast&lt;ssize_t&gt;(result); } } } // check multipart uploading if(0 &lt; upload_id.length()){ mp_size += wsize; if(S3fsCurl::GetMultipartSize() &lt;= mp_size){ // over one multipart size if(0 != (result = NoCacheMultipartPost(fd, mp_start, mp_size))){ S3FS_PRN_ERR(&quot;failed to multipart post(start=%lld, size=%lld) for file(%d).&quot;, static_cast&lt;long long int&gt;(mp_start), static_cast&lt;long long int&gt;(mp_size), fd); return result; } // [NOTE] // truncate file to zero and set length to part offset + size // after this, file length is (offset + size), but file does not use any disk space. // if(-1 == ftruncate(fd, 0) || -1 == ftruncate(fd, (mp_start + mp_size))){ S3FS_PRN_ERR(&quot;failed to truncate file(%d).&quot;, fd); return -EIO; } mp_start += mp_size; mp_size = 0; } } return wsize; } 读取文件网络请求流程 通过网络请求 S3 的数据，S3FS 分为了两种，一种是单次请求，一种是多次请求，请求流程如下： 单次请求：默认情况下单次请求的大小在 20M 以下，S3FS 会通过单个请求完成数据的请求。 多次请求：默认情况下如果请求的数据在 20M 以上，S3FS 会切割数据进行多次请求，每次请求 10M 的数据，对单个文件的请求每次最多启动 5 个线程来进行数据的获取，并且是在 5 个线程都请求完成后才会启动下一轮请求。 请求 S3 int FdEntity::Load(off_t start, off_t size, bool lock_already_held, bool is_modified_flag) { AutoLock auto_lock(&amp;fdent_lock, lock_already_held ? AutoLock::ALREADY_LOCKED : AutoLock::NONE); S3FS_PRN_DBG(&quot;[path=%s][fd=%d][offset=%lld][size=%lld]&quot;, path.c_str(), fd, static_cast&lt;long long int&gt;(start), static_cast&lt;long long int&gt;(size)); if(-1 == fd){ return -EBADF; } AutoLock auto_data_lock(&amp;fdent_data_lock, lock_already_held ? AutoLock::ALREADY_LOCKED : AutoLock::NONE); int result = 0; // check loaded area &amp; load fdpage_list_t unloaded_list; if(0 &lt; pagelist.GetUnloadedPages(unloaded_list, start, size)){ for(fdpage_list_t::iterator iter = unloaded_list.begin(); iter != unloaded_list.end(); ++iter){ if(0 != size &amp;&amp; start + size &lt;= iter-&gt;offset){ // reached end break; } // check loading size off_t need_load_size = 0; if(iter-&gt;offset &lt; size_orgmeta){ // original file size(on S3) is smaller than request. need_load_size = (iter-&gt;next() &lt;= size_orgmeta ? iter-&gt;bytes : (size_orgmeta - iter-&gt;offset)); } // download if(S3fsCurl::GetMultipartSize() &lt;= need_load_size &amp;&amp; !nomultipart){ // parallel request result = S3fsCurl::ParallelGetObjectRequest(path.c_str(), fd, iter-&gt;offset, need_load_size); }else{ // single request if(0 &lt; need_load_size){ S3fsCurl s3fscurl; result = s3fscurl.GetObjectRequest(path.c_str(), fd, iter-&gt;offset, need_load_size); }else{ result = 0; } } if(0 != result){ break; } // Set loaded flag pagelist.SetPageLoadedStatus(iter-&gt;offset, iter-&gt;bytes, (is_modified_flag ? PageList::PAGE_LOAD_MODIFIED : PageList::PAGE_LOADED)); } PageList::FreeList(unloaded_list); } return result; } 源码 原理概述： 代码层面 s3fs 的实现方法主要是：利用 fuse 库实现一个文件系统并挂载到本地文件系统的某个目录下，该文件系统的底层并不使用磁盘存储，而是使用 s3 os 存储(因为文件系统是自己实现的，用什么存储，怎么存储可以自己定)； 代码结构 s3fs 项目一共有 11 个头文件，除去一个测试代码文件，一共有10个文件系统实现相关的文件，分别是： curl.h：用于请求s3 os中的文件，多线程 fdcache.h：缓存文件相关类，包括fdpage、pagelist、FdEntity、FdManager，主要用于os中文件分页缓存到本地以及缓存文件的相关管理(比如检查存在、创建、清除等) 注意，若文件句柄持有线程为0，则该文件缓存被清除； psemaphore.h：信号量机制实现类 s3fs.h：s3文件系统实现类，用于利用s3 os 为存储介质，构造一个文件系统挂载到本地文件系统目录中 s3fs_util.h：一些工具类 s3fs_auth.h：用于s3 os的认证 string_util.h：用于处理http协议的(header,body) cache.h：文件缓存，将已读取或加载的文件缓存到本地，以备以后使用(相当于cpu的三级缓存) common.h：主要用于日志记录 add_head.h 同类产品 ossfs AliCloud 官方介绍 Github Repo ossfs 能让您在Linux系统中，将对象存储OSS的存储空间（Bucket）挂载到本地文件系统中，您能够像操作本地文件一样操作OSS的对象（Object），实现数据的共享。 obsfs HuaweiCloud 官方介绍 Github Repo Obsfs 是基于开源 s3fs 修改的。Obsfs 继承了 s3fs 的一些功能，并为华为云OBS服务开发了一些独特的功能。obsfs 允许 Linux 和 Mac OS X 通过 FUSE 挂载 S3 bucket。 References goofys - similar to s3fs but has better performance and less POSIX compatibility. Performance First, POSIX Second We can try to implement self-define backend object storage service. But we must use Go-Language to call functions given by our object operations sdk. s3acker - mount an S3 bucket as a single file S3Proxy - combine with s3fs to mount Backblaze B2, EMC Atmos, Microsoft Azure, and OpenStack Swift buckets. It is used as a S3 Proxy for other object storage service and implements S3 API. s3ql - similar to s3fs but uses its own object format YAS3FS - similar to s3fs but uses SNS to allow multiple clients to mount a bucket 参考链接 [1] AWS: 利用 S3fs 在Amazon EC2 Linux 实例上挂载 S3 存储桶 [2] CSDN: s3fs 实现原理剖析 [3] 滴滴云：基于滴滴云搭建 S3FS 及其实现机制剖析 ","link":"https://blog.shunzi.tech/post/s3fs/"},{"title":"Bigtable: A Distributed Storage System for Structured Data","content":" 2006 年 Google 发表的 Bigtable (Google 三大论文 GFS/MapReduce/BigTable 之一). 原文链接见参考链接【1】 2006 年 Google 发表的 Bigtable (Google 三大论文 GFS/MapReduce/BigTable 之一). 原文链接见参考链接【1】 BigTable 的理论模型之一即为 LSM Tree，BigTable 和底层 GFS 进行了结合 如今的 LSM Tree 的相关实现主要参考了 BigTable。 Abstract BigTable 是一个分布式的结构化数据存储系统，它被设计用来处理海量数据：通常是分布在数千台普通服务器上的 PB 级的数据。Google 的很多项目使用Bigtable 存储数据，包括 Web 索引、Google Earth、 Google Finance。这些应用对 Bigtable 提出的要求差异非常大，无论是在数据量上（从URL到网页到卫星图像）还是在响应速度上（从后端的批量处理到实时数据服务）均如此。尽管应用需求差异很大，但是，针对 Google 的这些产品，Bigtable 还是成功的提供了一个灵活的、高性能的解决方案。 BigTable 实现了多个目标：广泛的适用性，可伸缩性，高性能和高可用性 Data Model 使用一个三元组来作为索引 (row:string, column:string, time:int64)，对应的 value 也为 string. 图示中，则是一个 WebTable 表，用于存储不同网站的所有页面及其相关信息，URL 作为 row key,然后根据信息的类型进行列的区分，如 内容列， 引用了该页面的 anchor 文本列（对应有两个引用者，则对应两列信息）。而 Content 列又对应了多个版本的内容，用不同的时间戳表示版本的不同。 Rows 表中的行键是任意字符串（当前大小最大为64KB，尽管对于大多数用户而言，大小通常是10-100字节）。 单个行键下的每次数据读取或写入都是原子性的（无论该行中读取或写入的不同列的数量如何），该设计决策使客户端在出现并发更新到同一行时更容易推断系统行为。 Bigtable 按行键的字典顺序维护数据。表的行区间是动态分区的。每个行区间称为一个Tablet，它是分配和负载均衡的单位。因此对于行的小范围读取只需要与少部分机器通信。 eg. 例如在 Webtable 中，通过反转 URL 的主机名部分，可以将同一域中的页面分组为连续的行，使得相同域名的页面可以存储在比较靠近的位置，从而便于访问。 Column Families 列键被分组成称为列族的集合，这些集合构成访问控制的基本单元。 列族中存储的所有数据通常都是同一类型（同一列族中的数据会被压缩在一起）。 必须先创建一个列族，然后才能将数据存储在该族中的任何列键下。 创建族后，可以使用族中的任何列键。 Column Key 由 family:qualifier 的形式组成，列族（ column family）名称必须是可打印的，但限定词（qualifier）可以是任意字符串。 访问控制以及磁盘和内存统计均在列族层次执行. eg. Webtable的一个示例列族是 language，它存储编写网页所用的语言。我们在语言族中仅使用一个列键，并且它存储每个网页的语言 ID。此表的另一个有用的列族是锚； 该族中的每个列键都代表一个锚。限定符是引用站点的名称。单元格内容是链接文本。 Timestamps Bigtable 中的每个单元格可以包含同一数据的多个版本；这些版本通过时间戳索引。 Bigtable 时间戳是64位整数。它们可以由 Bigtable 分配，在这种情况下，它们以微秒为单位表示“真实时间”，也可以由客户端应用程序明确分配。需要避免冲突的应用程序必须自己生成唯一的时间戳。单元格 Cell 的不同版本以时间戳的降序存储，因此可以首先读取到最新版本的数据。 为了减少版本化数据的管理工作，我们支持每个列族的两个设置，这些设置告诉Bigtable自动垃圾回收单元格版本。 客户端可以指定仅保留单元格的最后n个版本，或者仅保留足够新的版本（例如，仅保留最近7天写入的值）。 eg. 在我们的Webtable示例中，我们将 content: 列中存储的爬虫网页的时间戳设置为实际爬虫这些页面版本的时间。 上述的垃圾收集机制使我们仅保留每个页面的最新三个版本。 数据示例 数据结构： table{ // ... &quot;aaaaa&quot; : { //一行 &quot;A:foo&quot; : { //一列 15 : &quot;y&quot;, //一个版本 4 : &quot;m&quot; }, &quot;A:bar&quot; : { //一列 15 : &quot;d&quot;, }, &quot;B:&quot; : { //一列 6 : &quot;w&quot; 3 : &quot;o&quot; 1 : &quot;w&quot; } }, // ... } 查询时，如果只给出行列，那么返回的是最新版本的数据；如果给出了行列时间戳，那么返回的是时间小于或等于时间戳的数据。比如，我们查询&quot;aaaaa&quot;/&quot;A:foo&quot;，返回的值是&quot;y&quot;；查询&quot;aaaaa&quot;/&quot;A:foo&quot;/10，返回的结果就是&quot;m&quot;；查询&quot;aaaaa&quot;/&quot;A:foo&quot;/2，返回的结果是空。 API Bigtable API 提供了用于创建和删除表和列族的功能。它还提供了用于更改集群，表和列族元数据的功能，例如访问控制权限。 eg.1 客户端应用程序可以在 Bigtable 中写入或删除值，可以从各个行中查找值，也可以遍历表中的数据子集。 如下代码显示了使用 RowMutation 抽象（对象）来执行一系列更新的 C++ 代码。（省略了详细信息，以使示例简短）通过调用 Apply 对 Webtable 进行了原子修改：它将一个锚点（列）添加到 www.cnn.com 并删除另一个锚点（列）。 // Open the table Table *T = OpenOrDie(&quot;/bigtable/web/webtable&quot;); // Write a new anchor and delete an old anchor RowMutation r1(T, &quot;com.cnn.www&quot;); r1.Set(&quot;anchor:www.c-span.org&quot;, &quot;CNN&quot;); r1.Delete(&quot;anchor:www.abc.com&quot;); Operation op; Apply(&amp;op, &amp;r1); eg.2 显示了使用Scanner抽象（对象）对特定行中的所有锚点进行迭代的C ++代码。客户端可以迭代多个列族，并且有几种机制可以限制扫描产生的行，列和时间戳。 例如，我们可以将上面的扫描限制为仅生成其列与正则表达式 anchor:*.cnn.com 匹配的锚，或者仅生成其时间戳在当前时间的十天内之内的锚。 Scanner scanner(T); ScanStream *stream; stream = scanner.FetchColumnFamily(&quot;anchor&quot;); stream-&gt;SetReturnAllVersions(); scanner.Lookup(&quot;com.cnn.www&quot;); for (; !stream-&gt;Done(); stream-&gt;Next()) { printf(&quot;%s %s %lld %s\\n&quot;, scanner.RowName(), stream-&gt;ColumnName(), stream-&gt;MicroTimestamp(), stream-&gt;Value()); } eg.3 Bigtable支持其他几种功能，这些功能允许用户以更复杂的方式操作数据。 首先，Bigtable 支持单行事务（single-row transaction），该事务可用于对存储在单个行键下的数据执行原子的 “读-修改-写”（read-modify-write） 序列。 Bigtable目前不支持跨行键的常规事务，尽管它提供了用于在客户端跨行键批处理写入的接口。 其次，Bigtable 允许将单元格用作整数计数器。 最后，Bigtable 支持在服务器的地址空间中执行客户端提供的脚本。 可以和 MapReduce 结合。 Building Blocks Bigtable 建立在 Google 其他几个基础架构之上。 Bigtable 使用分布式 Google 文件系统（GFS）存储日志和数据文件。 Bigtable 集群通常运行在与多种其他分布式应用程序共享的服务器池中，并且Bigtable进程通常与其他应用程序的进程共享同一台计算机。 Bigtable 依靠集群管理系统来调度作业、来管理共享计算机上的资源、来处理计算机故障以及监视计算机状态。 SSTable 存储 BigTable 数据 SSTable 提供了从键到值都可以持久化、有序的、不可变的映射表（map），其中键和值都是任意字节字符串。提供操作以查找与指定键相关联的值，并遍历指定键范围内的所有键/值对。 在内部，每个 SSTable 包含一系列块（通常每个块的大小为 64KB，但这是可配置的）；块的索引（存储在 SSTable 的末尾）用于定位块。 读 打开 SSTable 时，索引将加载到内存中。可以使用单次磁盘寻址（ disk seek）执行一次查找：首先对内存中的索引执行二分搜索来找到对应的块索引，然后从磁盘读取相应的块。 可选项是可以将一个 SSTable 全部映射到内存中，这使我们无需与磁盘进行 io 即可执行查找和扫描。 分布式锁 Chubby 提供粗粒度的分布式锁，比如 leader 选举、服务发现。 提供小数据的可靠存储 重点关注可靠性、一致性、扩展性而不是性能，一致性依靠 paxos 解决。 提供简单的语义 参考链接 [1] 知乎：Chubby 分布式锁服务总结 [2] OSDI06 Google Research: The Chubby lock service for loosely-coupled distributed systems BigTable With Chubby 确保任何时候最多一个活跃的 master（active master）； 存储 Bigtable 数据的引导位置（bootstrap location）； 发现 Tablet 服务器并确定 Tablet 服务器的死机； 存储 Bigtable 模式（schema）信息（每个表的列族信息）； 存储用于访问控制的信息而组成的列表 Implementation Components Master Server Master 负责检测集群中的 Tablet Server 组成以及它们的加入和退出事件，会将 Tablet 分配至 Tablet Server，并负责均衡 Tablet Server 间的存储负载以及从 GFS 上回收无用的文件。除外，Master 还负责管理如 Table、Column Family 的创建和删除等 Schema 修改操作。 Tablet Server 每个 Tablet Server 会负责管理若干个由 Master 指定的 Tablet，负责处理针对这些 Tablet 的读写请求，并负责在 Tablet 变得过大时对其进行切分。 客户端数据不会传输到主服务器（master）：客户端直接与Tablet服务器通信以进行读取和写入数据。由于Bigtable客户端不依赖主服务器（master）获取 Tablet 的位置信息，所以大多数客户端从不与主服务器（master）通信。结果，在实践中主服务器（master）是低负载的。 Tablet Server &amp; Table Bigtable 集群会管理若干个 Table，每个 Table 由若干个 Tablet 组成，每个 Tablet 都会关联一个指定的 Row Key 范围，那么这个 Tablet 就包含了该 Table 在该范围内的所有数据。初始时，Table 会只有一个 Tablet，随着 Tablet 增大被 Tablet Server 自动切分，Table 就会包含越来越多的 Tablet。 默认情况下每个Tablet的大小约为100-200 MB。 Tablet Location 使用类似于 B+ 树的三级层次结构来存储 Tablet 位置信息。 第一级是存储在 Chubby 中的文件，它包含 Root Tablet 的位置。 Root Tablet 包含特殊的 METADATA table 中所有Tablet的位置。 每个 METADATA Tablet 都包含一组 User Tablets 的位置。 Root Tablet 只是 METADATA table 中的第一个 Tablet，但经过特殊处理（从不切分），以确保 Tablet 位置层次结构不超过三个级别。 第一级：Chubby 中的一个文件 第二级：METADATA tables（第一个 METADATA table 比较特殊，所以在图中单独画出 (Root Tablet)，但它其实和其他 METADATA table 都属于第二级，即 METADATA tables = 图示中的 1st METADATA Tablet (Root Tablet) + Other METADATA Tablets） 第三级：User Tables 客户端库缓存 Tablet 的位置信息。 如果客户端不知道Tablet的位置，或者发现缓存的位置信息不正确，则它将在Tablet位置层级中向上递归（查找想要的位置信息）。 如果客户的缓存为空，则定位算法需要进行三次网络往返，包括从Chubby中读取一次。 如果客户的缓存过时，则定位算法最多可能需要进行六次往返，因为过时的缓存项仅在未命中时才被发现（假设METADATA Tablet的移动频率不高）。 尽管Tablet位置存储在内存中，所以不需要GFS访问，但在常见情况下，我们通过让客户端库预取Tablet位置来进一步降低了此成本：每当读取METADATA表时，它都会读取一个以上Tablet的元数据。 Tablet Assignment Bigtable Master 利用了 Chubby 来探测 Tablet Server 加入和离开集群的事件。每个 Tablet Server 在 Chubby 上都会有一个对应的唯一文件，Tablet Server 在启动时便会拿到该文件在 Chubby 上的互斥锁，Master 则通过监听这些文件的父目录来检测 Tablet Server 的加入。如果 Tablet Server 失去了互斥锁，那么 Master 就会认为 Tablet Server 已退出集群。尽管如此，只要该文件仍然存在，Tablet Server 就会不断地尝试再次获取它的互斥锁；如果该文件已被删除（见下文），那么 Tablet Server 就会自行关闭。 在了解了集群中有哪些 Tablet Server 后，Master 便需要将 Tablet 分配给 Tablet Server。同一时间，一个 Tablet 只能被分配给一个 Tablet Server。Master 会通过向 Tablet Server 发送 Tablet 载入请求来分配 Tablet。除非该载入请求在 Master 失效前仍未被 Tablet Server 接收到，那么就可以认为此次 Tablet 分配操作已成功：Tablet Server 只会接受来自当前 Master 的节点的请求。当 Tablet Server 决定不再负责某个 Tablet 时，它也会发送请求通知 Master。 Master 在检测到 Tablet Server 失效（互斥锁丢失）后，便会将其负责的 Tablet 重新分配。为此，Master 会尝试在 Chubby 上获取该 Tablet Server 对应的文件的互斥锁，并在成功获取后删除该文件，确保 Tablet Server 能够正确下线。之后，Master 便可顺利将 Tablet 分配至其他 Tablet Server。 如果 Master 与 Chubby 之间的通信连接断开，那么 Master 便会认为自己已经失效并自动关闭。Master 失效后，新 Master 恢复的过程如下： 在 Chubby 上获取 Master 独有的锁，确保不会有另一个 Master 同时启动 利用 Chubby 获取仍有效的 Tablet Server 从各个 Tablet Server 处获取其所负责的 Tablet 列表，并向其表明自己作为新 Master 的身份，确保 Tablet Server 的后续通信能发往这个新 Master Master 确保 Root Tablet 及 METADATA 表的 Tablet 已完成分配 Master 扫描 METADATA 表获取集群中的所有 Tablet，并对未分配的 Tablet 重新进行分配 在开始此扫描（步骤4）之前，如果在步骤3中未找到针对Root Tablet的分配，则主服务器会将Root Tablet添加到未分配Tablet的集合中。此添加操作确保了将对Root Tablet进行分配。由于Root Tablet包含所有METADATA Tablet的名称，因此主服务器在扫描了Root Tablet之后便知道了所有这些名称。 Tablet Serving Tablet的持久化状态存储在GFS中，如图所示。 更新被提交（commit）到一个提交日志（commit log），这些日志存储着重做的记录（redo records）。在这些更新当中，最近提交的更新被存储到内存当中的一个被称为 Memtable 的排序缓冲区，比较老的更新被存储在一系列 SSTable 中。 为了恢复 Tablet，Tablet 服务器从 METADATA table 读取其元数据。该元数据包含 SSTables 列表，该 SSTables 包含一个 Tablet 和一个重做点（redo point）的集合 ，这些重做点（redo point）是指向任何可能包含该 Tablet 数据的提交日志的指针。服务器将 SSTables 的索引读入内存，并通过应用自重做点以来已提交的所有更新来重建 Memtable。 Tablet Server 在载入 Tablet 时，首先需要从 METADATA 表中获取 Tablet 对应的 SSTable 文件及 Commit Log 的日志，并利用 Commit Log 中的条目恢复出 Tablet 的 MemTable。 Write 当写操作到达 Tablet 服务器时，服务器将检查其格式是否正确，以及发送方是否有权执行这个更改（mutation）。通过从 Chubby 文件中读取允许的作者列表来执行授权（这在Chubby客户端缓存中几乎总是命中）。有效的更改（mutation）将写入提交日志（commit log）。整组提交（group commit）用于提高许多小更改的吞吐量。提交写入后，其内容将插入到 memtable 中。其中 MemTable 保持其内部的数据有序。而对于那些已经持久化的数据则会作为一个个 SSTable 文件保存在 GFS 中。 Read 当读操作到达 Tablet 服务器时，同样会检查其格式是否正确以及是否获得适当的授权。在 SSTables 和memtable 序列的合并视图上执行有效的读取操作。由于 SSTables 和 memtable 是按字典顺序排序的数据结构，因此可以有效地形成合并视图。切分和合并 Tablet 时，传入的读写操作可以继续。 首先尝试从 MemTable 中获取所需的最新数据，如果无法查得再从 SSTable 中进行查找。 Compactions Minor Compaction 随着写操作的执行，memtable 的大小增加。 当 memtable 大小达到阈值时，该 memtable 被冻结，创建新的 memtable，并将冻结的 memtable (Immutable Memtable)转换为 SSTable 并写入 GFS。 Immutable Memtable 到 SSTable 的这个过程是一个压缩过程，称为 Minor Compaction。有两个目标： 减少 Tablet 服务器的内存使用量； 如果该服务器死机，那么在恢复期间，压缩将减少必须从提交日志中读取的数据量。 发生压缩时，传入的读取和写入操作可以继续。 Merging Compaction 每一次 Minor Compaction 都会产生一个新的 SSTable 文件，而过多的 SSTable 文件会导致后续的读操作需要扫描更多的 SSTable 文件以获得最新的正确数据。为了限制 SSTable 文件数，Bigtable 会周期地进行 Merging Compaction，将若干个 SSTable 和 MemTable 中的数据原样地合并成一个 SSTable。 Major Compaction Bigtable 还会周期地执行一种被称为 Major Compaction 的特殊 Merging Compaction 操作：在这个过程中，Bigtable 除了会将若干个 SSTable 合并为一个 SSTable，同时将 SSTable 中那些应后续变更或删除操作而被标记为无效的条目移除。 由 non-major compaction（非大型压缩）产生的SSTable可以包含特殊的删除条目（这里删除条目视为存储着：起到删除功能的指令，然而执行指令在：major compaction阶段），这些条目用于删除掉仍然存在于旧SSTable中逻辑上视为已删除的数据（逻辑上视为已删除的数据：客户端无法读取这些数据，即对客户端不可见，然而磁盘上这些数据还在。逻辑上已经不存在，物理上还存在）。 另一方面，major compaction（大型压缩）会产生一个SSTable，该表不包含删除信息或已删除的数据。 Bigtable会遍历其所有Tablet，并定期对其应用major compaction（大型压缩）。 这些major compaction（大型压缩）使Bigtable可以回收已删除数据所使用的资源，还可以确保Bigtable及时地从系统中删除已删除的数据，这对于存储敏感数据的服务很重要。 Refinements Locality Group Bigtable 允许客户端为 Column Family 指定一个 Locality Group，并以 Locality Group 为基础指定其实际的文件存储格式以及压缩方式。 首先，在进行上面我们提到的 Compaction 操作时，Bigtable 会为 Tablet 中的每个 Locality Group 生成独立的 SSTable 文件。由此，用户便可将那些很少同时访问的 Column Famliy 放入到不同的 Locality Group 中，以提高查询效率。除外 Bigtable 也提供了其他基于 Locality Group 的调优参数设置，如设置某个 Locality Group 为 in-memory 等。 在压缩方面，Bigtable 允许用户指定某个 Locality Group 是否要对数据进行压缩以及使用何种格式进行压缩。值得注意的是，Bigtable 对 SSTable 的压缩是基于 SSTable 文件的 Block 进行的，而不是对整个文件直接进行压缩。尽管这会让压缩的效率下降，但这也使得用户在读取数据时 Bigtable 只需要对 SSTable 的某些 Block 进行解压。 eg. Webtable 中的页面元数据（例如语言以及校验和）可以在一个 locality group 中，而页面的内容可以在另一个组中：想要读取元数据的应用程序不需要通读所有页面内容。 主要是根据数据访问的局部性原理与在操作系统中内存页的缓存算法是同理。 Compression 客户端可以控制是否压缩 locality group 的 SSTable，以及如果压缩，则使用哪种压缩格式。用户指定的压缩格式将应用于每个SSTable块（其大小可通过 locality group 的特定的调整参数来控制）。尽管我们通过分别压缩每个块而损失了一些空间，但我们的好处是因为：可以读取SSTable的一小部分而无需解压缩整个文件。 许多客户端使用两阶段自定义压缩方案。第一阶段使用Bentley和McIlroy的方案，该方案在一个大窗口中压缩长的公共字符串。第二阶段使用快速压缩算法，该算法在一个小的16 KB数据窗口中查找重复项。两种压缩过程都非常快——在现代机器上，它们的编码速度为100-200 MB/s，解码速度为 400-1000 MB/s。尽管在选择压缩算法时我们强调速度而不是减少空间，但这种两阶段压缩方案的效果出奇地好。 eg. 例如，在Webtable中，我们使用这种压缩方案来存储Web页面内容。在一个实验中，我们将大量文档存储在一个压缩的 locality group 中。为了进行实验，我们将自己限制为每个文档的一个版本，而不是存储所有可用的版本。该方案使空间减少了10比1。由于Webtable行的布局方式，这比HTML页面上通常的Gzip压缩（3比1或4比1）要好得多：来自单个主机的所有页面都存储得彼此靠近。这使Bentley-McIlroy算法可以识别来自同一主机的页面中的大量共享样板。许多应用程序（不仅是Webtable）都选择其行名致使相似的数据最终聚集在一起，因此实现了很好的压缩率。当我们在Bigtable中存储相同值的多个版本时，压缩率甚至会更高。 Caching for read performance &amp; Bloom Filter 了解过 LSM Tree 的读者可能已经意识到，Bigtable 使用的存储方式正是 LSM Tree：这种存储方式可以将对磁盘的随机写转换为顺序写，代价则是读取性能的下降。LSM Tree 被应用在 Bigtable 上是合情合理的，毕竟 Bigtable 的文件实际上存储在 GFS 中，而 GFS 主要针对顺序写进行优化，对随机写的支持可以说是极差。那么 Bigtable 在使用 LSM Tree 确保了写入性能后，当然就要通过其他的方式来确保自己的读性能了。 Cache 为了提高读取性能，Tablet服务器使用两个级别的缓存。 Scan Cache是一个更高层次的缓存，它将SSTable接口返回的键值对缓存到Tablet服务器。 Block Cache是较低层次的缓存，它缓存从GFS读取的SSTables块。 Scan Cache对于倾向于重复读取相同数据的应用程序最有用。 对于倾向于读取与其最近读取的数据接近的数据的应用程序（例如，顺序读取或对热点行中同一个 locality group 中不同列的随机读取），Block Cache非常有用。 Bloom Filter 读取操作必须从构成Tablet状态的所有SSTable中读取。如果这些SSTable不在内存中，我们可能最终会进行许多磁盘访问。通过允许客户端指定应为特定 locality group 中的SSTable创建Bloom过滤器，我们减少了访问次数。 布隆过滤器允许我们询问SSTable是否可以包含指定行/列对的任何数据。对于某些应用程序，用于存储布隆过滤器的少量Tablet服务器的内存会大大减少读取操作所需的磁盘搜寻次数。 我们对Bloom过滤器的使用还意味着对于不存在的行或列的大多数查找都不需要接触磁盘。 Commit-log implementation Bigtable 使用了 Write-Ahead Log 的做法来确保数据高可用，那么便涉及了大量对 Commit Log 的写入，因此这也是个值得优化的地方。 首先，如果 Bigtable 为不同的 Tablet 使用不同的 Commit Log，那么系统就会有大量的 Commit Log 文件同时写入，提高了底层磁盘寻址的时间消耗。为此，Tablet Server 会把其接收到的所有 Tablet 写入操作写入到同一个 Commit Log 文件中。 这样的设计带来了另一个问题：如果该 Tablet Server 下线，其所负责的 Tablet 可能会被重新分配到其他若干个 Tablet Server 上，它们在恢复 Tablet MemTable 的过程中会重复读取上一个 Tablet Server 产生的 Commit Log。为了解决该问题，Tablet Server 在读取 Commit Log 前会向 Master 发送信号，Master 就会发起一次对原 Commit Log 的排序操作：原 Commit Log 会按 64 MB 切分为若干部分，每个部分并发地按照 (table, row name, log sequence number) 进行排序。完成排序后，Tablet Server 读取 Commit Log 时便可只读取自己需要的那一部分，减少重复读取。 为了保护变化免受GFS延迟高峰的影响，每个Tablet服务器实际上都有两个日志写入线程（一个是被激活也就是正在使用的线程，一个是备用线程），每个线程都写入自己的日志文件。一次仅积极使用这两个线程之一。如果对激活的（active 有些人翻译：活跃的）日志文件的写入性能不佳，则日志文件的写入将切换到另一个线程，并且提交日志队列中的数据变化记录将由新激活的日志写线程进行写入。日志条目包含序列号，以允许恢复过程清除此日志切换过程产生的重复条目。 Speeding up tablet recovery 如果主服务器（master）将 Tablet 从一台 Tablet 服务器移动到另一台 Tablet 服务器，则源 Tablet 服务器首先对该 Tablet 进行 minor compaction（小型压缩）。 这种压缩通过减少 Tablet 服务器的提交日志中未压缩状态的数量来减少恢复时间。 完成这次压缩后，Tablet 服务器将停止为 Tablet 提供服务。 在实际卸载 Tablet 之前，Tablet 服务器会进行另一次（通常非常快） minor compaction（小型压缩）来消除执行第一次 minor compaction（小型压缩）时到达 Tablet 服务器的日志当中任何剩余的未压缩状态。 在完成第二次 minor compaction（小型压缩）后，可将 Tablet 加载到另一台 Tablet 服务器上，而无需恢复日志条目。 Exploiting immutability 除了SSTable缓存外，我们生成的所有SSTable都是不可变的，从而简化了Bigtable系统的其他各个部分。例如，当从SSTables读取数据时，我们不需要对文件系统的访问进行任何同步。结果，可以非常有效地实现对行的并发控制。读取和写入均访问的唯一可变数据结构是memtable。为了减少在读取memtable期间的竞争，我们使每个 memtable 的行使用写时复制 CoW 的策略，并允许读取和写入并行进行。 由于SSTable是不可变的，因此永久删除已删除数据（前面讲过的发出删除指令，但未被执行的数据）的问题被转换为垃圾收集过期的SSTable。每个Tablet的SSTables都注册在 METADATA table 中。主服务器（master）删除过时的SSTables作为SSTables集合上的标记再清除式的垃圾收集[25]，其中 METADATA table 包含根集合（按照前文：METADATA table 记录了这些 SSTable 的对应的 tablet 的 root）。最后，SSTables的不变性使我们能够快速拆分Tablet。我们不必为每个子 Tablet 生成一组新的SSTable，而是让子 Tablet 共享 Tablet 的SSTable。 Evaluation 见论文 Conclusion Reference [1] Google Research: BigTable [2] Paper Google Bigtable 翻译与总结 [3] Bigtable 论文详述 ","link":"https://blog.shunzi.tech/post/BigTable-A-Distributed-Storage-System-for-Structured-Data/"},{"title":"The Log-Structured Merge-Tree (LSM-Tree)","content":" 1996 年发表的 LSM Tree. 原文链接见参考链接【1】 作为 KV 存储中应用最为广泛的存储引擎（数据结构），近年来提出了大量新的优化方案。 此篇为考古文，了解 LSM 设计之初的一些想法以及 LSM 底层原理。 该篇论文只是数据结构和思想的提出，和实际的实现有一定出入。后续结合 Google BigTable 2006 继续学习。 其实我只是为了写作业的文献综述才来考古。/:惭愧&lt; Abstract 原文首先介绍了一个场景来引入 LSM 诞生的必要性。针对有高性能要求的交易系统，往往需要历史记录表来用于相关交易记录的查询或者故障恢复。当交易系统数据量较大时，就需要借助索引来保证查询效率的高校了。但索引显然是一种空间换时间的算法，在后期新数据写入时，维护索引数据的变化将会带来很大的开销，最常用的基于磁盘的索引结构 B Tree在实时维护索引时常常会 double IO 开销，整个系统的开销也将有大幅度的增加。故需要一种低开销的维护索引的方案。 日志结构合并树(LSM-tree)是一种基于磁盘的数据结构，旨在为长时间内经历高记录插入(和删除)率的文件提供低成本索引。核心思想就是将写入推迟(Defer)并转换为批量(Batch)写，首先将大量写入缓存在内存，当积攒到一定程度后，将他们批量写入文件中，这要一次I/O可以进行多条数据的写入，充分利用每一次I/O。同时指出 LSM Tree 更适用于写密集型应用。 Introduction The five minute rule 适当地引入内存，来规划存储系统的总开销将在成本和效率上都能获益。 参考：https://kernelmaker.github.io/lsm-tree Design LSM Tree 的组成 LSM Tree 主要由两部分组成，内存组件和磁盘组件，图示中的 C0 和 C1。C0 组件较小（出于成本考虑，位于在内存中，无需存储大量数据），C1 组件相对较大。 C0 常被用于 C1 的缓冲区，用于吸收突发的读写，C0 类似于 C1 的缓存层。 写流程：首先写日志文件（WAL机制用于故障恢复），然后写 C0，到达一定阈值之后，C0 合并到 C1（刷回）。C0 到 C1 延迟相对较大，数据容易丢，故需要 WAL 机制保证一致性。 读流程：首先读 C0 看是否存在对应的 KV，不存在再去读 C1。 磁盘组件 磁盘组件旨在利用磁盘的顺序读写性能远好于随机读写性能的优势，将数据进行排序，顺序地对磁盘进行 IO。数据结构类似于 B+ Tree，满节点，每一个节点对应数据页。 LSM Tree 中包含两类 Block，Single-Page Block 和 Multi-Page Block。Single-Page Block 其实就是指根节点以及每一层里的单页节点，而 Multi-Page Block 是指根目录下的每个层级上的单页节点序列会被打包，然后一起放入连续的多页磁盘块中（囊括了根节点以下的节点），利于磁盘顺序访问。 内存组件 C0 树不同于 C1 树，由于主要位于内存中，不需要考虑和磁盘交互的数据大小，可以是任意大小，从而充分利用 CPU，所以 C0 树通常不是 B+ 树，一般会是 AVL 树一样的数据结构，效率相对更高。 Rolling Merge 先读取包含C1树的叶节点的多页块，这会使得C1中的一系列节点条目驻留到缓存 然后每次合并都会去读取已经被缓存的C1树的一个磁盘页大小的叶节点 接着将第二步中读取到的C1树叶节点上的条目与C0树的叶节点条目进行合并，并减少C0树的大小 合并完成后，会为C1树创建一个已合并的新叶节点（在缓存中，填满后被刷入磁盘） How a Two Component LSM-tree Grows Rolling Merge Process 当增长中的C0树第一次达到阈值的时候 最靠左的一系列条目会以高效批量形式从C0树中删除 然后被重组到C1树(按key递增顺序)，将被完全填满 连续的C1树的叶节点会按从左到右的顺序，首先被放置到常驻内存的多页块内的若干初始页上 持续上一步直到该多页块被填满 然后该多页块被刷到磁盘，成为C1树叶节点层的第一部分，常驻磁盘 随着这些连续的叶节点添加的过程，一个C1树的目录节点结构会在内存缓存中被创建（这些上层目录节点被存在单独的多页块缓存或单独的页缓存中，为了更高效利用内存和磁盘。其中还包含分隔点索引，可以将访问精确匹配导向某个下一层级的单页节点而不是多页块，类似B树。这样一来，我们就可在滚动合并或长范围搜索时使用多页块，而在索引精确匹配访问时使用单页节点） 在发生如下情况时，C1的目录节点会被强制刷盘 包含目录节点的多页块缓存满了 -&gt; 只有该多页块会被刷盘 根节点分裂，增加了C1树的深度(大于2) -&gt; 所有多页块刷盘 checkpoint被执行-&gt; 所有多页块刷盘 Others 滚动合并过程中有许多针对一致性和并发访问控制的设计细节。 在磁盘组件 C1 中，合并过程常常会将需要合并的 Multi-Page Block 区分为两种状态，一种是 empty block，即正在取出数据进行合并的 block，一种是 filling block，用于表示现在的合并结果。对应的存在 Empty Node 和 Filling Node，在并发访问时，合并过程中的 Node 将拒绝对该 Node 的其他操作的请求。 一致性的保证则主要体现在合并过后生成的新的数据，仍然需要先写对应的日志文件，再将数据追加写入到磁盘，即存入到磁盘中的新的连续的位置，所以 LSM Tree 中是不存在修改操作的，但是针对老数据对应的块需要进行回收才能保证空间的利用率。 LSM Tree 中本身是受 LogStructured FileSystem 的启发，即将随机小写给转换成批量的顺序写，从而充分利用磁盘的顺序 IO 能力，并减小和磁盘的交互次数。同时其垃圾回收的策略也受 LSF 的影响，由于都是追加写入的方式，需要对以往的老数据块进行垃圾回收。LSF 回收的块可能不全为空，而 LSM 中回收的块全为空，所以 LSM 比 LSF 在垃圾回收方面相对较好。 Log Structured File Systems. 核心思想：使用一个位于内存中的日志文件来吸收大量的随机小写，当到达一定的阈值（通常为 LFS 中内存和磁盘交互的数据单元段的大小）之后（或者周期地进行执行合并操作）将内存中的数据写入磁盘。 写入磁盘的方式可以使用 RAID 来进行优化，从而在 IO 时充分利用每一个磁盘的带宽，特别是在读数据的时候，因为需要首先从磁盘中读取出相应的数据段，然后根据数据段中存储的一些数据块的地址信息重建出日志文件，再进行相应的读操作。所以 LFS 本身对于读操作是极不友好的。 LSF 和 Journal File System 的区别就是，LFS 只有一个日志文件，而Journal File System 则是有单独的日志和数据，是分开存储的，在写入数据之前首先写日志，数据提交到磁盘以后相应的会删除日志文件。 [Youtube] Preliminaries (Log Structured File System) - Georgia Tech - Advanced Operating Systems [知乎]: Log-structured File System Operations Query LSM Tree 的查询操作由于存在多个组件，故需要在多个 组件都进行查询，从 C0 到 Ck，为了保证LSM中所有条目都被检查到，就必须让每个精确匹配或范围查找要访问每个Ci组件的索引结构。故后来有很多研究提出了各种各样的对 LSM Tree 写性能的优化方式。 如果记录的生成逻辑就能保证索引唯一性，比如时间戳，那么在较小的一个Ci组件内搜索到结果就可以直接停止搜索直接返回。 再比如，搜索条件中使用了最近的时间戳，我们可以限制搜索范围为那些还没有移动到最大组件的记录所处的序号小的组件上。当合并游标在（Ci，Ci + 1）对中循环徘徊时，我们通常有理由保留Ci中那些最近插入的条目（比如在最近τi秒内），而只允许那些较旧的条目移动到Ci+1中。 在最频繁的查询指向最近插入的值的这种场景中，许多搜索都可在C0树中就完成，这样就使得C0树完全发挥了了自己的内存缓存价值。这是十分重要的性能优化策略。举个例子，被用作短期事务UNDO日志索引，在中止事件中被访问，在创建这些索引后会有大比例是访问短期内的数据，所以我们可以预期大多数这样的索引会驻留在内存中。 通过跟踪每个事务的开始时间，我们可以保证在最后的τ0秒内启动事务的所有日志，例如，将在组件C0中找到，而无需搜索位于磁盘的其他LSM树组件。 Delete Update 删除操作则是新写入一个指定索引的删除状态，在后期进行滚动合并时相应地针对该删除状态和对应的数据进行处理，合并到更大的组件中或者比较对应的数据块无效，在进行查询时对应地进行过滤，垃圾回收时相应地清空对应的存储空间。 更新操作则需要根据实际情况进行区分，如果只改value，相应地更改对应索引地址的数据，如果修改ID，即一些会影响数据索引的操作，则将修改操作对应的转换为 删除+插入。 相关参数理论计算 计算证明过程参考对应的参考链接。 并发访问 并发问题 查询操作不能同时去访问另一个进程的rolling merge正在修改的磁盘组件的节点内容 针对C0组件的查询和插入操作也不能与正在进行的rolling merge的同时对树的相同部分进行访问 从Ci-1到Ci的rolling merge的游标有时需要越过从Ci到Ci+1的rolling merge的游标，因为数据从Ci-1移出速率&gt;=从Ci移出的速率，这意味着Ci-1所关联的游标的循环周期要更快。因此无论如何，所采用的并发访问机制必须允许这种交错发生，而不能强制要求在交会点，一个进程(移出数据到Ci的那个)必须阻塞在另一个进程(从Ci移出数据的那个)之后。 磁盘组件并发问题 LSM树中用于并发控制访问基于磁盘的组件而导致冲突，所以加锁的单位是树的节点： 正在因滚动合并被修改的节点会被加上写锁。滚动游标使用的写锁会在合并到更大的组件后被释放 正在因搜索而读取的节点会被加上读锁。叶节点上的条目被扫描完了，就会释放 为了防止死锁，设计了目录锁相关方法 内存组件中，假设数据结构为 2-3 树。可以用写锁锁住2-3树的目录节点一个子树，该节点包含要合并到C1节点时受影响范围内的所有条目; 同时，查找操作会用读锁锁定那些处于搜索路径上的2-3树的所有节点，这是一个排他锁。 简书： AVL树，2-3 树，红黑树 为了提高并发，前面章节提到过的C1树的empty block和filling block都会包含整数个C1树中的页大小的节点，并驻留在内存。在合并重组节点时，这些节点会被加上写锁，以阻止对这些记录的其他类型并发访问。 内存组件到磁盘组件的并发 前面讨论的都是基于磁盘的组件间merger时的并发情况，现在说说C0到C1的合并时的并发情况。与其他合并步骤相同，CPU应该专注于合并任务，所以其他访问会被排他的写锁拒绝，当然这个时间会尽可能短。那些会被合并的C0条目应该被提前计算、提前加写锁。除此之外，CPU时间还会由于C0组件以批量的形式删除条目节省时间，而不是每次单独删除而尝试再平衡；C0树可以在整个合并步骤完成后被完全的平衡。 Failover 借鉴了 Log Structured File System 中的 Checkpoint 机制，LSM 中的故障恢复主要是为了保证在 Rolling Merge 过程中可能产生的数据丢失，设置 Checkpoint 则是在某一次 Rolling Merge 之前插入一条对应的日志记录，该日志主要保存此次 Rolling Merge 中插入的相关数据，以及原本缓存在内存中的磁盘组件的相关数据（即一些 Rolling Merge 依赖的磁盘组件上的数据）。 具体流程参考：https://www.cnblogs.com/siegfang/archive/2013/01/12/lsm-tree.html Summary LSM 树借鉴了 LSF 的思想，相比于 B Tree，写性能表现更好，主要归功于采用了批量延迟写的方式，同时利用了磁盘顺序 IO 远好于随机 IO 的特性，从而充分利用磁盘的性能。而在数据检索方面，由于多级结构的存在，导致数据的查询效率不高，LSM 更多的是做了内存和磁盘成本的折中妥协。 后续大量研究都从优化 LSM Tree 的读性能，以及优化 Compaction（Merge）过程带来的性能损失为目标，包括一些具体的工业实现 BigTable, LevelDB, RocksDB。 参考链接 [1] The Log-Structured Merge-Tree (LSM-Tree) [2] 知乎 - 大道始于一 - 从SSTable到LSM-Tree之二 [3] CSDN - 惜暮 - B树、B+树、LSM树以及其典型应用场景 [[4] Github Page - 【Paper笔记】The Log structured Merge-Tree（LSM-Tree）(https://kernelmaker.github.io/lsm-tree) [5] CSDN - 论文阅读(翻译) -The Log-Structured Merge-Tree (LSM-Tree) [6] 博客园：日志结构的合并树 The Log-Structured Merge-Tree [7] Wikipedia: LSM Tree Extend ","link":"https://blog.shunzi.tech/post/the-log-structured-merge-tree-lsm-tree/"},{"title":"DistCache: Provable Load Balancing for LargeScale Storage Systems with Distributed Caching","content":" FAST19 Best Paper DistCache: Provable Load Balancing for LargeScale Storage Systems with Distributed Caching FAST19 Best Paper DistCache: Provable Load Balancing for LargeScale Storage Systems with Distributed Caching BestPaper 以及有很多大佬都对该文章有相应的解读，所以想看下原文，恰巧现阶段的项目中也有涉及分布式缓存的东西，学习一下相关思想。 主要设计了一个保证存储系统负载均衡的分布式缓存，并提供了相关理论证明 由于相关解读资料很多，此处只简单记录一些思想和关键点。也是一种新的阅读论文方式的尝试。 Abstract 负载均衡是大规模存储系统需要实现的功能，从而达到严苛的 SLO(service-level objectives) 实现负载均衡的方式通常为建立缓存系统，而传统的缓存系统方式往往会有缓存节点数据不均衡或者一致性开销较高的问题。 本文提出的 DistCache机制，使用 independent hash function 做 item partition，query 则受启发于 power-of-two-choices。 Introduction 大规模存储系统在 scale out 时一个最大的问题就是 负载均衡。具体表现则是，部分节点的数据被频繁访问，导致该节点可能出现过载，以及产生热点数据，从而过载的节点将成为整个系统的瓶颈，可能导致高尾延迟、低吞吐量。 负载均衡的解决方案通常都是使用缓存。而理论证明（SOCC2011 Small cache, big effect: Provable load balancing for randomly partitioned cluster services）,n个存储节点，无论query是怎样的分布，缓存最热的O(nlogn)个对象即可均衡负载。即缓存大小只是和存储节点的数目相关，和存储节点上存储的对象数无关。相关论文有 SwitchKV（NSDI 2016），NetCache（SOSP 2017） 但是上面的小Cache解决热点的方案不能很好地scale out到多个集群，原因是一个集群一个Cache节点只能保证集群内的负载均衡，而集群间的负载均衡无法保证。为了保证集群间的负载均衡，最简单的方式则是再添加一个缓存节点来保证每个集群的负载均衡，但此时整个集群的吞吐量将受限于新添加的缓存节点，不足以支撑多个集群的吞吐，因此集群间需要一层分布式的Cache。而分布式Cache的难点在于如果简单地replicate热点数据到多个Cache节点会有一致性问题，而简单地按照hash partition又会造成Cache Node之间的负载不均衡导致吞吐受限于一个节点。所以关键就是避免Cache Node间的不均衡以及保证Cache一致性的开销。 提出了分布式缓存 DistCache。核心思想：使用独立的 HASH 函数在不同层的缓存节点之间对热数据进行分区，并将 The Power of Two Random Choices 应用到自适应的数据路由算法中。 参考链接 [知乎]：The Power of Two Random Choices [知乎]：Small Cache, Big Effect Background 同时保证集群内部和集群间负载均衡的方案： 图b 关键问题：如何将热数据分配到分布式缓存层？ 缓存分区：多个热数据可能被分区到同一个上层缓存节点上，导致上层缓存节点之间负载不均衡，使得缓存吞吐量不能线性增长，会受限于负载最高的节点。 缓存复制：虽然保证了负载均衡，但引入了较大的缓存一致性开销，需要更新上层缓存节点上所有副本。 故需要实现：避免上层缓存节点之间的负载不均现象，并减少缓存一致性的开销。 Design 缓存分配时，上下两层采用不同的 HASH 算法，如图所示 C3 过载时，其热点数据将会均匀分布在上层缓存节点 C0,C1,C2，上层节点将主要吸收对于热点数据的 IO 操作。 缓存分配机制只是提供了一种负载均衡的方式，但未提供查询的分布方法。故 DistCache 方案采用了 The Power of Two Random Choices，如图所示，GET A 操作将被路由到 C1 或者 C3 节点，根据 C1 和 C3 的负载决定最终的选择。从而无需依赖中心节点来统计集群信息。 对于缓存的更新操作，则是保证缓存数据一致性的常用方案。DistCache 使用了两阶段更新协议，即在更新 Cache 数据之前，标记老数据无效，然后首先更新 server 上的数据，再更新 cache 数据。 理论证明推导 详见论文。 Instance Switch-Based Caching 基于可编程的交换机缓存系统的设计 Architecture Component Controller：用于计算缓存分区，并通知缓存交换机。它更新系统重新配置下的缓存分配。 Cache switches： 功能1：缓存热点数据 功能2：分发用于查询路由的交换机负载信息。 ToR switches at client racks： 提供查询路由 Storage servers：使用开源存储软件进行 KV 数据的存储，同时实现缓存数据一致性。 读流程 R3 -&gt; S6 -&gt; Upper layer(S0, S1) -&gt; S1 hit R3 -&gt; S6 -&gt; Upper layer(S0, S1) -&gt; S0 -&gt; S3 hit 写流程 Evaluation 读性能 读性能表现 DistCache 与 Cache Replication 一样好 写性能 DistCache 在写占比较低的时候能够保证高吞吐 Cache Replication 一致性开销较大，随着写占比上升，吞吐量下降的较快。 参考链接 [1] FAST 2019 DistCache ","link":"https://blog.shunzi.tech/post/distcache-provable-load-balancing-for-largescale-storage-systems-with-distributed-caching/"},{"title":"C++基础快速入门","content":" 项目开发需要使用 C++，曾经学过相关课程，但很久没与使用，有些遗忘。 此处将和自己比较擅长的 Java 语言其中的概念进行类比，加深理解。 记录重要笔记，主要是C++ 一些独有的概念和一些基础知识 前半部分是 C 和 C++ 共通的部分，后半部分为面向对象的相关特性。 参考链接 https://www.runoob.com/cplusplus/cpp-tutorial.html 基本概念 标准库组成 核心语言，提供了所有构件块，包括变量、数据类型和常量，等等。 C++ 标准库，提供了大量的函数，用于操作文件、字符串等。 标准模板库（STL），提供了大量的方法，用于操作数据结构等。 环境搭建 参考百度/Google 基本语法 面向对象编程语言的基本概念。类、对象、方法（函数） 命名空间 namespace 数据类型 bool char = '\\0' int = 0 float = 0 double = 0 void wchar_t 宽字符型 typedef short int wchar_t; 枚举、指针、数组、引用、数据结构、类 pointer = NULL 基本数据类型修饰符 signed unsigned short long 类型限定符 const 类型的对象在程序执行期间不能被修改改变。 volatile 修饰符 volatile 告诉编译器不需要优化volatile声明的变量，让程序可以直接从内存中读取变量。对于一般的变量编译器会对变量进行优化，将内存中的变量值放在寄存器中以加快读写效率。 restrict 由 restrict 修饰的指针是唯一一种访问它所指向的对象的方式。只有 C99 增加了新的类型限定符 restrict。 typedef 为一个已有的类型取一个新的名字: typedef int feet; feet distance; 枚举类型 默认枚举对应数值 0，1，2；也可以指定 enum color { red, green, blue } c; c = blue; enum color { red, green=5, blue }; 变量 extern 外部引用关键字 extern int d = 3, f = 5; // d 和 f 的声明，引入其他文件中定义的变量 int d = 3, f = 5; // 定义并初始化 d 和 f byte z = 22; // 定义并初始化 z char x = 'x'; // 变量 x 的值为 'x' 声明 #include &lt;iostream&gt; using namespace std; // 变量声明 extern int a, b; extern int c; extern float f; // 函数声明 int func(); int main () { // 变量定义 int a, b; int c; float f; // 实际初始化 a = 10; b = 20; c = a + b; cout &lt;&lt; c &lt;&lt; endl ; f = 70.0/3.0; cout &lt;&lt; f &lt;&lt; endl ; // 函数调用 int i = func(); return 0; } // 函数定义 int func() { return 0; } 作用域 在函数或一个代码块内部声明的变量，称为局部变量。 在函数参数的定义中声明的变量，称为形式参数。 在所有函数外部声明的变量，称为全局变量。 #include &lt;iostream&gt; using namespace std; // 全局变量声明 int g; int main () { // 局部变量声明 int a, b; int c; // 实际初始化 a = 10; b = 20; c = a + b; g = a + b; cout &lt;&lt; c; cout &lt;&lt; g; return 0; } 在程序中，局部变量和全局变量的名称可以相同，但是在函数内，局部变量的值会覆盖全局变量的值 常量 定义常量 #define 域处理器 #include &lt;iostream&gt; using namespace std; #define LENGTH 10 #define WIDTH 5 #define NEWLINE '\\n' int main() { int area; area = LENGTH * WIDTH; cout &lt;&lt; area; cout &lt;&lt; NEWLINE; return 0; } const 关键字 #include &lt;iostream&gt; using namespace std; int main() { const int LENGTH = 10; const int WIDTH = 5; const char NEWLINE = '\\n'; int area; area = LENGTH * WIDTH; cout &lt;&lt; area; cout &lt;&lt; NEWLINE; return 0; } 存储类 auto （C++17 弃用） 声明变量时根据初始化表达式自动推断该变量的类型、声明函数时函数返回值的占位符。 auto f=3.14; //double auto s(&quot;hello&quot;); //const char* auto z = new auto(9); // int* auto x1 = 5, x2 = 5.0, x3='r';//错误，必须是初始化为同一类型 register （C++17 弃用） 用于定义存储在寄存器中而不是 RAM 中的局部变量。这意味着变量的最大尺寸等于寄存器的大小（通常是一个字），且不能对它应用一元的 '&amp;' 运算符（因为它没有内存位置）。 static 指示编译器在程序的生命周期内保持局部变量的存在，而不需要在每次它进入和离开作用域时进行创建和销毁。因此，使用 static 修饰局部变量可以在函数调用之间保持局部变量的值。也可以应用于全局变量。当 static 修饰全局变量时，会使变量的作用域限制在声明它的文件内。 #include &lt;iostream&gt; // 函数声明 void func(void); static int count = 10; /* 全局变量 */ int main() { while(count--) { func(); } return 0; } // 函数定义 void func( void ) { static int i = 5; // 局部静态变量 i++; std::cout &lt;&lt; &quot;变量 i 为 &quot; &lt;&lt; i ; std::cout &lt;&lt; &quot; , 变量 count 为 &quot; &lt;&lt; count &lt;&lt; std::endl; } 变量 i 为 6 , 变量 count 为 9 变量 i 为 7 , 变量 count 为 8 变量 i 为 8 , 变量 count 为 7 变量 i 为 9 , 变量 count 为 6 变量 i 为 10 , 变量 count 为 5 变量 i 为 11 , 变量 count 为 4 变量 i 为 12 , 变量 count 为 3 变量 i 为 13 , 变量 count 为 2 变量 i 为 14 , 变量 count 为 1 变量 i 为 15 , 变量 count 为 0 inline C++ 内联函数是通常与类一起使用。如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方。 ​ 对内联函数进行任何修改，都需要重新编译函数的所有客户端，因为编译器需要重新更换一次所有的代码，否则将会继续使用旧的函数。 内联函数inline：引入内联函数的目的是为了解决程序中函数调用的效率问题，这么说吧，程序在编译器编译的时候，编译器将程序中出现的内联函数的调用表达式用内联函数的函数体进行替换，而对于其他的函数，都是在运行时候才被替代。这其实就是个空间代价换时间的节省。所以内联函数一般都是1-5行的小函数。 注意事项： 1.在内联函数内不允许使用循环语句和开关语句、静态变量、递归 2.内联函数的定义必须出现在内联函数第一次调用之前； #include &lt;iostream&gt; using namespace std; inline int Max(int x, int y) { return (x &gt; y)? x : y; } // 程序的主函数 int main( ) { cout &lt;&lt; &quot;Max (20,10): &quot; &lt;&lt; Max(20,10) &lt;&lt; endl; cout &lt;&lt; &quot;Max (0,200): &quot; &lt;&lt; Max(0,200) &lt;&lt; endl; cout &lt;&lt; &quot;Max (100,1010): &quot; &lt;&lt; Max(100,1010) &lt;&lt; endl; return 0; } //运行结果 Max (20,10): 20 Max (0,200): 200 Max (100,1010): 1010 extern 用于提供一个全局变量的引用，全局变量对所有的程序文件都是可见的。当您使用 'extern' 时，对于无法初始化的变量，会把变量名指向一个之前定义过的存储位置。extern 是用来在另一个文件中声明一个全局变量或函数。 // main.cpp #include &lt;iostream&gt; int count ; extern void write_extern(); int main() { count = 5; write_extern(); } // suport.cpp #include &lt;iostream&gt; extern int count; void write_extern(void) { std::cout &lt;&lt; &quot;Count is &quot; &lt;&lt; count &lt;&lt; std::endl; } 在这里，第二个文件中的 extern 关键字用于声明已经在第一个文件 main.cpp 中定义的 count。 现在 ，编译这两个文件，如下所示： $ g++ main.cpp support.cpp -o write mutable 说明符仅适用于类的对象。它允许对象的成员替代常量。也就是说，mutable 成员可以通过 const 成员函数修改。 thread_local (C++11) 使用 thread_local 说明符声明的变量仅可在它在其上创建的线程上访问。 变量在创建线程时创建，并在销毁线程时销毁。 每个线程都有其自己的变量副本。 thread_local 说明符可以与 static 或 extern 合并。 可以将 thread_local 仅应用于数据声明和定义，thread_local 不能用于函数声明或定义。 thread_local int x; // 命名空间下的全局变量 class X { static thread_local std::string s; // 类的static成员变量 }; static thread_local std::string X::s; // X::s 是需要定义的 void foo() { thread_local std::vector&lt;int&gt; v; // 本地变量 } 运算符 和 语句 此处不表，只是需要注意指针的相关运算符，-&gt;, *, &amp; 循环语句、条件语句同理 函数 函数定义/声明/调用 此处不表 函数参数 传值/指针/引用 与 Java 区分 （Java 只有值传递和引用传递） 传值：该方法把参数的实际值赋值给函数的形式参数。在这种情况下，修改函数内的形式参数对实际参数没有影响。 指针：该方法把参数的地址赋值给形式参数。在函数内，该地址用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。 引用：该方法把参数的引用赋值给形式参数。在函数内，该引用用于访问调用中要用到的实际参数。这意味着，修改形式参数会影响实际参数。 函数参数默认值 定义一个函数，您可以为参数列表中后边的每一个参数指定默认值。当调用函数时，如果实际参数的值留空，则使用这个默认值。 int sum(int a, int b=20) lamda C++11 提供了对匿名函数的支持,称为 Lambda 函数(也叫 Lambda 表达式)。 Lambda 表达式把函数看作对象。Lambda 表达式可以像对象一样使用，比如可以将它们赋给变量和作为参数传递，还可以像函数一样对其求值。 Lambda 表达式本质上与函数声明非常类似。 // [capture](parameters)-&gt;return-type{body} [](int x, int y){ return x &lt; y ; } // [capture](parameters){body} []{ ++global_x; } [](int x, int y) -&gt; int { int z = x + y; return z + x; } [] // 沒有定义任何变量。使用未定义变量会引发错误。 [x, &amp;y] // x以传值方式传入（默认），y以引用方式传入。 [&amp;] // 任何被使用到的外部变量都隐式地以引用方式加以引用。 [=] // 任何被使用到的外部变量都隐式地以传值方式加以引用。 [&amp;, x] // x显式地以传值方式加以引用。其余变量以引用方式加以引用。 [=, &amp;z] // z显式地以引用方式加以引用。其余变量以传值方式加以引用。 另外有一点需要注意。对于[=]或[&amp;]的形式，lambda 表达式可以直接使用 this 指针。但是，对于[]的形式，如果要使用 this 指针，必须显式传入： [this]() { this-&gt;someFunc(); }(); 数字 double cos(double); double sin(double); double tan(double); double log(double); // 自然对数 double pow(double, double); double hypot(double, double); // 两个参数的平方总和的平方根 double sqrt(double); int abs(int); double fabs(double); double floor(double); // 向下取整 随机数 #include &lt;iostream&gt; #include &lt;ctime&gt; #include &lt;cstdlib&gt; using namespace std; int main () { int i,j; // 设置种子 srand( (unsigned)time( NULL ) ); /* 生成 10 个随机数 */ for( i = 0; i &lt; 10; i++ ) { // 生成实际的随机数 j= rand(); // 伪随机 cout &lt;&lt;&quot;随机数： &quot; &lt;&lt; j &lt;&lt; endl; } return 0; } 数组 此处不表，注意在 C/C++ 中和 指针的联系 字符串 C 风格 char greeting[6] = {'H', 'e', 'l', 'l', 'o', '\\0'}; char greeting[] = &quot;Hello&quot;; cstring #include &lt;iostream&gt; #include &lt;cstring&gt; using namespace std; int main () { char str1[11] = &quot;Hello&quot;; char str2[11] = &quot;World&quot;; char str3[11]; int len ; // 复制 str1 到 str3 strcpy( str3, str1); cout &lt;&lt; &quot;strcpy( str3, str1) : &quot; &lt;&lt; str3 &lt;&lt; endl; // 连接 str1 和 str2 strcat( str1, str2); cout &lt;&lt; &quot;strcat( str1, str2): &quot; &lt;&lt; str1 &lt;&lt; endl; // 连接后，str1 的总长度 len = strlen(str1); cout &lt;&lt; &quot;strlen(str1) : &quot; &lt;&lt; len &lt;&lt; endl; return 0; } C++ string #include &lt;iostream&gt; #include &lt;string&gt; using namespace std; int main () { string str1 = &quot;Hello&quot;; string str2 = &quot;World&quot;; string str3; int len ; // 复制 str1 到 str3 str3 = str1; cout &lt;&lt; &quot;str3 : &quot; &lt;&lt; str3 &lt;&lt; endl; // 连接 str1 和 str2 str3 = str1 + str2; cout &lt;&lt; &quot;str1 + str2 : &quot; &lt;&lt; str3 &lt;&lt; endl; // 连接后，str3 的总长度 len = str3.size(); cout &lt;&lt; &quot;str3.size() : &quot; &lt;&lt; len &lt;&lt; endl; return 0; } 指针和引用 指针和引用的相同点和不同点 相同点： 都是地址的概念， 指针指向一块内存，内容是内存的地址 引用则是某块内存的别名 不同点： 指针是一个实体，引用是一个别名 引用只能在定义时被初始化一次，之后不可变，指针可变 引用没有 const，即无 int&amp; const a; 但指针有 const 引用不能为空，指针可以为空 sizeof(引用) 得到的是所指向的变量（对象）的大小，而 sizeof(指针) 得到的是指针本身的大小 指针和引用的自增运算意义不同 引用是类型安全的，指针不是，（引用比指针多了类型检查） 日期/时间 C++ 标准库没有提供所谓的日期类型。C++ 继承了 C 语言用于日期和时间操作的结构和函数。为了使用日期和时间相关的函数和结构，需要在 C++ 程序中引用 头文件。 有四个与时间相关的类型：clock_t、time_t、size_t 和 tm。类型 clock_t、size_t 和 time_t 能够把系统时间和日期表示为某种整数。 struct tm { int tm_sec; // 秒，正常范围从 0 到 59，但允许至 61 int tm_min; // 分，范围从 0 到 59 int tm_hour; // 小时，范围从 0 到 23 int tm_mday; // 一月中的第几天，范围从 1 到 31 int tm_mon; // 月，范围从 0 到 11 int tm_year; // 自 1900 年起的年数 int tm_wday; // 一周中的第几天，范围从 0 到 6，从星期日算起 int tm_yday; // 一年中的第几天，范围从 0 到 365，从 1 月 1 日算起 int tm_isdst; // 夏令时 } 实例 获取当前时间 #include &lt;iostream&gt; #include &lt;ctime&gt; using namespace std; int main( ) { // 基于当前系统的当前日期/时间 time_t now = time(0); // 把 now 转换为字符串形式 char* dt = ctime(&amp;now); cout &lt;&lt; &quot;本地日期和时间：&quot; &lt;&lt; dt &lt;&lt; endl; // 把 now 转换为 tm 结构 tm *gmtm = gmtime(&amp;now); dt = asctime(gmtm); cout &lt;&lt; &quot;UTC 日期和时间：&quot;&lt;&lt; dt &lt;&lt; endl; } 格式化时间 #include &lt;iostream&gt; #include &lt;ctime&gt; using namespace std; int main( ) { // 基于当前系统的当前日期/时间 time_t now = time(0); cout &lt;&lt; &quot;1970 到目前经过秒数:&quot; &lt;&lt; now &lt;&lt; endl; tm *ltm = localtime(&amp;now); // 输出 tm 结构的各个组成部分 cout &lt;&lt; &quot;年: &quot;&lt;&lt; 1900 + ltm-&gt;tm_year &lt;&lt; endl; cout &lt;&lt; &quot;月: &quot;&lt;&lt; 1 + ltm-&gt;tm_mon&lt;&lt; endl; cout &lt;&lt; &quot;日: &quot;&lt;&lt; ltm-&gt;tm_mday &lt;&lt; endl; cout &lt;&lt; &quot;时间: &quot;&lt;&lt; ltm-&gt;tm_hour &lt;&lt; &quot;:&quot;; cout &lt;&lt; ltm-&gt;tm_min &lt;&lt; &quot;:&quot;; cout &lt;&lt; ltm-&gt;tm_sec &lt;&lt; endl; } 输入/输出 &lt;iostream&gt; ： 该文件定义了 cin、cout、cerr 和 clog 对象，分别对应于标准输入流、标准输出流、非缓冲标准错误流和缓冲标准错误流。 &lt;iomanip&gt; ： 该文件通过所谓的参数化的流操纵器（比如 setw 和 setprecision），来声明对执行标准化 I/O 有用的服务。 &lt;fstream&gt; ： 该文件为用户控制的文件处理声明服务。我们将在文件和流的相关章节讨论它的细节。 #include &lt;iostream&gt; #include &lt;iomanip&gt; using namespace std; int main() { cout&lt;&lt;setiosflags(ios::left|ios::showpoint); // 设左对齐，以一般实数方式显示 cout.precision(5); // 设置除小数点外有五位有效数字 cout&lt;&lt;123.456789&lt;&lt;endl; cout.width(10); // 设置显示域宽10 cout.fill('*'); // 在显示区域空白处用*填充 cout&lt;&lt;resetiosflags(ios::left); // 清除状态左对齐 cout&lt;&lt;setiosflags(ios::right); // 设置右对齐 cout&lt;&lt;123.456789&lt;&lt;endl; cout&lt;&lt;setiosflags(ios::left|ios::fixed); // 设左对齐，以固定小数位显示 cout.precision(3); // 设置实数显示三位小数 cout&lt;&lt;999.123456&lt;&lt;endl; cout&lt;&lt;resetiosflags(ios::left|ios::fixed); //清除状态左对齐和定点格式 cout&lt;&lt;setiosflags(ios::left|ios::scientific); //设置左对齐，以科学技术法显示 cout.precision(3); //设置保留三位小数 cout&lt;&lt;123.45678&lt;&lt;endl; return 0; } 结构体 直接看实例 // 声明一个结构体类型 Books #include &lt;iostream&gt; #include &lt;cstring&gt; using namespace std; void printBook( struct Books *book ); struct Books { char title[50]; char author[50]; char subject[100]; int book_id; }; int main( ) { Books Book1; // 定义结构体类型 Books 的变量 Book1 Books Book2; // 定义结构体类型 Books 的变量 Book2 // Book1 详述 strcpy( Book1.title, &quot;C++ 教程&quot;); strcpy( Book1.author, &quot;Runoob&quot;); strcpy( Book1.subject, &quot;编程语言&quot;); Book1.book_id = 12345; // Book2 详述 strcpy( Book2.title, &quot;CSS 教程&quot;); strcpy( Book2.author, &quot;Runoob&quot;); strcpy( Book2.subject, &quot;前端技术&quot;); Book2.book_id = 12346; // 通过传 Book1 的地址来输出 Book1 信息 printBook( &amp;Book1 ); // 通过传 Book2 的地址来输出 Book2 信息 printBook( &amp;Book2 ); return 0; } // 该函数以结构指针作为参数 void printBook( struct Books *book ) { cout &lt;&lt; &quot;书标题 : &quot; &lt;&lt; book-&gt;title &lt;&lt;endl; cout &lt;&lt; &quot;书作者 : &quot; &lt;&lt; book-&gt;author &lt;&lt;endl; cout &lt;&lt; &quot;书类目 : &quot; &lt;&lt; book-&gt;subject &lt;&lt;endl; cout &lt;&lt; &quot;书 ID : &quot; &lt;&lt; book-&gt;book_id &lt;&lt;endl; } 面向对象 类和对象 基础实例 #include &lt;iostream&gt; using namespace std; class Line { public: int getLength( void ); Line( int len ); // 简单的构造函数 Line( const Line &amp;obj); // 拷贝构造函数 ~Line(); // 析构函数 private: int *ptr; }; // 成员函数定义，包括构造函数 Line::Line(int len) { cout &lt;&lt; &quot;调用构造函数&quot; &lt;&lt; endl; // 为指针分配内存 ptr = new int; *ptr = len; } Line::Line(const Line &amp;obj) { cout &lt;&lt; &quot;调用拷贝构造函数并为指针 ptr 分配内存&quot; &lt;&lt; endl; ptr = new int; *ptr = *obj.ptr; // 拷贝值 } Line::~Line(void) { cout &lt;&lt; &quot;释放内存&quot; &lt;&lt; endl; delete ptr; } int Line::getLength( void ) { return *ptr; } void display(Line obj) { cout &lt;&lt; &quot;line 大小 : &quot; &lt;&lt; obj.getLength() &lt;&lt;endl; } // 程序的主函数 int main( ) { Line line1(10); Line line2 = line1; // 这里也调用了拷贝构造函数 display(line1); display(line2); return 0; } 友元函数 类的友元函数是定义在类外部，但有权访问类的所有私有（private）成员和保护（protected）成员。尽管友元函数的原型有在类的定义中出现过，但是友元函数并不是成员函数。 友元可以是一个函数，该函数被称为友元函数；友元也可以是一个类，该类被称为友元类，在这种情况下，整个类及其所有成员都是友元。 如果要声明函数为一个类的友元，需要在类定义中该函数原型前使用关键字 friend #include &lt;iostream&gt; using namespace std; class Box { double width; public: friend void printWidth(Box box); friend class BigBox; void setWidth(double wid); }; class BigBox { public : void Print(int width, Box &amp;box) { // BigBox是Box的友元类，它可以直接访问Box类的任何成员 box.setWidth(width); cout &lt;&lt; &quot;Width of box : &quot; &lt;&lt; box.width &lt;&lt; endl; } }; // 成员函数定义 void Box::setWidth(double wid) { width = wid; } // 请注意：printWidth() 不是任何类的成员函数 void printWidth(Box box) { /* 因为 printWidth() 是 Box 的友元，它可以直接访问该类的任何成员 */ cout &lt;&lt; &quot;Width of box : &quot; &lt;&lt; box.width &lt;&lt; endl; } // 程序的主函数 int main() { Box box; BigBox big; // 使用成员函数设置宽度 box.setWidth(10.0); // 使用友元函数输出宽度 printWidth(box); // 使用友元类中的方法设置宽度 big.Print(20, box); getchar(); return 0; } 内联函数 C++ 内联函数是通常与类一起使用。如果一个函数是内联的，那么在编译时，编译器会把该函数的代码副本放置在每个调用该函数的地方。 对内联函数进行任何修改，都需要重新编译函数的所有客户端，因为编译器需要重新更换一次所有的代码，否则将会继续使用旧的函数。 如果想把一个函数定义为内联函数，则需要在函数名前面放置关键字 inline，在调用函数之前需要对函数进行定义。如果已定义的函数多于一行，编译器会忽略 inline 限定符。 在类定义中的定义的函数都是内联函数，即使没有使用 inline 说明符。 作用 引入内联函数的目的是为了解决程序中函数调用的效率问题，这么说吧，程序在编译器编译的时候，编译器将程序中出现的内联函数的调用表达式用内联函数的函数体进行替换，而对于其他的函数，都是在运行时候才被替代。这其实就是个空间代价换时间的i节省。所以内联函数一般都是1-5行的小函数。在使用内联函数时要留神： 1.在内联函数内不允许使用循环语句和开关语句； 2.内联函数的定义必须出现在内联函数第一次调用之前； 3.类结构中所在的类说明内部定义的函数是内联函数。 类中的静态成员 使用 static 关键字来把类成员定义为静态的。当我们声明类的成员为静态时，这意味着无论创建多少个类的对象，静态成员都只有一个副本。 静态成员在类的所有对象中是共享的。如果不存在其他的初始化语句，在创建第一个对象时，所有的静态数据都会被初始化为零。我们不能把静态成员的初始化放置在类的定义中，但是可以在类的外部通过使用范围解析运算符 :: 来重新声明静态变量从而对它进行初始化。 #include &lt;iostream&gt; using namespace std; class Box { public: static int objectCount; // 构造函数定义 Box(double l=2.0, double b=2.0, double h=2.0) { cout &lt;&lt;&quot;Constructor called.&quot; &lt;&lt; endl; length = l; breadth = b; height = h; // 每次创建对象时增加 1 objectCount++; } double Volume() { return length * breadth * height; } static int getCount() { return objectCount; } private: double length; // 长度 double breadth; // 宽度 double height; // 高度 }; // 初始化类 Box 的静态成员 int Box::objectCount = 0; int main(void) { // 在创建对象之前输出对象的总数 cout &lt;&lt; &quot;Inital Stage Count: &quot; &lt;&lt; Box::getCount() &lt;&lt; endl; Box Box1(3.3, 1.2, 1.5); // 声明 box1 Box Box2(8.5, 6.0, 2.0); // 声明 box2 // 在创建对象之后输出对象的总数 cout &lt;&lt; &quot;Final Stage Count: &quot; &lt;&lt; Box::getCount() &lt;&lt; endl; return 0; } 构造函数 简书：C++11 =default 和 =delete 一个类通过定义五种特殊的成员函数来控制对象的拷贝、移动、赋值和销毁操作。 拷贝构造函数（copy constructor） 拷贝赋值运算符（copy-assignment operator） 移动构造函数（move constructor） 移动赋值运算符（move-assignment operator） 析构函数（destructor） 这些操作统称为拷贝控制操作（copy control）。 在定义任何类时，拷贝控制操作都是必要部分。 拷贝构造函数（The Copy Constructor） 如果一个构造函数的第一个参数是自身类类型的引用（几乎总是const引用），且任何额外参数都有默认值，则此构造函数是拷贝构造函数。 class Foo { public: Foo(); // default constructor Foo(const Foo&amp;); // copy constructor // ... }; 由于拷贝构造函数在一些情况下会被隐式使用，因此通常不会声明为explicit的。 如果类未定义自己的拷贝构造函数，编译器会为类合成一个。一般情况下，合成拷贝构造函数（synthesized copy constructor）会将其参数的非static成员逐个拷贝到正在创建的对象中。 class Sales_data { public: // other members and constructors as before // declaration equivalent to the synthesized copy constructor Sales_data(const Sales_data&amp;); private: std::string bookNo; int units_sold = 0; double revenue = 0.0; }; // equivalent to the copy constructor that would be synthesized for Sales_data Sales_data::Sales_data(const Sales_data &amp;orig): bookNo(orig.bookNo), // uses the string copy constructor units_sold(orig.units_sold), // copies orig.units_sold revenue(orig.revenue) // copies orig.revenue { } // empty bod 使用直接初始化时，实际上是要求编译器按照函数匹配规则来选择与实参最匹配的构造函数。使用拷贝初始化时，是要求编译器将右侧运算对象拷贝到正在创建的对象中，如果需要的话还要进行类型转换。 string dots(10, '.'); // direct initialization string s(dots); // direct initialization string s2 = dots; // copy initialization string null_book = &quot;9-999-99999-9&quot;; // copy initialization string nines = string(100, '9'); // copy initialization 拷贝初始化通常使用拷贝构造函数来完成。但如果一个类拥有移动构造函数，则拷贝初始化有时会使用移动构造函数而非拷贝构造函数来完成。 发生拷贝初始化的情况： 用=定义变量。 将对象作为实参传递给非引用类型的形参。 从返回类型为非引用类型的函数返回对象。 用花括号列表初始化数组中的元素或聚合类中的成员。 当传递一个实参或者从函数返回一个值时，不能隐式使用explicit构造函数。 vector&lt;int&gt; v1(10); // ok: direct initialization vector&lt;int&gt; v2 = 10; // error: constructor that takes a size is explicit void f(vector&lt;int&gt;); // f's parameter is copy initialized f(10); // error: can't use an explicit constructor to copy an argument f(vector&lt;int&gt;(10)); // ok: directly construct a temporary vector from an int 拷贝赋值运算符（The Copy-Assignment Operator） 重载运算符（overloaded operator）的参数表示运算符的运算对象。 如果一个运算符是成员函数，则其左侧运算对象会绑定到隐式的this参数上。 赋值运算符通常应该返回一个指向其左侧运算对象的引用。 class Foo { public: Foo&amp; operator=(const Foo&amp;); // assignment operator // ... }; 标准库通常要求保存在容器中的类型要具有赋值运算符，且其返回值是左侧运算对象的引用。 如果类未定义自己的拷贝赋值运算符，编译器会为类合成一个。一般情况下，合成拷贝赋值运算符（synthesized copy-assignment operator）会将其右侧运算对象的非static成员逐个赋值给左侧运算对象的对应成员，之后返回左侧运算对象的引用。 // equivalent to the synthesized copy-assignment operator Sales_data&amp; Sales_data::operator=(const Sales_data &amp;rhs) { bookNo = rhs.bookNo; // calls the string::operator= units_sold = rhs.units_sold; // uses the built-in int assignment revenue = rhs.revenue; // uses the built-in double assignment return *this; // return a reference to this object } =default 可以通过将拷贝控制成员定义为=default来显式地要求编译器生成合成版本。 class Sales_data { public: // copy control; use defaults Sales_data() = default; Sales_data(const Sales_data&amp;) = default; ~Sales_data() = default; // other members as before }; 在类内使用=default修饰成员声明时，合成的函数是隐式内联的。如果不希望合成的是内联函数，应该只对成员的类外定义使用=default。 只能对具有合成版本的成员函数使用=default。 阻止拷贝（Preventing Copies）=delete 大多数类应该定义默认构造函数、拷贝构造函数和拷贝赋值运算符，无论是显式地还是隐式地。 在C++11新标准中，将拷贝构造函数和拷贝赋值运算符定义为删除的函数（deleted function）可以阻止类对象的拷贝。删除的函数是一种虽然进行了声明，但是却不能以任何方式使用的函数。定义删除函数的方式是在函数的形参列表后面添加=delete。 struct NoCopy { NoCopy() = default; // use the synthesized default constructor NoCopy(const NoCopy&amp;) = delete; // no copy NoCopy &amp;operator=(const NoCopy&amp;) = delete; // no assignment ~NoCopy() = default; // use the synthesized destructor // other members }; =delete和=default有两点不同： =delete可以对任何函数使用；=default只能对具有合成版本的函数使用，defaulted 函数特性仅用于类的特殊成员函数，且该特殊成员函数没有默认参数。 =delete必须出现在函数第一次声明的地方；=default既能出现在类内，也能出现在类外。 析构函数不能是删除的函数。对于析构函数被删除的类型，不能定义该类型的变量或者释放指向该类型动态分配对象的指针。 如果一个类中有数据成员不能默认构造、拷贝或销毁，则对应的合成拷贝控制成员将被定义为删除的。 在旧版本的C++标准中，类通过将拷贝构造函数和拷贝赋值运算符声明为private成员来阻止类对象的拷贝。在新标准中建议使用=delete而非private。 继承 #include &lt;iostream&gt; using namespace std; // 基类 Shape class Shape { public: void setWidth(int w) { width = w; } void setHeight(int h) { height = h; } protected: int width; int height; }; // 基类 PaintCost class PaintCost { public: int getCost(int area) { return area * 70; } }; // 派生类 class Rectangle: public Shape, public PaintCost { public: int getArea() { return (width * height); } }; int main(void) { Rectangle Rect; int area; Rect.setWidth(5); Rect.setHeight(7); area = Rect.getArea(); // 输出对象的面积 cout &lt;&lt; &quot;Total area: &quot; &lt;&lt; Rect.getArea() &lt;&lt; endl; // 输出总花费 cout &lt;&lt; &quot;Total paint cost: $&quot; &lt;&lt; Rect.getCost(area) &lt;&lt; endl; return 0; } 重载 多态 抽象 封装 此处不表。注意虚函数的概念 虚函数 是在基类中使用关键字 virtual 声明的函数。在派生类中重新定义基类中定义的虚函数时，会告诉编译器不要静态链接到该函数。 我们想要的是在程序中任意点可以根据所调用的对象类型来选择调用的函数，这种操作被称为动态链接，或后期绑定。 class Shape { protected: int width, height; public: Shape( int a=0, int b=0) { width = a; height = b; } // pure virtual function virtual int area() = 0; }; 抽象类：等同于 Java 的接口 #include &lt;iostream&gt; using namespace std; // 基类 class Shape { public: // 提供接口框架的纯虚函数 virtual int getArea() = 0; void setWidth(int w) { width = w; } void setHeight(int h) { height = h; } protected: int width; int height; }; // 派生类 class Rectangle: public Shape { public: int getArea() { return (width * height); } }; class Triangle: public Shape { public: int getArea() { return (width * height)/2; } }; int main(void) { Rectangle Rect; Triangle Tri; Rect.setWidth(5); Rect.setHeight(7); // 输出对象的面积 cout &lt;&lt; &quot;Total Rectangle area: &quot; &lt;&lt; Rect.getArea() &lt;&lt; endl; Tri.setWidth(5); Tri.setHeight(7); // 输出对象的面积 cout &lt;&lt; &quot;Total Triangle area: &quot; &lt;&lt; Tri.getArea() &lt;&lt; endl; return 0; } 指针 智能指针 知乎：C++智能指针 简而言之就是不需要程序员每次去负责指针的释放，而是采用诸如引用计数的方法来自动释放指针。 C++里面的四个智能指针: auto_ptr, unique_ptr,shared_ptr, weak_ptr 其中后三个是C++11支持，并且第一个已经被C++11弃用。 智能指针主要用于管理在堆上分配的内存，它将普通的指针封装为一个栈对象。当栈对象的生存周期结束后，会在析构函数中释放掉申请的内存，从而防止内存泄漏。C++ 11中最常用的智能指针类型为shared_ptr,它采用引用计数的方法，记录当前内存资源被多少个智能指针引用。该引用计数的内存在堆上分配。当新增一个时引用计数加1，当过期时引用计数减一。只有引用计数为0时，智能指针才会自动释放引用的内存资源。对shared_ptr进行初始化时不能将一个普通指针直接赋值给智能指针，因为一个是指针，一个是类。可以通过make_shared函数或者通过构造函数传入普通指针。并可以通过get函数获得普通指针。 智能指针的作用是管理一个指针，因为存在以下这种情况：申请的空间在函数结束时忘记释放，造成内存泄漏。使用智能指针可以很大程度上的避免这个问题，因为智能指针是一个类，当超出了类的实例对象的作用域时，会自动调用对象的析构函数，析构函数会自动释放资源。所以智能指针的作用原理就是在函数结束时自动释放内存空间，不需要手动释放内存空间。 std::unique_ptr std::unique_ptr是std::auto_ptr的替代品，其用于不能被多个实例共享的内存管理。这就是说，仅有一个实例拥有内存所有权。它的使用很简单： class Fraction { private: int m_numerator = 0; int m_denominator = 1; public: Fraction(int numerator = 0, int denominator = 1) : m_numerator(numerator), m_denominator(denominator) { } friend std::ostream&amp; operator&lt;&lt;(std::ostream&amp; out, const Fraction &amp;f1) { out &lt;&lt; f1.m_numerator &lt;&lt; &quot;/&quot; &lt;&lt; f1.m_denominator; return out; } }; int main() { std::unique_ptr&lt;Fraction&gt; f1{ new Fraction{ 3, 5 } }; cout &lt;&lt; *f1 &lt;&lt; endl; // output: 3/5 std::unique_ptr&lt;Fraction&gt; f2; // 初始化为nullptr // f2 = f1 // 非法，不允许左值赋值 f2 = std::move(f1); // 此时f1转移到f2，f1变为nullptr // C++14 可以使用 make_unique函数 auto f3 = std::make_unique&lt;Fraction&gt;(2, 7); cout &lt;&lt; *f3 &lt;&lt; endl; // output: 2/7 // 处理数组，但是尽量不用这样做，因为你可以用std::array或者std::vector auto f4 = std::make_unique&lt;Fraction[]&gt;(4); std::cout &lt;&lt; f4[0] &lt;&lt; endl; // output: 0/1 cin.ignore(10); return 0; } std::shared_ptr std::shared_ptr与std::unique_ptr的主要区别在于前者是使用引用计数的智能指针。引用计数的智能指针可以跟踪引用同一个真实指针对象的智能指针实例的数目。这意味着，可以有多个std::shared_ptr实例可以指向同一块动态分配的内存，当最后一个引用对象离开其作用域时，才会释放这块内存。 #include &lt;iostream&gt; #include &lt;memory&gt; // for std::shared_ptr class Resource { public: Resource() { std::cout &lt;&lt; &quot;Resource acquired\\n&quot;; } ~Resource() { std::cout &lt;&lt; &quot;Resource destroyed\\n&quot;; } }; int main() { auto ptr1 = std::make_shared&lt;Resource&gt;(); cout &lt;&lt; ptr1.use_count() &lt;&lt; endl; // output: 1 { auto ptr2 = ptr1; // 通过复制构造函数使两个对象管理同一块内存 std::shared_ptr&lt;Resource&gt; ptr3; // 初始化为空 ptr3 = ptr1; // 通过赋值，共享内存 cout &lt;&lt; ptr1.use_count() &lt;&lt; endl; // output: 3 cout &lt;&lt; ptr2.use_count() &lt;&lt; endl; // output: 3 cout &lt;&lt; ptr3.use_count() &lt;&lt; endl; // output: 3 } // 此时ptr2与ptr3对象析构了 cout &lt;&lt; ptr1.use_count() &lt;&lt; endl; // output: 1 cin.ignore(10); return 0; } std::weak_ptr std::shared_ptr可以实现多个对象共享同一块内存，当最后一个对象离开其作用域时，这块内存被释放。但是仍然有可能出现内存无法被释放的情况，联想一下“死锁”现象，对于std::shared_ptr会出现类似的“循环引用”现象： class Person { public: Person(const string&amp; name): m_name{name} { cout &lt;&lt; m_name &lt;&lt; &quot; created&quot; &lt;&lt; endl; } virtual ~Person() { cout &lt;&lt; m_name &lt;&lt; &quot; destoryed&quot; &lt;&lt; endl; } friend bool partnerUp(std::shared_ptr&lt;Person&gt;&amp; p1, std::shared_ptr&lt;Person&gt;&amp; p2) { if (!p1 || !p2) { return false; } p1-&gt;m_partner = p2; p2-&gt;m_partner = p1; cout &lt;&lt; p1-&gt;m_name &lt;&lt; &quot; is now partenered with &quot; &lt;&lt; p2-&gt;m_name &lt;&lt; endl; return true; } private: string m_name; std::shared_ptr&lt;Person&gt; m_partner; }; int main() { { auto p1 = std::make_shared&lt;Person&gt;(&quot;Lucy&quot;); auto p2 = std::make_shared&lt;Person&gt;(&quot;Ricky&quot;); partnerUp(p1, p2); // 互相设为伙伴 } cin.ignore(10); return 0; } 对象没有被析构！出现内存泄露！仔细想想std::shared_ptr对象是什么时候才能被析构，就是引用计数变为0时，但是当你想析构p1时，p2内部却引用了p1，无法析构；反过来也无法析构。互相引用造成了“死锁”，最终内存泄露！ std::weak_ptr可以包含由std::shared_ptr所管理的内存的引用。但是它仅仅是旁观者，并不是所有者。那就是std::weak_ptr不拥有这块内存，当然不会计数，也不会阻止std::shared_ptr释放其内存。但是它可以通过lock()方法返回一个std::shared_ptr对象，从而访问这块内存。这样我们可以用std::weak_ptr来解决上面的“循环引用”问题 class Person { public: Person(const string&amp; name): m_name{name} { cout &lt;&lt; m_name &lt;&lt; &quot; created&quot; &lt;&lt; endl; } virtual ~Person() { cout &lt;&lt; m_name &lt;&lt; &quot; destoryed&quot; &lt;&lt; endl; } friend bool partnerUp(std::shared_ptr&lt;Person&gt;&amp; p1, std::shared_ptr&lt;Person&gt;&amp; p2) { if (!p1 || !p2) { return false; } p1-&gt;m_partner = p2; // weak_ptr重载的赋值运算符中可以接收shared_ptr对象 p2-&gt;m_partner = p1; cout &lt;&lt; p1-&gt;m_name &lt;&lt; &quot; is now partenered with &quot; &lt;&lt; p2-&gt;m_name &lt;&lt; endl; return true; } private: string m_name; std::weak_ptr&lt;Person&gt; m_partner; }; int main() { { auto p1 = std::make_shared&lt;Person&gt;(&quot;Lucy&quot;); auto p2 = std::make_shared&lt;Person&gt;(&quot;Ricky&quot;); partnerUp(p1, p2); // 互相设为伙伴 } cin.ignore(10); return 0; } ","link":"https://blog.shunzi.tech/post/cpp-basic/"},{"title":"Minio","content":" 开源支持 S3 协议的对象存储 支持多个客户端访问，支持分布式集群部署，容器化部署 考虑集成在项目中，作为后端存储进行测试 Introduction Features Open source Object Storage: MINIO Amazon S3, Apache License v2.0 Client/Server Mode Support Erasure Code Website: https://min.io/ Github Repo：https://github.com/minio/minio Details MinIO 是一个基于Apache License v2.0开源协议的对象存储服务。它兼容亚马逊S3云存储服务接口，非常适合于存储大容量非结构化的数据，例如图片、视频、日志文件、备份数据和容器/虚拟机镜像等，而一个对象文件可以是任意大小，从几kb到最大5T不等。 MinIO是一个非常轻量的服务,可以很简单的和其他应用的结合，类似 NodeJS, Redis 或者 MySQL。 [Java Client API] [Java Client API -zh] 官方测试地址：https://play.minio.io:9000 Start Docker Pull Image docker pull minio/minio Run image (without access_key &amp; secret_key) # -it 运行参数 # -p 指定端口 # -d 后台运行 docker run -it -p 9000:9000 -d minio/minio server /data Run image docker run -p 9000:9000 --name minio \\ -d --restart=always \\ -e &quot;MINIO_ACCESS_KEY=admin&quot; \\ -e &quot;MINIO_SECRET_KEY=admin123456&quot; \\ -v /home/data:/data \\ -v /home/config:/root/.minio \\ minio/minio server /data AWS S3 SDK for C++ Prerequisite Install CMake/GCC etc. Download AWS sdk. Build sdk firstly. 参考链接：博客园 AWS SDK for C++调用第三方S3 API 单独编译要使用到的 aws-cpp-sdk-core 和 aws-cpp-sdk-s3 make -j `nproc` -C aws-cpp-sdk-core make -j `nproc` -C aws-cpp-sdk-s3 安装头文件和库到一个目录 make install DESTDIR=/c++/C-Example/linux/install -C ../build/aws-cpp-sdk-s3 make install DESTDIR=/c++/C-Example/linux/install -C ../build/aws-cpp-sdk-core Sample Code: List Buckets Test Code: 使用 Minio 示例测试。 #include &lt;iostream&gt; #include &lt;aws/s3/S3Client.h&gt; #include &lt;aws/core/Aws.h&gt; #include &lt;aws/core/auth/AWSCredentialsProvider.h&gt; using namespace Aws::S3; using namespace Aws::S3::Model; using namespace std; int main(int argc, char* argv[]) { Aws::SDKOptions options; options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Trace; Aws::InitAPI(options); Aws::Client::ClientConfiguration cfg; cfg.endpointOverride = &quot;https://play.minio.io:9000&quot;; // S3服务器地址和端口 cfg.scheme = Aws::Http::Scheme::HTTP; cfg.verifySSL = false; Aws::Auth::AWSCredentials cred(&quot;Q3AM3UQ867SPQQA43P2F&quot;, &quot;zuf+tfteSlswRu7BJ86wekitnifILbZam1KYY3TG&quot;); // 认证的Key S3Client client(cred, cfg, Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, false); auto response = client.ListBuckets(); if (response.IsSuccess()) { auto buckets = response.GetResult().GetBuckets(); for (auto iter = buckets.begin(); iter != buckets.end(); ++iter) { cout &lt;&lt; iter-&gt;GetName() &lt;&lt; &quot;\\t&quot; &lt;&lt; iter-&gt;GetCreationDate().ToLocalTimeString(Aws::Utils::DateFormat::ISO_8601) &lt;&lt; endl; } } else { cout &lt;&lt; &quot;Error while ListBuckets &quot; &lt;&lt; response.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; response.GetError().GetMessage() &lt;&lt; endl; } Aws::ShutdownAPI(options); return 0; } IDE Configuration (VS Code Remote) VS Code Remote 开发时可能缺少对 gcc 的相关配置，示例如下： { &quot;configurations&quot;: [ { &quot;name&quot;: &quot;Linux&quot;, &quot;includePath&quot;: [ &quot;${workspaceFolder}/install/**&quot; ], &quot;defines&quot;: [], &quot;compilerPath&quot;: &quot;/usr/bin/gcc&quot;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++14&quot;, &quot;intelliSenseMode&quot;: &quot;clang-x64&quot; }, { &quot;name&quot;: &quot;huawei-cloud&quot;, &quot;includePath&quot;: [ &quot;${workspaceFolder}/install/**&quot; ], &quot;defines&quot;: [], &quot;compilerPath&quot;: &quot;/usr/bin/gcc&quot;, &quot;cStandard&quot;: &quot;c11&quot;, &quot;cppStandard&quot;: &quot;c++14&quot;, &quot;intelliSenseMode&quot;: &quot;gcc-x64&quot; } ], &quot;version&quot;: 4 } CMake Env Configuration 编写 CMakeList.txt cmake_minimum_required (VERSION 2.8) # 项目信息 project (Demo) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) # 指定生成目标 add_executable(Demo object_storage.cpp) target_link_libraries(Demo -laws-cpp-sdk-core -laws-cpp-sdk-s3) Build And Run 运行 CMake 编译成可执行文件 cmake . make make 时可能报错，缺少依赖，如 aws-c-common, aws-checksums and aws-c-event-stream. 在安装 AWS SDK 时会默认将这些依赖包放在 .deps/install/lib，如果没有安装，自行 make，make 完成之后导出 export LD_LIBRARY_PATH=/c++/C-Example/linux/.deps/install/lib64 重新编译我们的 demo cmake . make 编译成功，运行可执行文件 ./Demo # 输出结果 只列出部分 ezz 2020-03-17T02:13:55Z ezz1 2020-03-17T02:19:09Z f0569eee-6888-4d7b-a40c-bf0ee4ce65ad 2020-03-10T06:09:44Z f0953c46-45d2-4442-b0b4-24d4e34206ef 2020-03-14T00:46:38Z f3d50918-fee0-438a-8d9d-5c80fab1e43c 2020-03-17T19:30:46Z flink 2020-02-25T21:00:54Z foobar 2020-03-12T19:16:42Z foobbar 2020-03-12T19:00:40Z form 2020-03-09T05:47:00Z foundation-qa 2020-03-24T00:39:02Z j2n1g7fjsa9rjhceprqhphfktreq5hjx 2020-02-26T02:21:04Z jalgasdlkglskadg 2020-02-29T18:27:11Z jayce 2020-03-03T20:56:08Z jgjhghgj 2020-03-16T00:07:13Z jh6-gateway 2020-03-23T10:19:00Z jianmeng2 2020-03-02T09:16:31Z jjj 2020-03-05T23:56:21Z jvtest 2020-03-19T23:53:31Z jyyy-enclosure 2020-03-03T15:10:44Z kannappan300 2020-03-19T02:55:18Z kdyotzwd9278ke3gvukngmuqovylerxu 2020-03-20T10:49:53Z kek 2020-03-18T06:46:26Z kgramlkxni5sbd116ep81gz3d38gysqd 2020-03-07T05:12:23Z kkk 2020-03-05T23:56:12Z kopia-test-00040bab8a787438 2020-03-21T12:20:30Z kopia-test-fe2c2dc0cc28b253 2020-03-04T12:46:37Z mikbucket 2020-03-19T21:16:31Z minio-backup-bucket-name-here 2020-03-26T16:38:50Z zzz01 2020-03-16T08:02:36Z zzz02 2020-03-16T08:02:38Z Sample Code 2 GET/PUT #include &lt;iostream&gt; #include &lt;aws/s3/S3Client.h&gt; #include &lt;aws/core/Aws.h&gt; #include &lt;aws/core/auth/AWSCredentialsProvider.h&gt; // Include object request header #include &lt;aws/s3/model/PutObjectRequest.h&gt; #include &lt;aws/s3/model/GetObjectRequest.h&gt; #include &lt;fstream&gt; using namespace Aws::S3; using namespace Aws::S3::Model; using namespace std; int main(int argc, char* argv[]) { // Set log options and init aws sdk. Aws::SDKOptions options; options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Trace; Aws::InitAPI(options); // Create the client with configuration (endpoint, protocol, ssl, credential) Aws::Client::ClientConfiguration cfg; cfg.endpointOverride = &quot;http://localhost:9000&quot;; // S3服务器地址和端口 cfg.scheme = Aws::Http::Scheme::HTTP; cfg.verifySSL = false; // Create credential with access key and secret key. Aws::Auth::AWSCredentials cred(&quot;admin&quot;, &quot;admin123456&quot;); // 认证的Key S3Client client(cred, cfg, Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, false); // Call api provided by aws sdk. // List buckets auto response = client.ListBuckets(); if (response.IsSuccess()) { // Parse response auto buckets = response.GetResult().GetBuckets(); for (auto iter = buckets.begin(); iter != buckets.end(); ++iter) { // cout bucket name and creation date. cout &lt;&lt; iter-&gt;GetName() &lt;&lt; &quot;\\t&quot; &lt;&lt; iter-&gt;GetCreationDate().ToLocalTimeString(Aws::Utils::DateFormat::ISO_8601) &lt;&lt; endl; } } else { // Error handle. cout &lt;&lt; &quot;Error while ListBuckets &quot; &lt;&lt; response.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; response.GetError().GetMessage() &lt;&lt; endl; } // Read Objects // Parameters std::string BucketName = &quot;test&quot;; std::string KeyName = &quot;key&quot;; // Local path to store the data returned by Cloud Storage std::string PathKey = &quot;./test-data/key.png&quot;; // Create the object get request Aws::S3::Model::GetObjectRequest object_get_request; object_get_request.WithBucket(BucketName.c_str()).WithKey(KeyName.c_str()); // Send Get Request auto get_response = client.GetObject(object_get_request); // Verify the status of response. if (get_response.IsSuccess()) { // Read data and write into the file. Aws::OFStream local_file(PathKey.c_str(), std::ios::trunc | ::ios::out | std::ios::binary); if (local_file) { std::cout &lt;&lt; &quot;File exsits!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;File not exsits!&quot; &lt;&lt; std::endl; } local_file &lt;&lt; get_response.GetResult().GetBody().rdbuf(); if (local_file) { std::cout &lt;&lt; &quot;After write, File exsits!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;After write, File not exsits!&quot; &lt;&lt; std::endl; } std::cout &lt;&lt; &quot;Done!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;GetObject error: &quot; &lt;&lt; get_response.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; get_response.GetError().GetMessage() &lt;&lt; std::endl; } // Write Objects // Parameters std::string writeKeyName = &quot;keyw&quot;; // Local path to store the data returned by Cloud Storage std::string writePathKey = &quot;./test-data/key.png&quot;; PutObjectRequest putObjectRequest; putObjectRequest.WithBucket(BucketName.c_str()).WithKey(writeKeyName.c_str()); auto input_data = Aws::MakeShared&lt;Aws::FStream&gt;(&quot;PutObjectInputStream&quot;, writePathKey.c_str(), std::ios_base::in | std::ios_base::binary); putObjectRequest.SetBody(input_data); auto putObjectResult = client.PutObject(putObjectRequest); if (putObjectResult.IsSuccess()) { std::cout &lt;&lt; &quot;Upload Done!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;PutObject error: &quot; &lt;&lt; putObjectResult.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; putObjectResult.GetError().GetMessage() &lt;&lt; std::endl; } // Deallocate AWS resources Aws::ShutdownAPI(options); return 0; } 注意路径的格式，相对路径和绝对路径的区分，上例使用的是相对路径。 将 aws-s3-cpp-sdk 封装为 C library Create CPP Class Project Tree ├── CMakeLists.txt ├── include │ ├── S3Util.h └── src ├── CMakeLists.txt ├── main.cpp └── S3Util.cpp define header S3Util.h #ifndef S3UTIL_H #define S3UTIL_H #include &lt;iostream&gt; #include &lt;aws/s3/S3Client.h&gt; #include &lt;aws/core/Aws.h&gt; #include &lt;aws/core/auth/AWSCredentialsProvider.h&gt; // Include object request header #include &lt;aws/s3/model/PutObjectRequest.h&gt; #include &lt;aws/s3/model/GetObjectRequest.h&gt; #include &lt;aws/s3/model/CreateBucketRequest.h&gt; #include &lt;fstream&gt; using namespace Aws::S3; using namespace Aws::S3::Model; using namespace std; class S3Util { private: static Aws::SDKOptions options; static S3Client* client; public: S3Util(); bool createBucket(char* BucketName); bool uploadfile(char* BucketName, char* objectKey, char* pathkey); bool downloadfile(char* BucketName, char* objectKey, char* pathkey); ~S3Util(); }; #endif S3Util.cpp #include &quot;S3Util.h&quot; Aws::SDKOptions S3Util::options = {}; S3Client* S3Util::client = {}; S3Util::S3Util() { // Set log options and init aws sdk. options.loggingOptions.logLevel = Aws::Utils::Logging::LogLevel::Trace; Aws::InitAPI(options); // Create the client with configuration (endpoint, protocol, ssl, credential) Aws::Client::ClientConfiguration cfg; cfg.endpointOverride = &quot;http://114.116.234.136:9000&quot;; // S3服务器地址和端口 cfg.scheme = Aws::Http::Scheme::HTTP; cfg.verifySSL = false; // Create credential with access key and secret key. Aws::Auth::AWSCredentials cred(&quot;admin&quot;, &quot;admin123456&quot;); // 认证的Key client = new S3Client(cred, cfg, Aws::Client::AWSAuthV4Signer::PayloadSigningPolicy::Never, false); } bool S3Util::createBucket(char* BucketName) { CreateBucketRequest createBucketReq; createBucketReq.WithBucket(BucketName); auto createResponse = client-&gt;CreateBucket(createBucketReq); if (createResponse.IsSuccess()) { std::cout &lt;&lt; &quot;Create Bucket Done!&quot; &lt;&lt; std::endl; return true; } else { std::cout &lt;&lt; &quot;Create Bucket error: &quot; &lt;&lt; createResponse.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; createResponse.GetError().GetMessage() &lt;&lt; std::endl; return false; } } bool S3Util::uploadfile(char* BucketName, char* objectKey, char* pathkey) { PutObjectRequest putObjectRequest; putObjectRequest.WithBucket(BucketName).WithKey(objectKey); auto input_data = Aws::MakeShared&lt;Aws::FStream&gt;(&quot;PutObjectInputStream&quot;, pathkey, std::ios_base::in | std::ios_base::binary); putObjectRequest.SetBody(input_data); auto putObjectResult = client-&gt;PutObject(putObjectRequest); if (putObjectResult.IsSuccess()) { std::cout &lt;&lt; &quot;Upload Done!&quot; &lt;&lt; std::endl; return true; } else { std::cout &lt;&lt; &quot;PutObject error: &quot; &lt;&lt; putObjectResult.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; putObjectResult.GetError().GetMessage() &lt;&lt; std::endl; return false; } } bool S3Util::downloadfile(char* BucketName, char* objectKey, char* pathkey) { // Create the object get request Aws::S3::Model::GetObjectRequest object_get_request; object_get_request.WithBucket(BucketName).WithKey(objectKey); // Send Get Request auto get_response = client-&gt;GetObject(object_get_request); // Verify the status of response. if (get_response.IsSuccess()) { // Read data and write into the file. Aws::OFStream local_file(pathkey, std::ios::trunc | ::ios::out | std::ios::binary); if (local_file) { std::cout &lt;&lt; &quot;File exsits!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;File not exsits!&quot; &lt;&lt; std::endl; } local_file &lt;&lt; get_response.GetResult().GetBody().rdbuf(); if (local_file) { std::cout &lt;&lt; &quot;After write, File exsits!&quot; &lt;&lt; std::endl; } else { std::cout &lt;&lt; &quot;After write, File not exsits!&quot; &lt;&lt; std::endl; } std::cout &lt;&lt; &quot;Done!&quot; &lt;&lt; std::endl; return true; } else { std::cout &lt;&lt; &quot;GetObject error: &quot; &lt;&lt; get_response.GetError().GetExceptionName() &lt;&lt; &quot; &quot; &lt;&lt; get_response.GetError().GetMessage() &lt;&lt; std::endl; return false; } } S3Util::~S3Util() { Aws::ShutdownAPI(options); } 测试程序 main.cpp #include &quot;S3Util.h&quot; #include &lt;iostream&gt; using namespace std; int main(int argc, char* argv[]) { std::cout &lt;&lt; &quot;Start!&quot;&lt;&lt; std::endl; char *BucketName = &quot;test&quot;; char *objectKey = &quot;key&quot;; char *path = &quot;./../test-data/key.png&quot;; S3Util* util = new S3Util(); bool result = util-&gt;downloadfile(BucketName, objectKey, path); std::cout &lt;&lt; &quot;Download reslut &quot; &lt;&lt; result &lt;&lt; std::endl; std::cout &lt;&lt; &quot;End!&quot;&lt;&lt; std::endl; return 0; } CMake build Outside CMakeLists.txt #CMake最低版本号要求 cmake_minimum_required(VERSION 2.8) #指定项目名称 project(s3util) #指定版本信息 set(CMAKE_SYSTEM_VERSION 1) #若是需要指定编译器路径 #set(CROSS_TOOLCHAIN_PREFIX &quot;/path/arm-linux-&quot;) #指定编译器 #set(CMAKE_C_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}gcc&quot;) #set(CMAKE_CXX_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}g++&quot;) #使用默认路径的g++指定编译器 #set(CMAKE_CXX_COMPILER &quot;g++&quot;) #指定编译选项 set(CMAKE_BUILD_TYPE Debug ) #指定编译目录 set(PROJECT_BINARY_DIR ${PROJECT_SOURCE_DIR}/build) #添加子目录,这样进入源码文件src目录可以继续构建 add_subdirectory(${PROJECT_SOURCE_DIR}/src) Inside CMakeLists.txt #查找当前目录下的所有源文件， #并将名称保存到DIR_LIB_SRCS目录 #aux_source_directory(. DIR_LIB_SRCS) #指定头文件目录,PROJECT_SOURCE_DIR为工程的根目录 include_directories(${PROJECT_SOURCE_DIR}/include) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) #指定可执行文件的输出目录，输出到bin下面 set(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin) #生成动态库 add_library(s3_shared_demo SHARED S3Util.cpp) #设置库输出名为 shared =&gt; libshared.so set_target_properties(s3_shared_demo PROPERTIES OUTPUT_NAME &quot;s3shared&quot;) #生成静态库 add_library(s3_static_demo STATIC S3Util.cpp) #设置输库出名为 static =&gt; libstatic.a set_target_properties(s3_static_demo PROPERTIES OUTPUT_NAME &quot;s3static&quot;) #指定库文件输出路径 set(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib) #在指定目录下查找库，并保存在LIBPATH变量中 find_library(LIBPATHS shared ${PROJECT_SOURCE_DIR}/lib) #指定生成目标 add_executable(main main.cpp) #链接共享库 target_link_libraries(main ${LIBPATHS}) target_link_libraries(main -laws-cpp-sdk-core -laws-cpp-sdk-s3 s3_shared_demo) 编写 Wrapper Project Tree ├── CMakeLists.txt ├── include │ ├── S3TestWrapper.h └── src ├── CMakeLists.txt ├── main.cpp └── S3Util.cpp define header #ifndef S3TESTWRAPPER_H #define S3TESTWRAPPER_H #ifdef __cplusplus extern &quot;C&quot; { #endif int createBucket(char* BucketName); int uploadfile(char* BucketName, char* objectKey, char* pathkey); int downloadfile(char* BucketName, char* objectKey, char* pathkey); #ifdef __cplusplus } #endif #endif define wrapper #include &quot;S3TestWrapper.h&quot; #include &quot;S3Util.h&quot; #ifdef __cplusplus extern &quot;C&quot; { #endif int createBucket(char* BucketName) { S3Util* util = new S3Util(); return util-&gt;createBucket(BucketName); } int uploadfile(char* BucketName, char* objectKey, char* pathkey) { S3Util* util = new S3Util(); return util-&gt;uploadfile(BucketName, objectKey, pathkey); } int downloadfile(char* BucketName, char* objectKey, char* pathkey) { S3Util* util = new S3Util(); return util-&gt;downloadfile(BucketName, objectKey, pathkey); } #ifdef __cplusplus }; #endif 测试程序 main.cpp #include &quot;S3TestWrapper.h&quot; #include &lt;iostream&gt; using namespace std; int main(int argc, char* argv[]) { std::cout &lt;&lt; &quot;Start!&quot;&lt;&lt; std::endl; char *BucketName = &quot;test&quot;; char *objectKey = &quot;key&quot;; char *path = &quot;./../test-data/key.png&quot;; bool result = downloadfile(BucketName, objectKey, path); std::cout &lt;&lt; &quot;Download reslut &quot; &lt;&lt; result &lt;&lt; std::endl; std::cout &lt;&lt; &quot;End!&quot;&lt;&lt; std::endl; return 0; } Inside CMakeLists.txt #查找当前目录下的所有源文件， #并将名称保存到DIR_LIB_SRCS目录 aux_source_directory(. DIR_LIB_SRCS) #指定头文件目录,PROJECT_SOURCE_DIR为工程的根目录 include_directories(${PROJECT_SOURCE_DIR}/include) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/s3util/include) link_directories(/c++/C-Example/linux/s3util/lib) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) #指定可执行文件的输出目录，输出到bin下面 set(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin) #生成动态库 add_library(s3_shared_demo_c SHARED S3TestWrapper.cpp) #设置库输出名为 shared =&gt; libshared.so set_target_properties(s3_shared_demo_c PROPERTIES OUTPUT_NAME &quot;s3sharedC&quot;) #生成静态库 add_library(s3_static_demo_c STATIC S3TestWrapper.cpp) #设置输库出名为 static =&gt; libstatic.a set_target_properties(s3_static_demo_c PROPERTIES OUTPUT_NAME &quot;s3staticC&quot;) #指定库文件输出路径 set(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib) #在指定目录下查找库，并保存在LIBPATH变量中 find_library(LIBPATHS shared ${PROJECT_SOURCE_DIR}/lib) #指定生成目标 add_executable(main main.cpp) #链接共享库 target_link_libraries(main ${LIBPATHS}) target_link_libraries(main -ls3shared -laws-cpp-sdk-core -laws-cpp-sdk-s3 s3_shared_demo_c) #指定生成目标 add_executable(test test.c) #链接共享库 target_link_libraries(test ${LIBPATHS}) target_link_libraries(test -ls3shared -laws-cpp-sdk-core -laws-cpp-sdk-s3 s3_shared_demo_c) Outside CMakeLists.txt #CMake最低版本号要求 cmake_minimum_required(VERSION 2.8) #指定项目名称 project(s3wrapperC) #指定版本信息 set(CMAKE_SYSTEM_VERSION 1) #若是需要指定编译器路径 #set(CROSS_TOOLCHAIN_PREFIX &quot;/path/arm-linux-&quot;) #指定编译器 #set(CMAKE_C_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}gcc&quot;) #set(CMAKE_CXX_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}g++&quot;) #使用默认路径的g++指定编译器 #set(CMAKE_CXX_COMPILER &quot;g++&quot;) #指定编译选项 set(CMAKE_BUILD_TYPE Debug ) #指定编译目录 set(PROJECT_BINARY_DIR ${PROJECT_SOURCE_DIR}/build) #添加子目录,这样进入源码文件src目录可以继续构建 add_subdirectory(${PROJECT_SOURCE_DIR}/src) 使用 C 语言测试 main.c main.c #include &quot;S3TestWrapper.h&quot; #include&lt;stdio.h&gt; int main(int argc, char* argv[]) { printf(&quot;start!\\n&quot;); char *BucketName = &quot;test&quot;; char *objectKey = &quot;key&quot;; char *path = &quot;./../test-data/key-test.png&quot;; int result = downloadfile(BucketName, objectKey, path); printf(&quot;Download Result: %d!\\n&quot;, result); printf(&quot;end!\\n&quot;); return 0; } CMakeLists.txt cmake_minimum_required (VERSION 2.8) # 项目信息 project (Demo) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) include_directories(/c++/C-Example/linux/s3util/include) link_directories(/c++/C-Example/linux/s3util/lib) include_directories(/c++/C-Example/linux/s3-c/test/include) link_directories(/c++/C-Example/linux/s3-c/test/lib) # 指定生成目标 add_executable(Demo main.c) target_link_libraries(Demo -laws-cpp-sdk-core -laws-cpp-sdk-s3 -ls3shared -ls3sharedC) 集成 TCMU 参考资料 [1] 简书：Docker安装Minio存储服务器详解 [2] (minIO) aws sdk for C++ ","link":"https://blog.shunzi.tech/post/minio/"},{"title":"FlashBlox: Achieving Both Performance Isolation and Uniform Lifetime for Virtualized SSDs","content":" 虚拟化课程论文分享，文章来自 FAST17 SSD 虚拟化相关，顺便补一下之前 SSD 的坑 主要解决云计算领域中虚拟化 SSD 设备中的隔离性问题 预备知识 闪存 参考链接 [1] 知乎 - 老狼 - 杂说闪存一：关公战秦琼之 UFS VS NVMe [2] 知乎 - 老狼 - 杂谈闪存二：NOR和NAND Flash [3] 知乎 - 老狼 - 杂谈闪存三：FTL [4] 知乎 - 老狼 - 杂说闪存四：闪存硬盘接口大比拼 [5] 存储产业技术创新战略联盟 SSD 相关培训资料 [6] 阿里云云栖号：存储系统设计——NVMe SSD性能影响因素一探究竟 简单总结 此处简单总结 Paper 中可能将会使用到的一些术语和概念以及一些 SSD 相关的基本常识，结合该总结食用 Paper 更佳。 SSD 基本结构：控制单元（主机接口，SSD控制器，DRAM），存储单元（NAND FLASH） NVMe 与 PCIe 的关系？：NVMe 的目标是取代传统的 SATA 接口，NVMe 本质是一种通信协议，在通信协议里属于应用层，使用PCIe 协议作为数据和链路层。 NVMe 取代 SATA 的动力？：SATA接口采用AHCI规范，其已经成为制约SSD速度的瓶颈。AHCI只有1个命令队列，队列深度32；而NVMe可以有65535个队列，每个队列都可以深达65536个命令。NVMe也充分使用了MSI的2048个中断向量优势，延迟大大减小。 NVMe 和 UFS 对比？：UFS 是为了取代 eMMC 嵌入式场景的协议，常用于手机；NVMe主要应用于计算机平台。UFS 通常使用 2 条 lane，NVMe 常使用 4 条 lane。UFS3.0，PCIe 4.0 NOR FLASH 和 NAND FLASH：NOR 常用于 BIOS，NAND 主要用于存储卡 NAND FLASH 颗粒：SLC/MLC/TLC/QLC - FTL 产生的背景：NAND Flash相对NOR Flash更可能发生比特翻转，就必须采用错误探测/错误更正(EDC/ECC)算法，同时NAND Flash随着使用会渐渐产生坏块。如何才能平衡各块的擦写和为可能的坏块寻找替换呢？通常需要有一个特殊的软件层次，实现坏块管理、擦写均衡、ECC、垃圾回收等的功能，这一个软件层次称为 FTL（Flash Translation Layer）。编程可以让 FLASH 的 bit 从 1 变到 0，而从 0 到 1 只能进行擦除。写入的最小单元为 Page，擦除的最小单元为 Block，NAND flash的寿命是由其擦写次数决定的(P/E数 (Program/Erase Count)来衡量的)，频繁的擦除慢慢的会产生坏块。需要一套机制ll来平衡整块 FLASH 的整体擦写次数 NAND FLASH 结构 Package: 也就是chip即Flash芯片，就是我们经常在M.2的SSD上看到的NAND flash颗粒。一个封装好的芯片就是一个chip。 Die: 一个NAND颗粒是由一颗或者多颗Die封装在一起而成，这种封装可是平排的，也可以是层叠的。die内部可以通过3D 堆叠技术扩展容量，譬如三星的V-NAND每层容量都有128Gb（16GB），通过3D堆叠技术可以实现最多24层堆叠，这意味着24层堆叠的总容量将达到384GB！就像盖楼房一样。Die也是可以单独执行命令和返回状态的最小单位。 Plane: 一个die可以包含几个Plane. 一个plane就是一个存储矩阵，包含若干个Block Block: 重要的概念，它是擦除操作的最小单位。 Page：也很重要，它是写入动作的最小单位。读的最小单位也是 Page Cell：每个16KB的Page页又是由大量的Cell单元构成。Cell是闪存的最小工作单位，执行数据存储的任务。闪存根据每个单元内可存储的数据量分成SLC（1bit/Cell）、MLC（2bit/Cell）、TLC（3bit/Cell）和QLC（4bit/Cell） NAND FLASH 存储原理：，其工作原理是利用 浮栅上是否储存有电荷或储存电荷的多少来改变晶体管 的阈值电压，通过读取到的晶体管阈值电压来实现数据 信息的表征。 写操作本质是向浮栅注入电荷，擦除操作是从浮栅挪走 电荷 FTL原理：维护了一个逻辑Block地址（LBA，logical block addresses ）和物理Block地址（PBA, physical block addresses）的对应关系，有了这层映射关系，我们需要修改时就不需要改动原来的物理块，只需要标记原块为废块，同时找一个没用的新物理块对应到原来的逻辑块上就好了 寿命均衡（Wear Levelling）：LBA/PBA的映射本身会对寿命均衡产生正面影响。就如我们SD卡上的FAT文件系统，文件分配表会被经常修改，但由于修改的是逻辑块，我们可以让每次物理块不同而避免经常擦写相同的物理块，这本身就保证不会有物理块被经常擦写。但是有一种情况它没有办法处理，即冷的数据块（cold block），它们被写入后没有更改，就一直占据某些物理块，而这些物理块寿命还很长，而别的热的块却在飞速损耗中。这种情况怎么办呢？我们只有在合适的时机帮它们换个位置了，如何选择这个时机很重要，而且这个搬家动作本身也会损耗寿命本身。这些策略也是各个FTL算法的精华了。 LBA/PBA表存储在哪里：在大部分的NAND Flash里，还有些空闲块，我们叫它OP(Over Provisioning)。这些空闲的块可以极大的帮助我们改善NAND flash的性能，它可以： A． 坏块处理。发现坏块，这些后备的可以立刻顶上，因为有映射机制，上层软件完全感受不到。 B． 存储LBA/PBA表 C． 给GC和Wear Levelling留下极大的腾挪空间。 D. 减少写入放大（Write Amplification） 事实上，现在几乎所有主流SSD等NAND die上都有OP。譬如我们拿到标称容量240GB的SSD，实际空间可能有256GB甚至更高（一般&gt;7.37%），只不过这些多余的空间我们用不到，感受不到，它完全被SSD固件藏做私用而已。 SSD 关键概念：容量（用户容量）、接口类型、介质类型、顺序读/写、随机读/写、读/写延迟、典型功耗、可靠性、冗余空间、写放大系数、UBER（发生不可纠正ECC的几率）、数据保持里（Retention）、寿命（DWPD，每日整盘写入次数） SSD 技术趋势：新型接口 Ruler/M.3，新架构 OpenChannel/盘内计算，新介质QLC/SCM 新型接口 Ruler/M.3：存储服务器新型接口 新架构 OpenChannel/盘内计算： 数据库（Rocks DB，key-value），优化原有 SSD，提高性能/寿命 新介质QLC/SCM：QLC取代大容量硬盘，SCM用于内存数据库 系统框图： SSD 读写流程： Paper Course Requirements：motivation, idea, design, experiments and what you think. Finial Report Request: Select one area, survey the area, and write a survey report, at least 3000 words, 30+ references. Finial report: Before May 30. Abstract SSD 虚拟化领域的一个长期目标是在共享SSD 存储设备的多个租户之间提供性能隔离。然而，由于资源隔离和设备寿命之间的根本冲突，虚拟化 SSD 在传统上一直是一个挑战——现有虚拟化 SSD 的目标是使SSD Flash 的所有区域一致老化（即负载均衡保证比较平均的使用寿命），但这不利于隔离。我们建议利用 SSD 的并行性来改善虚拟固态硬盘之间的隔离问题，方法则是在专用SSD通道和芯片上实现并行。除此以外，我们还提供了一个负载均衡的解决方案。我们提出了允许不同通道和 LUN 的负载在比较小的时间粒度上分散，从而支持隔离性并能在一个大的时间粒度上调控对应的负载。我们的实验表明，与软件隔离的虚拟固态硬盘相比，新的固态硬盘磨损均衡方案，在各种多租户设置下，存储操作的第99百分位延迟（尾延迟）减少了3.1倍。 Die也被称作LUN（逻辑单元），也是闪存内可执行命令并回报自身状态的最小独立单元。 [1] 参考链接：闪存结构全解：读懂固态硬盘中的“黑话” Dies Introduction 随着近年来的硬件技术的发展，SSD 的成本越发接近 HDD，但其性能表现远高于 HDD。所以 SSD 在云计算领域中得到了大量的应用，到如今已经显得不可或缺。对于 SSD 而言，硬件技术的发展使得可以通过增加芯片的数量来直接增加 SSD 的带宽和容量，大带宽大容量的出现则使得云计算领域考虑在 SSD 上虚拟化构建共享存储设备的可能性。但是由于 SSD 自身结构的原因（主要是指 SSD 的管理模块 FTL），屏蔽了大量的技术细节，特别是 SSD 的并行性并未在云计算的多租户场景下得到有效的利用。原有 SSD 在多租户场景下表现出来的瓶颈其中之一就是尾延迟，云计算场景下的云存储和数据库系统通常就构建在同一个 SSD 集群上，多个应用多租户的场景则加重了 SSD 尾延迟的问题。 从单个 SSD 的角度考虑尾延迟，造成尾延迟的根本原因是 SSD 控制器 FTL 中复杂的管理算法。管理算法从 SSD 发展开始至今，主要目标未发生过显著变化，主要是为了隐藏内部 FLASH 颗粒的相关特性，统一对外暴露出块接口，这些算法将 SSD 的磨损均衡（为了延长 SSD 的使用寿命）和 资源利用（利用并行性）合并在一起，导致在虚拟化 SSD 的场景下，常常引入了多租户之间的干扰。 【1】科普向 FTL 简书 - SSD之FTL技术 【2】科普向 FTL 老狼 - 杂谈闪存三：FTL 现有的优化方案中，主要有应用层面上通过利用物理涉设备的水平并行性来提升吞吐量，但并没有实际减少多租户共享 SSD 时的之间的干扰。这些租户不能有效地利用 flash 并行性来隔离它们，即使它们都分别是 flash 友好的，因为 FTL 隐藏了并行性。新的 SSD 接口提议将原始并行直接暴露给更高的层，这在为租户获得隔离方面提供了更大的灵活性，但是它们使跨不同并行单元的磨损均衡机制的实现复杂化 。 在这项工作中，我们建议利用当前SSD中固有的并行性来增强共享SSD的多个承租者之间的隔离，通过创建虚拟 SSD，根据租户的容量和性能需求，将其固定到特定数量的通道 channel 和 die 上，channel 和 die 可以一定程度上独立地运行工作，所以可以避免对彼此之间的性能产生影响。不同的工作负载可以以不同的比例和不同的读写模式运行，可能导致channel 和 die 不同程度上老化。例如固定在TPC-C数据库实例上的通道比固定在TPCE数据库实例上的通道耗损速度快12倍，从而显著降低了SSD的生存期。这种不一致的老化造成了不可预测的SSD生存期行为，这使数据中心集群的配置和负载平衡变得复杂 为了解决这个问题，我们提出了包含了两个部分的负载均衡模型，该模型使用不同的策略来平衡每个虚拟SSD内部和跨虚拟SSD之间的磨损。 对于每个虚拟 SSD 内部，利用了现存的 SSD 的负载均衡机制来管理，虚拟SSD内部的磨损是在粗粒度上平衡的，通过使用新的机制来减少干扰。 对于跨虚拟 SSD，使用一个数学模型来控制虚拟SSD之间的磨损不平衡，并证明了新的磨损平衡模型可以在几乎不影响租户的情况下确保SSD的接近理想寿命。 实际工作 提出了一个名为FlashBlox的系统，使用该系统，租户可以通过在专用通道上工作，以最小的干扰共享一个SSD。 提出了一种新的磨损平衡机制，允许测量磨损量的不平衡，以获得更好的性能隔离之间的租户 提出了一个分析模型和一个系统，控制channel 和 die 之间的磨损不平衡，使它们一致老化，对租户的干扰可以忽略不计 其他 我们设计并实现了FlashBlox及其新的耐磨调平机制在一个开放通道的SSD堆栈（Open Channel SSD），基于微软数据中心的多租户存储负载进行了测试评估，新的 SSD 显示出了原有 1.6 倍的吞吐量，99th 尾延迟减少了 3.1倍，我们实现的负载均衡机制保证了 SSD 寿命能够达到理想寿命的 95%，即便在同一个通道上进行写操作，其他通道进行写操作这样严重影响 SSD 寿命的负载。 SSD 虚拟化 - 机遇和挑战 IaaS、PaaS 和 DaaS 中的存储服务为了达到对应的服务水平目标，常常需要使用性能远好于 HDD 的 SSD，而存储虚拟化则是帮助这些存储服务在多个客户和实例之间进行资源的分割，从而更高效地利用 SSD 的容量和性能。例如 DaaS 中的数据库存储服务容量需求大约为 10GB - 1TB，对应的服务器则需要拥有容量超过 20T 的 SSD。除此以外，存储虚拟化还常常需要使用诸如令牌桶算法之类的能够智能调控 IO 的技术来对带宽、IOPS等进行一定程度上的限制，实现定制化的需求。但是 SSD 虚拟化领域缺少一个类似的机制在实现 SSD 共享的同时保留 SSD 低延迟的特性，一个实例的延迟表现仍然取决于其他实例的前端负载和垃圾回收。 越来越有必要将不同的工作负载(例如延迟关键型应用程序和批处理作业)放在一起，以提高资源利用率，同时保持隔离，虚拟化和容器技术正在发展，以利用内存、CPU、缓存和网络的硬件隔离来支持这种场景。通过提供与硬件隔离的ssd，我们将这条研究线扩展到ssd，并提供了一个解决方案，以解决由于不同工作负载的租户之间的物理闪存分区而产生的磨损不平衡问题。 硬件隔离 VS 磨损均衡 此处比较两种硬件资源共享的方式： 第一种方法从所有flash通道(总共8个)的所有工作负载中提取数据，就像现有的ssd所做的那样。该方案为每个IO提供了最大的吞吐量，并使用Linux容器和Docker使用的软件速率限制器来实现资源的加权公平共享 。**注意，在软件隔离的情况下，实例不会与其他共区的实例共享物理flash块。**该方法消除了消除了其中一个实例垃圾回收对其他实例的读性能的影响。 第二种方使用我们提出的机制中的配置，通过为每个实例分配一定数量的通道来提供硬件隔离。 为了比较两种方法，我们使用了四种IO敏感型负载，这些负载分别请求 1/8，1/4，1/4，3/8 的共享存储资源，在第一种方法中将比例对应的设置为速率限制器的权重值，相应的方法二中则分配 1，2，2，3个通道。workloads 2 和 4 使用了 100% 的写，1 和 3 使用了 100% 的读，所有负载都使用 64KB 的 IO 大小。 测试结果表明硬件隔离相比于软件隔离，延迟降低约 1.7 倍，吞吐量提升 10.8%，然而硬件隔离方案中的将通道固定到指定实例，阻止了硬件自动在多通道之间实现负载均衡的机制的运行，如图d 所示，我们夸大了写速率的差异，以便更好地解决由虚拟ssd的硬件隔离引起的损耗不平衡问题。为了进一步激发这个问题，我们必须首先探索SSD硬件中可用的并行性，以及在第一种方法中导致干扰的 FTLs 的各个方面 利用并行性保证隔离性 传统的 SSD 设备常常采用分层的结构来组织，从通道到 dies 到planes 到 blocks 到 pages，每两层之间都是一个一对多的关系，对应每一层的数量也会因为厂商的不同和产品的版本不同而不同，通常，一个 drive，8-32 个 channles，1 个 channel 约有 4-8 个 dies，一个 die 又有 2-4 个planes，每一个 planes 又有数千个 blocks，每一个 block 又包含 128 - 256 个 pages。该结构对定义隔离边界有很大的影响，通道(仅共享整个SSD的公共资源)提供了最强大的隔离，die以完全独立的方式执行它们的命令，但是它们必须与同一通道上的其他die共享一个总线，Planes 的隔离是有限的，因为die只包含一个地址缓冲区，控制器可以将数据隔离到不同的 planes，但是对这些数据的操作必须在不同的时间发生，或者在一个die中的每个 planes 上对相同的地址进行操作 目前市面上流行的 SSD 设备，没有将这些内部组件的灵活性暴露给上层，相反，驱动器优化为一个单一的IO模式：非常大或连续的IO. FTL 逻辑上将所有的 planes 组合成一个大的单元，创造出的“super pages”和“super blocks”比它们的基本单元大上数百倍，例如，一个包含4MB块和256个planes的驱动器对应的有一个1GB的超级块。条带增加了连续的大型IOs的吞吐量，但是引入了共享驱动器的多个租户之间的干扰的负面影响，由于所有数据都是分段的，每个租户的读、写和擦除操作可能与其他租户的操作发生冲突。 以前的工作提出了新的技术来帮助租户放置他们的数据，这样底层的flash页面就可以从单独的块中分配。这有助于通过减少写放大因子来提高性能。缺少块共享有一个令人满意的副作用，那就是将垃圾聚集到更少的块中，从而提高垃圾收集(GC)的效率，从而减少ssd的延迟。然而，租户之间仍然存在显著的干扰，因为当数据被条带化时，每个租户使用每个通道、die和plane来存储数据，并且一个租户的存储操作可能会延迟其他租户，软件隔离技术相当于直接公平地分割了 SSD 资源，然而，当底层存在资源争用时，由于通道、dies 和 planes 等独立资源的强制共享，它们无法最大限度地利用flash并行性。 OpenChannel SSD 则开放了内部组件给操作系统，通过使用专用通道，可以帮助共享SSD的租户避免这些陷阱，然而，不同租户以不同速度和比例写的 channel 之间的磨损不平衡问题仍然没有得到解决。我们提出了一个整体的方法来解决这个问题，通过暴露的flash channel 和 dies 作为虚拟ssd，而系统下磨损水平在粗糙的时间粒度上每个vSSD 的 channel 和 dies 保证负载均衡。FlashBlox 只关心在一个NVMe SSD内共享资源，公平共享机制，将PCIe总线带宽分割到多个 NVMe设备、网络接口卡、图形处理单元和其他PCIe设备超出了本工作的范围。 设计和实现 架构 组成 资源管理器：允许租户分配和释放虚拟 SSD Host-Level 闪存管理器：通过在粗略的时间力度上平衡通道之间和 dies 之间的负载来实现实现虚拟 SSD 内部的负载均衡 SSD-Level 闪存管理器：实现虚拟 SSD 内部的其他负载均衡，以及 FTL 的其他功能。 设计 FlashBlox提供的一个关键的新抽象是虚拟SSD (vSSD)，它可以减少尾部延迟，它使用专用的闪存硬件资源，如 channel 和 die，可以独立操作。并暴露创建 vSSD 的 API：isolationLevel, tputLevel, capacity 参数使得用户可以根据对隔离性和吞吐量的不同级别的要求来创建对应的 vSSD vssd t AllocVirtualSSD(int isolationLevel, int tputLevel, size t capacity) 这些参数与性能和经济成本水平相兼容，如DaaS系统中宣传的，以简化使用和管理。租户可以通过创建支持大小的多个vssd来扩展容量，就像今天在DaaS系统中所做的那样。对应的解除 SSD 则使用： void DeallocVirtualSSD(vssd t vSSD). channel 、die 和 plane 用于提供不同级别的性能隔离。这为多租户场景带来了显著的性能优势，因为它们可以相互独立运作。隔离级别越高，资源分配的粒度越大。因此，与 dies 粒度的分配相比，channel 粒度的分配具有更多的内部碎片。然而，由于几个原因，FlashBlox的设计不太关心这一点： 第一：一个典型的数据中心服务器可以容纳八个NVMe ssd，因此对应的支持的最大数量的 channel 隔离 和 die 隔离的数量分别为 128 和 1024（假设使用的是 16 通道 SSD），此外，根据我们与微软服务提供商的对话，拥有32个频道的ssd将使vssd的数量增加一倍，这应该足够了。 第二，DaaS系统提供的差异化存储允许租户从一定数量的性能和容量类中进行选择，这允许云提供商降低复杂性。在这样的应用程序中，动态更改容量和IOPS的灵活性是通过更改专用于应用程序的分区数量来实现的。FlashBlox的批量通道/die分配的设计与这种模型非常吻合。 第三，差异化的隔离级别与现有的云存储平台成本模型相匹配，在云存储平台中，更好的服务需要更高的定价，对于 FlashBlox 来说，这是一个自然的选择，因为在FlashBlox中，channe 比die更昂贵，性能也更好 channel 隔离的 vSSD 拥有16个通道的SSD的资源管理器可以承载最多16个与通道隔离的vSSD，每个vSSD包含一个或多个其他vSSD无法访问的通道 channel 的分配 吞吐量水平和目标容量决定分配给通道隔离的vSSD的通道数量。FlashBlox 允许数据中心/DaaS管理员实现 size t tputToChannel(int tputLevel) 函数，该函数映射吞吐量级别和所需的通道数量。因此，分配给 vSSD 的信道数是 tputToChannel(tputLevel) 的最大值，capacity / capacityPerChannel 向上取整。 在vSSD中，系统跨其分配的通道对数据进行条带处理，这与传统ssd类似，这通过在通道上并行操作来最大化峰值吞吐量。因此，图4中vSSD A的超级块大小是vSSD B的一半，超级块中的页面也跨通道条带化，类似于现有的物理ssd。通过硬件并行，通道之间的硬件级隔离允许一个vSSD上的读、编程和擦除操作在很大程度上不受其他vSSD上的操作的影响，这样的隔离使对延迟敏感的应用程序能够显著减少它们的尾延迟。 与跨所有通道从所有应用程序提取数据的SSD相比，vSSD(在较少的通道上)提供了SSD的部分全通道带宽。DaaS系统的客户通常按照固定的带宽/IOPS级别进行分配和收费，软件速率限制器积极地控制他们的消耗，因此，如果不为每个vSSD提供峰值带宽功能，就不会失去任何机会。 负载均衡的挑战 通道隔离的一个重要副作用是SSD中通道的不均匀老化风险，因为不同的vssd可能以不同的速率写入，图5显示了不同的存储工作负载如何以不同的速率擦除块，这表明如果不进行检查，被简单地固定到vssd上的通道将以不同的速率老化。这种不均匀的老化可能会在其他通道失效之前很久就耗尽一个通道的寿命，即使是单一信道的过早死亡也会造成严重的容量损失(&gt; 6%在我们的SSD中)，单个 channel 的过早死亡将导致一个机会的丧失，即永远无法为服务器生命周期的其余部分创建一个跨越所有16个通道的vSSD。考虑到服务器中的其他组件(如CPU、网络和内存)不会过早地丧失功能，服务器能力的这种不平衡意味着机会成本的损失，此外，功能中不可预测的变化也使负载平衡器的工作复杂化，负载平衡器通常采用统一的或可预测的非统一的(根据设计)功能，因此，有必要确保所有的通道以相同的速度老化。 通道内的磨损均衡 为了确保所有通道的均匀老化，FlashBlox使用了一个简单而有效的损耗均衡方案：到目前为止，已经引起最大磨损的通道定期与磨损率最小的通道交换。通道的磨损率是自最后一次交换通道以来擦除块的平均速率。这可以防止最老的通道出现高磨损率，从而直观地延长它们的寿命，以匹配系统中其他通道的寿命。 我们对来自Microsoft数据中心工作负载跟踪的实验表明，这种方法在实践中效果良好。通过这种机制，我们可以确保几乎完美的磨损平衡，并且每隔几周进行一次更换，此外，在15分钟的迁移期间，对尾部延迟的影响仍然很低，我们在§3.1.4中分析推导了最小必要频率，并在§3.1.5中给出了迁移机制的设计。 交换频率分析 设 si 为第 i 通道的磨损量(到日期为止所有块的擦除总数)。x= smax/savg 表示磨损不平衡不能超过1 + d; smax = Max (s1,:::; sN) savg = Avg (s1,:::; sN), N为通道总数，d为不平衡度。 当设备是新的，显然不可能确保x≤1 + d，因为没有交换通道。另一方面，必须在服务器生命周期的早期将其限制在一定范围内(通常为150-250周)，以便所有通道在服务器的生命周期内尽可能多地可用。ssd配备了一个目标擦除工作负载，我们对相同的工作负载进行分析：假设每周擦除M个。我们用数学方法研究了磨损不平衡与迁移频率(f)之间的权衡关系。表明 f 的可管理值可以提供可接受的磨损不平衡，当 aL 周后x低于 1+d 时，a 在0到1之间。 FlashBlox最坏的情况是所有的写操作都转移到一个通道，单个通道的带宽可以处理全部分配的带宽的假设对于现代ssd是有效的：大多数ssd配备了每个cell 3000 - 10000个擦除以维持使用150 - 250周。因此，为1TB SSD准备的擦除速率为M=21 - 116 MBPS，这低于通道擦除带宽(通常为64 - 128MBPS) 对于具有N通道的SSD，理想磨损水平的磨损不平衡为x = 1，而FlashBlox最坏情况下的工作负载为x = N：smax / savg = M∗time/(M∗time/N =N 在任何交换之前。分析了通过N个通道循环写工作负载的简单交换策略(每个通道的写工作负载花费1/ f周)。我们假设在经过K轮循环之后，KN / f≥aL成立 也就是说，已经过去了1周，x小于1 + d，并且继续保持不变，此时x = 1。因此，smax = MK, savg = 然后在下一次交换之后，smax = MK +M和savg = MK + M/N。为了保证不平衡总是有限的，我们需要: x = smax/savg = (MK +M)/(MK +M/N) ≤ (1 + d )。这意味着 K ≥ (N−1−d)/(Nd)，其上界为1/d。因此，要保证 x≤ (1+d)，在 aL 个星期内，交换NK = N/d次就足够了。这意味着，在五年的时间里，如果a是0.9，那么必须每12天(= 1/ f)交换一次 d = 0.1 (N = 16)。表2显示了交换的频率如何随着通道的数量增加而增加(显示为减少的时间段)。但是，对于实际的工作负载，它们没有固定带宽的倾斜写模式，必须根据工作负载模式(见表5)自适应地执行交换，以减少交换的数量，同时保持平衡的磨损 自适应迁移机制 出于分析的目的，我们假设M的写速率是恒定的，但实际上，写的速度非常快。高写率必须触发频繁的交换，而在低写率期间可能不需要频繁地交换，为了实现这一点，FlashBlox为每个频道维护一个计数器，表示自上次交换以来每个通道中擦除的空间量(MB)。一旦其中一个计数器超过某个阈值g，就会执行交换，指针相应清零。如果通道在两次交换之间发生了最坏情况下的写工作负载，则将g设置为擦除的空间量。(M/ f) 这种机制背后的基本原理是，channel 必须始终以一种能够在最坏情况下赶上的方式进行定位。然后FlashBlox与smax和lmin交换通道，其中li表示第i个通道的磨损率，lmin = Min(l1;:::;lN)。FlashBlox使用原子块交换机制逐步将候选通道迁移到它们的新位置，而不涉及任何应用程序，该机制为每个vSSD使用一个擦除块粒度映射表(在§3.4中进行了描述)，以一致和持久的方式进行维护。 迁移分为四个步骤： 首先，FlashBlox将停止并对与正在交换的两个擦除块相关的所有正在进行的读、编程和擦除操作进行排队。 其次，将擦除块读入内存缓冲区。 第三，擦除块被写入到它们的新位置。 第四，停止的操作将被退出队列继续执行。 **注意，只有针对vSSD中的交换擦除块的IO操作才会排队并延迟。**对于其他块的IO请求仍然以更高的优先级发出，以减少迁移开销。 迁移会影响所涉及的vssd的吞吐量和延迟。但是，这种情况很少见(对于实际工作负载来说，一个月发生一次以下)，而且只会发生15分钟就结束了。作为未来的优化，我们希望修改DaaS系统，以便在其他副本上执行读操作，以进一步减少影响，对于仅在主副本上执行读操作的系统，可以在副本集中执行迁移，以便当前正在进行vSSD迁移的副本(如果可能的话)首先转换为备份。这样的优化可以减少迁移对复制的应用程序中读操作的影响。 DIE 隔离的 vSSD 适用于能承受一定干扰的应用(即。，中等隔离)，如免费的云数据库产品。vSSD 中的 dies 的数量的最大值为 tputToDie(tputLevel) （由管理员定义)和 capacity / capacityPerDie。它们的超级块和页面横跨vSSD中的所有模块，以最大化吞吐量。然而，这些vssd具有较弱的隔离保证，因为通道内的终端必须共享总线 因为是基于 DIE 隔离的，所以磨损均衡机制也必须基于 DIE 进行设计优化。因此，我们将FlashBlox中的磨损调平机制拆分为两个子机制:通道级和die级。通道级磨损平衡机制的工作是确保所有通道以大致相同的速度老化；DIE级磨损平衡机构的工作是确保通道内的所有 DIE 以大致相同的速度老化。 如§3.1.4所示，N通道SSD必须至少交换N/d次，以保证在目标时间段内x≤(1 + d)，该数学分析同样适用于 DIE，对于今天的ssd，每个通道有4个die, FlashBlox必须在每个通道中交换die 在最坏的情况下，在SSD的整个生命周期中有40次，或者每个月一次。 作为一种优化，我们利用channel 级迁移，根据 dies 必须随着 channel 级迁移的事实，来是实现die 级磨损平衡的目标。在每个通道级别的迁移过程中，磨损最大的迁移通道中的 dir 与相应通道中写速率最低的 die 进行交换。实际工作负载的实验表明，这种简单的优化可以有效地为ssd提供满意的生存期。 软件隔离的 SSD 对于隔离要求更低的应用程序，比如Azure的基本数据库服务，使用 plane 级别的隔离的可能性也就出现了。但是 plane 相比于 channel 和 die 缺少足够的灵活性：每个 die 允许一次操作一个 plane 或在同一地址偏移处操作所有 plane。因此，我们使用一种方法，其中所有的 planes 同时操作，但他们的 带宽/IOPS 是使用软件来进行隔离。 默认情况下，每个die被划分为四个大小相等的区域，称为软平面。在FlashBlox中，每个软平面的大小为4gb(也支持其他配置)，plane 本来是 DIE 中的物理结构，而软平面只是通过在 DIE 中所有 planes 上对数据进行条带化得到的，所以每一个 DIE 中的软平面都可以获得一个 DIE 中块总数的相等份额。他们也得到来自 DIE 的公平的带宽份额，这样做的原因是为了让数据中心/PaaS管理员更容易地将租户所需的吞吐量水平映射为量化的软平面数量。 使用软平面创建的vssd与使用软件速率限制器在多个承租者之间分割SSD的传统虚拟SSD是不可区分的。与这些设置类似，我们使用最先进的令牌桶速率限制器，它已被广泛用于Linux容器和Docker，同时提高了隔离性和利用率。我们的实际实施类似于之前工作中的加权公平分配机制，此外，使用单独的队列将请求排队到每个die。 用于创建这些vssd的软平面的数量与前面的情况类似：tputToSoftPlane(tputLevel) 和 capacity / capacityPerSoftPlane。图4说明了vssd E和F，它们分别包含三个软平面。这种vSSD使用的超级块只是在vSSD使用的所有软平面上进行条带化，我们使用这些vssd作为比较通道和die隔离vssd的基线。 该软件机制允许对每个vSSD的flash块进行隔离裁剪，从而减少GC干扰。但是，它不能解决一个软平面上的擦除操作偶尔会阻塞共享die上其他软平面上的所有操作的情况。因此，这种vssd只能提供比 die 级隔离更低的软件隔离。 除了这些隔离的vSSD, FlashBlox还支持一个类似于软件隔离vSSD的非隔离vSSD模型。但是，并没有使用公平的共享机制来隔离这些vssd。为了保证当今云平台上的vssd之间的公平性，FlashBlox中默认启用了软件隔离的vssd，以满足较低的隔离要求。 对于软件隔离和非隔离的vssd，它们的磨损平衡策略保持不变，而不是交换软平面。这样做的理由是，模具的软平面之间的隔离是通过软件提供的，而不是通过将vssd固定到物理闪存plant上。因此，更传统的磨平机制是简单地在 DIE 的软平面之间旋转块，这就足以保证 DIR 内的软平面以大致相同的速度老化。 Intra Channel/Die Wear-Leveling 目标：相同速率老化。同时，通过避免多个间接层和跨这些层的冗余功能的缺陷，使应用程序能够有效地访问数据。 归功于 DIE 级别的磨损均衡机制和 DIE 内部的磨损均衡机制，FlashBlox也不可避免地实现了通道内部磨损均衡的目标：所有的 DIE 在每个通道和所有的块在每个 DIE 年龄均匀。 我们利用flash友好的应用程序或文件系统逻辑来执行GC和压缩，并简化设备级映射。我们还利用了驱动器的功能来管理坏块，而不必给应用程序带来纠错、检测和清除的负担， 应用程序/文件系统级别的日志 API 在设计时考虑了日志结构的系统，它所施加的惟一限制是，应用程序或文件系统执行日志压缩的粒度与底层vSSD的擦除粒度相同。当基于FlashBlox的日志结构应用程序或文件系统需要清除包含活动对象(比如O)的擦除块时： (1)首先通过AllocBlock分配一个新块; (2)通过ReadData读取O对象 (3)通过WriteData将对象O写入新块; (4)修改索引以反映对象O的新位置 (5)通过FreeBlock释放旧块。 注意，新分配的块仍然有许多可以写入的页面，这些页面可以用作日志的头部，用于从其他已清除的块写入活动数据或写入新数据。FlashBlox并不假设日志结构的系统以相同的速度释放所有分配的擦除块，这样的限制将迫使系统实现顺序日志清理器/压缩器，而不是考虑其他方面(如垃圾收集效率)的技术。相反，FlashBlox确保了较低级别的擦除块的均匀磨损。 设备级的映射 该层的主要目标：(1)一个die内所有的eraseblocks被按相同的比例擦除；(2)即将发生故障的擦除块将其数据迁移到另一个擦除块，而擦除块将对应用程序永久隐藏。两者都不需要更改应用程序 通过设备级映射，物理擦除块的地址不会暴露给应用程序，只有逻辑擦除块地址才会暴露给上层软件。也就是说，设备将每个die公开为使用块粒度FTL的单个SSD，而对于应用程序级别的日志，FlashBlox确保上层只发出块级别的分配和解除分配调用。间接开销很小，因为它们保持在擦除块粒度(每TB SSD需要8MB)。 与传统的ssd不同，在FlashBox中，租户不能共享预先擦除的块。虽然这样做的好处是租户可以控制自己的写放大因子、写和GC性能，但缺点是租户中的频繁写操作不能投机地从整个设备中使用预先擦除的块。在FlashBlox中，每个die都有自己的私有块粒度映射表，以及一个IO队列，该队列的默认深度为256(可配置)，以支持基本的存储操作和软件隔离的vssd的软件速率限制器。每个块中的带外元数据(使用16个字节)用于记录物理擦除块的逻辑地址;这支持原子的、一致的和持久的操作。逻辑地址是一个惟一的全局8字节数，由die ID和die中的块ID组成。元数据的其他8个字节用于2字节擦除计数器和6字节擦除时间戳。FlashBlox在主机内存中缓存映射表和所有其他带外元数据。当系统崩溃时，FlashBlox利用反向映射和时间戳来恢复映射表 设备级映射层既可以在主机上实现，也可以在设备的固件本身中实现(如果设备的控制器没有充分利用资源)，我们的方案中选了在 host 端实现。在FlashBlox中，错误检测、校正和掩蔽，以及其他低级别的flash管理系统仍然没有修改。 应用程序/文件系统级别的日志和设备级别的映射都需要过度提供，但原因不同。为了提高垃圾收集效率，日志需要过度配置。在这里，我们依赖于日志结构、flashaware应用程序和文件系统中的现有逻辑来执行它们自己的过度配置，以适应它们的工作负载。为了消除容易出错的擦除块，设备级映射需要自己的过度配置。在我们的实现中，我们根据之前工作中的错误率分析将其设置为1%。 实现细节 原型SSD 使用 CNEX SSD（它是一个开放通道的SSD，包含1 TB东芝A19闪存和一个开放控制器，允许从主机访问物理资源。）这个硬件提供了基本的I/O控制命令来对闪存执行读、写和擦除操作。我们使用CNEX固件/驱动程序堆栈的修改版本，它允许我们独立地将请求排队到每个die 原型应用程序和文件系统 我们能够修改LevelDB键值存储和ShoreMT数据库引擎，分别使用38和22个LoC修改来使用FlashBlox，同时使用了前文表三所列举的 API。此外，我们实现了一个基于FUSE的用户空间日志结构文件系统(vLFS)，它有1,809个LoC(只有26个LoC来自FlashBlox API)，用于不能修改的应用程序 资源分配 对于创建vSSD的每个调用，资源管理器对所有可用的通道、DIES 和 Soft Plane 进行线性搜索，以满足需求，相应地为此维护了一个空闲列表，在释放资源期间，资源管理器获取释放的通道、DIES 和 Soft Plane，并在可能的情况下合并它们。例如，如果通道的所有四个 DIES 都是空闲的，则资源管理器将这些 DIES 合并到一个通道中，并将该通道添加到空闲通道集。 测试 实验表明 ： FlashBlox 的开销和 FTL 的基本相当（CPU 和写放大系数）。 利用flash并行可以实现不同级别的硬件隔离，并且它们的性能优于软件隔离 硬件隔离允许对延迟敏感的应用程序(如web搜索)与带宽密集型工作负载(如MapReduce作业)有效共享SSD 负载均衡造成的迁移对数据中心应用程序性能的影响较低 FlashBlox的磨损水平接近理想 测试环境 工具： FIO Benchmark 负载：NoSQL YCSB A-F；DataBase ；DataCenter 测试运行的应用：基于 FlashBlox 修改的 LevelDB（KV） 和 Short-MT (Database) 参数：wear-imbalance : 1.1 Microbenchmarks 同 CNEX SSD 对照，测试写放大系数。2 vSSDS。FlashBlox 的写放大系数更小，因为 FlashBlox 基本上不会共享实际的物理闪存块。FlashBlox不同类型的vssd具有相似的WAFs，因为它们都使用单独的块，但是由于更高的隔离级别，它们提供了不同的吞吐量和尾延迟级别 此外，与未修改的CNEX相比，FlashBlox的系统CPU总使用率最高可达6%。尽管通过使用FlashBlox api将文件系统的索引与FTL的索引合并，相比现有的openchannel 降低了延迟，但由于设备映射层会在每一个关键路径上都被访问到，引入了额外的 CPU 开销。 隔离级别 VS 尾延迟 记录最低线程数所获得的最大吞吐量，对于相同数量的线程，记录事务的平均延迟和尾部延迟。 硬件隔离 vs 软件隔离 2 个 LevelDB 实例，运行在两个 vSSD 上，使用三种配置，对应不同的隔离级别。1 channell, 4 dies, 16 soft-planes , 跑 YCSB 负载 32 GB，KV 1KB，50 million CRUD。选择数据库的大小和操作的数量，以便总是触发GC。 channel 隔离，2 vSSDs 分配到 2 channels，die 隔离， vSSDs 共享 channel，channel 中的 dies 级别隔离。soft-plane 所有 vSSD 分片横跨所有 dies。 吞吐量：channel 级别比 dies 约好 1.3x，比 software 好 1.6x 延迟：channel 级别比 dies 约好 1.7x，比 software 好 2.6x 4 个 LevelDB 实例 延迟敏感型和带宽敏感型的应用之间的影响 此处采用了 WebSearch 和 MapReduce 共享 SSD 进行了测试。主要为了证明硬件隔离提供的优点。和软件隔离、未隔离之间进行对比。 实验表明： 硬件隔离在带宽和延迟之间取得了最佳的折中。与未隔离的vSSD相比，MapReduce作业的带宽减少了36%，web搜索工作负载的尾部延迟减少了2倍以上。MapReduce的吞吐量下降是可以预料的，因为它只有非隔离情况下的一半，在这种情况下，由于缺乏任何隔离技术，它的大型sequentialIOs最终会不公平地消耗带宽 用于web搜索和MapReduce的软件隔离的vSSD可以将web搜索的尾部延迟降低到与通道隔离的情况相同的水平，但是与未隔离的vSSD相比，MapReduce作业的带宽减少了4倍多。这也是可以预见的，因为SSD可以执行的工作是IOPS和带宽的一个组合。当与MapReduce共享带宽时，Web搜索需要大量的小IOPS，这反过来又减少了MapReduce可用的总带宽 磨损均衡效率和开销测试 FlashBlox 主要有两个层面的磨损均衡。所有DIE均匀老化，以及DIE中的所有块均匀老化。 迁移开销 图12，LevelDB 迁移，迁移总共64GB通道中的1GB数据，单线程迁移，78.9MBPS，64GB全部迁移需要大概15分钟 图13，WebSearch 和 MapReduce 迁移 迁移频率分析测试 仿真实验，使用 vLFS 文件系统，基于 FlashBlox API，测试了一些极端情况下的负载，针对单个通道进行压力测试，其他通道用于读操作。图14显示了各种损耗均衡方案的SSD寿命。 如果不进行平均磨损（NoSwap 图14），SSD会在不到4个月的时间内死亡，而FlashBlox始终可以在通道和杀手级工作负载的迁移频率（≤4周一次）中保证95％的理想寿命。 FlashBlox中的自适应损耗均衡方案通过调整写入速率来自动迁移通道。 使用混合负载测试了相应的不平衡系数。 图16显示了通道的绝对擦除计数(包括迁移和gc所需的擦除)。与理想的损耗平衡相比，一周一次的交换频率基本就能达到理想效果。 相关工作 开放架构的 SSD，但缺少云计算领域多租户之间的性能影响的研究。 SSD 层面的优化，大量研究致力于优化 FTL，本篇文章进行了扩展，在现有的优化基础上，将通道进行隔离，并使用新的负载均衡策略，内部闪存颗粒的管理沿用前人的有优化方案。 SSD 接口，主要用于虚拟化场景下和应用程序之间的对接，FlashBlox 沿用了研究成果并设计了 FlashBlox API。 存储隔离 结论 利用SSD中存在的通道和DIE级并行来为共享SSD的延迟敏感应用程序提供隔离。此外，FlashBlox提供了近乎理想的生存期，尽管各个应用程序以不同的速度写入各自的通道和DIES。FlashBlox通过在粗糙时间粒度的通道和DIES之间迁移应用程序来实现这一点。 未来希望研究如何与虚拟硬盘驱动程序集成，以便虚拟机可以利用flashblox而无需修改。希望了解FlashBlox如何与多资源数据中心调度器集成，以帮助应用程序获得可预测的端到端性能 ","link":"https://blog.shunzi.tech/post/flashblox-achieving-both-performance-isolation-and-uniform-lifetime-for-virtualized-ssds/"},{"title":"Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems","content":" FAST 2020 Controlled Data Migration in the Expansion of Decentralized Object-Based Storage Systems 主要针对集群扩容/缩容场景的数据迁移过程进行优化 基于Ceph CRUSH进行了修改，减少了增扩容过程中的数据迁移。 Abstract 数据布局策略对于去中心化的对象存储系统的可扩展性的保证是至关重要的。CRUSH 作为最先进的数据布局算法，不依赖中心化的目录来将对象副本放置在对应的存储设备中。在受益于去中心化带来的扩展性和健壮性的好处时，CRUSH 算法在集群扩容时容易产生不受控制的数据迁移，特别是集群扩容较大规模的时候，系统性能降级现象将尤为严重。 该方案提出了 MAPX，基于CRUSH算法的一个扩展实现，使用了一个额外的时间维度的映射策略（对象创建时间到集群扩容时间）来保证集群扩容过程中的数据迁移可控。每一次扩容被看成 CRUSH MAP 中新的一层，被表示成为 CRUSH 根节点下的一个虚拟节点，MAPX 通过操纵 PGs 的时间戳来控制对象到层级 layer 的映射。MAPX 适用于各种各样的基于对象的存储场景，可以将对象的时间戳维护成更高级别的元数据。例如，我们将 MAPX 用于 Ceph-RBD，通过扩展 RBD 元数据来维护和检索在扩展层粒度上的对象创建时间。实验结果表明基于 MAPX 的无迁移系统比基于 CRUSH 的系统（扩展之后忙于数据迁移操作）的尾延迟降低了 4.25 倍。 Introduction 对象存储系统应用广泛，分布式文件存储，远程块存储，小对象（图片）存储、二进制大对象（视频）存储等，相比于传统文件系统，对象存储系统通过简化了数据布局策略，直接通过唯一标识符暴露对象的读写接口，同时也减小了后端管理的复杂度。 以 CRUSH 为代表的去中心化数据放置策略，通常比中心化的方法要好是因为客户端可以直接通过计算找到需要放置该数据的 OSD 节点。CRUSH 是一种先进的放置算法，它允许从对象到层次集群映射的结构化，层次集群由表示osd、机器、机架等的节点组成。CRUSH 被广泛采用，如 Ceph, Ursa。但 CRUSH 算法最大的问题在于集群扩容或者添加中间 PGs 时会造成不受控制的数据迁移，虽然迁移可以在扩展之后立即重新平衡整个系统的负载，但是在扩展规模较大时会导致显著的性能下降。（如以机架的规模进行扩容） 实际部署中都尽可能地在扩容后避免数据迁移，即便是会以暂时的负载不均衡为代价。Ceph 为了减小 CRUSH 的影响，在实现上进行了优化，将迁移的比例尽可能限制到较小，如果被写的对象正在等待迁移的话，写操作将在原来的 OSD 上执行，但是所有对象的副本最终都将迁移到经过 CRUSH 计算的目标节点上去，就会造成长时间的性能降级。 相反，传统的中心化的布局方法就能较好地控制集群扩容过程中的数据迁移。Haystack 和 HDFS 都使用了中心化的目录来记录对象的位置，来保证现有对象在扩容过程中不受影响，只把新的对象数据放置在新添加的 OSD 节点上。 该论文提出的 MAPX 适用于各种各样的基于对象存储的存储场景，无论是块设备（类似于Ceph-RBD）还是文件系统（CephFS），该方案将对象创建时间戳维护到上层的元数据中。RBD 中利用 rbd_header 的元数据数据结构来在扩展层的粒度上维护检索对象创建时间；CephFS 则是扩展了 inode 元数据结构，来将文件的创建时间作为文件对象的创建时间。更复杂的应用则可以基于 RBD 和 CephFS 进行构建。 Background CRUSH CRUSH 逻辑上使用了一个 ClusterMap 的结构来抽象表示集群的分层信息。集群信息如图所示有三种结构，root 节点代表一整个集群，集群由多个 cabinets 组成，也就是所谓的机架 rack，而 cabinets 又由 shelves 组成，即所谓的物理机。每一个物理机又安装了许多 OSD 节点，对应磁盘。该层次中的内部节点 root/cabinets/shelves 统称为 bucket。 每个OSD都有一个由管理员分配的权重，用于控制OSD的相对存储数据量，所以每一个 OSD 的负载是按照权重来进行分配的，而 bucket 的权重则是该节点对应的子节点的权重之和。如图所示，节点中数字表示节点对应的权重。 CRUSH Steps Step 1. 通过计算对象名称哈希的模，将对象分为若干个 PGs（类似于简单的 HASH 操作） pgid = HASH(name) % PG_NUM Step 2. 单个 PG 的对象根据 CRUSH 算法映射到一组 OSDs（根据副本数量） PG -&gt; OSDs CRUSH 算法支持灵活的配置，可以把故障域的信息编码到 ClusterMap 中，也可以定制 Placement Rule 来明确如何通过递归选择bucket项来放置副本。通常的 Placement Rule 又大致分为四步： 1. take(root) 选择存储层级结构的根节点，由于是唯一入口，可以保证操作顺序执行。 2. select(3, cabinet) 找到三个满足故障域的 rack，3 为副本数量。 重复计算如下等式，根据图中的例子，则是从 四个机架中选出三个。i 指定对应四个节点中的哪一个节点，其中 pgid 为 PG 的编号，r 为argmax 计算的参数，HASH 是有三个输入参数的 HASH 函数，ID(i) 和 W(i) 则是节点 i 对应的 ID 和权重。 为了选择 x 个节点出来，通常该等式的执行次数大于 x 次，因为可能存在计算出来的节点已经被选择过了或者计算出的节点已经失效了。 3. select(1, shelf) 从 cabinet 中找到一个 shelf。机架中选择一个 物理机 同样执行步骤二中的等式，只是 x = 1，后续步骤一致。 4. select(1, osd) 从 shelf 中找到一个 osd。从物理机中选择一个 osd。 如上图所示，一个物理机对应了四个 OSD，每个 OSD 代入自己的 id 和 pgid，crush_hash(osd id, pgid) 计算出一个随机值 r（其中crush_hash可以简单地看成是一个伪随机的hash函数：返回一个固定范围内的随机值。同样的输入下其返回值是确定的，但是任何一个参数的改变都会导致其返回值发生变化），每一个 osd 对应一个 r 值。用公式 ri * wi 计算得到 osd 的 straw 值，然后选取 straw 值最大的 osd 放置 PG。 参考链接：知乎 - CRUSH算法的原理与实现 CRUSH 缺点 CRUSH 实现了不依赖中心化目录的统计意义上的负载均衡，并且可以在存储集群发生变化的时候自动调整对应的负载。但是扩容过程中可能导致不受控制的的数据迁移。如图所示，在机架4上新加一个物理机 4-3，同样带四个 OSD 节点，此时将影响对应的父节点的权重，直到根节点为止，所以将可能导致该机架上的其他物理机的数据迁移到 4-3 上，同时也可能导致其他机架的数据迁移到机架 4 上，h * ▲w/W。h 为改分层结构的层数， ▲w 为扩容造成的权重的变化，W 为所有OSD节点的权重和。 实验测试，两个 Ceph 集群，均为三层结构，三副本，一个机架作为故障域，一个机架由八个 host 组成，每一个 host 十个 OSD 节点。集群 1 对应 3 个机架，24 个物理机，240 个 OSDs，24000 PGs，集群 2 有 125 个机架，1000 个物理机，10000 个 OSDs，100w PGs。两个集群都执行扩容操作，粒度分别为 一个 OSD，一个物理机，一个机架。结果如图所。如果扩容规模较大时，可能会有接近 60% 的 PG 将会被影响，在整个迁移期间不可避免地会导致性能降级。 MAPX 之所以 CRUSH 算法扩容过程中会导致大量的数据迁移，是因为该算法破坏了 新旧对象/OSD 之间的差异。为了解决这个问题，MAPX 延伸了通用的 CRUSH 实现，引入了额外的时间映射机制。 可控数据迁移的扩容 以 Haystack and HDFS 为代表的中心化目录管理的存储系统为了避免数据的迁移，往往不惜以短暂的负载不均衡为代价。随着新对象存储到新的 OSD 中，可用的容量对应随着时间下降，因此整个系统将渐渐实现负载均衡。数据迁移可以根据需求来决定何时执行。 受中心化数据布局策略的启发，致力于实现扩容过程中数据迁移的可控，所以基于 CRUSH 设计实现了 MAPX，核心思想就是引入时间维度的映射机制来区分 新老 对象/OSD ，同时保留 CRUSH 算法随机和均匀的优点。 核心思想 集群变化 如图 a 描述了两种扩容情况，第一种红色框表示的给每一个机架添加一个物理机，第二种蓝色框表示的给集群添加 m 个机架。此时不再像通用的 CRUSH 那样单独更新 ClusterMap，MAPX 将每一次扩容和原本的集群当作一个单独的层，其中不仅包含新的叶子osd，而且包含从叶子osd到根的所有内部 bucket。 实现思路 为了尽可能少地修改原有的 CRUSH 算法，在原本的 CRUSH 根节点下插入一个虚拟层，如图 b 所示，每一个虚拟节点代表一次扩容。虚拟层对应的通过使用 MAPX 实现可控数据迁移，通过在执行原本的 CRUSH 算法之前将新的对象映射到新的 layer，因为新的 layer 不会影响原有 layer 的权重，原有对象的放置还是和以前一样，不会发生改变。 算法流程 Mapping objects to PGs Mapping PGs to OSDs Mapping objects to PGs 每一次扩容，新的 layer 将被分配指定数量的新创建的 PGs，且每一个 PGs 会带上一个时间戳信息 t_pg，该时间戳信息等于该新 layer 的产生时间，也就是扩容时间 t_layer。 当读/写一个对象 O 的时候，该对象携带创建时间信息 t0，先计算该对象对应的 pgid。其中 name 为 object name INIT PG NUM[i] 是第 i layer的初始 PG 数量 第 j layer 有最近的时间戳 tl &lt; t0，即最新的 layer 尽管 PG 可能会因为负载均衡被映射到其他 layer，但 INIT PG NUM 是常量，因此从对象到 PGs 的映射是不会变化的。所以每个对象在创建时 都会被映射到一个确定的 PG，该 PG 会有所有 PGs 中距离 t0 最近的时间戳 t_pg &lt; t0。 算法简单描述：新的对对象的 IO 请求，会被简单 HASH 到新建的 layer 对应的 PG 上。（该 PG 的计算则是通过 HASH 值对该层 PG 数量的简单取模 + 前面所有 layer 对应的 PGs 总数偏移） 如图所示，假设有三个 RBD，分别在layer0，layer1，layer2的扩容后创建，RBD1，RBD2，RBD3 将使用三层的 INIT PG NUM 分别计算他们在 layer0，layer1，layer2 的 PGs。 Mapping PGs to OSDs MAPX 类似于 CRUSH，会把 PG 映射到一组 OSDs，该过程按照用户定义的 Placement Rule 中定义的顺序去执行，如图所示，MAPX 暗中增加了一个操作 select(1, layer)，是为了实现时间维度的从 PGs 到 layers 的映射，而不需要管理节点的参与。 MAPX 延伸了 CRUSH 算法中 select 操作的实现，以支持 layer 类型的 select 操作。算法如下所示，如果不是 layer 类型，仍按照 CRUSH 处理，否则将初始化一个 layer 数组用于存储所有位于正在处理的 bucket （通常为 root）下的 layers，并按照 layer 的时间戳排序。同时也会初始化 layer_num 来表示 layers 的数量以及 pg 和 output list （select 出来的结果）。循环则是将 layers 中指定个数的 layers 添加到输出列表中。大多数场景下，number 均为 1，即只需要选出一个 layer 即可，但也不排除需要更多的 layers 的场景，例如需要选两个 layers，其中一个作为另一个的镜像。 需要注意的是，对象的副本没必要放在最新的 layer 上。例如，假设新建的 layer2 新增了两个机架，但是后一步的操作需要选出三个机架，根据 CRUSH 回溯的机制，将造成 (select(1, layer)) 被调用两次才能满足规则：当一个 select 操作在 layer 之下不能选出足够的节点，MAPX 将保留已经选中的节点，然后回溯到根节点，再去前一个 layer 中选择缺少的节点，从而避免回溯造成的一个 layer 被多次选中。双重保证使得在这类情况也能得到处理，前面提到的例子中则将先返回 layer2 再返回 layer1。 数据迁移控制管理 MAPX 能保证每一层的负载均衡，主要是因为通用 CRUSH 算法的随机性和均匀性，随着时间的迁移，新的 layer 中的数据增多到和前一个 layer 时则实现了 layers 层面的负载均衡。但是一个 layer 的负载可能会因为 对象的删除、OSD 的宕机发生一些不可预测的负载变化。如图所示，当 layer1 中的负载跟原始集群 layer0 的负载一样高时，则可能会执行一次扩容产生 layer2，假设 layer1 中执行了大量的对象删除操作，则会造成不同 layers 之间的负载不均衡。 为了解决这个问题，MAPX 设计了三种灵活的策略来动态管理 MAPX 中的负载： PG 重映射：可以控制 PGs 到 Layer 的映射，来保证 layers 负载均衡 集群缩容：缩容时需要进行 PG 重映射调整负载，但也要保留部分元数据来保证映射关系不变 layers 合并：使用时间戳来保证物理层的变化在逻辑层 layer 上保持相同以实现负载均衡 PG remapping 每一个 PG 有两个时间戳，一个静态的时间戳 t_pgs，等于创建该 PG时对应 layer 的初始化的时间戳；一个动态的时间戳 t_pgd，可以被设置为任意 layer 的扩容时间。 对象到 PG 的映射使用了静态的时间戳，PGs 到 layers 的映射通过比较 PGs 的动态时间戳和 layers 的时间戳来执行。 所以通过修改动态时间戳对应的数值则可以实现将 PG 映射到指定 layer，同时也会通过内部 map 信息增量更新的方式通知所有的 OSD 节点。而对于时间戳的存储开销其实是较小的，每一个 PG 的时间戳可以使用 one byte 的索引来指向对应 layer 的初始化时间，故可以支持 2^8 = 256 个 layers，假设一个机器有 20 个 OSDs 对应 200 个 PGs，那么1000台机器组成的集群的时间戳开销则为 1000 x 20 x 200 x 2 x 1B = 8MB。 Cluster Shrinking 当一个 layer 负载低于一个设定的阈值之后，MAPX 将移除该 layer 中多余的机器或者 OSD 从集群中，相当于扩容的反向操作。 假定 layer a 将要从集群中被移除，首先会将 layer a 对应的所有 PG 分配给余下的其他 layer，并按照其他 layer 的权重进行分配（此处为了简化重分配的过程没有考虑 layer 实际的负载情况），然后使用上一节提到的重新映射来进行数据的迁移。 在移除 layer a 之后，即执行了缩容操作之后，layer a 的逻辑意义不会发生改变，特别是 INIT PG NUM（仍然保留），但是不会保留任何物理设备，逻辑意义不改变是为了保证对象到 PG 的映射的关系保持不变。 Layer merging 层级之间的合并可以巧妙利用前文提到的 PGs 到 OSDs 的映射机制，假设要合并 layer a 和 b，可以直接将 a 的扩容时间 t_layer 直接设置为 b 的扩容时间，此时在逻辑意义上 a 和 b 就是同一 layer 了。 在 CEPH 中实现 MAPX 从上图中我们可以发现内部的 bucket 在 MAPX 中可能会同时属于多个 layers（譬如只进行了 OSD 层面的扩容），因此我们将内部设备分配到一个特定的 layer，例如在一个特定虚拟节点之下，使用虚拟设备 ID 将物理设备 ID 和 layer 时间戳连接在一起，使用虚拟节点的权重对应的属性记录 layers 的时间戳，将在选择 layer 的过程中和 PG 的动态时间戳进行对比。 MAPX 不能为每一个对象都维护一个时间戳信息，因为这样的开销和中心化目录的开销就大致相同了，所以 MAPX 不适用于那种最底层最用的对象存储系统，而是适用于那些可以使用上层一点的元数据管理的对象存储系统。 Ceph RBD MAPX 使用了 rbd_header （RBD本身的元数据管理数据结构）来管理时间戳信息，当一个客户端使用 rbd_open 来挂载 RBD 时将检索 rbd_header。因为RBD的对象可以在任何扩展之后创建，所以我们继承当前 layer 的时间戳（创建该 layer 的时间）作为对象的时间戳，因此我们在 rbd_header 中为每一个对象添加了 object_timestamp 属性，该属性对应指向 layer 的创建时间戳。假设每一个对象对应的索引属性的大小为 one byte，每个对象的大小是 4MB，那么对于一个 4TB 大小的 RBD object_timestamp 数组的开销为 1MB。 CephFS CephFS 的元数据对应存储在 inode 节点中，客户端打开一个文件会去获取对应的创建时间，此时对应地会访问 inode 数据，现在我们让一个文件对应的所有对象都继承该文件创建的时间戳，这样就能控制一个文件对应对象的时间维度的映射。假设一个文件的大小达到了阈值 T，假定 T = 100MB，我们会将其分为多个小于 100 MB 的子文件，文件的元数据中会维护源文件到子文件的映射信息以及每一个子文件的创建时间，所以同样可以控制子文件对应的时间维度的映射。 测试 比较对象：MAPX 和 传统 CRUSH 硬件环境：3台主机，每台主机对应： 20 核 Xeon E5-2630 2.20GHz CPU 128G RAM 10GbE NIC 5.5 TB HDDs 软件环境： OS：CentOS 7.0 Ceph：12.2 Luminous，BlueStore，Monitor co-located with one of the storage servers Client：fio benchmark 扩容过程中的 IO 性能 参数使用 Ceph 默认参数，除开 OSD_max_back_fills 前面中提到过 Ceph 自身对 CRUSH 造成的数据迁移的优化是通过设置 OSD_max_back_fills &gt;= 1 来对性能降级过程的严重程度和持续时间来做权衡。该参数默认值为 1，将使得迁移的优先级最低，所以在迁移操作完成之前 PGs 中的对象的数据迁移速度将很慢，相应地将严重延长数据迁移的时间，同时增加节点对应的写负载：因为对 等待迁移的 PGs 的写操作，需要首先在原始 OSD 上执行，然后再异步地迁移到目标 OSD，该方式性能降级现象表现得较为轻微，但会持续很长一段时间。我们将 OSD_max_back_fills 设为 10，该参数在此次实验中更为合理，因为这样迁移操作才会有更高的优先级来显示 MAPX 和 CRUSH 之间的区别。 Ceph 初始集群，三台存储机，每台机器两个 OSDs，三副本，128 个 PGs，对应每个 OSD 会负责 64 PGs。创建 40*20GB 的块设备，我们会给每个机器添加一到两个 OSDs，然后测试 MAPX 和 CRUSH 下对应的性能表现（I/O 延迟和 IOPS），I/O 大小为 4KB，FIO 的 iodepth 分别为 1 和 128，对应延迟和 IOPS 测试 延迟测试实验结果如图所示，初始集群大小为 6 个 OSDs，然后分别扩容到 9 个和 12 个。云存储通常只关心尾延迟 99th，99.9th or 99.99th，因为尾延迟对于服务等级影响相比于其他比例的延迟最大。MAPX 相比于 CRUSH 降低了 4.25x 的延迟，主要是以内 CRUSH 中的数据迁移将和普通的 IO 请求发生严重的资源竞争。 下图显示了 IOPS 测试结果，每个结果都 run 了 20 次，忽略了部分误差，因为占比较小。结果显示 MAPX IOPS 优于 CRUSH 约 74.3% 计算开销 模拟大约有 600-19200 个 OSDs 的集群来比较 MAPX 和 CRUSH 的计算开销，CRUSH 和 MAPX 均可以在 10us 内完成对象到 OSD 的映射，MAPX 相比于 CRUSH 略高因为需要时间维度上的映射计算。 缩容过程中的 IO 性能 三台机器，每个机器三个 OSDs，先给每一台机器加一个 OSD，然后每一个机器减去一个 OSD，为了控制迁移的速度，将并发迁移PG的数量设置为 8 图示均显示了 99th 的尾延迟。图示结果并不一定表明缩容过程中 MAPX 的延迟比 CRUSH 低，因为采用了不同的调节机制，因为移除一个 OSD 对于 CRUSH 而言造成的是权重降低然后导致不必要的数据迁移，但对于 MAPX 而言因为根本没有造成之前节点的权重变化，缩容只是 layer 层面上的，根本不会发生数据的迁移。 层级之间的合并 使用了 CrushTool 来模拟 MAPX 中的层级合并。三副本策略，一个集群对应五个机架，一个机架对应20台物理机，一个机器对应20个OSDs，即100台机器，2000个OSDs，20w PGs，扩容四次，每次加一个新机架，20台物理机，400个OSDs，4w PGs，MAPX将所有新加入的 PGs 映射到 OSDs 上因此没有迁移发生，四次扩容后将第一次扩容和第二次扩容总计 40 台机器的进行了合并，然后测量受影响的 PG 数量。 结果表明对应合并的 8w 个 OSDs，其中受影响的 OSDs 约为 70910，由于 CRUSH 没有合并操作，就只是只能对扩容操作影像的 PG 数量进行了了测量。 Related Word Ceph CRUSH Ceph CRUSH 原理中 Object 到 PG 的双重映射 CRUSH 抽签算法的演变 Straw2 负载均衡 &amp; 迁移开销 Ceph 通过降低迁移的优先级来避免扩容造成的突发的数据迁移，但是由于 CRUSH 算法的特性，迁移不可避免，只是时间早晚的问题，保守的迁移控制只会延长迁移的时间，但这又导致了等待迁移的 PGs 的数据写入的复杂性，不可避免低会增加负载。 其他的一些去中心化的分布式存储系统使用了分布式哈希表的一致性哈希算法来实现数据路由。相比于CRUSH，DHT不能表示集群的层级结构，需要额外的机制来进行建模，相比于 CRUSH 不够灵活。 存储系统 去中心化对象存储系统：Ambry，F4。 中心化对象存储系统：Haystack，Lustre，HDFS 块存储系统：Ursa，Salus，Blizzard，PARIX 文件存储系统：GFS，Zebra，BPFS，OptFS Conclusion 在大规模存储系统领域关于中心化和去中心化的争论已久，去中心化的 CRUSH 表现出了高可扩展、健壮性和性能上较好，但在扩容过程中会造成不受控制的数据迁移，MAPX引入了额外的时间粒度映射机制，同时继承了 CRUSH 算法的随机性和均匀性。未来将致力于减少对象时间戳存储的开销，以便于将 MAPX 应用于更广泛的基于对象存储的存储系统。 ","link":"https://blog.shunzi.tech/post/controlled-data-migration-in-the-expansion-of-decentralized-object-based-storage-systems/"},{"title":"Ceph ObjectStore","content":" Ceph OSD 后端存储实现架构和源码分析 结合 Ceph 已经实现的存储引擎，考虑实现新的后端存储 理解 ObjectStore 和 Ceph IO 流的调用关系 Review 源码目录结构 IO 流 librados -&gt; OSDC -&gt; OSD -&gt; OS -&gt; ObjectStore PrimayLogPG 中使用 Objecter 发送 Operation 消息来执行对应的操作 PrimaryLogPG 执行构造函数时创建了相应的 PGBackend. PrimaryLogPG::PrimaryLogPG(OSDService *o, OSDMapRef curmap, const PGPool &amp;_pool, const map&lt;string,string&gt;&amp; ec_profile, spg_t p) : PG(o, curmap, _pool, p), pgbackend( PGBackend::build_pg_backend( _pool.info, ec_profile, this, coll_t(p), ch, o-&gt;store, cct)), object_contexts(o-&gt;cct, o-&gt;cct-&gt;_conf-&gt;osd_pg_object_context_cache_count), new_backfill(false), temp_seq(0), snap_trimmer_machine(this) { recovery_state.set_backend_predicates( pgbackend-&gt;get_is_readable_predicate(), pgbackend-&gt;get_is_recoverable_predicate()); snap_trimmer_machine.initiate(); } 构建 PGBackend 时会根据系统配置的后端存储类型，进行相应的实例化。Ceph 中的后端存储又主要分成 ECBackend 和 ReplicatedBackend，分别使用了纠删码和多副本来保证数据的一致性。在构造 PGBackend 时又相应地指定了 ObjectStore 的存储类型。此处以 ReplicatedBackend 为例。 读操作直接执行相关函数调用，写操作相应地使用事务进行封装提交 int ReplicatedBackend::objects_read_sync( const hobject_t &amp;hoid, uint64_t off, uint64_t len, uint32_t op_flags, bufferlist *bl) { return store-&gt;read(ch, ghobject_t(hoid), off, len, *bl, op_flags); } int ReplicatedBackend::objects_readv_sync( const hobject_t &amp;hoid, map&lt;uint64_t, uint64_t&gt;&amp;&amp; m, uint32_t op_flags, bufferlist *bl) { interval_set&lt;uint64_t&gt; im(std::move(m)); auto r = store-&gt;readv(ch, ghobject_t(hoid), im, *bl, op_flags); if (r &gt;= 0) { m = std::move(im).detach(); } return r; } void ReplicatedBackend::submit_transaction( const hobject_t &amp;soid, const object_stat_sum_t &amp;delta_stats, const eversion_t &amp;at_version, PGTransactionUPtr &amp;&amp;_t, const eversion_t &amp;trim_to, const eversion_t &amp;min_last_complete_ondisk, const vector&lt;pg_log_entry_t&gt; &amp;_log_entries, std::optional&lt;pg_hit_set_history_t&gt; &amp;hset_history, Context *on_all_commit, ceph_tid_t tid, osd_reqid_t reqid, OpRequestRef orig_op) { parent-&gt;apply_stats( soid, delta_stats); vector&lt;pg_log_entry_t&gt; log_entries(_log_entries); ObjectStore::Transaction op_t; PGTransactionUPtr t(std::move(_t)); set&lt;hobject_t&gt; added, removed; generate_transaction( t, coll, log_entries, &amp;op_t, &amp;added, &amp;removed, get_osdmap()-&gt;require_osd_release); ceph_assert(added.size() &lt;= 1); ceph_assert(removed.size() &lt;= 1); auto insert_res = in_progress_ops.insert( make_pair( tid, ceph::make_ref&lt;InProgressOp&gt;( tid, on_all_commit, orig_op, at_version) ) ); ceph_assert(insert_res.second); InProgressOp &amp;op = *insert_res.first-&gt;second; op.waiting_for_commit.insert( parent-&gt;get_acting_recovery_backfill_shards().begin(), parent-&gt;get_acting_recovery_backfill_shards().end()); issue_op( soid, at_version, tid, reqid, trim_to, min_last_complete_ondisk, added.size() ? *(added.begin()) : hobject_t(), removed.size() ? *(removed.begin()) : hobject_t(), log_entries, hset_history, &amp;op, op_t); add_temp_objs(added); clear_temp_objs(removed); parent-&gt;log_operation( log_entries, hset_history, trim_to, at_version, min_last_complete_ondisk, true, op_t); op_t.register_on_commit( parent-&gt;bless_context( new C_OSD_OnOpCommit(this, &amp;op))); vector&lt;ObjectStore::Transaction&gt; tls; tls.push_back(std::move(op_t)); parent-&gt;queue_transactions(tls, op.op); if (at_version != eversion_t()) { parent-&gt;op_applied(at_version); } } Architecture 无论是哪种后端存储，都主要是通过实现 ObjectStore 所定义的相关接口来实现 IO ObjectStore 在现阶段的 Ceph 中主要有两种实现：FileStore，BlueStore 其中 FileStore 又主要提供了三种文件系统的实现：btrfs，ext4，xfs，（zfs） 其中 BlueStore 的主要实现又包括 RocksDB 和 BlueFS ObjectStore ObjectStore.h 中定义了相关方法，具体的实现由具体的存储引擎来决定，主要涉及到的方法如下：read、readv、queue_transactions （其中我们将重点关注 read、queue_transactions） class ObjectStore { protected: std::string path; public: using Transaction = ceph::os::Transaction; CephContext* cct; /** * create - create an ObjectStore instance. * This is invoked once at initialization time. * @param type type of store. This is a std::string from the configuration file. * @param data path (or other descriptor) for data * @param journal path (or other descriptor) for journal (optional) * @param flags which filestores should check if applicable */ static ObjectStore *create(CephContext *cct, const std::string&amp; type, const std::string&amp; data, const std::string&amp; journal, osflagbits_t flags = 0); /** * probe a block device to learn the uuid of the owning OSD * @param cct cct * @param path path to device * @param fsid [out] osd uuid */ static int probe_block_device_fsid( CephContext *cct, const std::string&amp; path, uuid_d *fsid); //... // Transactions Collections. // Transactions in one collection will be applied in sequence. // Transactions in different collections will be applied in parallel. struct CollectionImpl : public RefCountedObject { const coll_t cid; virtual void flush() = 0; virtual bool flush_commit(Context *c) = 0; //... } int queue_transaction(CollectionHandle&amp; ch, Transaction&amp;&amp; t, TrackedOpRef op = TrackedOpRef(), ThreadPool::TPHandle *handle = NULL) { std::vector&lt;Transaction&gt; tls; tls.push_back(std::move(t)); return queue_transactions(ch, tls, op, handle); } virtual int queue_transactions( CollectionHandle&amp; ch, std::vector&lt;Transaction&gt;&amp; tls, TrackedOpRef op = TrackedOpRef(), ThreadPool::TPHandle *handle = NULL) = 0; /** * read_meta - read a simple configuration key out-of-band * * Read a simple key value to an unopened/mounted store. * * Trailing whitespace is stripped off. * * @param key key name * @param value pointer to value std::string * @returns 0 for success, or an error code */ virtual int read_meta(const std::string&amp; key, std::string *value); /** * read -- read a byte range of data from an object * * Note: if reading from an offset past the end of the object, we * return 0 (not, say, -EINVAL). * * @param cid collection for object * @param oid oid of object * @param offset location offset of first byte to be read * @param len number of bytes to be read * @param bl output ceph::buffer::list * @param op_flags is CEPH_OSD_OP_FLAG_* * @returns number of bytes read on success, or negative error code on failure. */ virtual int read( CollectionHandle &amp;c, const ghobject_t&amp; oid, uint64_t offset, size_t len, ceph::buffer::list&amp; bl, uint32_t op_flags = 0) = 0; /** * readv -- read specfic intervals from an object; * caller must call fiemap to fill in the extent-map first. * * Note: if reading from an offset past the end of the object, we * return 0 (not, say, -EINVAL). Also the default version of readv * reads each extent separately synchronously, which can become horribly * inefficient if the physical layout of the pushing object get massively * fragmented and hence should be overridden by any real os that * cares about the performance.. * * @param cid collection for object * @param oid oid of object * @param m intervals to be read * @param bl output ceph::buffer::list * @param op_flags is CEPH_OSD_OP_FLAG_* * @returns number of bytes read on success, or negative error code on failure. */ virtual int readv( CollectionHandle &amp;c, const ghobject_t&amp; oid, interval_set&lt;uint64_t&gt;&amp; m, ceph::buffer::list&amp; bl, uint32_t op_flags = 0) { int total = 0; for (auto p = m.begin(); p != m.end(); p++) { bufferlist t; int r = read(c, oid, p.get_start(), p.get_len(), t, op_flags); if (r &lt; 0) return r; total += r; // prune fiemap, if necessary if (p.get_len() != t.length()) { auto save = p++; if (t.length() == 0) { m.erase(save); // Remove this empty interval } else { save.set_len(t.length()); // fix interval length bl.claim_append(t); } // Remove any other follow-up intervals present too while (p != m.end()) { save = p++; m.erase(save); } break; } bl.claim_append(t); } return total; } FileStore PGBackend 端的操作同 BlueStore 一样，区别只在于 FileStore 对相关读写方法实现的区别。 FileStore 由于支持了多种文件系统作为存储后端，所以又提供了 FileStoreBackend 相关接口，便于各种文件系统提供基于该接口的实现。 写操作 写操作流程： FileStore::queue_transactions do_transactions(tls, op); _op_journal_transactions(tbl, orig_len, op, ondisk, osd_op); FileStore::_do_transaction _write(cid, oid, off, len, bl, fadvise_flags); lfn_open(cid, oid, true, &amp;fd); bl.write_fd(**fd, offset); lfn_close(fd); 写操作实现： int FileStore::queue_transactions(CollectionHandle&amp; ch, vector&lt;Transaction&gt;&amp; tls, TrackedOpRef osd_op, ThreadPool::TPHandle *handle) { Context *onreadable; Context *ondisk; Context *onreadable_sync; ObjectStore::Transaction::collect_contexts( tls, &amp;onreadable, &amp;ondisk, &amp;onreadable_sync); if (cct-&gt;_conf-&gt;objectstore_blackhole) { dout(0) &lt;&lt; __FUNC__ &lt;&lt; &quot;: objectstore_blackhole = TRUE, dropping transaction&quot; &lt;&lt; dendl; delete ondisk; ondisk = nullptr; delete onreadable; onreadable = nullptr; delete onreadable_sync; onreadable_sync = nullptr; return 0; } utime_t start = ceph_clock_now(); OpSequencer *osr = static_cast&lt;OpSequencer*&gt;(ch.get()); dout(5) &lt;&lt; __FUNC__ &lt;&lt; &quot;: osr &quot; &lt;&lt; osr &lt;&lt; &quot; &quot; &lt;&lt; *osr &lt;&lt; dendl; ZTracer::Trace trace; if (osd_op &amp;&amp; osd_op-&gt;pg_trace) { osd_op-&gt;store_trace.init(&quot;filestore op&quot;, &amp;trace_endpoint, &amp;osd_op-&gt;pg_trace); trace = osd_op-&gt;store_trace; } if (journal &amp;&amp; journal-&gt;is_writeable() &amp;&amp; !m_filestore_journal_trailing) { Op *o = build_op(tls, onreadable, onreadable_sync, osd_op); //prepare and encode transactions data out of lock bufferlist tbl; int orig_len = journal-&gt;prepare_entry(o-&gt;tls, &amp;tbl); if (handle) handle-&gt;suspend_tp_timeout(); op_queue_reserve_throttle(o); journal-&gt;reserve_throttle_and_backoff(tbl.length()); if (handle) handle-&gt;reset_tp_timeout(); uint64_t op_num = submit_manager.op_submit_start(); o-&gt;op = op_num; trace.keyval(&quot;opnum&quot;, op_num); if (m_filestore_do_dump) dump_transactions(o-&gt;tls, o-&gt;op, osr); if (m_filestore_journal_parallel) { dout(5) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (parallel) &quot; &lt;&lt; o-&gt;op &lt;&lt; &quot; &quot; &lt;&lt; o-&gt;tls &lt;&lt; dendl; trace.keyval(&quot;journal mode&quot;, &quot;parallel&quot;); trace.event(&quot;journal started&quot;); _op_journal_transactions(tbl, orig_len, o-&gt;op, ondisk, osd_op); // queue inside submit_manager op submission lock queue_op(osr, o); trace.event(&quot;op queued&quot;); } else if (m_filestore_journal_writeahead) { dout(5) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (writeahead) &quot; &lt;&lt; o-&gt;op &lt;&lt; &quot; &quot; &lt;&lt; o-&gt;tls &lt;&lt; dendl; osr-&gt;queue_journal(o); trace.keyval(&quot;journal mode&quot;, &quot;writeahead&quot;); trace.event(&quot;journal started&quot;); _op_journal_transactions(tbl, orig_len, o-&gt;op, new C_JournaledAhead(this, osr, o, ondisk), osd_op); } else { ceph_abort(); } submit_manager.op_submit_finish(op_num); utime_t end = ceph_clock_now(); logger-&gt;tinc(l_filestore_queue_transaction_latency_avg, end - start); return 0; } if (!journal) { Op *o = build_op(tls, onreadable, onreadable_sync, osd_op); dout(5) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (no journal) &quot; &lt;&lt; o &lt;&lt; &quot; &quot; &lt;&lt; tls &lt;&lt; dendl; if (handle) handle-&gt;suspend_tp_timeout(); op_queue_reserve_throttle(o); if (handle) handle-&gt;reset_tp_timeout(); uint64_t op_num = submit_manager.op_submit_start(); o-&gt;op = op_num; if (m_filestore_do_dump) dump_transactions(o-&gt;tls, o-&gt;op, osr); queue_op(osr, o); trace.keyval(&quot;opnum&quot;, op_num); trace.keyval(&quot;journal mode&quot;, &quot;none&quot;); trace.event(&quot;op queued&quot;); if (ondisk) apply_manager.add_waiter(op_num, ondisk); submit_manager.op_submit_finish(op_num); utime_t end = ceph_clock_now(); logger-&gt;tinc(l_filestore_queue_transaction_latency_avg, end - start); return 0; } ceph_assert(journal); //prepare and encode transactions data out of lock bufferlist tbl; int orig_len = -1; if (journal-&gt;is_writeable()) { orig_len = journal-&gt;prepare_entry(tls, &amp;tbl); } uint64_t op = submit_manager.op_submit_start(); dout(5) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (trailing journal) &quot; &lt;&lt; op &lt;&lt; &quot; &quot; &lt;&lt; tls &lt;&lt; dendl; if (m_filestore_do_dump) dump_transactions(tls, op, osr); trace.event(&quot;op_apply_start&quot;); trace.keyval(&quot;opnum&quot;, op); trace.keyval(&quot;journal mode&quot;, &quot;trailing&quot;); apply_manager.op_apply_start(op); trace.event(&quot;do_transactions&quot;); int r = do_transactions(tls, op); if (r &gt;= 0) { trace.event(&quot;journal started&quot;); _op_journal_transactions(tbl, orig_len, op, ondisk, osd_op); } else { delete ondisk; ondisk = nullptr; } // start on_readable finisher after we queue journal item, as on_readable callback // is allowed to delete the Transaction if (onreadable_sync) { onreadable_sync-&gt;complete(r); } apply_finishers[osr-&gt;id % m_apply_finisher_num]-&gt;queue(onreadable, r); submit_manager.op_submit_finish(op); trace.event(&quot;op_apply_finish&quot;); apply_manager.op_apply_finish(op); utime_t end = ceph_clock_now(); logger-&gt;tinc(l_filestore_queue_transaction_latency_avg, end - start); return r; } 读操作 读操作流程： FileStore::read lfn_open(cid, oid, false, &amp;fd); safe_pread(**fd, bptr.c_str(), len, offset); lfn_close(fd); Read 函数实现如下：其中 lfnopen 是从文件句柄缓冲池中拿句柄信息，对应 POSIX 接口中的 open 系统调用，safe_pread 则是对 pread 系统调用的封装。 int FileStore::read( CollectionHandle&amp; ch, const ghobject_t&amp; oid, uint64_t offset, size_t len, bufferlist&amp; bl, uint32_t op_flags) { int got; tracepoint(objectstore, read_enter, ch-&gt;cid.c_str(), offset, len); const coll_t&amp; cid = !_need_temp_object_collection(ch-&gt;cid, oid) ? ch-&gt;cid : ch-&gt;cid.get_temp(); dout(15) &lt;&lt; __FUNC__ &lt;&lt; &quot;: &quot; &lt;&lt; cid &lt;&lt; &quot;/&quot; &lt;&lt; oid &lt;&lt; &quot; &quot; &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; len &lt;&lt; dendl; auto osr = static_cast&lt;OpSequencer*&gt;(ch.get()); osr-&gt;wait_for_apply(oid); FDRef fd; int r = lfn_open(cid, oid, false, &amp;fd); if (r &lt; 0) { dout(10) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (&quot; &lt;&lt; cid &lt;&lt; &quot;/&quot; &lt;&lt; oid &lt;&lt; &quot;) open error: &quot; &lt;&lt; cpp_strerror(r) &lt;&lt; dendl; return r; } if (offset == 0 &amp;&amp; len == 0) { struct stat st; memset(&amp;st, 0, sizeof(struct stat)); int r = ::fstat(**fd, &amp;st); ceph_assert(r == 0); len = st.st_size; } #ifdef HAVE_POSIX_FADVISE if (op_flags &amp; CEPH_OSD_OP_FLAG_FADVISE_RANDOM) posix_fadvise(**fd, offset, len, POSIX_FADV_RANDOM); if (op_flags &amp; CEPH_OSD_OP_FLAG_FADVISE_SEQUENTIAL) posix_fadvise(**fd, offset, len, POSIX_FADV_SEQUENTIAL); #endif bufferptr bptr(len); // prealloc space for entire read got = safe_pread(**fd, bptr.c_str(), len, offset); if (got &lt; 0) { dout(10) &lt;&lt; __FUNC__ &lt;&lt; &quot;: (&quot; &lt;&lt; cid &lt;&lt; &quot;/&quot; &lt;&lt; oid &lt;&lt; &quot;) pread error: &quot; &lt;&lt; cpp_strerror(got) &lt;&lt; dendl; lfn_close(fd); return got; } bptr.set_length(got); // properly size the buffer bl.clear(); bl.push_back(std::move(bptr)); // put it in the target bufferlist #ifdef HAVE_POSIX_FADVISE if (op_flags &amp; CEPH_OSD_OP_FLAG_FADVISE_DONTNEED) posix_fadvise(**fd, offset, len, POSIX_FADV_DONTNEED); if (op_flags &amp; (CEPH_OSD_OP_FLAG_FADVISE_RANDOM | CEPH_OSD_OP_FLAG_FADVISE_SEQUENTIAL)) posix_fadvise(**fd, offset, len, POSIX_FADV_NORMAL); #endif if (m_filestore_sloppy_crc &amp;&amp; (!replaying || backend-&gt;can_checkpoint())) { ostringstream ss; int errors = backend-&gt;_crc_verify_read(**fd, offset, got, bl, &amp;ss); if (errors != 0) { dout(0) &lt;&lt; __FUNC__ &lt;&lt; &quot;: &quot; &lt;&lt; cid &lt;&lt; &quot;/&quot; &lt;&lt; oid &lt;&lt; &quot; &quot; &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; got &lt;&lt; &quot; ... BAD CRC:\\n&quot; &lt;&lt; ss.str() &lt;&lt; dendl; ceph_abort_msg(&quot;bad crc on read&quot;); } } lfn_close(fd); dout(10) &lt;&lt; __FUNC__ &lt;&lt; &quot;: &quot; &lt;&lt; cid &lt;&lt; &quot;/&quot; &lt;&lt; oid &lt;&lt; &quot; &quot; &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; got &lt;&lt; &quot;/&quot; &lt;&lt; len &lt;&lt; dendl; if (cct-&gt;_conf-&gt;filestore_debug_inject_read_err &amp;&amp; debug_data_eio(oid)) { return -EIO; } else if (oid.hobj.pool &gt; 0 &amp;&amp; /* FIXME, see #23029 */ cct-&gt;_conf-&gt;filestore_debug_random_read_err &amp;&amp; (rand() % (int)(cct-&gt;_conf-&gt;filestore_debug_random_read_err * 100.0)) == 0) { dout(0) &lt;&lt; __func__ &lt;&lt; &quot;: inject random EIO&quot; &lt;&lt; dendl; return -EIO; } else { tracepoint(objectstore, read_exit, got); return got; } } BlueStore MSST17 测试数据 硬件环境： Admin Server x1 CPU：Intel® Xeon® CPU E5-2640 v3 Mem：128GB OSD Server x4 CPU：Intel® Xeon® CPU E5-2640 v3 Mem：32 GB Storage： HGST UCTSSC600 600 GB x4 Samsung PM1633 960 GB x4 Intel® 750 series 400 GB x2 软件环境：Linux 4.4.43 kernel，Ceph Jewel LTS version (v10.2.5) 工具：ftrace（可能引入开销导致 Ceph 性能下降），或者修改内核记录写入的扇区数来测试写放大 负载解释 Microbenchmark 首先，我们分析在不同情况下，当我们向RBD发出一个写请求时，WAF如何变化。在默认情况下，整个RBD空间被划分为一组4MiB对象。 1st write：there is a write to an empty object 2nd write：there is a write next to the 1ST WRITE 3rd write：there is a write to the middle of the object leaving a hole between the 2ND WRITE and the current write。（即从 2MB 的位置开始写） overwrite：there is an overwrite to the location written by the 1ST WRITE 通过将请求大小从4KiB翻倍到4KiB来重复相同的实验 Long-Term EXPERIMENT 使用 FIO 生成随机 4K 写到 RBD，周期性地测试 IOPS 和 WAF，对于每个存储后端，我们将所有写请求分类为几个类别，并计算每个类别的 WAF，因为 RBD 常用于虚拟桌面基础设施 VDI，所以写请求通常是随机的，大小从 4K 到 8K 不等，所以我们测试了 4K 写，我们修改内核来记录写入的扇区数来测试写放大 step1. 安装 ceph，创建 64GB 的 krbd step2. 删除页面缓存，调用同步并等待600秒将所有脏数据刷新到磁盘 step3. 执行 4KiB 随机写操作，队列深度为 128 (QD=128)对 krbd 分区使用 fio，直到总写量达到容量的 90%（即 57.6GB） 注意：所有的实验都是先使用 16 HDDs 测试，然后使用 16 SSDs 测试，对于 HDD 队列深度设置为 128 ，使用单个写线程即可让 OSD 饱和，但是对于 SSD 不行，所以使用了 2 个写线程和 128 的队列深度。 测试结果 MicroBenchmark 基于 XFS 的 FileStore，CephJournal 放在 NVMe SSD 上，数据被分类为： Ceph data Ceph metadata：因为 Ceph data 和 Ceph metadata 不好区分，所以这里采用的方式是写入 XFS 之前的数据总量减去 Ceph data 的数量（Ceph data 的数量则是在客户端处统计 * 副本数），所以此处的 Ceph metadata 包括了写入 LevelDB 的数据量（即 Metadata Attributes） Ceph journal File system metadata File system journal 随着请求大小的增大，WAF 下降明显，因为对象的元数据、KV 对、文件系统日志和元数据相对于写入的数据变得更小了，到 4MB 的时候收敛到了大约 6 倍，6 则主要是 3 副本和写前日志 Ceph Journal 引入的双写叠加起来的。 下表所示的 2nd write 和 3rd write 对应在 2M 和 4M 大小的统计 WAF 系数为空 是因为对于单个请求大小为 2M 时，1st 写入就已经写入了 2M，2nd 写入和 3rd 写入是等价的，所以此处只记录了 3rd 从 2MB 开始写入。 而对于单个请求大小为 4MB 时，1st 就已经写满了 4MB 的 RBD 对象大小，所以 2nd 和 3rd 写入无法继续进行，故统计为空。 BlueStore 数据被划分为以下类型： Ceph data Ceph metadata：写到 RocksDB 以及 RocksDB WAL 的数据 Compaction：RocksDB Compaction 引入的写入 Zero-filled data：Data filled with zeroes by Ceph OSD daemons。即当写入的 chunk 小于最小分配大小，chunk 中会有空白的部分也就是所说的 holes，使用 0 填充 3rd write 相比另外两种写操作的写放大更为严重，特别是在小 IO 的时候（小于16KB），因为 BlueStore 默认的最小分配 chunk 大小设置为了 64KB，小 IO 的话对应就需要进行填充（1st 和 2nd 小 IO 之后进行 3rd IO 需要填充前两次 IO 对应的 chunk），当请求大小达到 32KB 的时候，整个 chunk 因为第一次和第二次写入就已经被写满了（32+32），所以就不需要填充数据了，此时填充带来的写放大影响就是最小的。 当请求大小小于 64KB 的时候，overwrite 的写放大比 1st write 要严重，这是因为如果请求需要部分覆盖现有块，BlueStore 会尝试通过将数据写入 WAL 设备来维护数据一致性，以防止突然断电，如果直接对现有块执行部分覆盖，在写操作被中断的情况下，我们将无法恢复原始数据。 BlueStore 相比于其他存储后端的写放大表现相对更好是因为没有使用本地文件系统来存储对象数据，减少了写流量，和其他存储后端不一样的是，只要不是部分覆盖写操作的话 BlueStore 就不会有双写的问题，当请求大小大于或等于块（chunk）大小时，这使得 BlueStore 中的 WAF 收敛为3。 Long-Term EXPERIMENT FileStore 的 IOPS 曲线和 Ceph Journal 的写流量是比较相近的，因为对于 FIleStore 下的客户端而言，需要等待三个写入请求都被封装成事务写入到日志中才会确认该请求成功写入。 HDD 和 SSD 的表现完全不同，HDD 下，IOPS 保持在接近 4000 ops/秒，持续约 1000 秒，但之后会降到 3000 ops/秒以下，直到实验结束。这种性能下降主要是由于 HDD 的速度较慢和文件存储中使用的节流（throttling）机制，开始写入的时候，无其他数据，Ceph 日志首先吸收传入的写事务，所以能够很快确认写入完成，由于 FileStore 一直以只追加的方式将 Ceph 日志写入NVMe SSD，所以对 HDD 的重写速度赶不上 Ceph 日志的速度，为了防止延迟骤降，FileStore 会检查日志项中还未刷入 HDD 的数量是否达到阈值然后来决定是否限制来自上层的事务写入（journal_throttle_low_threshold, journal_throttle_high_threshold），当向 HDD 写入了一定的事务之后，取消限制，高性能 Ceph 日志再次快速吸收写请求，直到它又达到阈值，所以会产生周期性的性能波动，写事务经常被限制，IOPS 曲线也会波动。 SSD 下的性能表现就相对稳定，实验开始时所写的Ceph日志与实验结束时所写的基本相同，在整个实验过程中，在数量上几乎没有变化。这意味着，装载 XFS 文件系统的底层 SSD 处理排队事务的速度与 Ceph 日志写入的速度一样快 BlueStore 与 FileStore 的一个显著区别是，在实验开始时，会有大量零填充的数据流量。当前 chunk 大小小于最小分配的单元大小时就需要进行填充，零填充的数据量会随着时间的推移而减少，因为对每个块只执行一次零填充操作。 因为 BlueStore 按顺序将块分配给原始块设备，在实验开始时零填充的数据将会很快写入完成，即便是在较慢的 HDD 上。但是对于已经分配好的 chunk 进行连续的 4KiB 随机写操作会导致对 HDD 的随机访问，就会让 HDD 的 IOPS 降低，但是相比于 FileStore，IOPS 表现更为稳定。当在 SSD 上运行的时候，实验开始阶段 IOPS 还略有上升，这是因为不像 HDD 那样，初始阶段之后的随机写不会成为 SSD 的瓶颈，因为 SSD 有更好的随机写性能相比于 HDD，IOPS 会随着由零填充数据引起的额外写操作的减少而增加一点 当使用 SAS SSD 作为 BlueStore 的主存储时，我们发现作为 RocksDB 和 RocksDB WAL 使用的 NVMe SSD 设备可能会出现瓶颈，因为 SAS SSD 和 NVMe SSD 的性能差异不显著。其中 RocksDB WAL 尤为明显，因为大部分随机的 4K 写将被转换成对于现有的存在的 chunk 的部分覆盖写，此时就需要 BlueStore 将数据保存在 RocksDB WAL 中来保证数据的一致性。 由于 FileStore 依赖于外部 Ceph 日志记录，传统的看法是，由于冗余的数据写入 Ceph 日志，它会使 WAF 翻倍，但是下表所示无论是 HDD 还是 SSD，不只是翻倍，而是三倍，这是因为 FileStore 不仅将数据写入日志，还把元数据和其他属性写入了日志。除此以外还会有 journaling of journal 问题，写放大增加了约 4 倍，这意味着文件系统元数据和文件系统日志的数量甚至大于每个OSD服务器中的实际数据大小。 对于 BlueStore 无论是 SSD 还是 HDD，由 RocksDB + RocksDB WAL + Compaction 造成的总流量很大，占据了写放大因子的 65.7~69.3%。 如下表所示，无论哪种存储后端，对于 HDD，Ceph 的 WAF 很高，从 14.56 到 71.03，考虑到使用了三副本，对于单个 4KB 的写入实质放大为 4.85x ∼ 23.68x，其中 FileStore 的 IOPS 和平均延迟上表现最好，但是尾延迟最差，但 BlueStore 的尾延迟最低，BlueStore 的写放大更大是因为虽然避免了 Ceph Journal 但还是需要对小写进行写前日志的操作来保证一致性。 SSD 上 FileStore 仍然表现最好，尾延迟也是如此，主要是因为主要存储变成 SSD 后速度也足够快能够赶上 Ceph Journal 的写入速度，大多数场景下，客户端在写入 Ceph Journal 后就能确认当前操作完成，也一定程度上减小了延迟。 BlueStore 似乎是在延迟敏感情况下最有前途的存储后端，特别是在 HDD 用作主存储介质时。SSD 上仍然是 FileStore 表现更好，但是 BlueStore 差距也不大。 SOSP19 测试数据 硬件环境： 16-node Ceph cluster 16-core Intel E5-2698Bv3 Xeon 2GHz CPU 64GiB RAM 400GB Intel P3600 NVMe SSD 4TB 7200RPM Seagate ST4000NM0023 HDD 软件环境： Linux kernel 4.15 on Ubuntu 18.04 Ceph Luminous release (v12.2.11) 测试结果 Bare RADOS Benchmarks 如图所示以队列深度 128 写入的不同对象大小的吞吐量。在稳定状态下，BlueStore的吞吐量比FileStore大50-100%，因为 BlueStore 避免了双写和一致性开销 如图显示了对象写入RADOS的95%以上的延迟。BlueStore 的尾部延迟比 FileStore 低一个数量级。此外，正如预期的那样，使用 BlueStore 时，尾部延迟会随着对象大小的增加而增加，而使用 FileStore 时，即使是很小的对象写操作也可能有很高的尾部延迟，这是由于缺乏对写操作的控制 读取性能在 BlueStore(没有显示)是类似或更好的相比于 FileStore 当 I/O 大小大于 128 KiB;对于较小的 I/O大小，FileStore 更好，因为内核提前读取，BlueStore 无意实现预读。预期在RADOS上实现的应用程序将执行它们自己的预读。 RADOS Block Device Benchmarks 对于 I/O 大小大于 BlueStore 的 512 KiB、顺序写和随机写吞吐量平均分别高出 1.7 倍和 2 倍，同样主要是由于避免了重复写，BlueStore 还显示了一个显著更低的吞吐量差异，因为它可以确定地将数据推送到磁盘，另一方面，在 FileStore 中，任意触发的回写与前台对 WAL 的写入冲突，并引入了长请求延迟 对于中等 I/O 大小(128 512 KiB)，顺序写操作的吞吐量差异会减小，因为XFS在文件存储中屏蔽了重复写操作的部分成本。对于中等 I/O 大小，对 WAL 的写操作不会完全利用磁盘，这样就留下了足够的带宽让另一个写入流通过，并且不会对前台对 WAL 的写入产生很大影响。将数据同步写入到 WAL 后，FileStore 再将其异步写入文件系统。XFS 缓冲这些异步写操作，并在将它们发送到磁盘之前将它们转换为一个大的顺序写操作。XFS 不能对随机写操作执行同样的操作，这就是为什么即使对于中等大小的随机写操作，高吞吐量的差异仍然存在。 对于小于 64KiB(未显示)的 I/O 大小，BlueStore 的吞吐量比 FileStore 高 20%。对于这些 I/O 大小，BlueStore 执行延迟写操作，首先将数据插入 RocksDB，然后异步覆盖对象数据以避免碎片。 为什么使用 BlueStore I/O 放大严峻：放大主要包括两个方面：在 FileStore 实现事务带来的放大 和 文件系统本身的放大 由于有的文件系统不支持事务，或者部分支持内部事务的文件系统如 btrfs（但实践表明事务中途发生故障的时候可能出现事务部分提交的情况），故 FileStore 需要实现一套 WAL 机制来实现事务（FileJournal），相应地也就引入了放大 许多文件系统后端本身就是日志文件系统，内部保证数据一致性也实现了写前日志，因此也存在一定的写放大 本地文件系统的元数据性能可能严重影响分布式系统的整体性能：Ceph 面对的一个很大的挑战就是“如何快速地枚举文件夹中数百万项的内容，如何保证返回的结果有序”。基于 Btrfs 和 XFS 的后端存储往往都会有这样的问题，同时用于分配元数据负载的目录分割操作与系统策略其实是有一定冲突的，整个系统的性能会受到元数据性能的影响。 新型存储器件向文件系统提出了挑战：文件系统日趋成熟带来的影响就是显得更加的保守和死板，不能较好地适配现在很多摒弃了块接口的新型存储器件。面向数据中心的新型存储器件往往都需要在原有应用程序接口层面做较大的修改。诸如为了提升容量， HDD 正在向 SMR 过渡，同时支持 Zone Interface；为了减小 SSD 中由于 FTL 造成的 IO 尾延迟，引入了 Zoned Namespace SSD 技术，支持 Zone Interface；云计算和云存储供应商也在调整他们的软件栈来适配 Zone 设备。分布式文件系统在这方面目前缺乏较好的支持。 架构 BlueStore 整体架构分为三个部分：BlockDevice、BlueFS 和 RocksDB BlockDevice 为最底层的块设备，通常为 HDD 或者 SSD ， BlueStore 直接操作块设备，抛弃了 XFS 等本地文件系统。 BlockDevice 在用户态直接以 linux 系统实现的 AIO 直接操作块设备，由于操作系统支持的 aio 操作只支持 directIO，所以对 BlockDevice 的写操作直接写入磁盘，并且需要按照 page 对齐。 RocksDB 是 Facebook 在 leveldb 上开发并优化的 KV 存储系统。本身是基于文件系统的，不是直接操作裸设备。它将系统相关的处理抽象成 Env，用户可实现相应的接口。BlueFS 的主要的目的，就是支持 RocksDB Env 接口，保证 RocksDB 的正常运行。 BlueFS 是一个小的文件系统，其文件系统的文件和目录的元数据都保存在全部缓存在内存中，持久化保存在文件系统的日志文件中， 当文件系统重新 mount 时，重新 replay 该日志文件中保存的操作，就可以加载所有的元数据到内存中。其数据和日志文件都直接保存在依赖底层的 BlockDevice 中。主要还实现了RocksDB::Env所需要的接口。BlueFS 在设计上支持把 .log 和 .sst 分开存储，.log 使用速度更快的存储介质(NVME等)，从而提高 WAL 日志的性能。(即可以在 OSD 的配置中指定 wal/db path) bluestore block db path =/dev/sdb2 bluestore block wal path =/dev/ram0 bluestore block path = /dev/sdb4 BlueStore 是最终基于 RocksDB 和 BlockDevice 实现的 Ceph 的对象存储，其所有的元数据都保存在 RocksDB 这个KV存储系统中，包括对象的集合，对象，存储池的 omap 信息，磁盘空间分配记录等都保存 RocksDB 里, 其对象的数据直接保存在 BlockDevice 上，不使用本地文件系统，直接接管裸设备，并且只使用一个原始分区。 Allocator: 最新的实现中有 AvlAllocator, BitmapAllocator, StupidAllocator, ZonedAllocator BlueFS 数据结构 BlueFS 会为每一个文件维护一个 inode，包括分配给该文件的区段列表也会维护相应的 inode。超级块存储在确定的物理地址上，且包含了日志的 inode 信息，其文件系统的文件和目录的元数据都保存在全部缓存在内存中，持久化保存在文件系统的日志文件中， 当文件系统重新 mount 时，重新 replay 该日志文件中保存的操作，就可以加载所有的元数据到内存中。当日志大小达到阈值时，日志文件将被压缩并写到一个新的日志文件中，同时将日志的新地址信息记录到超级块中 WAL 对于提升RocksDB的性能至关重要，所以 BlueFS 在设计上支持把 .log 和 .sst 分开存储，.log 使用速度更快的存储介质(NVME等)。 BlueFS 与传统文件系统不同另外一个地方是并没有设计单独存储fnode的存储空间，而是将其存储在WAL（Write Ahead Log）日志当中。当文件系统挂载的时候通过回放该日志实现内存数据结构的构建。这样，在内存中就可以查到磁盘中的目录和文件信息，从而可以实现对文件的读写。BlueFS本身就是一个功能阉割的，迷你文件系统。BlueFS可以这么实现得益于其只服务于RocksDB，其文件数量非常有限，使用场景也非常有限。 class BlueFS { public: CephContext* cct; // 支持不同种类的块设备 static constexpr unsigned MAX_BDEV = 5; static constexpr unsigned BDEV_WAL = 0; static constexpr unsigned BDEV_DB = 1; static constexpr unsigned BDEV_SLOW = 2; static constexpr unsigned BDEV_NEWWAL = 3; static constexpr unsigned BDEV_NEWDB = 4; enum { WRITER_UNKNOWN, WRITER_WAL, // RocksDB的log文件 WRITER_SST, // RocksDB的sst文件 }; // 文件 struct File : public RefCountedObject { MEMPOOL_CLASS_HELPERS(); bluefs_fnode_t fnode; // 文件inode int refs; // 引用计数 uint64_t dirty_seq; // dirty序列号 bool locked; bool deleted; boost::intrusive::list_member_hook&lt;&gt; dirty_item; // 读写计数 std::atomic_int num_readers, num_writers; std::atomic_int num_reading; void* vselector_hint = nullptr; ... 写操作 针对具体的存储引擎的实现，由于数据都是先写后读，此处将从写操作入手，再到读操作。 写操作的调用关系如下： ReplicatedBackend::submit_transaction parent-&gt;queue_transactions(tls, op.op); //调用PrimaryLogPG::queue_transactions osd-&gt;store-&gt;queue_transactions(ch, tls, op, NULL); //调用BlueStore::queue_transactions 而在 BlueStore 中具体的 queue_transactions 中： int BlueStore::queue_transactions( CollectionHandle&amp; ch, vector&lt;Transaction&gt;&amp; tls, TrackedOpRef op, ThreadPool::TPHandle *handle) { FUNCTRACE(cct); /** * Collect contexts * on_applied: will queue it in queue transactions, it's readable but we can callback in async way * on_applied_sync: will call back in queue_transactions, because it's readable immediately in bluestore. * on_commit: will call it when kvdb transactions committed. **/ list&lt;Context *&gt; on_applied, on_commit, on_applied_sync; ObjectStore::Transaction::collect_contexts( tls, &amp;on_applied, &amp;on_commit, &amp;on_applied_sync); auto start = mono_clock::now(); Collection *c = static_cast&lt;Collection*&gt;(ch.get()); OpSequencer *osr = c-&gt;osr.get(); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; ch &quot; &lt;&lt; c &lt;&lt; &quot; &quot; &lt;&lt; c-&gt;cid &lt;&lt; dendl; // Create transaction context // 创建 KVDB 的事务同时获取PG对应的OpSequencer(每个PG有一个OpSequencer)用来保证PG上的IO串行执行 // prepare TransContext *txc = _txc_create(static_cast&lt;Collection*&gt;(ch.get()), osr, &amp;on_commit); // 遍历收集到的 list&lt;Context *&gt; 根据事务对应的操作码分别进行处理 // Collection Ops：remove_collection、create_collection、split_collection、merge_collection、collection hint objects // Object Ops：OP_CREATE、OP_TOUCH（修改时间属性）、OP_WRITE、OP_ZERO、OP_TRUNCATE、OP_REMOVE、OP_SETATTR、OP_RMATTR(remove attributes)、OP_CLONE // OMap Ops for (vector&lt;Transaction&gt;::iterator p = tls.begin(); p != tls.end(); ++p) { txc-&gt;bytes += (*p).get_num_bytes(); _txc_add_transaction(txc, &amp;(*p)); } // 计算事务开销 _txc_calc_cost(txc); // 写 ONode 数据 （ONode 是常驻内存的数据结构，主要用于管理对象元数据） // 持久化时，ONode 数据将被写入到 RocksDB 中 _txc_write_nodes(txc, txc-&gt;t); // journal deferred items if (txc-&gt;deferred_txn) { txc-&gt;deferred_txn-&gt;seq = ++deferred_seq; bufferlist bl; encode(*txc-&gt;deferred_txn, bl); string key; get_deferred_key(txc-&gt;deferred_txn-&gt;seq, &amp;key); txc-&gt;t-&gt;set(PREFIX_DEFERRED, key, bl); } _txc_finalize_kv(txc, txc-&gt;t); if (handle) handle-&gt;suspend_tp_timeout(); auto tstart = mono_clock::now(); if (!throttle.try_start_transaction( *db, *txc, tstart)) { // ensure we do not block here because of deferred writes dout(10) &lt;&lt; __func__ &lt;&lt; &quot; failed get throttle_deferred_bytes, aggressive&quot; &lt;&lt; dendl; ++deferred_aggressive; deferred_try_submit(); { // wake up any previously finished deferred events std::lock_guard l(kv_lock); if (!kv_sync_in_progress) { kv_sync_in_progress = true; kv_cond.notify_one(); } } throttle.finish_start_transaction(*db, *txc, tstart); --deferred_aggressive; } auto tend = mono_clock::now(); if (handle) handle-&gt;reset_tp_timeout(); logger-&gt;inc(l_bluestore_txc); // execute (start) _txc_state_proc(txc); // we're immediately readable (unlike FileStore) for (auto c : on_applied_sync) { c-&gt;complete(0); } if (!on_applied.empty()) { if (c-&gt;commit_queue) { c-&gt;commit_queue-&gt;queue(on_applied); } else { finisher.queue(on_applied); } } log_latency(&quot;submit_transact&quot;, l_bluestore_submit_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age); log_latency(&quot;throttle_transact&quot;, l_bluestore_throttle_lat, tend - tstart, cct-&gt;_conf-&gt;bluestore_log_op_age); return 0; } 写操作调用关系： BlueStore::queue_transactions _txc_add_transaction(txc, &amp;(*p)); //调用 BlueStore::_txc_add_transaction(TransContext *txc, Transaction *t) _write(txc, c, o, off, len, bl, fadvise_flags); //调用BlueStore::_write _do_write(txc, c, o, offset, length, bl, fadvise_flags); txc-&gt;write_onode(o); BlueStore::_do_write _choose_write_options(c, o, fadvise_flags, &amp;wctx); o-&gt;extent_map.fault_range(db, offset, length); _do_write_data(txc, c, o, offset, length, bl, &amp;wctx); r = _do_alloc_write(txc, c, o, &amp;wctx); 写操作的函数流程： int BlueStore::_write(TransContext *txc, CollectionRef&amp; c, OnodeRef&amp; o, uint64_t offset, size_t length, bufferlist&amp; bl, uint32_t fadvise_flags) { dout(15) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; c-&gt;cid &lt;&lt; &quot; &quot; &lt;&lt; o-&gt;oid &lt;&lt; &quot; 0x&quot; &lt;&lt; std::hex &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; length &lt;&lt; std::dec &lt;&lt; dendl; int r = 0; if (offset + length &gt;= OBJECT_MAX_SIZE) { r = -E2BIG; } else { _assign_nid(txc, o); r = _do_write(txc, c, o, offset, length, bl, fadvise_flags); txc-&gt;write_onode(o); } dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; c-&gt;cid &lt;&lt; &quot; &quot; &lt;&lt; o-&gt;oid &lt;&lt; &quot; 0x&quot; &lt;&lt; std::hex &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; length &lt;&lt; std::dec &lt;&lt; &quot; = &quot; &lt;&lt; r &lt;&lt; dendl; return r; } int BlueStore::_do_write( TransContext *txc, CollectionRef&amp; c, OnodeRef o, uint64_t offset, uint64_t length, bufferlist&amp; bl, uint32_t fadvise_flags) { int r = 0; dout(20) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; o-&gt;oid &lt;&lt; &quot; 0x&quot; &lt;&lt; std::hex &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; length &lt;&lt; &quot; - have 0x&quot; &lt;&lt; o-&gt;onode.size &lt;&lt; &quot; (&quot; &lt;&lt; std::dec &lt;&lt; o-&gt;onode.size &lt;&lt; &quot;)&quot; &lt;&lt; &quot; bytes&quot; &lt;&lt; &quot; fadvise_flags 0x&quot; &lt;&lt; std::hex &lt;&lt; fadvise_flags &lt;&lt; std::dec &lt;&lt; dendl; _dump_onode&lt;30&gt;(cct, *o); if (length == 0) { return 0; } uint64_t end = offset + length; GarbageCollector gc(c-&gt;store-&gt;cct); int64_t benefit = 0; auto dirty_start = offset; auto dirty_end = end; WriteContext wctx; _choose_write_options(c, o, fadvise_flags, &amp;wctx); o-&gt;extent_map.fault_range(db, offset, length); //***************************************************** // 执行真正的数据写入的操作 _do_write_data(txc, c, o, offset, length, bl, &amp;wctx); //***************************************************** r = _do_alloc_write(txc, c, o, &amp;wctx); if (r &lt; 0) { derr &lt;&lt; __func__ &lt;&lt; &quot; _do_alloc_write failed with &quot; &lt;&lt; cpp_strerror(r) &lt;&lt; dendl; goto out; } if (wctx.extents_to_gc.empty() || wctx.extents_to_gc.range_start() &gt; offset || wctx.extents_to_gc.range_end() &lt; offset + length) { benefit = gc.estimate(offset, length, o-&gt;extent_map, wctx.old_extents, min_alloc_size); } // NB: _wctx_finish() will empty old_extents // so we must do gc estimation before that _wctx_finish(txc, c, o, &amp;wctx); if (end &gt; o-&gt;onode.size) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; extending size to 0x&quot; &lt;&lt; std::hex &lt;&lt; end &lt;&lt; std::dec &lt;&lt; dendl; o-&gt;onode.size = end; } if (benefit &gt;= g_conf()-&gt;bluestore_gc_enable_total_threshold) { wctx.extents_to_gc.union_of(gc.get_extents_to_collect()); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; perform garbage collection for compressed extents, &quot; &lt;&lt; &quot;expected benefit = &quot; &lt;&lt; benefit &lt;&lt; &quot; AUs&quot; &lt;&lt; dendl; } if (!wctx.extents_to_gc.empty()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; perform garbage collection&quot; &lt;&lt; dendl; r = _do_gc(txc, c, o, wctx, &amp;dirty_start, &amp;dirty_end); if (r &lt; 0) { derr &lt;&lt; __func__ &lt;&lt; &quot; _do_gc failed with &quot; &lt;&lt; cpp_strerror(r) &lt;&lt; dendl; goto out; } dout(20)&lt;&lt;__func__&lt;&lt;&quot; gc range is &quot; &lt;&lt; std::hex &lt;&lt; dirty_start &lt;&lt; &quot;~&quot; &lt;&lt; dirty_end - dirty_start &lt;&lt; std::dec &lt;&lt; dendl; } o-&gt;extent_map.compress_extent_map(dirty_start, dirty_end - dirty_start); o-&gt;extent_map.dirty_range(dirty_start, dirty_end - dirty_start); r = 0; out: return r; } 真正执行数据写入的函数 _do_write_data： void BlueStore::_do_write_data( TransContext *txc, CollectionRef&amp; c, OnodeRef o, uint64_t offset, uint64_t length, bufferlist&amp; bl, WriteContext *wctx) { uint64_t end = offset + length; bufferlist::iterator p = bl.begin(); // 判断写入的数据大小是否处于一个 min_alloc_size 大小中 // min_alloc_size 通常为 block_size 大小的整数倍 if (offset / min_alloc_size == (end - 1) / min_alloc_size &amp;&amp; (length != min_alloc_size)) { // 执行小写 // we fall within the same block _do_write_small(txc, c, o, offset, length, p, wctx); } else { // 如果写入的数据大小超过 min_alloc_size，则会进行划分 // 一部分进行大写，一部分进行小写 uint64_t head_offset, head_length; uint64_t middle_offset, middle_length; uint64_t tail_offset, tail_length; head_offset = offset; head_length = p2nphase(offset, min_alloc_size); tail_offset = p2align(end, min_alloc_size); tail_length = p2phase(end, min_alloc_size); middle_offset = head_offset + head_length; middle_length = length - head_length - tail_length; if (head_length) { _do_write_small(txc, c, o, head_offset, head_length, p, wctx); } if (middle_length) { // 执行大写 _do_write_big(txc, c, o, middle_offset, middle_length, p, wctx); } if (tail_length) { _do_write_small(txc, c, o, tail_offset, tail_length, p, wctx); } } } 当一个写请求按照min_alloc_size进行拆分后，就会分为对齐写，对应到do_write_big，非对齐写（即落到某一个min_alloc_size区间的写I/O（对应到do_write_small） do_write_big: 对齐到min_alloc_size的写请求处理起来比较简单，有可能是多个min_alloc_size的大小，在处理时会根据实际大小新生成lextent和blob，这个lextent跨越的区域是min_alloc_size的整数倍，如果这段区间是之前写过的，会将之前的lextent记录下来便于后续的空间回收。 do_write_small: 在处理落到某个min_alloc_size区间的写请求时，会首先根据offset去查找有没有可以复用的blob，因为最小分配单元是min_alloc_size，默认64KB，如果一个4KB的写I/O就只会用到blob的一部分，blob里剩余的还能放其他的。 读操作 读操作的流程： ReplicatedBackend::objects_read_sync store-&gt;read(ch, ghobject_t(hoid), off, len, *bl, op_flags) BlueStore::read _do_read(c, o, offset, length, bl, op_flags); BlueStore::_do_read ReplicatedBackend::objects_readv_sync store-&gt;readv(ch, ghobject_t(hoid), im, *bl, op_flags); BlueStore::readv _do_readv(c, o, m, bl, op_flags); BlueStore::_do_readv 真正执行读操作的 _do_read 和 _do_readv 通过libaio的方式进行读写操作。实现的时候抽象出BlockDevice基类类型，统一管理各种类型的设备，如Kernel, NVME和NVRAM等，为裸盘的使用者(BlueFS/BlueStore)提供统一的操作接口 int BlueStore::_do_read( Collection *c, OnodeRef o, uint64_t offset, size_t length, bufferlist&amp; bl, uint32_t op_flags, uint64_t retry_count) { FUNCTRACE(cct); int r = 0; int read_cache_policy = 0; // do not bypass clean or dirty cache dout(20) &lt;&lt; __func__ &lt;&lt; &quot; 0x&quot; &lt;&lt; std::hex &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; length &lt;&lt; &quot; size 0x&quot; &lt;&lt; o-&gt;onode.size &lt;&lt; &quot; (&quot; &lt;&lt; std::dec &lt;&lt; o-&gt;onode.size &lt;&lt; &quot;)&quot; &lt;&lt; dendl; bl.clear(); if (offset &gt;= o-&gt;onode.size) { return r; } // generally, don't buffer anything, unless the client explicitly requests // it. bool buffered = false; if (op_flags &amp; CEPH_OSD_OP_FLAG_FADVISE_WILLNEED) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; will do buffered read&quot; &lt;&lt; dendl; buffered = true; } else if (cct-&gt;_conf-&gt;bluestore_default_buffered_read &amp;&amp; (op_flags &amp; (CEPH_OSD_OP_FLAG_FADVISE_DONTNEED | CEPH_OSD_OP_FLAG_FADVISE_NOCACHE)) == 0) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; defaulting to buffered read&quot; &lt;&lt; dendl; buffered = true; } if (offset + length &gt; o-&gt;onode.size) { length = o-&gt;onode.size - offset; } auto start = mono_clock::now(); o-&gt;extent_map.fault_range(db, offset, length); log_latency(__func__, l_bluestore_read_onode_meta_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age); _dump_onode&lt;30&gt;(cct, *o); // for deep-scrub, we only read dirty cache and bypass clean cache in // order to read underlying block device in case there are silent disk errors. if (op_flags &amp; CEPH_OSD_OP_FLAG_BYPASS_CLEAN_CACHE) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; will bypass cache and do direct read&quot; &lt;&lt; dendl; read_cache_policy = BufferSpace::BYPASS_CLEAN_CACHE; } // build blob-wise list to of stuff read (that isn't cached) ready_regions_t ready_regions; blobs2read_t blobs2read; _read_cache(o, offset, length, read_cache_policy, ready_regions, blobs2read); // read raw blob data. start = mono_clock::now(); // for the sake of simplicity // measure the whole block below. // The error isn't that much... vector&lt;bufferlist&gt; compressed_blob_bls; IOContext ioc(cct, NULL, true); // allow EIO r = _prepare_read_ioc(blobs2read, &amp;compressed_blob_bls, &amp;ioc); // we always issue aio for reading, so errors other than EIO are not allowed if (r &lt; 0) return r; int64_t num_ios = length; if (ioc.has_pending_aios()) { num_ios = -ioc.get_num_ios(); bdev-&gt;aio_submit(&amp;ioc); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; waiting for aio&quot; &lt;&lt; dendl; ioc.aio_wait(); r = ioc.get_return_value(); if (r &lt; 0) { ceph_assert(r == -EIO); // no other errors allowed return -EIO; } } log_latency_fn(__func__, l_bluestore_read_wait_aio_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age, [&amp;](auto lat) { return &quot;, num_ios = &quot; + stringify(num_ios); } ); bool csum_error = false; r = _generate_read_result_bl(o, offset, length, ready_regions, compressed_blob_bls, blobs2read, buffered, &amp;csum_error, bl); if (csum_error) { // Handles spurious read errors caused by a kernel bug. // We sometimes get all-zero pages as a result of the read under // high memory pressure. Retrying the failing read succeeds in most // cases. // See also: http://tracker.ceph.com/issues/22464 if (retry_count &gt;= cct-&gt;_conf-&gt;bluestore_retry_disk_reads) { return -EIO; } return _do_read(c, o, offset, length, bl, op_flags, retry_count + 1); } r = bl.length(); if (retry_count) { logger-&gt;inc(l_bluestore_reads_with_retries); dout(5) &lt;&lt; __func__ &lt;&lt; &quot; read at 0x&quot; &lt;&lt; std::hex &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; length &lt;&lt; &quot; failed &quot; &lt;&lt; std::dec &lt;&lt; retry_count &lt;&lt; &quot; times before succeeding&quot; &lt;&lt; dendl; } return r; } int BlueStore::_do_readv( Collection *c, OnodeRef o, const interval_set&lt;uint64_t&gt;&amp; m, bufferlist&amp; bl, uint32_t op_flags, uint64_t retry_count) { FUNCTRACE(cct); int r = 0; int read_cache_policy = 0; // do not bypass clean or dirty cache dout(20) &lt;&lt; __func__ &lt;&lt; &quot; fiemap &quot; &lt;&lt; m &lt;&lt; std::hex &lt;&lt; &quot; size 0x&quot; &lt;&lt; o-&gt;onode.size &lt;&lt; &quot; (&quot; &lt;&lt; std::dec &lt;&lt; o-&gt;onode.size &lt;&lt; &quot;)&quot; &lt;&lt; dendl; // generally, don't buffer anything, unless the client explicitly requests // it. bool buffered = false; if (op_flags &amp; CEPH_OSD_OP_FLAG_FADVISE_WILLNEED) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; will do buffered read&quot; &lt;&lt; dendl; buffered = true; } else if (cct-&gt;_conf-&gt;bluestore_default_buffered_read &amp;&amp; (op_flags &amp; (CEPH_OSD_OP_FLAG_FADVISE_DONTNEED | CEPH_OSD_OP_FLAG_FADVISE_NOCACHE)) == 0) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; defaulting to buffered read&quot; &lt;&lt; dendl; buffered = true; } // this method must be idempotent since we may call it several times // before we finally read the expected result. bl.clear(); // call fiemap first! ceph_assert(m.range_start() &lt;= o-&gt;onode.size); ceph_assert(m.range_end() &lt;= o-&gt;onode.size); auto start = mono_clock::now(); o-&gt;extent_map.fault_range(db, m.range_start(), m.range_end() - m.range_start()); log_latency(__func__, l_bluestore_read_onode_meta_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age); _dump_onode&lt;30&gt;(cct, *o); IOContext ioc(cct, NULL, true); // allow EIO vector&lt;std::tuple&lt;ready_regions_t, vector&lt;bufferlist&gt;, blobs2read_t&gt;&gt; raw_results; raw_results.reserve(m.num_intervals()); int i = 0; for (auto p = m.begin(); p != m.end(); p++, i++) { raw_results.push_back({}); _read_cache(o, p.get_start(), p.get_len(), read_cache_policy, std::get&lt;0&gt;(raw_results[i]), std::get&lt;2&gt;(raw_results[i])); r = _prepare_read_ioc(std::get&lt;2&gt;(raw_results[i]), &amp;std::get&lt;1&gt;(raw_results[i]), &amp;ioc); // we always issue aio for reading, so errors other than EIO are not allowed if (r &lt; 0) return r; } auto num_ios = m.size(); if (ioc.has_pending_aios()) { num_ios = ioc.get_num_ios(); bdev-&gt;aio_submit(&amp;ioc); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; waiting for aio&quot; &lt;&lt; dendl; ioc.aio_wait(); r = ioc.get_return_value(); if (r &lt; 0) { ceph_assert(r == -EIO); // no other errors allowed return -EIO; } } log_latency_fn(__func__, l_bluestore_read_wait_aio_lat, mono_clock::now() - start, cct-&gt;_conf-&gt;bluestore_log_op_age, [&amp;](auto lat) { return &quot;, num_ios = &quot; + stringify(num_ios); } ); ceph_assert(raw_results.size() == (size_t)m.num_intervals()); i = 0; for (auto p = m.begin(); p != m.end(); p++, i++) { bool csum_error = false; bufferlist t; r = _generate_read_result_bl(o, p.get_start(), p.get_len(), std::get&lt;0&gt;(raw_results[i]), std::get&lt;1&gt;(raw_results[i]), std::get&lt;2&gt;(raw_results[i]), buffered, &amp;csum_error, t); if (csum_error) { // Handles spurious read errors caused by a kernel bug. // We sometimes get all-zero pages as a result of the read under // high memory pressure. Retrying the failing read succeeds in most // cases. // See also: http://tracker.ceph.com/issues/22464 if (retry_count &gt;= cct-&gt;_conf-&gt;bluestore_retry_disk_reads) { return -EIO; } return _do_readv(c, o, m, bl, op_flags, retry_count + 1); } bl.claim_append(t); } if (retry_count) { logger-&gt;inc(l_bluestore_reads_with_retries); dout(5) &lt;&lt; __func__ &lt;&lt; &quot; read fiemap &quot; &lt;&lt; m &lt;&lt; &quot; failed &quot; &lt;&lt; retry_count &lt;&lt; &quot; times before succeeding&quot; &lt;&lt; dendl; } return bl.length(); } 参考链接 [1] CSDN：Ceph 撸源码系列（三）：Ceph OSDC源码分析 （1 of 2） [2] CSDN：Ceph 撸源码系列（一）：Ceph开源项目源代码的关键目录介绍 [3] CSDN：ceph bluestore 写操作源码分析（上） [4] CSDN：ceph bluestore 写操作源码分析（下） [5] Sysnote：ceph存储引擎bluestore解析 [6] DieInADream - 分布式存储系统 Ceph 的后端存储引擎研究 [7] tom-sun - BlueStore-先进的用户态文件系统《一》 [8] Understanding Write Behaviors of Storage Backends in Ceph Object Store slides [9] 知乎 - 分布式存储 BlueStore 源码分析之架构设计 BlueStore 源码分析之 BitMap 分配器 BlueStore 源码分析之 Stupid 分配器 BlueStore 源码分析之 FreelistManager BlueStore 源码分析之 Cache BlueStore 源码分析之对象 IO BlueStore 源码分析之事物状态机 [10] ceph存储引擎bluestore解析 ","link":"https://blog.shunzi.tech/post/ceph-objectstore/"},{"title":"VLDBJ 2018：LSM-based storage techniques: a survey","content":" VLDBJ 2018：LSM-based storage techniques: a survey 偶然的机会遇到一篇综述类型的文章，在考虑深入 LSM 的研究之前决定先阅读此篇 主要内容为 LSM 树的问题剖析相关方案的总结。 ... Abstract 近年来 LSM 树随着 NoSQL 的流行逐渐成为了数据库领域和操作系统领域的研究重点之一，旨在各方面对 LSM 树进行优化。这篇文章主要调研了 LSM 树的一些最新研究成果，便于读者了解 LSM 的发展动向。并使用了传统的分类方法对这些方案进行了分类，总结了优缺点。同时也提供了几个比较代表性的方案来探讨未来的发展方向。 关键字：LSM-tree，NoSQL，Storage Management，Indexing Introduction LSM 树被广泛应用于现代的 NoSQL 存储系统中，如 BigTable，Dynamo，HBase，Cassandra，LevelDB，RocksDB and AsterixDB。LSM Tree 与传统的就地更新的索引结构有所不同，首先缓存将所有的写操作缓存在内存中，到达一定的阈值之后顺序地进行刷回到磁盘并进行合并。主要的优点则是较好的写性能，空间利用率高，具有可调性，并发控制和故障恢复机制较简单。所以 LSM Tree 能够为各种负载提供服务，典型代表 RocksDB 就被用于实时数据处理，图处理，流处理和联机事务处理(OLTP)。 由于 LSM 在现代存储系统中的流行，学术界提供了很多关于 LSM Tree的优化方案。本篇论文主要调查了近年来关于 LSM Tree 的相关研究，旨在为LSM研究者们提供一个指南。我们主要针对各个方案对于 LSM Tree的优化方案进行了分类，然后详细介绍各种改进，并讨论其优缺点。同时为了反映 LSM Tree的实际应用情况，我们也调查了五种开源的LSM Tree的NoSQL实现方案。 LSM Tree 基础 LSM Tree 发展历史 索引结构的更新策略 索引结构一般有两种更新策略。就地更新（in-place updates）和异地更新（out-of-place updates）。 就地更新 in-place updates：就地更新的典型代表为 B+树，直接将新数据写入覆盖老数据，因为确保了每次读的时候结果都是最新的，所以是读性能相对较好的，但牺牲了写性能，因为更新操作可能导致随机 IO，随机 IO 的性能一般比顺序 IO 差，除此以外，索引页可能因为删除和更新操作产生大量碎片，使得空间利用率降低。 优点：读性能较好 缺点：写性能较差，空间利用率低 异地更新 out-of updates：对比之下，LSM Tree 这种异地更新的方案，通常都是存储新的数据到新的位置，而不是直接覆盖老旧的数据。这种方案对应地就实现了 IO 的顺序写入，从而提升了写性能。由于没有覆盖老数据，数据恢复流程也得到了简化。但存在的最大问题就是牺牲了读性能，因为数据可能被存储在很多层，这样的结构就要求需要一套单独的数据重排机制来进行提升存储和查询的性能。 优点：写性能较好，数据恢复流程简单 缺点：读性能较差 异地更新/顺序写 思想起源 保证顺序写的异地更新思想早在上世纪七十年代就已经被应用到数据库系统中了，1976 年提出来的 Differential Files[1]，就是将所有的更新首先写入到不同的文件中，然后由主文件周期地对其他文件进行合并。 80年代提出来的 Postgres[2] 则提倡使用日志结构化的数据库存储，通过将所有的写追加写入到顺序的日志文件中，从而支持快速的恢复和基于时间的查询，然后使用一个后台进程持续地对日志文件中的垃圾数据进行回收。该思想对应地后来被应用到文件系统领域，从而充分利用磁盘的写入带宽，例如日志结构化文件系统（LFS）[3]。 日志结构化存储的历史问题 在提出 LSM Tree 之前，日志结构化存储受限于几个关键问题， 数据存储在追加写的日志文件中导致的较低的查询性能。因为许多相关的数据是分散在日志文件中的； 老旧的数据没有被及时删除导致空间利用率不高。尽管设计了很多种数据重组方案，但是仍然缺少一个条理化的模型来分析写成本、读成本和空间利用率之间的权衡，这使得早期的日志结构存储很难进行参数调优。数据重新布局之后就很容易导致系统产生瓶颈。 LSM 的提出 1996 年提出的 LSM Tree [4]则巧妙地解决了上述存在的问题。其中多层结构主要是借鉴 B+ 树，每一个层级都使用了 B+ 树的结构，并提出水平合并的策略 leveling merge policy [5] [6] 来进行数据压缩。最初提出的滚动合并过程由于其实现的复杂性而没有被今天的基于 LSM 的存储系统所使用。LSM 树的原始论文进一步表明，在稳定的工作负载下，当所有相邻组件之间的大小比 Ti = |Ci+1|/|Ci| 相同时，且层数保持不变的情况下，写性能得到了优化。这个原则影响了 LSM 树的所有后续实现和改进。 为了提升 LSM Tree 的并行性，Jagadish 等人提出了类似的结构和逐步合并策略，以获得更好的写性能。它将组件组织成一个 Level，当Level L 充满了 T 组件时，这些 T 组件将合并到 Level L+1 的新组件中。这个策略变成了现在的 LSM-tree 实现中使用的分层合并策略 tiering merge policy [5] [6]。 如今的 LSM Tree 基础结构 如今的 LSM Tree 仍然使用异地更新的策略来减少随机 IO，所有的写操作都首先追加写入到内存组件中，插入和更新操作都是直接写入一个新的键值对，而删除操作则是写入一个标记对来标志一个 Key 对应的 Entry 已经被删除了。然而如今的 LSM Tree 在实现过程中利用了磁盘组件的不变性来简化并发控制和恢复过程。多个磁盘组件被合并到一个新的磁盘组件中，而不用修改已经存在的组件。（区别于原论文提出的滚动合并机制） 针对 LSM Tree 的内部具体实现，内存组件常常使用并发的数据结构来实现，如跳表或者 B+ 树来组织他们的内存组件，磁盘组件一般使用 B+ 树或排序字符串表 SSTables 来组织。一个 SSTable 包含一个数据块列表和一个索引块; 数据块存储按键排序的键值对，索引块存储所有数据块对应的键范围。 对于 LSM-tree 的查询必须搜索多个组件来执行协调，即查找每个键对应的值的最新版本。获取特定键值的点查询可以逐个地从新到旧地搜索所有组件，并在找到第一个匹配项后立即停止。范围查询可以同时搜索所有组件，将搜索结果放入优先级队列中执行协调。 随着时间的推移，磁盘组件越来越多，LSM-tree 的查询性能会下降，因为查询时必须扫描更多的组件。为了解决这个问题，将逐步合并磁盘组件以减少组件的总数。通常有两种合并策略，都将磁盘组件组织成逻辑级别(或层)，并由大小比例 T 控制。两种策略分别为 Leveling Merge Policy 和 Tiering Merge Policy Leveling Merge Policy 每一层只保留一个组件，但层级之间的大小比例仍保持为T，因此，组件在 Level L 将多次与 Level L-1 的传入组件合并，直到它被填满，然后它将被合并到 Level L+1。如图所示，L0 和 L1 合并以后将使得 L1 变得更大，因为每一层只保留一个组件。 Tiering Merge Policy，该种合并策略每一层则维护了 T 个组件，当一层满了之后，对应的 T 个组件将合并成一个新的位于 L+1 层的组件，在图中，Level 0 的两个组件合并在一起，形成 Level 1 的新组件。应该注意的是，如果 Level L 已经是配置的最高级别，那么结果仍然位于Level L。 通常情况下，leveling merge policy 的查询性能更好，因为查询时需要检索的组件数更少，而 tiering merge policy 的写性能更好，因为减小了合并的频率。 Leveling Merge Policies 典型应用 RocksDB. 具体过程可以参考 RocksDB 的合并实现 Leveled Compaction：https://github.com/facebook/rocksdb/wiki/Leveled-Compaction Tiering Merge Policy 典型应用 Cassendra 具体过程可以参考 https://www.scylladb.com/2018/01/17/compaction-series-space-amplification/ 参考资料 [1] 知乎：LSM Tree的Leveling 和 Tiering Compaction 一些其他方面的优化方案 有两种在绝大多数 LSM 存储中都使用到的优化方案：BloomFilter 和 Partitioning。 BloomFilter 一种节省空间的概率数据结构，用于响应集合成员查询。针对该数据结构的介绍请参考其他资料，此处不再赘述。 布隆过滤器主要是两个操作，一个是插入一个Key，一个是检查一个键的成员关系（是否被包含） 插入操作的过程主要是使用多个 HASH 函数将 Key 映射到一个位向量中的多个位置，并将这些位置的位设置为 1。 检查该 Key 是否存在时则仍先把 Key 给 HASH 到多个位置，如果所有的位置对应的位的值都为 1，那么说明该 Key 大概率存在。如果不全为 1，则该 Key 肯定不存在。 布隆过滤器在 LSM 中最主要的应用，可以在磁盘组件的基础上进行构建从而显著提高点查询的性能。要扫描磁盘组件时，首先查询布隆过滤器，如果布隆过滤器判断该 Key 存在时才对应地去搜索磁盘组件对应的数据结构，如 B+ 树。还可以将布隆过滤器应用到具体的叶子节点对应的 Page 上。一个点查询首先通过查询 B+ 树查询非叶子节点页（非叶子节点页应该尽可能小，便于缓存在内存中加速检索），对应的定位到叶子节点的 Page，在真正执行取页之前首先利用布隆过滤器判断该页中是否包含该 Key，如果不包含则对应地不取该页，从而减少磁盘 IO 次数。由于布隆过滤器是概率型数据结构，还是会有不存在的Key检索引起不必要的IO操作的现象，量化方式可以参见原文，通常使用 10bits/key 作为默认配置，对应的错误率约为 1%。 布隆过滤器由于其所占存储空间较小，通常会被缓存在内存中来减小查询过程中的 IO 次数。 Partitioning 另一个常用的优化方法是将 LSM 树的磁盘组件划分为多个(通常是固定大小的)小分区，LevelDB 中的 SSTable 文件就是如此。 优势在于，首先，分区将大型组件合并操作分解为多个较小的操作，限制每个合并操作的处理时间以及创建新组件所需的临时磁盘空间，此外，分区可以通过仅合并具有重叠键范围的组件来适应顺序创建键或 skewed updates 的工作负载。对于顺序创建的键，基本上不执行合并，因为不存在键范围重叠的组件；对于 skewed updates，具有冷更新范围的组件的合并频率可以大大降低。传统的 LSM Tree 的滚动合并机制已经利用了分区的优势，然而，由于其滚动合并的实现复杂性，目前的 LSM-tree 实现通常选择实际的物理分区，而不是滚动合并。 LSM 树的分区应用中常常使用到了 PE[7] 文件，一个 PE 文件包含很多分区，每个分区在逻辑上可以被看作是一个独立的 LSM 树，当一个分区变得太大时，可以将它进一步划分为两个分区。但是，这种设计强制了分区之间严格的键范围边界，降低了合并的灵活性。 至于当今的 LSM-tree 实现中使用的分区优化，需要注意的是，分区与合并策略是正交的;可以调整 Leveling Merge Policy 和 Tiering Merge Policy (以及其他新兴的合并策略)来支持分区。据我们所知，只有分区调平策略 partitioned leveling policy 被工业的基于 LSM 的存储系统(如LevelDB[8] 和 RocksDB[9] )完全实现。最近的一些论文提出了不同形式的分区分层合并策略，以获得更好的写性能。 由 LevelDB 提出的分区 Leveling Merge Policy 中，每一层的磁盘组件被分成了确定大小的 SSTables。如图所示，其中 Level0 由于直接是内存刷回的，所以没有进行分区，这种设计还可以帮助系统吸收写突发，因为它可以在 Level0 容忍多个未分区组件,要将一个 SSTable 从 Level L 合并到 Level L + 1，需要选择它在 Level L + 1 上的所有重叠的 SSTable，将 SSTables 与它合并，生成仍然处于 Level L + 1 的新 SSTables。以图为例，Level1 层的范围为 0-30 SSTable 要和 Level2 层的范围分别为 0-15 和 16-32 SSTables 进行合并。合并之后在 Level2 层上的分布则为 0-10，11-19，20-32；旧的 SSTable 将被垃圾回收，可以使用不同的策略来选择下一步在每个级别合并哪个 SSTable。LevelDB 利用了轮询的机制来减小总的写开销。RocksDB 选择要压缩的文件方式为：https://github.com/facebook/rocksdb/wiki/Choose-Level-Compaction-Files 分区优化方案也可以应用到 Tiering Merge Policy 中，这样做的一个主要问题是，每个级别可能包含多个具有重叠键范围的SSTables。必须根据最近的时间对这些 SSTables 进行适当的排序，以确保正确性。可以使用两种可能的方案来组织每个级别的SSTables，即垂直分组和水平分组。 垂直分组方案将具有重叠键范围的 SSTables 分组在一起，从而使组具有不相交的键范围。可以理解为 Leveling Merge Policy 的扩展来支持 Tiering。如图所示，垂直分组是将具有重叠键范围的 SSTables 分为一组，然后不同的组之间范围不重叠，在合并操作期间，将组中的所有 SSTables 合并在一起，根据下一层重叠组的键范围生成结果 SSTables，然后将这些键范围添加到这些重叠组中。举例说明就是，图中 Level1 里的第一组的 0-31 和 0-30 的两个 SSTable 文件合并到了一起生成 0-12 和 17-31 两个新的 SSTable 文件，然后将被添加到 Level2 中具有重叠键的组中。在合并之前，0-30 和 0-31 两个 SSTable 有重叠的键范围，点查询对这两个 SSTable 都将进行扫描，然而在合并之后，变成了 0-12 和 13-31 两个不重叠的 SSTable，点查询将只会扫描其中一个 SSTable 文件。还应该指出，在这个方案下，SSTables 不再是固定大小的，因为它们是根据下一层重叠组的键范围产生的。 而水平分组方案中，每个逻辑磁盘组件都被分区到一组 SSTables 中，直接作为一个组，这允许基于 SSTables 单元增量地形成磁盘组件。通过将水平分组划分为一组固定大小的 SSTables，直接充当逻辑组，每一层都会包含一个活跃的 group，也适用于接收上层发送的需要合并的 SSTable 数据的第一个组，在未分区的情况下，这个活动组可以看作是在 L - 1 级合并组成的部分组件，合并操作从某个级别的所有组中选择具有重叠键范围的SSTables，并将生成的 SSTables 添加到下一个级别的活动组中。如图中的例子所示，Level1 的 35-70 和 35-65 SSTables 被合并到一起，新生成的 35-52 和 53-70 被添加到 Level2 中的第一组中。即便在水平分组方案中的 SSTable 大小是固定的，组中的一个SSTable可能会与其余组中的大量SSTable重叠，这仍然是可能的。 Concurrency control and recovery 对于并发控制，LSM-tree需要处理并发读和写，并处理并发刷新和合并操作。确保并发读写的正确性是数据库系统中访问方法的一般要求。根据事务隔离需求，目前的 LSM-tree 实现要么使用加锁的方式，要么使用多版本控制。多版本控制方案能在 LSM 中比较好地进行应用，因为针对旧版本的 Key 和数据，可以利用垃圾回收机制进行垃圾回收。而并发的刷新和合并操作是 LSM 系统独有的操作，这些操作会修改 LSM Tree 的元数据，如活跃的组件列表等。因此，访问组件元数据的操作必须正确地进行同步，为了避免一个正在被使用的组件被删除，每个组件必须维护一个计数器进行引用计数，在访问一个组件之前，查询操作必须获取组件的快照并对该组件的使用计数进行+1操作。 因为所有的写操作首先都是先写入到内存中，WAL 机制主要用于保证数据的持久化。为了简化数据的恢复流程，现有系统通常采用无窃取缓冲区管理策略。也就是说，只有在所有活动的写事务都已终止时，才能将内存组件刷回，在对 LSM-tree 进行恢复期间，将重播事务日志以 redo 所有成功的事务，但是由于使用了no-steal策略，所以不需要撤消 undo。同时，在发生崩溃时，还必须恢复活动磁盘组件的列表。对于未分区的 LSM Tree，可以通过向每个磁盘组件添加一对时间戳来实现，该时间戳指示所存储条目的时间戳范围，此时间戳可以简单地使用本地时钟时间或单调递增的序列号生成，要重构组件列表，恢复过程只需找到所有具有不相交时间戳的组件，如果多个组件具有重叠的时间戳，则选择时间戳范围最大的组件，其余的则可以简单地删除，因为它们将被合并形成所选的组件。对于分区 LSM Tree，使用一种传统的方式，在 LevelDB 和 RocksDB 中广泛应用的方式，即维护一个单独的元数据日志文件来存储所有结构型元数据的变化，例如添加和删除 SSTables。在故障恢复时可以通过重播元数据日志文件来恢复 LSM Tree 的结构。 redo log 和 undo log 区别： https://zhuanlan.zhihu.com/p/35574452 开销分析 为了理解 LSM Tree 在性能上做的权衡，我们可以通过分析写操作，点查询，范围查询和空间放大带来的开销。衡量写和查询的开销的方式是统计每一次操作对应的磁盘 IO 次数，此处针对未分区的 LSM Tree 进行了分析，同时讨论了开销最大的情况。 预先假设 假设 LSM Tree 有 L 层。每一层之间的大小比例为 T，对于一个稳定的 LSM Tree 来说（稳定是指插入和删除的数据大小相等），L 通常是不变的。假设 B 为页的大小，即每个数据页可以存储的 KV 条目数，假设 P 是内存组件中的页数。所以内存组件中最多包含 B*P 个键值对，那么第 i 层中将包含最多 $T^{i+1} * B * P $ 个键值对。给定 N 个键值对，那么最大层大约有 $ N * \\frac{T}{T+1}$ 个键值对，因此建立等式，可以得到层数大约为以 T 为底的 log⁡T(NB∗P∗TT+1)\\log_T(\\frac{N}{B * P} * \\frac{T} {T + 1})logT​(B∗PN​∗T+1T​)。 写开销 写开销（部分文献以写放大来衡量）主要是指插入一个键值对到 LSM Tree 中的平均开销，应该注意的是，这个成本度量了将这个条目合并到最大级别的总体 I/O 成本，因为将一个条目插入到内存中不会导致任何磁盘 I/O。 在 Leveling Merge Policy 中，每一层的组件在被该层数据满之前都会被合并 T - 1 次，然后传到下一层 Tiering Merge Policy 中每一层的多个组件只会合并一次然后直接传到下一层。 因为每一个页包含 B 个键值对，每一个键值对的写开销对于 Level 方案将为 O(T∗LBT * \\frac{L}{B}T∗BL​)，对于 Tiering 策略将为 O(LB\\frac{L}{B}BL​) 点查询开销 查询的 IO 开销将取决于 LSM Tree 的组件数量。如果没有布隆过滤器，点查询的 IO 开销将为 O(L) Level 和 O(T * L) Tiering，然而布隆过滤器也可能提高点查询的开销，诸如一些无效的查询，查询一些不存在键对应的数据，布隆过滤器将增加查询的开销。假设所有的布隆过滤器都有 M 个 bit 位，每一层都有相同的误报率，N 个 Key，每一个布隆过滤器的误报率约为 O(e−MN)O(e^{-\\frac{M}{N}})O(e−NM​)，那么无效查询的开销约为 对于 Level - O(L∗e−MN)O(L * e^{-\\frac{M}{N}})O(L∗e−NM​)，对于 Tiering - O(T∗e−MN)O(T * e^{-\\frac{M}{N}})O(T∗e−NM​)。而至于查询有效的 Key，至少要执行一次 IO 操作才能得到对应的键值对。实际应用中布隆过滤器的误报率远小于 1，所以一次成功的点查询开销无论是 Level 还是 Tiering 均为 O(1) 范围查询开销 范围查询的开销取决于查询范围的选择，假设一次范围查询访问的 Key 的数量为 s，如果 sB&gt;2∗L\\frac{s}{B} &gt; 2 * LBs​&gt;2∗L，则认为该范围查询较 long，否则较 short。差别主要为 long range query 的开销主要受最高 Level 影响，因为最高 Level 包含所有数据的绝大部分，而相对而言 short range query 会受每一层的影响，因为查询会向每个磁盘组件发起一次 IO。因此 long range query 的开销为 Level - O(sB)O(\\frac{s}{B})O(Bs​) 和 Tiering - O(T∗sB)O(T * \\frac{s}{B})O(T∗Bs​)。short range query 的开销约为 Level - O(L) 和 Tiering - O(T * L) 空间放大系数 我们量化了 LSM Tree 中的空间放大，空间放大即为最终的键值对总数除以键值对单位数。Level 中，最糟糕的情况为所有的数据都位于 L - 1 层时，大约包含总数据量的 1 / T，此时更新数据然后写入到 L 层，因此该情况下的空间放大系数为 O(T+1T)O(\\frac{T + 1}{T})O(TT+1​)。而对于 Tiering，最坏的情况为在最高层额磁盘组件都包含相同的键集合，此时的空间放大系数将为 O(T)O(T)O(T)。实际环境中，空间放大是一个部署存储系统过程中需要考虑的重要的因素，因为它直接决定了在给定负载的情况下的存储开销。 总结 通过总结不同合并策略下的 LSM Tree 的开销，可以观察到层级之间的大小比例 T 对开销的影响。对于 Leveling，通过每层维护一个磁盘组件，使得查询性能和和空间放大方面表现较好，但因为磁盘组件不可避免地需要经常进行合并，所以写性能上将严重受层级比例 T 的影响。相比之下， Tiering 的合并策略，主要时在每层维护 T 个磁盘组件，写性能表现较好，但对应地其查询性能和空间放大的开销都会严重受层级比例 T 的影响。 LSM Tree 是具有很大的调整空间的，例如改变合并策略从 Leveling 到 Tiering，可以极大地提高写性能，但对点查询的负面影响因为布隆过滤器的应用可以变得很小，但是，范围查询和空间利用率将受到显著影响。最近关于改进 lsm tree 的文献中的每种方法都要进行一定的性能权衡，主要是在读开销 R, 写开销 U 和存储开销 M 上进行权衡。 LSM-Tree 的优化方案 我们提出了一个分类法，用于对现有的改善lsm树的研究工作进行分类。然后，我们提供了LSM-tree文献的深入调查，这些文献遵循了提出的分类法的结构。 LSM Tree 改进方案的分类 尽管LSM-tree在现代NoSQL系统中非常流行，但是基本的LSM-tree设计却存在着各种各样的缺陷和不足。现在，我们确定了基本LSM-tree设计的主要问题，并根据这些缺陷进一步提出了LSM-tree改进的分类。 优化方案的分类 写放大：尽管 LSM Tree 相比于如 B+ 树这种就地更新的数据结构可以提供更好的写性能，因为减少了随机 IO，即便是在 LevelDB 和 RocskDB 中广泛应用的 Leveling 合并策略，仍然会引起相对较为严重的写放大问题，严重的写放大问题不仅限制了 LSM Tree 的写性能，也因为频繁的磁盘写操作减少了 SSD 的寿命，大量的研究致力于减小 LSM Tree 的写放大问题。 合并操作：合并操作对于 LSM Tree 的性能影响至关重要，因此需要仔细实现，另外，合并操作可能还会给整个系统带来负面影响，包括在大量合并过程后出现的缓冲区缓存不命中（合并之后导致的缓存失效）。一些优化方案在合并策略上做了一些改进来解决合并过程带来的性能问题。 硬件：为了最大化性能，LSM Tree 必须仔细实现以充分利用硬件平台。传统的 LSM Tree 是基于磁盘设计，目标为减少随机 IO。近年来，新的硬件平台给数据库系统提出了新的机会以实现更好的性能，所以最近大量研究致力于通过充分利用硬件平台来提升 LSM Tree 的性能，主要涉及大内存、多核、SSD/NVM 和 native storage。？ 特殊负载：针对确定特殊负载的优化也能实现较好的性能，需要根据负载的特征对 LSM Tree 进行调整和定制。 自动调整：基于 RUM [10]猜想，没有方法可以同时对读、对写和对空间利用率进行优化。对于给定的工作负载，LSM 树的可调优性是实现最佳权衡的一个很有前途的解决方案，然而，由于有太多的调优选项和参数，例如内存分配、合并策略、大小比等，所以很难对 LSM 树进行调优。为了解决这个问题，文献中提出了几种自动调优技术。 二级索引：给定的 LSM-tree 只提供一个简单的键-值接口。为了支持对非键属性的查询的有效处理，必须维护辅助索引。该领域中一个问题就是如何高效地维护一组关联二级索引，且在写性能方面损耗较小。各种基于 LSM 的二级索引结构和技术也进行了设计和测试。 基于这些针对基础 LSM Tree 设计的主要问题，我们提出了一种分类方法来强调现如今的研究试图优化的方向，如图所示。根据这种分类，表2 进一步根据每个改进的主要和次要关注点对LSM-tree改进进行了分类 减小写放大 在本节中，我们将回顾文献中旨在减少 LSM 树写放大的改进。大多数这些改进都是基于 Tiering 的，因为 Tiering 的写性能比 Leveling 好得多。其他提出的改进已经开发了执行 Merge Skipping 或利用数据倾斜的新技术。 Tiering 从前文的叙述中。我们可以知道 Tiering 的比 Leveling 的写放大系数更小，但是会导致更差的查询性能和空间放大问题。基于 Tiering 的优化方案基本都是在第 2.2.2 节提到的水平或垂直分区 Tiering 设计基础上进行了一定的修改优化。Partitioned tiering with vertical grouping 或者 Partitioned tiering with horizontal grouping WriteBuffer Tree(WB-Tree)：一种垂直分组 Tiering 设计的变体。主要做了以下修改，首先，利用 HASH 分区来实现了负载均衡，从而使得每个 SSTable Group 存储等量的数据；其次，该方案将 SSTable Group 组织成一个类似于 B+ 树的结构来利用 B+ 树的自平衡来减小 LSM 树的层数。具体而言，每一个 SSTable Group 将被当作一个 B+ 树中的节点。当一个非叶子节点满了之后，即拥有了 T 个 SSTable 之后，这 T 个 SSTables 将合并到一起形成新的将要被添加到其子节点中的 SSTables；当一个叶子节点满了之后，将合并后的 SSTables 按照范围拆分成两个叶子节点，所以每个叶子节点将会收到大约 T/2 个 SSTables。 Lightweight Compaction Tree(LWC-Tree)：同样是垂直分组 Tiering 设计的变体，进一步提出了一种实现 SSTable Group 负载均衡的方法，回顾前文中提到的垂直分组模式下，SSTable 不再时严格固定大小的，因为 SSTable 是基于下一层的重叠键范围产生的。该方案中，如果一个 Group 包含了太多键值对，将在该组被合并之后缩减改组的范围并扩充其兄弟 Group 的范围来实现负载均衡。 PebblesDB：垂直分组 Tiering 设计的变体，主要的不同是该方案收跳表的启发使用了 Guard（哨兵）的思想来确定 SSTable 的键范围。Guards ，即 SSTable 的键范围，将基于插入的键被概率性地选择，从而实现负载均衡。一旦一个 Guard 被选中，他将在下一次合并过程中被应用。PebblesDB 进一步实现了 SSTables 的并行查询来提升范围查询的性能。 dCompaction：引入了虚拟的 SSTable 和虚拟的合并操作以减少合并的频率。一个虚拟的合并操作生成虚拟的的 SSTable，该 SSTable 可能指向多个实际输入的 SSTables，但不执行真正的合并操作。但因为一个虚拟的 SSTable 指向了具有重叠键范围的多个实际 SSTables，查询性能将有一定的损失。为了解决这个问题，dCompaction 基于实际的 SSTable 数量引入了一个阈值来控制触发实际合并操作的时机。如果查询过程中发现一个虚拟 SSTable 指向太多 SSTables，也是会触发合并操作的。总之，dCompaction 方案就是将合并操作进行了延迟直到多个 SSTables 可以被合并到一起时，因此该方案可以被看成 Tireing 合并策略的变体。 以上描述的四种方案都是基于相似的结构，垂直分组 Tiering 设计，区别主要在于负载均衡的实现方式。例如 WB-Tree 使用了 HASH，但也就在范围查询方面做了舍弃。LWC-Tree 动态调整了 SSTable Group 的范围，PebblesDB 依赖概率性的 Guards 机制。相比之下，dCompaction 不提供内置的结构来实现负载均衡，而使用算法层面的优化策略来实现。不清楚倾斜的 SSTable Group 如何影响这些结构的性能，需要进一步的研究来理解这个问题并测试这些负载均衡策略。 Zhang et al. 和 SifrDB 则是基于水平分组 Tiering 设计的。SifrDB 还提出了一种早期清理技术，以减少合并期间的磁盘空间利用率。在合并操作期间，SifrDB 增量地激活新生成的SSTables，并停用旧的 SSTables。SifrDB 进一步利用 I/O 并行性，通过检查多个 SSTables 来提高查询性能。 Merge Skipping skip-tree：提出了一种 Merge Skipping 的想法来提升写性能。传统的 LSM Tree 中，观察结果是，每个条目必须从0级向下合并到最大级别。如果某些条目可以通过跳过逐层合并直接推到更高的级别，这样总写成本就会降低。如下图所示，在 L 级的合并过程中，skip-tree 直接将一些键推入 L + K级的可变缓冲区，这样就可以跳过一些逐级合并。同时，在随后的合并过程中，可调缓冲区中的跳过项将与L + K 级别的 SSTables 合并。为了确保正确性，只有在 L +1 到 L + K - 1 的任何一个中间级别中都没有出现该键时，才可以将该键从 L 级推到 L + K 级。如何判断中间层是否存在该键，可以通过每一个 Level 总的布隆过滤器进行高效地判断。该方案进一步实现了写前日志 WAL 来保证存储在可变缓冲区中的数据的持久性。为了减小日志的开销，skip-tree 只记录了 key 和原始 SSTable 的 ID，同时要避免 SSTable 被删除，如果该 SSTable 被可变缓冲区中的任何 Key 所引用的话将不能删除该 SSTable。即便 skip-tree 是一个在减小写放大方面很有意思的一个方案，但该方案为了管理可变缓冲区引入了较高的实现复杂度。此外，该方案本质是减少中间层的合并，还不清楚相比于可调 LSM Tree 中的减小大小比例的效果如何。 Exploiting data skew TRIAD：针对倾斜更新负载减小了写放大，倾斜更新负载是指一些热点数据经常被更新。主要思想是在内存组件中将冷热数据键进行分离，从而保证只有冷数据键会被刷回到磁盘组件中。所以，当热数据键被更新时，该键对应的老版本的数据可以直接被丢弃，不用写回到磁盘。即便热数据没有刷回到磁盘，但是回周期性地拷贝到新的事务日志中，老的事务日志就可以进行回收。该方案也通过采用延迟 Level0 层的合并操作（直到 Level 0 含有许多 SSTables）来减小写放大。该方案提供了一种优化，避免在刷新之后创建新的磁盘组件，取而代之的是，事务日志被当作磁盘组件并在其上构建索引结构提升查询性能。然而，范围查询性能仍然会受到消极的影响，因为存储再日志文件中的键值对不是有序的。 总结 这类中的所有改进，以及后面几节中的一些改进，都声称它们可以极大地提高 LSM Tree 的写性能，但是它们的性能评估常常没有考虑到 LSM Tree 的可调性。也就是说，这些改进主要是针对 LevelDB 或RocksDB 的默认(未调优的)配置进行评估的，它们使用大小比为 10 的 leveling 合并策略。目前还不清楚这些改进与可调的 LSM-trees 相比效果如何。为了解决这个问题，一个可能的解决方案是调整 RocksDB 的参数（调整层级之间的大小比例或者采用 Tiering 合并策略） 以实现与上文提到的优化方案基本相同的写吞吐量，然后测试这些优化方案提高查询性能和减小空间放大的效果。此外，这些优化方案主要研究了查询性能，空间放大问题往往被忽视。这将是一个有用的实验研究，以充分评估这些改进与调优的基线 LSM Tree，以评估它们的实际用途。我们也希望在未来的研究中可以避免这种情况，在评估所提出的改进时考虑 LSM 树的可调性。 优化合并操作 提升合并性能 Improving merge performance VT-tree：提出了一种改进合并性能的拼接方法。基本思想是，在合并多个SSTables时，如果一个页面的键范围来自一个输入 SSTable，且该 数据页 和来自别的 SSTable 的数据页的键范围没有重叠时，就可以让 SSTable 简单地指向这个页面，而无需再次读取和复制。尽管拼接可以提高某些工作负载的合并性能，但它也有一些缺点。首先，因为数据页不再连续第存储在磁盘上，很容易产生碎片。为了减小碎片化，VT-tree 引入了一个拼接阈值 K，当至少有来自同一个 SSTable 的 K 个连续的页时，才会触发拼接操作。此外，因为合并过程中，拼接的页中的键不再会被扫描，对应的布隆过滤器也不会被创建，为了解决这个问题，VT-tree 使用了 quotient filters，因为多个 quotient filters 可以直接组合而不需要访问原始键。 **Zhang et al. **：提出了一种流水线的合并操作实现从而更好地利用 CPU 和 IO 的并行性来提升合并性能。观察发现合并过程包含很多个阶段，包括读阶段、归并排序阶段和写阶段，读阶段从输入的 SSTables 中读取数据页，在归并排序阶段，读出来的数据页将被归并排序并生成新的数据页，最后在写阶段新的数据页将被写回磁盘。因此，读阶段和写阶段过程中，IO压力比较大，而归并排序阶段，CPU压力较大。为了更好地利用 CPU 和 IO的并行性，提出了一种方案将上述三个阶段的执行流水线化，如图所示。在该例中，在第一个输入页被读取之后，该方法持续地读第二个输入页（利用磁盘），第一个数据页相应地执行归并排序（利用CPU）。 减小缓冲缓存不命中 （提升缓存命中率） 合并操作会对系统的缓存行为产生影响，当生成新的组件时，查询操作可能会经常出现缓存miss的情况，因为新组件还没来得及缓存，简单的write-through缓存策略没法解决这样的问题。新组件的所有数据页都将在合并操作中被缓存，从而导致缓存中其他有效的数据页可能被淘汰，这些数据页被淘汰之后又会造成缓存不命中。 Ahmad et al 研究了合并操作对整个系统的性能影响，研究表明合并操作会消耗大量的 CPU 和 I/O 资源，并在查询响应时间方面带来极大的开销。为了解决这个问题，该方案提出通过 offload 合并操作放在远程服务器上执行以减小影响，合并操作完成后，设计一个智能的缓存预热算法来获取新组件的数据从而减小缓存丢失率。核心思想是增量地切换到新组件，一个块接着一个块，以便顺利地将传入的查询从旧组件重定向到新组件。最终，缓冲缓存未命中的突发被分解成大量的更小的突发，从而最小化了组件切换对查询性能的负面影响。 LSbM-tree：上述方法的局限是合并操作必须卸载到独立的服务器上，随后发现，由于新生成的页面和现有的热页面之间存在争用，仅使用缓存增量预热算法是不够的，为了解决这个局限，新提出了方案LSbM-tree。如图所示，L 层的 SSTable 合并到 L+1 层时，老旧的 L 层的 SSTable 将不会被立马删除而是追加写入到事先分配给 L+1 层的缓冲区中，L+1 层的就SSTable 将不会被追加到缓冲区中，因为此时 L+1 层的旧 SSTable 的数据是来自于更上层的，且已经被缓存过。查询的时候将会检索到被缓存的 SSTable 以提升缓存命中率，同时根据这些缓存中的 SSTable 的访问频率渐渐被删除，该方法不会造成任何额外的磁盘IO，因为该方案只是延迟了老 SSTable 的删除时机。然而该方法只针对那种只有小部分的数据会被经常访问的倾斜负载比较有效。对于没有被缓存的冷数据反而会引入额外的访问开销，尤其是那些不能使用Bloom过滤器判断的操作，如范围查询。 最小化Write Stalls（写停顿） 尽管 LSM 相比于 B+ 树提供过了很高的写吞吐量，但因为后台任务中大量繁重的操纵如刷回、合并等，LSM 经常会出现写停顿以及一些不可预知的写延迟。 bLSM：该方案针对未分区的 leveling 合并策略提供了一种 spring-and-gear 的合并调度器，以尽可能减少写停顿。它的基本思想是在每个 Level 上引入一个额外的组件，以便在不同 Level 上的合并可以并行进行，而合并调度器控制整个合并的过程以确保 Level L 生成新组件（要合并到 Level L+1 中的组件）的过程在 Level L+1 上执行的合并完成之后进行。这最终会限制内存组件的最大写速度，从而减少大量的写停顿。但是，该方案也有自己的局限，即仅针对未分区的 leveling 合并策略；该方案只是限制了写入内存组件的最大延迟，而队列延迟(通常是性能变化的主要来源)被忽略了。 总结 该分类下针对 LSM Tree 的优化方案主要表现在对合并操作的优化，主要涉及性能、缓存命中率和写停顿等方面。VT-Tree 为了避免输入数据页的复制引入了拼接操作，但可能引入较多对硬盘不友好的碎片，同时该方案与布隆过滤器不兼容。Pipelined merge 通过利用 CPU 和 IO 的并行性较大地提升了合并性能，业界中也已经有很多优化方案使用了流水线相关技术，通过利用磁盘的预读和延迟写等操作。 Ahmed et al. 和 LSbM-tree 则展示了两种缓解由于合并操作造成的缓存不命中的方法，但两种方法都有较为明显的局限，前者需要专门的服务器做合并操作，后者延迟了老旧组件的删除可能会给冷数据的查询带来额外的性能开销。 写停顿是 LSM Tree 中独有的问题，因为其异地更新的机制。bLSM 是唯一在该问题上做研究的方案，但是该方案只是限制了内存组件的最大写延迟，端到端的延迟还是有很大的变数，因为其中排队延迟占了较高的比重。 基于新硬件的优化 LSM 的优化方案对应的硬件平台大致有大容量内存、多核、SSD/NVM 以及本地存储。基于硬件的优化方案都遵循着一种范式----修改 LSM 的基础设计从而尽可能地利用硬件平台的性能。 大容量内存 大容量内存肯定是有效的优化方案，不仅能减少 LSM 的磁盘组件层级数，在读写性能方面都能有很大提高。但是，对大容量内存场景下的内存组件的管理也面临着一些挑战。如果内存组件是直接使用的堆上数据结构来实现的话，大容量内存可能会产生大量的小对象，导致频繁的GC操作。相反，如果使用堆外数据结构实现的话，例如并发 B+ Tree，大容量内存会导致较大的查询开销，因为大容量内存意味着 B+ 树的深度会很大，在写的时候也可能造成大量的 CPU 缓存不命中等情况，因为写操作必须先查询到对应地址。 FloDB：设计了一个两层结构来管理内存组件，上层是一个较小的 HASH 表来支持快速写，下层是一个大的跳表来支持高效的范围查询。HASH 表满了之后，将使用一个批处理算法将键值对高效地迁移到跳表中。通过将随机写限制在很小的一个内存区域，显著地提高了内存中的写吞吐量。FloDB 要求范围查询必须等待哈希表被清空，以便仅搜索跳表来响应查询。然而存在两个主要的问题，对于同时包含写操作和范围查询的负载，因为可能存在写操作和范围查询操作频繁切换的情况，所以该方案对这种负载的优化效果不好。其次，跳表可能占用大量的内存空间，导致内存的利用率不高。 Accordion：为了解决 FloDB 方案中的问题，该方案设计了一个多层的架构来管理内存组件。如图所示，设计了一个很小的 mutable 内存组件在最上层来处理写操作，当该层满了以后，不刷到磁盘，而是使用一个内存中的 flush 操作刷入到 immutable 内存组件，相似地，immutable 内存组件也可以在内存中进行合并操作来提高查询性能，并回收老旧的键值对对应的空间（垃圾回收）。需要注意的是，内存中的刷回和合并操作不会引入任何磁盘 IO，所以通过利用大容量内存可以降低磁盘 IO 开销。 多核 cLSM：针对多核机器进行了优化，并针对不同的 LSM 操作提供了新的并发控制算法。该方案将 LSM 组件放置在一个并发链表中以减小由于同步操作造成的阻塞。刷回和合并操作经过精心设计，因此它们只会对链表进行原子性修改，而不会阻塞查询。当内存组件满了之后，在内存组件将被刷回的时候会创建一个新的内存组件。为了避免此时的写操作将数据插入到旧的内存组件中，写入操作在进行修改之前需要获取一个共享锁，刷回线程在刷回之前需要一个独占锁。cLSM 通过多版本控制和原子读-修改-写操作进行快照扫描，并使用乐观并发控制方法，该方法利用了所有写操作都涉及到内存组件这一事实。 SSD/NVM 区别于传统的机械硬盘只是在顺序 IO 方面表现高效，以 SSD/NVM 为代表的新型存储设备在随机 IO 方面也表现较好。NVM 还提供了高效的字节寻址随机访问策略，并提供了数据持久化保证。 FD-tree：使用了一个类似 LSM Tree 的设计来减小 SSD 上的随机写操作。一个主要的区别是FD-tree利用分级级联来提高查询性能，而不是使用Bloom过滤器。对于每一层的组件，FD-tree还存储了指向下一层每个页面的指针。如图12所示，Level 1 的键 1、27、51、81 指向了 Level 2 的数据页。在 Level 0 层执行了一个二分查找之后，查询操作可以通过指针遍历所有的层级。然而该方案为合并操作引入了额外的复杂度，Level L 的组件合并到 L+1 层时，所有 0 到 L-1 层也都必须合并从而才能重建对应的页面指针。但由于没有了布隆过滤器，对于不存在的键的点查询可能就会造成磁盘 I/O。正是因为这些原因，现代 LSM Tree 的设计都更喜欢采用 BloomFilter 而不是 分级级联。 FD+tree：主要提升了 FD-Tree 中的合并性能。当从 Level 0 向 Level L 合并时，FD-Tree 的方案导致Level 0 到 Level L 层都需要创造新组件，将临时导致空间放大问题。为了解决这个问题，FD+tree 增量地激活新组件，并从没有被任何活跃的查询操作使用的旧组件中回收页面。 Evaluation References [1] Severance, D.G., Lohman, G.M.: Differential files: their application to the maintenance of large databases. ACM TODS1(3),256–267 (1976)] [2] Stonebraker, M.: The design of the Postgres storage system. In:VLDB, pp. 289–300 (1987) [3] Rosenblum, M., Ousterhout, J.K.: The design and implementation of a log-structured file system. ACM TOCS10(1), 26–52 (1992) [4] O’Neil, P., et al.: The log-structured merge-tree (LSM-tree). ActaInf.33(4), 351–385 (1996) [5] Dayan, N., Idreos, S.: Dostoevsky: Better space-time trade-offs forLSM-tree based key-value stores via adaptive removal of superflu-ous merging. In: ACM SIGMOD, pp. 505–520 (2018) [6] Dayan, N., et al.: Monkey: optimal navigable key-value store. In:ACM SIGMOD, pp. 79–94 (2017) [7] Jermaine, C., et al.: The partitioned exponential file for databasestorage management. VLDBJ16(4), 417–437 (2007) [8] LevelDB.http://leveldb.org/ [9] RocksDB.http://rocksdb.org/ [10] Athanassoulis, M., et al.: Designing access methods: the RUMconjecture. In: EDBT, vol. 2016, pp. 461–466 (2016) Problems Extend ","link":"https://blog.shunzi.tech/post/vldbj-2018lsm-based-storage-techniques-a-survey/"},{"title":"FAST 2017-2019","content":" Paper bewtween FAST 2017 and FAST 2019 To know about hot topic in storage. Will update to recent FAST. FAST 2019 Persistent Memory Systems 1. Reaping the performance of fast NVM storage with uDepot. Source Abstract Many applications require low-latency key-value storage, a requirement that is typically satisfied using key-value stores backed by DRAM. Recently, however, storage devices built on novel NVM technologies offer unprecedented performance compared to conventional SSDs. A key-value store that could deliver the performance of these devices would offer many opportunities to accelerate applications and reduce costs. Nevertheless, existing key-value stores, built for slower SSDs or HDDs, cannot fully exploit such devices. In this paper, we present uDepot, a key-value store built bottom-up to deliver the performance of fast NVM block-based devices. uDepot is carefully crafted to avoid inefficiencies, uses a two-level indexing structure that dynamically adjusts its DRAM footprint to match the inserted items, and employs a novel task-based IO run-time system to maximize performance, enabling applications to use fast NVM devices at their full potential. As an embedded store, uDepot's performance nearly matches the raw performance of fast NVM devices both in terms of throughput and latency, while being scalable across multiple devices and cores. As a server, uDepot significantly outperforms state-of-the-art stores that target SSDs under the YCSB benchmark. Finally, using a Memcache service on top of uDepot we demonstrate that data services built on NVM storage devices can offer equivalent performance to their DRAM-based counterparts at a much lower cost. Indeed, using uDepot we have built a cloud Memcache service that is currently available as an experimental offering in the public cloud. 2. Optimizing Systems for Byte-Addressable NVM by Reducing Bit Flipping. Source Abstract New byte-addressable non-volatile memory (BNVM) technologies such as phase change memory (PCM) enable the construction of systems with large persistent memories, improving reliability and potentially reducing power consumption. However, BNVM technologies only support a limited number of lifetime writes per cell and consume most of their power when flipping a bit's state during a write; thus, PCM controllers only rewrite a cell's contents when the cell's value has changed. Prior research has assumed that reducing the number of words written is a good proxy for reducing the number of bits modified, but a recent study has suggested that this assumption may not be valid. Our research confirms that approaches with the fewest writes often have more bit flips than those optimized to reduce bit flipping. To test the effectiveness of bit flip reduction, we built a framework that uses the number of bits flipped over time as the measure of &quot;goodness&quot; and modified a cycle-accurate simulator to count bits flipped during program execution. We implemented several modifications to common data structures designed to reduce power consumption and increase memory lifetime by reducing the number of bits modified by operations on several data structures: linked lists, hash tables, and red-black trees. We were able to reduce the number of bits flipped by up to 3.56× over standard implementations of the same data structures with negligible overhead. We measured the number of bits flipped by memory allocation and stack frame saves and found that careful data placement in the stack can reduce bit flips significantly. These changes require no hardware modifications and neither significantly reduce performance nor increase code complexity, making them attractive for designing systems optimized for BNVM. 3. Write-Optimized Dynamic Hashing for Persistent Memory. Source Abstract Low latency storage media such as byte-addressable persistent memory (PM) requires rethinking of various data structures in terms of optimization. One of the main challenges in implementing hash-based indexing structures on PM is how to achieve efficiency by making effective use of cachelines while guaranteeing failure-atomicity for dynamic hash expansion and shrinkage. In this paper, we present Cacheline-Conscious Extendible Hashing (CCEH) that reduces the overhead of dynamic memory block management while guaranteeing constant hash table lookup time. CCEH guarantees failure-atomicity without making use of explicit logging. Our experiments show that CCEH effectively adapts its size as the demand increases under the fine-grained failure-atomicity constraint and reduces the maximum query latency by over two-thirds compared to the state-of-the-art hashing techniques. 4. Software Wear Management for Persistent Memories. Source Abstract The commercial release of byte-addressable persistent memories (PMs) is imminent. Unfortunately, these devices suffer from limited write endurance—without any wear management, PM lifetime might be as low as 1.1 months. Existing wear-management techniques introduce an additional indirection layer to remap memory across physical frames and require hardware support to track fine-grain wear. These mechanisms incur storage overhead and increase access latency and energy consumption. We present Kevlar, an OS-based wear-management technique for PM that requires no new hardware. Kevlar uses existing virtual memory mechanisms to remap pages, enabling it to perform both wear leveling—shuffling pages in PM to even wear; and wear reduction—transparently migrating heavily written pages to DRAM. Crucially, Kevlar avoids the need for hardware support to track wear at fine grain. Instead, it relies on a novel wear estimation technique that builds upon Intel's Precise Event Based Sampling to approximately track processor cache contents via a software-maintained Bloom filter and estimate write-back rates at fine grain. We implement Kevlar in Linux and demonstrate that it achieves lifetime improvement of 18.4x (avg.) over no wear management while incurring 1.2% performance overhead. File Systems 1. Storage Gardening: Using a Virtualization Layer for Efficient Defragmentation in the WAFL File System. Source Abstract As a file system ages, it can experience multiple forms of fragmentation. Fragmentation of the free space in the file system can lower write performance and subsequent read performance. Client operations as well as internal operations, such as deduplication, can fragment the layout of an individual file, which also impacts file read performance. File systems that allow sub-block granular addressing can gather intra-block fragmentation, which leads to wasted free space. This paper describes how the NetApp® WAFL® file system leverages a storage virtualization layer for defragmentation techniques that physically relocate blocks efficiently, including those in read-only snapshots. The paper analyzes the effectiveness of these techniques at reducing fragmentation and improving overall performance across various storage media. 2. Pay Migration Tax to Homeland: Anchor-based Scalable Reference Counting for Multicores. Source Abstract The operating system community has been combating scalability bottlenecks for the past 10 years with victories or all the then-new multicore hardware. File systems, however, are in the midst of turmoil yet. One of the culprits behind performance degradation is reference counting widely used for managing data and metadata, and scalability is badly impacted under load with little or no logical contention, where the capability is desperately needed. To address this, we propose PAYGO, a reference counting technique that combines per-core hash of local reference counters with an anchor counter to make concurrent counting scalable as well as space-efficient, without having any other delay for managing counters. PAYGO imposes the restriction that decrement must be performed on the original local counter where the act of increment has occurred so that reclaiming zero-valued local counters can be done immediately. To this end, we enforce migrated processes running on different cores to update the anchor counter associated with the original local counter. We implemented PAYGO in the Linux page cache, and so our implementation is transparent to the file system. Experimental evaluation with underlying file systems (i.e., ext4, F2FS, btrfs, and XFS) demonstrated that PAYGO scales file systems better than other state-of-the-art techniques. 3. Speculative Encryption on GPU Applied to Cryptographic File Systems. Source Abstract Due to the processing of cryptographic functions, Cryptographic File Systems (CFSs) may require significant processing capacity. Parallel processing techniques on CPUs or GPUs can be used to meet this demand. The CTR mode has two particularly useful features: the ability to be fully parallelizable and to perform the initial step of the encryption process ahead of time, generating encryption masks. This work presents an innovative approach in which the CTR mode is applied in the context of CFSs seeking to exploit these characteristics, including the anticipated production of the cipher masks (speculative encryption) in GPUs. Techniques that demonstrate how to deal with the issue of the generation, storage and management of nonces are presented, an essential component to the operation of the CTR mode in the context of CFSs. Related to GPU processing, our methods work to perform the handling of the encryption contexts and control the production of the masks, aiming to produce them with the adequate anticipation and overcome the extra latency due to encryption tasks. The techniques were applied in the implementation of EncFS++, a user space CFS. Performance analyzes showed that it was possible to achieve significant gains in throughput and CPU efficiency in several scenarios. They also demonstrated that GPU processing can be efficiently applied to CFS encryption workload even when working by encrypting small amounts of data (4 KiB), and in scenarios where higher speed/lower latency storage devices are used, such as SSDs or memory. Deduplication 1. Sketching Volume Capacities in Deduplicated Storage. Source Abstract The adoption of deduplication in storage systems has introduced significant new challenges for storage management. Specifically, the physical capacities associated with volumes are no longer readily available. In this work we introduce a new approach to analyzing capacities in deduplicated storage environments. We provide sketch-based estimations of fundamental capacity measures required for managing a storage system: How much physical space would be reclaimed if a volume or group of volumes were to be removed from a system (the {\\em reclaimable} capacity) and how much of the physical space should be attributed to each of the volumes in the system (the {\\em attributed} capacity). Our methods also support capacity queries for volume groups across multiple storage systems, e.g., how much capacity would a volume group consume after being migrated to another storage system? We provide analytical accuracy guarantees for our estimations as well as empirical evaluations. Our technology is integrated into a prominent all-flash storage array and exhibits high performance even for very large systems. We also demonstrate how this method opens the door for performing placement decisions at the data center level and obtaining insights on deduplication in the field. 2. Finesse: Fine-Grained Feature Locality based Fast Resemblance Detection for Post-Deduplication Delta Compression. Source Abstract In storage systems, delta compression is often used as a complementary data reduction technique for data deduplication because it is able to eliminate redundancy among the non-duplicate but highly similar chunks. Currently, what we call 3. Sliding Look-Back Window Assisted Data Chunk Rewriting for Improving Deduplication Restore Performance. Source Abstract Data deduplication is an effective way of improving storage space utilization. The data generated by deduplication is persistently stored in data chunks or data containers (a container consisting of a few hundreds or thousands of data chunks). The data restore process is rather slow due to data fragmentation and read amplification. To speed up the restore process, data chunk rewrite (a rewrite is to store a duplicate data chunk) schemes have been proposed to effectively improve data chunk locality and reduce the number of container reads for restoring the original data. However, rewrites will decrease the deduplication ratio since more storage space is used to store the duplicate data chunks. To remedy this, we focus on reducing the data fragmentation and read amplification of container-based deduplication systems. We first propose a flexible container referenced count based rewrite scheme, which can make a better tradeoff between the deduplication ratio and the number of required container reads than that of capping which is an existing rewrite scheme. To further improve the rewrite candidate selection accuracy, we propose a sliding look-back window based design, which can make more accurate rewrite decisions by considering the caching effect, data chunk localities, and data chunk closeness in the current and future windows. According to our evaluation, our proposed approach can always achieve a higher restore performance than that of capping especially when the reduction of deduplication ratio is small. Storage Potpourri 1. DistCache: Provable Load Balancing for Large-Scale Storage Systems with Distributed Caching. Source Abstract Load balancing is critical for distributed storage to meet strict service-level objectives (SLOs). It has been shown that a fast cache can guarantee load balancing for a clustered storage system. However, when the system scales out to multiple clusters, the fast cache itself would become the bottleneck. Traditional mechanisms like cache partition and cache replication either result in load imbalance between cache nodes or have high overhead for cache coherence. We present DistCache, a new distributed caching mechanism that provides provable load balancing for large-scale storage systems. DistCache co-designs cache allocation with cache topology and query routing. The key idea is to partition the hot objects with independent hash functions between cache nodes in different layers, and to adaptively route queries with the power-of-two-choices. We prove that DistCache enables the cache throughput to increase linearly with the number of cache nodes, by unifying techniques from expander graphs, network flows, and queuing theory. DistCache is a general solution that can be applied to many storage systems. We demonstrate the benefits of DistCache by providing the design, implementation, and evaluation of the use case for emerging switch-based caching. 2. GearDB: A GC-free Key-Value Store on HM-SMR Drives with Gear Compaction. Source Abstract Host-managed shingled magnetic recording drives (HM-SMR) give a capacity advantage to harness the explosive growth of data. Applications where data is sequentially written and randomly read make the HM-SMR an ideal solution due to its capacity, predictable performance, and economical cost. Key-value stores based on the Log-Structured Merge Trees (LSM-trees) data structure is such a good fit due to their batched sequential writes. However, building an LSM-tree based KV store on HM-SMR drives presents severe challenges in maintaining the performance and space efficiency due to the redundant cleaning processes for applications and storage devices (i.e., compaction and garbage collections). To eliminate the overhead of on-disk garbage collections (GC) and improve compaction efficiency, this paper presents GearDB, a GC-free KV store tailored for HM-SMR drives, with three new techniques: a new on-disk data layout, compaction windows, and a novel gear compaction algorithm. We implement GearDB and evaluate it with LevelDB on a real HM-SMR drive. Our extensive experiments have shown that GearDB achieves good performance and space efficiency, i.e., on average 1.71×1.71\\times1.71× faster than LevelDB in random write with a space efficiency of 89.9%. 3. SPEICHER: Securing LSM-based Key-Value Stores using Shielded Execution. Source Abstract We introduce Speicher, a secure storage system that not only provides strong confidentiality and integrity properties, but also ensures data freshness to protect against rollback/forking attacks. Speicher exports a Key-Value (KV) interface backed by Log-Structured Merge Tree (LSM) for supporting secure data storage and query operations. Speicher enforces these security properties on an untrusted host by leveraging shielded execution based on a hardware-assisted trusted execution environment (TEE)—specifically, Intel SGX. However, the design of Speicher extends the trust in shielded execution beyond the secure SGX enclave memory region to ensure that the security properties are also preserved in the stateful (or non-volatile) setting of an untrusted storage medium, including system crash, reboot, or migration. More specifically, we have designed an authenticated and confidentiality-preserving LSM data structure. We have further hardened the LSM data structure to ensure data freshness by designing asynchronous trusted counters. Lastly, we designed a direct I/O library for shielded execution based on Intel SPDK to overcome the I/O bottlenecks in the SGX enclave. We have implemented Speicher as a fully-functional storage system by extending RocksDB, and evaluated its performance using the RocksDB benchmark. Our experimental evaluation shows that Speicher incurs reasonable overheads for providing strong security guarantees, while keeping the trusted computing base (TCB) small. NVM File and Storage Systems 1. SLM-DB: Single-Level Key-Value Store with Persistent Memory. Source Abstract This paper investigates how to leverage emerging byte-addressable persistent memory (PM) to enhance the performance of key-value (KV) stores. We present a novel KV store, the Single-Level Merge DB (SLM-DB), which takes advantage of both the B+-tree index and the Log-Structured Merge Trees (LSM-tree) approach by making the best use of fast persistent memory. Our proposed SLM-DB achieves high read performance as well as high write performance with low write amplification and near-optimal read amplification. In SLM-DB, we exploit persistent memory to maintain a B+-tree index and adopt an LSM-tree approach to stage inserted KV pairs in a PM resident memory buffer. SLM-DB has a single-level organization of KV pairs on disks and performs selective compaction for the KV pairs, collecting garbage and keeping the KV pairs sorted sufficiently for range query operations. Our extensive experimental study demonstrates that, in our default setup, compared to LevelDB, SLM-DB provides 1.07 - 1.96 and 1.56 - 2.22 times higher read and write throughput, respectively, as well as comparable range query performance. 2. Ziggurat: A Tiered File System for Non-Volatile Main Memories and Disks. Source Abstract Emerging fast, byte-addressable Non-Volatile Main Memory (NVMM) provides huge increases in storage performance compared to traditional disks. We present Ziggurat, a tiered file system that combines NVMM and slow disks to create a storage system with near-NVMM performance and large capacity. Ziggurat steers incoming writes to NVMM, DRAM, or disk depending on application access patterns, write size, and the likelihood that the application will stall until the write completes. Ziggurat profiles the application's access stream online to predict the behavior of individual writes. In the background, Ziggurat estimates the &quot;temperature&quot; of file data, and migrates the cold file data from NVMM to disks. To fully utilize disk bandwidth, Ziggurat coalesces data blocks into large, sequential writes. Experimental results show that with a small amount of NVMM and a large SSD, Ziggurat achieves up to 38.9x and 46.5x throughput improvement compared with EXT4 and XFS running on an SSD alone, respectively. As the amount of NVMM grows, Ziggurat's performance improves until it matches the performance of an NVMM-only file system. 3. Orion: A Distributed File System for Non-Volatile Main Memory and RDMA-Capable Networks. Source Abstract High-performance, byte-addressable non-volatile main memories (NVMMs) force system designers to rethink trade-offs throughout the system stack, often leading to dramatic changes in system architecture. Conventional distributed file systems are a prime example. When faster NVMM replaces block-based storage, the dramatic improvement in storage performance makes networking and software overhead a critical bottleneck. In this paper, we present Orion, a distributed file system for NVMM-based storage. By taking a clean slate design and leveraging the characteristics of NVMM and high-speed, RDMA-based networking, Orion provides high-performance metadata and data access while maintaining the byte addressability of NVMM. Our evaluation shows Orion achieves performance comparable to local NVMM file systems and outperforms existing distributed file systems by a large margin. Big Systems 1. INSTalytics: Cluster Filesystem Co-design for Big-data Analytics. Source Abstract We present the design, implementation, and evaluation of Instalytics, a co-designed stack of a cluster file system and the compute layer, for efficient big data analytics in large-scale data centers. Instalytics amplifies the well-known benefits of data partitioning in analytics systems; instead of traditional partitioning on one dimension, Instalytics enables data to be simultaneously partitioned on four different dimensions at the same storage cost, enabling a larger fraction of queries to benefit from partition filtering and joins without network shuffle. To achieve this, Instalytics uses compute-awareness to customize the 3-way replication that the cluster file system employs for availability. A new heterogeneous replication layout enables Instalytics to preserve the same recovery cost and availability as traditional replication. Instalytics also uses compute-awareness to expose a new {\\em sliced-read} API that improves performance of joins by enabling multiple compute nodes to read slices of a data block efficiently via co-ordinated request scheduling and selective caching at the storage nodes. We have implemented Instalytics in a production analytics stack, and show that recovery performance and availability is similar to physical replication, while providing significant improvements in query performance, suggesting a new approach to designing cloud-scale big-data analytics systems. 2. GraphOne: A Data Store for Real-time Analytics on Evolving Graphs. Source Abstract There is a growing need to perform real-time analytics on evolving graphs in order to deliver the values of big data to users. The key requirement from such applications is to have a data store to support their diverse data access efficiently, while concurrently ingesting fine-grained updates at a high velocity. Unfortunately, current graph systems, either graph databases or analytics engines, are not designed to achieve high performance for both operations. To address this challenge, we have designed and developed GraphOne, a graph data store that combines two complementary graph storage formats (edge list and adjacency list), and uses dual versioning to decouple graph computations from updates. Importantly, it presents a new data abstraction, GraphView, to enable data access at two different granularities with only a small data duplication. Experimental results show that GraphOne achieves an ingestion rate of two to three orders of magnitude higher than graph databases, while delivering algorithmic performance comparable to a static graph system. GraphOne is able to deliver 5.36x higher update rate and over 3x better analytics performance compared to a state-of-the-art dynamic graph system. 3. Automatic, Application-Aware I/O Forwarding Resource Allocation. Source Abstract The I/O forwarding architecture is widely adopted on modern supercomputers, with a layer of intermediate nodes sitting between the many compute nodes and backend storage nodes. This allows compute nodes to run more efficiently and stably with a leaner OS, offloads I/O coordination and communication with backend from the compute nodes, maintains less concurrent connections to storage systems, and provides additional resources for effective caching, prefetching, write buffering, and I/O aggregation. However, with many existing machines, these forwarding nodes are assigned to serve fixed set of compute nodes. We explore an automatic mechanism, DFRA, for application-adaptive dynamic forwarding resource allocation. With I/O monitoring data that proves affordable to acquire in real time and maintain for long-term history analysis, Upon each job's dispatch, DFRA conducts a history-based study to determine whether the job should be granted more forwarding resources or given dedicated forwarding nodes. Such customized I/O forwarding lets the small fraction of I/O-intensive applications achieve higher I/O performance and scalability, meanwhile effectively isolating disruptive I/O activities. We implemented, evaluated, and deployed DFRA on Sunway TaihuLight, the current No.2 supercomputer in the world. It improves applications' I/O performance by up to 16.0x, eliminates most of the inter-application I/O interference, and has saved over 200 million of core-hours during its deployment on TaihuLight for past 8 months. Finally, our proposed DFRA design is not platform-dependent, making it applicable to the management of existing and future I/O forwarding or burst buffer resources. Flash and Emerging Storage Systems 1. Design Tradeoffs for SSD Reliability. Source Abstract Flash memory-based SSDs are popular across a wide range of data storage markets, while the underlying storage medium—flash memory—is becoming increasingly unreliable. As a result, modern SSDs employ a number of in-device reliability enhancement techniques, but none of them offers a one size fits all solution when considering the multi-dimensional requirements for SSDs: performance, reliability, and lifetime. In this paper, we examine the design tradeoffs of existing reliability enhancement techniques such as data re-read, intra-SSD redundancy, and data scrubbing. We observe that an uncoordinated use of these techniques adversely affects the performance of the SSD, and careful management of the techniques is necessary for a graceful performance degradation while maintaining a high reliability standard. To that end, we propose a holistic reliability management scheme that selectively employs redundancy, conditionally re-reads, judiciously selects data to scrub. We demonstrate the effectiveness of our scheme by evaluating it across a set of I/O workloads and SSDs wear states. 2. Fully Automatic Stream Management for Multi-Streamed SSDs Using Program Contexts. Source Abstract Multi-streamed SSDs can significantly improve both the performance and lifetime of flash-based SSDs when their streams are properly managed. However, existing stream management solutions do not adequately support the multi-streamed SSDs for their wide adoption. No existing stream management technique works in a fully automatic fashion for general I/O workloads. Furthermore, the limited number of available streams makes it difficult to effectively manage streams when a large number of streams are required. In this paper, we propose a fully automatic stream management technique, PCStream, which can work efficiently for general I/O workloads with heterogeneous write characteristics. PCStream is based on the key insight that stream allocation decisions should be made on dominant I/O activities. By identifying dominant I/O activities using program contexts, PCStream fully automates the whole process of stream allocation within the kernel with no manual work. In order to overcome the limited number of supported streams, we propose a new type of streams, internal streams, which can be implemented at low cost. PCStream can effectively double the number of available streams using internal streams. Our evaluations on real multi-streamed SSDs show that PCStream achieves the same efficiency as highly-optimized manual allocations by experienced programmers. PCStream improves IOPS by up to 56% over the existing automatic technique by reducing the garbage collection overhead by up to 69%. 3. Large-Scale Graph Processing on Emerging Storage Devices. Source Abstract Graph processing is becoming commonplace in many applications to analyze huge datasets. Much of the prior work in this area has assumed I/O devices with considerable latencies, especially for random accesses, using large amount of DRAM to trade-off additional computation for I/O accesses. However, emerging storage devices, including currently popular SSDs, provide fairly comparable sequential and random accesses, making these prior solutions inefficient. In this paper, we point out this inefficiency, and propose a new graph partitioning and processing framework to leverage these new device capabilities. We show experimentally on an actual platform that our proposal can give 2X better performance than a state-of-the-art solution. Erasure Coding and Reliability 1. Fast Erasure Coding for Data Storage: A Comprehensive Study of the Acceleration Techniques. Source Abstract Various techniques have been proposed in the literature to improve erasure code computation efficiency, including optimizing bitmatrix design, optimizing computation schedule, common XOR operation reduction, caching management techniques, and vectorization techniques. These techniques were largely proposed individually previously, and in this work, we seek to use them jointly. In order to accomplish this task, these techniques need to be thoroughly evaluated individually, and their relation better understood. Building on extensive test results, we develop methods to systematically optimize the computation chain together with the underlying bitmatrix. This led to a simple design approach of optimizing the bitmatrix by minimizing a weighted cost function, and also a straightforward erasure coding procedure: use the given bitmatrix to produce the computation schedule, which utilizes both the XOR reduction and caching management techniques, and apply XOR level vectorization. This procedure can provide a better performance than most existing techniques, and even compete against well-known codes such as EVENODD, RDP, and STAR codes. Moreover, the result suggests that vectorizing the XOR operation is a better choice than directly vectorizing finite field operations, not only because of the better encoding throughput, but also its minimal migration efforts onto newer CPUs. 2. OpenEC: Toward Unified and Configurable Erasure Coding Management in Distributed Storage Systems. Source Abstract Erasure coding becomes a practical redundancy technique for distributed storage systems to achieve fault tolerance with low storage overhead. Given its popularity, research studies have proposed theoretically proven erasure codes or efficient repair algorithms to make erasure coding more viable. However, integrating new erasure coding solutions into existing distributed storage systems is a challenging task and requires non-trivial re-engineering of the underlying storage workflows. We present OpenEC, a unified and configurable framework for readily deploying a variety of erasure coding solutions into existing distributed storage systems. OpenEC decouples erasure coding management from the storage workflows of distributed storage systems, and provides erasure coding designers with configurable controls of erasure coding operations through a directed-acyclic-graph-based programming abstraction. We prototype OpenEC on two versions of HDFS with limited code modifications. Experiments on a local cluster and Amazon EC2 show that OpenEC preserves both the operational performance and the properties of erasure coding solutions; OpenEC can also automatically optimize erasure coding operations to improve repair performance. 3. Cluster storage systems gotta have HeART: improving storage efficiency by exploiting disk-reliability heterogeneity. Source Abstract Large-scale cluster storage systems typically consist of a heterogeneous mix of storage devices with significantly varying failure rates. Despite such differences among devices, redundancy settings are generally configured in a one-scheme-for-all fashion. In this paper, we make a case for exploiting reliability heterogeneity to tailor redundancy settings to different device groups. We present HeART, an online tuning tool that guides selection of, and transitions between redundancy settings for long-term data reliability, based on observed reliability properties of each disk group. By processing disk failure data over time, HeART identifies the boundaries and steady-state failure rate for each deployed disk group (e.g., by make/model). Using this information, HeART suggests the most space-efficient redundancy option allowed that will achieve the specified target data reliability. Analysis of longitudinal failure data for a large production storage cluster shows the robustness of HeART's failure-rate determination algorithms. The same analysis shows that a storage system guided by HeART could provide target data reliability levels with fewer disks than one-scheme-for-all approaches: 11–16% fewer compared to erasure codes like 10-of-14 or 6-of-9 and 33% fewer compared to 3-way replication. 4. ScaleCheck: A Single-Machine Approach for Discovering Scalability Bugs in Large Distributed Systems. Source Abstract We present ScaleCheck, an approach for discovering scalability bugs (a new class of bug in large storage systems) and for democratizing large-scale testing. ScaleCheck employs a program analysis technique, for finding potential causes of scalability bugs, and a series of colocation techniques, for testing implementation code at real scales but doing so on just a commodity PC. ScaleCheck has been integrated to several large-scale storage systems, Cassandra, HDFS, Riak, and Voldemort, and successfully exposed known and unknown scalability bugs, up to 512-node scale on a 16-core PC. FAST 2018 Failing and Recovering 1. Fail-Slow at Scale: Evidence of Hardware Performance Faults in Large Production Systems. Source Abstract Fail-slow hardware is an under-studied failure mode. We present a study of 101 reports of fail-slow hardware incidents, collected from large-scale cluster deployments in 12 institutions. We show that all hardware types such as disk, SSD, CPU, memory and network components can exhibit performance faults. We made several important observations such as faults convert from one form to another, the cascading root causes and impacts can be long, and fail-slow faults can have varying symptoms. From this study, we make suggestions to vendors, operators, and systems designers. 2. Protocol-Aware Recovery for Consensus-Based Storage. Source Abstract We introduce protocol-aware recovery (PAR), a new approach that exploits protocol-specific knowledge to correctly recover from storage faults in distributed systems. We demonstrate the efficacy of PAR through the design and implementation of corruption-tolerant replication (CTRL), a PAR mechanism specific to replicated state machine (RSM) systems. We experimentally show that the CTRL versions of two systems, LogCabin and ZooKeeper, safely recover from storage faults and provide high availability, while the unmodified versions can lose data or become unavailable. We also show that the CTRL versions have little performance overhead. 3. WAFL Iron: Repairing Live Enterprise File Systems. Source Abstract Consistent and timely access to an arbitrarily damaged file system is an important requirement of enterprise class systems. Repairing file system inconsistencies is accomplished most simply when file system access is limited to the repair tool. Checking and repairing a file system while it is open for general access present unique challenges. In this paper, we explore these challenges, present our online repair tool for the NetApp® WAFL® file system, and show how it achieves the same results as offline repair even while client access is enabled. We present some implementation details and evaluate its performance. To the best of our knowledge, this publication is the first to describe a fully functional online repair tool. Revealing Flashy Secrets 1. MQSim: A Framework for Enabling Realistic Studies of Modern Multi-Queue SSD Devices. Source Abstract Solid-state drives (SSDs) are used in a wide array of computer systems today, including in datacenters and enterprise servers. As the I/O demands of these systems have increased, manufacturers have evolved SSD design to keep up with this demand. For example, manufacturers have introduced new high-bandwidth interfaces to replace the traditional SATA protocol. These new interfaces, such as the NVMe protocol, are designed specifically to enable the high amount of concurrent I/O that SSDs are capable of delivering. While modern SSDs with sophisticated features such as the NVMe protocol are already on the market, SSD simulation tools have fallen behind, as they do not capture these new features. We compare the outputs of these simulators to the performance measured from real off-the-shelf SSDs, and find three major shortcomings in state-of-the-art SSD simulators. First, existing simulators do not model critical features of new protocols like NVMe, such as their use of multiple application-level queues for requests and the elimination of OS intervention. Second, existing simulators do not capture the effects of advanced SSD maintenance algorithms (e.g., garbage collection), as they do not properly emulate the steady-state conditions that exist in real SSDs. Third, existing simulators do not capture the full end-to-end latency of I/O requests, which can incorrectly skew the simulated behavior of SSDs that use emerging memory technologies, such as 3D XPoint. We show that without the accurate modeling of these features, results from existing simulators deviate significantly from real SSD performance. In this work, we introduce a new simulator, called MQSim, that accurately models the performance of both modern SSDs and conventional SATA-based SSDs. MQSim faithfully models new high-bandwidth protocol implementations, steady-state SSD conditions, and the full end-to-end latency for requests in modern SSDs. We validate MQSim using several modern SSDs, and show that MQSim uncovers several real and important issues that were not captured by existing simulators, such as the performance impact of inter-flow interference in modern SSDs. We plan to release MQSim as an open-source tool, and we hope that it can enable several new research directions in the future. 2. PEN: Design and Evaluation of Partial-Erase for 3D NAND-Based High Density SSDs. Source Abstract 3D NAND flash memories promise unprecedented flash storage capacities, which can be extremely important in certain application domains where both storage capacity and performance are first-class target metrics. However a block of 3D NAND flash contains many more pages than its 2D counterpart. This increased number of pages-per-block has numerous ramifications such as the longer erase latency, higher garbage collection costs, and increased write amplification factors, which can collectively prevent the 3D NAND flash products from becoming the mainstream in high-performance storage domain. In this paper, we introduce PEN, an architecture-level mechanism that enables partial-erase of flash blocks. Using our proposed partial-erase support, we also discuss how one can build a custom garbage collector for two types of flash translation layers (FTLs), namely, block-level FTL and hybrid FTL. Our experimental evaluations of PEN with a set of diverse real storage workloads indicate that the proposed approach can shorten the write latency by 44.3%44.3\\%44.3% and 47.9%47.9\\%47.9% for block-level FTL and hybrid FTL, respectively. 3. The CASE of FEMU: Cheap, Accurate, Scalable and Extensible Flash Emulator. Source Abstract We present FEMU, a QEMU-based flash emulator for fostering future full-stack software/hardware SSD research, with the following four &quot;CASE&quot; benefits. FEMU is cheap ($0) as it will be an open-sourced software; FEMU is relatively accurate, with only 0.5-38% variance from OpenChannel SSD in our tests; FEMU is scalable, upon our optimized QEMU stack, to support up to 32 parallel channels/chips without unintended queueing delays; FEMU is extensible, enabling various types of SSD research, such as internal-SSD, kernel-only and split-level research on it. Understanding the Meta(data) Story 1. Spiffy: Enabling File-System Aware Storage Applications. Source Abstract Many file system applications such as defragmentation tools, file system checkers or data recovery tools, operate at the storage layer. Today, developers of these storage applications require detailed knowledge of the file system format, which takes a significant amount of time to learn, often by trial and error, due to insufficient documentation or specification of the format. Furthermore, these applications perform ad-hoc processing of the file-system metadata, leading to bugs and vulnerabilities. We propose Spiffy, an annotation language for specifying the on-disk format of a file system. File system developers annotate the data structures of a file system, and we use these annotations to generate a library that allows identifying, parsing and traversing file-system metadata, providing support for both offline and online storage applications. This approach simplifies the development of storage applications that work across different file systems because it reduces the amount of file-system specific code that needs to be written. We have written annotations for the Linux Ext4, Btrfs and F2FS file systems, and developed several applications for these file systems, including a type-specific metadata corruptor, a file system converter, and an online storage layer cache that preferentially caches files for certain users. Our experiments show that applications that use the library to access file system metadata can achieve good performance and are robust against file system corruption errors. 2. Towards Robust File System Checkers. Source Abstract File systems may become corrupted for many reasons despite various protection techniques. Therefore, most file systems come with a checker to recover the file system to a consistent state. However, existing checkers are commonly assumed to be able to complete the repair without interruption, which may not be true in practice. In this work, we demonstrate via fault injection experiments that checkers of widely used file systems may leave the file system in an uncorrectable state if the repair procedure is interrupted unexpectedly. To address the problem, we first fix the ordering issue in the undo logging of e2fsck, and then build a general logging library (i.e., rfsck-lib) for strengthening checkers. To demonstrate the practicality, we integrate rfsck-lib with existing checkers and create two new checkers: (1) rfsck-ext, a robust checker for Ext-family file systems, and (2) rfsck-xfs, a robust checker for XFS file system, both of which require only tens of lines of modification to the original versions. Both rfsck-ext and rfsck-xfs are resilient to faults in our experiments. Also, both checkers incur reasonable performance overhead (i.e., up to 12%) comparing to the original unreliable versions. Moreover, rfsck-ext outperforms the patched e2fsck by up to nine times while achieving the same level of robustness. 3. The Full Path to Full-Path Indexing. Source Abstract This paper shows how to use full-path indexing in a file system to realize fast directory scans, writes, and renames. Prior results indicated that renames are prohibitively expensive in full-path indexing. The paper introduces a range-rename mechanism for efficient key-space changes in a write-optimized dictionary. This mechanism is encapsulated in the key-value-store API, and simplifies the overall design of the file system. We implemented this mechanism in ArborFS, an extension of the BetrFS in-kernel, local file system for Linux. For instance, ArborFS performs recursive greps 1.5x faster and random writes 1.2x faster than BetrFS, but renames are competitive with standard, indirection-based file systems for a range of sizes. ArborFS outperforms relative-path file systems such as BetrFS as well as traditional file systems such as ext4, xfs and zfs across a variety of workloads. Coding, Hashing, Hiding 1. Clay Codes: Moulding MDS Codes to Yield an MSR Code. Source Abstract With increase in scale, the number of node failures in a data center increases sharply. To ensure availability of data, failure-tolerance schemes such as Reed-Solomon (RS) or more generally, Maximum Distance Separable (MDS) erasure codes are used. However, while MDS codes offer minimum storage overhead for a given amount of failure tolerance, they do not meet other practical needs of today's data centers. Although modern codes such as Minimum Storage Regenerating (MSR) codes are designed to meet these practical needs, they are available only in highly-constrained theoretical constructions, that are not sufficiently mature enough for practical implementation. We present {\\em Clay codes} that extract the best from both worlds. Clay (short for Coupled-Layer) codes are MSR codes that offer a simplified construction for decoding/repair by using pairwise coupling across multiple stacked layers of any single MDS code. In addition, Clay codes provide the first practical implementation of an MSR code that offers (a) low storage overhead, (b) simultaneous optimality in terms of three key parameters: repair bandwidth, sub-packetization level and disk I/O, (c) uniform repair performance of data and parity nodes and (d) support for both single and multiple-node repairs, while permitting faster and more efficient repair. While all MSR codes are vector codes, none of the distributed storage systems support vector codes. We have modified Ceph to support any vector code, and our contribution is now a part of Ceph's master codebase. We have implemented Clay codes, and integrated it as a plugin to Ceph. Six example Clay codes were evaluated on a cluster of Amazon EC2 instances and code parameters were carefully chosen to match known erasure-code deployments in practice. A particular example code, with storage overhead 1.251.251.25x, is shown to reduce repair network traffic by a factor of 2.92.92.9 in comparison with RS codes and similar reductions are obtained for both repair time and disk read. 2. Towards Web-based Delta Synchronization for Cloud Storage Services. Source Abstract Delta synchronization (sync) is crucial for network-level efficiency of cloud storage services. Practical delta sync techniques are, however, only available for PC clients and mobile apps, but not web browsers---the most pervasive and OS-independent access method. To understand the obstacles of web-based delta sync, we implement a delta sync solution, WebRsync, using state-of-the-art web techniques based on rsync, the de facto delta sync protocol for PC clients. Our measurements show that WebRsync severely suffers from the inefficiency of JavaScript execution inside web browsers, thus leading to frequent stagnation and even hanging. Given that the computation burden on the web browser mainly stems from data chunk search and comparison, we reverse the traditional delta sync approach by lifting all chunk search and comparison operations from the client side to the server side. Inevitably, this brings considerable computation overhead to the servers. Hence, we further leverage locality-aware chunk matching and lightweight checksum algorithms to reduce the overhead. The resulting solution, WebR2sync+, outpaces WebRsync by an order of magnitude, and is able to simultaneously support 6800--8500 web clients' delta sync using a standard VM server instance based on a Dropbox-like system architecture. 3. Stash in a Flash. Source Abstract Encryption is a useful tool to protect data confidentiality. Yet it is still challenging to hide the very presence of encrypted, secret data from a powerful adversary. This paper presents a new technique to hide data in flash by manipulating the voltage level of pseudo-randomlyselected flash cells to encode two bits (rather than one) in the cell. In this model, we have one “public” bit interpreted using an SLC-style encoding, and extract a private bit using an MLC-style encoding. The locations of cells that encode hidden data is based on a secret key known only to the hiding user. Intuitively, this technique requires that the voltage level in a cell encoding data must be (1) not statistically distinguishable from a cell only storing public data, and (2) the user must be able to reliably read the hidden data from this cell. Our key insight is that there is a wide enough variation in the range of voltage levels in a typical flash device to obscure the presence of fine-grained changes to a small fraction of the cells, and that the variation is wide enough to support reliably re-reading hidden data. We demonstrate that our hidden data and underlying voltage manipulations go undetected by support vector machine based supervised learning which performs similarly to a random guess. The error rates of our scheme are low enough that the data is recoverable months after being stored. Compared to prior work, our technique provides 24x and 50x higher encoding and decoding throughput and doubles the capacity, while being 37x more power efficient. New Media and Old 1. Endurable Transient Inconsistency in Byte-Addressable Persistent B+-Tree. Source Abstract With the emergence of byte-addressable persistent memory (PM), a cache line, instead of a page, is expected to be the unit of data transfer between volatile and nonvolatile devices, but the failure-atomicity of write operations is guaranteed in the granularity of 8 bytes rather than cache lines. This granularity mismatch problem has generated interest in redesigning block-based data structures such as B+-trees, and attempts have been made to use in-memory data structures for PM. However, various methods of modifying B+-trees for PM degrade the efficiency of B+-trees due to the additional metadata and high rebalancing overhead caused by logging methods. In this study, we develop Failure-Atomic ShifT (FAST) and Failure-Atomic In-place Rebalance (FAIR) algorithms. FAST and FAIR modify legacy B+-trees in a byte-addressable fashion but solves the granularity mismatch problem. Every 8-byte store instruction used in the FAST and FAIR algorithms transforms a B+-tree into another consistent state or a transient inconsistent state that read operations can tolerate. By making read operations transient inconsistency, we can eliminate expensive copy-on-write, logging, and even the necessity of read latches so that read transactions can be non-blocking. Our experimental results show that legacy B+-trees with FAST and FAIR schemes outperform the state-of-the-art persistent indexing structures by a large margin. 2. RFLUSH: Rethink the Flush. Source Abstract A FLUSH command has been used for decades to enforce persistence and ordering of updates in a storage device. The command forces all the data in the volatile buffer to non-volatile media to achieve persistency. This lumpsum approach to flushing has two performance consequences. First, it slows down non-volatile materialization of the writes that actually need to be flushed. Second, it deprives the writes that need not to be flushed of an opportunity for absorbing future writes and coalescing. We attempt to characterize the problems of this semantic gap of flushing in storage devices and propose RFLUSH that allows a fine-grained control over flushing in them. The RFLUSH command delivers a range of LBAs that need to be flushed and thus enables the storage device to force only a subset of data in its buffer. We implemented this fine-grained flush command in a storage device using an open-source flash development platform and modified the F2FS file system to make use of the command in processing fsync requests as a case study. Performance evaluation using the prototype implementation shows that the inclusion of RFLUSH improves the throughput by up to 5.6x; reduces the write traffic by up to 43%; and eliminates the long tail in the response time. 3. Barrier-Enabled IO Stack for Flash Storage. Source Abstract This work is dedicated to eliminating the overhead required for guaranteeing the storage order in the modern IO stack. The existing block device adopts a prohibitively expensive approach in ensuring the storage order among write requests: interleaving the write requests with Transfer-and-Flush. Exploiting the cache barrier command for Flash storage, we overhaul the IO scheduler, the dispatch module, and the filesystem so that these layers are orchestrated to preserve the ordering condition imposed by the application with which the associated data blocks are made durable. The key ingredients of Barrier-Enabled IO stack are Epoch-based IO scheduling, Order-Preserving Dispatch, and Dual-Mode Journaling. Barrier-enabled IO stack can control the storage order without Transfer-and-Flush overhead. We implement the barrier-enabled IO stack in server as well as in mobile platforms. SQLite performance increases by 270% and 75%, in server and in smartphone, respectively. In a server storage, BarrierFS brings as much as by 43×\\times× and by 73×\\times× performance gain in MySQL and SQLite, respectively, against EXT4 via relaxing the durability of a transaction. Long Live the File System! 1. High-Performance Transaction Processing in Journaling File Systems. Source Abstract Journaling file systems provide crash-consistency to applications by keeping track of uncommitted changes in the journal area (journaling) and writing committed changes to their original area at a certain point (checkpointing). They generally use coarse-grained locking to access shared data structures and perform I/O operations by a single thread. For these reasons, journaling file systems often face the problem of lock contention and underutilization of I/O bandwidth on multi-cores with high-performance storage. To address these issues, we design journaling and checkpointing schemes that enable concurrent updates on data structures and parallelize I/O operations. We implement our schemes in EXT4/JBD2 and evaluate them on a 72-core machine with a high-performance NVMe SSD. The experimental results show that our optimized file system improves the performance by up to about 2.2x and 1.5x compared to the existing EXT4 file system and a state-of-art scalable file system, respectively. 2. Designing a True Direct-Access File System with DevFS. Source Abstract We present DevFS, a direct-access file system embedded completely within a storage device. DevFS provides direct access without compromising file system integrity, concurrency, crash consistency, and security. A novel reverse-caching mechanism enables the usage of host memory for inactive objects, thus reducing memory load upon the device. Evaluation of an emulated DevFS prototype shows more than 2x higher I/O throughput with direct access and up to a 78% reduction in device RAM utilization. 3. FStream: Managing Flash Streams in the File System. Source Abstract The performance and lifespan of a solid-state drive (SSD) depend not only on the current input workload but also on its internal media fragmentation formed over time. The recently proposed streams gives a means for the host system to control how data are placed on the physical media (abstrated by a stream) and effectively reduce the media fragmentation. This work proposes FStream, a file system approach to taking advantage of this facility. FStream extracts streams at the file system level and avoids complex application level data mapping to streams. Experimental results showed that FStream enhances the filebench performance by 5%-35% and reduces WAF (Write Amplification Factor) by 7%-46%. For a NoSQL database benchmark, performance is improved by up to 38% and WAF is reduced by up to 81%. Distribute and Conquer 1. Improving Docker Registry Design Based on Production Workload Analysis. Source Abstract Containers offer an efficient way to run workloads as independent microservices that can be developed, tested and deployed in an agile manner. To facilitate this process, container frameworks offer a registry service that enables users to publish and version container images and share them with others. The registry service plays a critical role in the startup time of containers since many container starts entail the retrieval of container images from a registry. To support research efforts on optimizing the registry service, large-scale and realistic traces are required. In this paper, we perform a comprehensive characterization of a large-scale registry workload based on traces that we collected over the course of 75 days from five IBM data centers hosting production-level registries. We present a trace replayer to perform our analysis and infer a number of crucial insights about container workloads, such as request type distribution, access patterns, and response times. Based on these insights, we derive design implications for the registry and demonstrate their ability to improve performance. Both the traces and the replayer are open-sourced to facilitate further research. 2. RAID+: Deterministic and Balanced Data Distribution for Large Disk Enclosures. Source Abstract Existing RAID solutions partition large disk enclosures so that each RAID group uses its own disks exclusively. This achieves good performance isolation across underlying disk groups, at the cost of disk under-utilization and slow RAID reconstruction from disk failures. We propose RAID+, a new RAID construction mechanism that spreads both normal I/O and reconstruction workloads to a larger disk pool in a balanced manner. Unlike systems conducting randomized placement, RAID+ employs deterministic addressing enabled by the mathematical properties of mutually orthogonal Latin squares, based on which it constructs 3-D data templates mapping a logical data volume to uniformly distributed disk blocks across all disks. While the total read/write volume remains unchanged, with or without disk failures, many more disk drives participate in data service and disk reconstruction. Our evaluation with a 60-drive disk enclosure using both synthetic and real-world workloads shows that RAID+ significantly speeds up data recovery while delivering better normal I/O performance and higher multi-tenant system throughput. 3. Logical Synchronous Replication in the Tintri VMstore File System. Source Abstract A standard feature of enterprise data storage systems is synchronous replication: updates received from clients by one storage system are replicated to a remote storage system and are only acknowledged to clients after having been stored persistently on both storage systems. Traditionally these replication schemes require configuration on a coarse granularity, e.g. on a LUN, filesystem volume, or whole-system basis. In contrast to this, we present a new architecture which operates on a fine granularity---individual files and directories. To implement this, we use a combination of novel per-file capabilities and existing techniques to solve the following problems: tracking parallel writes in flight on independent storage systems; replicating arbitrary filesystem operations; efficiently resynchronizing after a disconnect; and verifying the integrity of replicated data between two storage systems. Dedup: Last but Not Least 1. ALACC: Accelerating Restore Performance of Data Deduplication Systems Using Adaptive Look-Ahead Window Assisted Chunk Caching. Source Abstract Data deduplication has been widely applied in storage systems to improve the efficiency of space utilization. In data deduplication systems, the data restore performance is seriously hindered by read amplification since the accessed data chunks are scattered over many containers. A container consisting of hundreds or thousands data chunks is the data unit to be read from or write to the storage. Several schemes such as forward assembly, container-based caching, and chunk-based caching are used to reduce the number of container-reads during the restore process. However, how to effectively use these schemes to get the best restore performance is still unclear. In this paper, we first study the trade-offs of using these schemes in terms of read amplification and computing time. We then propose a combined data chunk caching and forward assembly scheme called ALACC (Adaptive Look-Ahead Chunk Caching) for improving restore performance which can adapt to different deduplication workloads with a fixed total amount of memory. This is accomplished by extending and shrinking the look-ahead window adaptively to cover an appropriate data recipe range and dynamically deciding the memory to be allocated to forward assembly area and chunk-based caching. Our evaluations show the restore throughput of ALACC is higher than that of the optimum case of various configurations using the fixed amount of memory allocated to forward assembly and to chunk-based caching. 2. UKSM: Swift Memory Deduplication via Hierarchical and Adaptive Memory Region Distilling. Source Abstract In cloud computing, deduplication can reduce memory footprint by eliminating redundant pages. The responsiveness of a deduplication process to newly generated memory pages is critical. State-of-the-art Content Based Page Sharing (CBPS) approaches lack responsiveness as they equally scan every page while finding redundancies. We propose a new deduplication system UKSM, which prioritizes different memory regions to accelerate the deduplication process and minimize application penalty. With UKSM, memory regions are organized as a distilling hierarchy, where a region in a higher level receives more CPU cycles. UKSM adaptively promotes/demotes a region among levels according to the region’s estimated deduplication benefit and penalty. UKSM further introduces an adaptive partial-page hashing scheme which adjusts a global page hashing strength parameter according to the global degree of page similarity. Experiments demonstrate that, with the same amount of CPU cycles in the same time envelop, UKSM can achieve up to 12.6and 5 more memory saving than CBPS approaches on static and dynamic workloads, respectively. FAST 2017 Garbage 1. Algorithms and Data Structures for Efficient Free Space Reclamation in WAFL. Source Abstract NetApp®WAFL®is a transactional file system that uses the copy-on-write mechanism to support fast write performance and efficient snapshot creation. However, copy-on-write increases the demand on the file system to find free blocks quickly; failure to do so may impede allocations for incoming writes. Efficiency is also important, because the task may consume CPU and other resources. In this paper, we describe the evolution (over more than a decade) of WAFL’s algorithms and data structures for reclaiming space with minimal impact on the overall storage appliance performance. 2. Tiny-Tail Flash: Near-Perfect Elimination of Garbage Collection Tail Latencies in NAND SSDs. Source Abstract TTFLASH is a “tiny-tail” flash drive (SSD) that eliminates GC-induced tail latencies by circumventing GCblocked I/Os with four novel strategies: plane-blocking GC, rotating GC, GC-tolerant read, and GC-tolerant flush. It is built on three SSD internal advancements: powerful controllers, parity-based RAIN, and capacitorbacked RAM, but is dependent on the use of intra-plane copyback operations. We show that TTFLASH comes significantly close to a “no-GC” scenario. Specifically, between 99–99.99th percentiles, TTFLASH is only 1.0 to 2.6× slower than the no-GC case, while a base approach suffers from 5–138× GC-induced slowdowns. 3. The Logic of Physical Garbage Collection in Deduplicating Storage. Source Abstract Most storage systems that write in a log-structured manner need a mechanism for garbage collection (GC), reclaiming and consolidating space by identifying unused areas on disk. In a deduplicating storage system, GC is complicated by the possibility of numerous references to the same underlying data. We describe two variants of garbage collection in a commercial deduplicating storage system, a logical GC that operates on the files containing deduplicated data and a physical GC that performs sequential I/O on the underlying data. The need for the second approach arises from a shift in the underlying workloads, in which exceptionally high duplication ratios or the existence of millions of individual small files result in unacceptably slow GC using the file-level approach. Under such workloads, determining the liveness of chunks becomes a slow phase of logical GC. We find that physical GC decreases the execution time of this phase by up to two orders of magnitude in the case of extreme workloads and improves it by approximately 10–60% in the common case, but only after additional optimizations to compensate for its higher initialization overheads. The System 1. File Systems Fated for Senescence? Nonsense, Says Science! Source Abstract File systems must allocate space for files without knowing what will be added or removed in the future. Over the life of a file system, this may cause suboptimal file placement decisions which eventually lead to slower performance, or However, this paper describes realistic as well as synthetic workloads that can cause these heuristics to fail, inducing large performance declines due to aging. For example, on ext4 and ZFS, a few hundred git pull operations can reduce read performance by a factor of 2; performing a thousand pulls can reduce performance by up to a factor of 30. We further present microbenchmarks demonstrating that common placement strategies are extremely sensitive to file-creation order; varying the creation order of a few thousand small files in a real-world directory structure can slow down reads by 15–175x, depending on the file system. We argue that these slowdowns are caused by poor layout. We demonstrate a correlation between read performance of a directory scan and the locality within a file system’s access patterns, using a In short, many file systems are exquisitely prone to read aging for a variety of write workloads. We show, however, that aging is not inevitable. BetrFS, a file system based on write-optimized dictionaries, exhibits almost no aging in our experiments. BetrFS typically outperforms the other file systems in our benchmarks; aged BetrFS even outperforms the unaged versions of these file systems, excepting Btrfs. We present a framework for understanding and predicting aging, and identify the key features of BetrFS that avoid aging. 2. To FUSE or Not to FUSE: Performance of User-Space File Systems. Source Abstract Traditionally, file systems were implemented as part of OS kernels. However, as complexity of file systems grew, many new file systems began being developed in user space. Nowadays, user-space file systems are often used to prototype and evaluate new approaches to file system design. Low performance is considered the main disadvantage of user-space file systems but the extent of this problem has never been explored systematically. As a result, the topic of user-space file systems remains rather controversial: while some consider user-space file systems a toy not to be used in production, others develop full-fledged production file systems in user space. In this paper we analyze the design and implementation of the most widely known user-space file system framework—FUSE—and characterize its performance for a wide range of workloads. We instrumented FUSE to extract useful statistics and traces, which helped us analyze its performance bottlenecks and present our analysis results. Our experiments indicate that depending on the workload and hardware used, performance degradation caused by FUSE can be completely imperceptible or as high as –83% even when optimized; and relative CPU utilization can increase by 31%. 3. Knockoff: Cheap Versions in the Cloud. Source Abstract Cloud-based storage provides reliability and ease-of-management. Unfortunately, it can also incur significant costs for both storing and communicating data, even after using techniques such as chunk-based deduplication and delta compression. The current trend of providing access to past versions of data exacerbates both costs. In this paper, we show that deterministic recomputation of data can substantially reduce the cost of cloud storage. Borrowing a well-known dualism from the fault-tolerance community, we note that any data can be equivalently represented by a log of the nondeterministic inputs needed to produce that data. We design a file system, called Knockoff, that selectively substitutes nondeterministic inputs for file data to reduce communication and storage costs. Knockoff compresses both data and computation logs: it uses chunk-based deduplication for file data and delta compression for logs of nondeterminism. In two studies, Knockoff reduces the average cost of sending files to the cloud without versioning by 21% and 24%; the relative benefit increases as versions are retained more frequently. 4. HopsFS: Scaling Hierarchical File System Metadata Using NewSQL Databases. Source Abstract Recent improvements in both the performance and scalability of shared-nothing, transactional, in-memory NewSQL databases have reopened the research question of whether distributed metadata for hierarchical file systems can be managed using commodity databases. In this paper, we introduce HopsFS, a next generation distribution of the Hadoop Distributed File System (HDFS) that replaces HDFS’ single node in-memory metadata service, with a distributed metadata service built on a NewSQL database. By removing the metadata bottleneck, HopsFS enables an order of magnitude larger and higher throughput clusters compared to HDFS. Metadata capacity has been increased to at least 37 times HDFS’ capacity, and in experiments based on a workload trace from Spotify, we show that HopsFS supports 16 to 37 times the throughput of Apache HDFS. HopsFS also has lower latency for many concurrent clients, and no downtime during failover. Finally, as metadata is now stored in a commodity database, it can be safely extended and easily exported to external systems for online analysis and free-text search. Edward Sharpe and the Magnetic Zeros 1. Evolving Ext4 for Shingled Disks. Source Abstract Drive-Managed SMR (ShingledMagnetic Recording) disks offer a plug-compatible higher-capacity replacement for conventional disks. For non-sequential workloads, these disks show bimodal behavior: After a short period of high throughput they enter a continuous period of low throughput. We introduce ext4-lazy 2. SMaRT: An Approach to Shingled Magnetic Recording Translation. Source Abstract Shingled Magnetic Recording (SMR) is a new technique for increasing areal data density in hard drives. Drivemanaged SMR (DM-SMR) drives employ a shingled translation layer to mask internal data management and support block interface to the host software. Two major challenges of designing an efficient shingled translation layer for DM-SMR drives are metadata overhead and garbage collection overhead. In this paper we introduce SMaRT, an approach to We implement SMaRT and compare it with a regular Hard Disk Drive (HDD) and a simulated Seagate DM-SMR drive. The experiments with several block I/O traces demonstrate that SMaRT performs better than the Seagate drive and even provides comparable performance as regular HDDs when drive space usage is below a certain threshold. 3. Facilitating Magnetic Recording Technology Scaling for Data Center Hard Disk Drives through Filesystem-Level Transparent Local Erasure Coding. Source Abstract This paper presents a simple yet effective design solution to facilitate technology scaling for hard disk drives (HDDs) being deployed in data centers. Emerging magnetic recording technologies improve storage areal density mainly through reducing the track pitch, which however makes HDDs subject to higher read retry rates. More frequent HDD read retries could cause intolerable tail latency for large-scale systems such as data centers. To reduce the occurrence of costly read retry, one intuitive solution is to apply erasure coding locally on each HDD or JBOD (just a bunch of disks). To be practically viable, local erasure coding must have very low coding redundancy, which demands very long codeword length (e.g., one codeword spans hundreds of 4kB sectors) and hence large file size. This makes local erasure coding mainly suitable for data center applications. This paper contends that local erasure coding should be implemented transparently within filesystems, and accordingly presents a basic design framework and elaborates on important design issues. Meanwhile, this paper derives the mathematical formulations for estimating its effect on reducing HDD read tail latency. Using Reed-Solomon (RS) based erasure codes as test vehicles, we carried out detailed analysis and experiments to evaluate its implementation feasibility and effectiveness. We integrated the developed design solution into ext4 to further demonstrate its feasibility and quantitatively measure its impact on average speed performance of various big data benchmarks. Corruption 1. Redundancy Does Not Imply Fault Tolerance: Analysis of Distributed Storage Reactions to Single Errors and Corruptions. Source Abstract We analyze how modern distributed storage systems behave in the presence of file-system faults such as data corruption and read and write errors. We characterize eight popular distributed storage systems and uncover numerous bugs related to file-system fault tolerance. We find that modern distributed systems do not consistently use redundancy to recover from file-system faults: a single file-system fault can cause catastrophic outcomes such as data loss, corruption, and unavailability. Our results have implications for the design of next generation fault-tolerant distributed and cloud storage systems. 2. Omid, Reloaded: Scalable and Highly-Available Transaction Processing. Source Abstract We present Omid—a transaction processing service that powers web-scale production systems at Yahoo. Omid provides ACID transaction semantics on top of traditional key-value storage; its implementation over Apache HBase is open sourced as part of Apache Incubator. Omid can serve hundreds of thousands of transactions per second on standard mid-range hardware, while incurring minimal impact on the speed of data access in the underlying key-value store. Additionally, as expected from always-on production services, Omid is highly available. 3. Application Crash Consistency and Performance with CCFS. Source Abstract Recent research has shown that applications often incorrectly implement crash consistency. We present ccfs, a file system that improves the correctness of application-level crash consistency protocols while maintaining high performance. A key idea in ccfs is the abstraction of a 4. High Performance Metadata Integrity Protection in the WAFL Copy-on-Write File System. Source Abstract We introduce a low-cost incremental checksum technique that protects metadata blocks against in-memory scribbles, and a lightweight digest-based transaction auditing mechanism that enforces file system consistency invariants. Compared with previous work, our techniques reduce performance overhead by an order of magnitude. They also help distinguish scribbles from logic bugs. We also present a mechanism to pinpoint the cause of scribbles on production systems. Our techniques have been productized in the NetApp® WAFL® (Write Anywhere File Layout) file system with negligible performance overhead, greatly reducing corruption-related incidents over the past five years, based on millions of runtime hours. Frameworks 1. Mirador: An Active Control Plane for Datacenter Storage. Source Abstract This paper describes 2. Chronix: Long Term Storage and Retrieval Technology for Anomaly Detection in Operational Data. Source Abstract Anomalies in the runtime behavior of software systems, especially in distributed systems, are inevitable, expensive, and hard to locate. To detect and correct such anomalies (like instability due to a growing memory consumption, failure due to load spikes, etc.) one has to automatically collect, store, and analyze the operational data of the runtime behavior, often represented as time series. There are efficient means both to collect and analyze the runtime behavior. But traditional time series databases do not yet focus on the specific needs of anomaly detection (generic data model, specific built-in functions, storage efficiency, and fast query execution). The paper presents Chronix, a domain specific time series database targeted at anomaly detection in operational data. Chronix uses an ideal compression and chunking of the time series data, a methodology for commissioning Chronix’ parameters to a sweet spot, a way of enhancing the data with attributes, an expandable set of analysis functions, and other techniques to achieve both faster query times and a significantly smaller memory footprint. On benchmarks Chronix saves 20%–68% of the space that other time series databases need to store the data and saves 80%–92% of the data retrieval time and 73%–97% of the runtime of analyzing functions. 3. Crystal: Software-Defined Storage for Multi-Tenant Object Stores. Source Abstract Object stores are becoming pervasive due to their scalability and simplicity. Their broad adoption, however, contrasts with their rigidity for handling heterogeneous workloads and applications with evolving requirements, which prevents the adaptation of the system to such varied needs. In this work, we present Solid State Records 1. WORT: Write Optimal Radix Tree for Persistent Memory Storage Systems. Source Abstract Recent interest in persistent memory (PM) has stirred development of index structures that are efficient in PM. Recent such developments have all focused on variations of the B-tree. In this paper, we show that the radix tree, which is another less popular indexing structure, can be more appropriate as an efficient PM indexing structure. This is because the radix tree structure is determined by the prefix of the inserted keys and also does not require tree rebalancing operations and node granularity updates. However, the radix tree as-is cannot be used in PM. As another contribution, we present three radix tree variants, namely, WORT (Write Optimal Radix Tree), WOART (Write Optimal Adaptive Radix Tree), and ART+CoW. Of these, the first two are optimal for PM in the sense that they only use one 8-byte failure-atomic write per update to guarantee the consistency of the structure and do not require any duplicate copies for logging or CoW. Extensive performance studies show that our proposed radix tree variants perform considerable better than recently proposed B-tree variants for PM such NVTree, wB+Tree, and FPTree for synthetic workloads as well as in implementations within Memcached. 2. SHRD: Improving Spatial Locality in Flash Storage Accesses by Sequentializing in Host and Randomizing in Device. Source Abstract Recent advances in flash memory technology have reduced the cost-per-bit of flash storage devices such as solid-state drives (SSDs), thereby enabling the development of large-capacity SSDs for enterprise-scale storage. However, two major concerns arise in designing SSDs. The first concern is the poor performance of random writes in an SSD. Server workloads such as databases generate many random writes; therefore, this problem must be resolved to enable the usage of SSDs in enterprise systems. The second concern is that the size of the internal DRAM of an SSD is proportional to the capacity of the SSD. The peculiarities of flash memory require an address translation layer called flash translation layer (FTL) to be implemented within an SSD. The FTL must maintain the address mapping table in the internal DRAM. Although the previously proposed demand map loading technique can reduce the required DRAM size, the technique aggravates the poor random performance. We propose a novel address reshaping technique called sequentializing in host and randomizing in device (SHRD), which transforms random write requests into sequential write requests in the block device driver by assigning the address space of the reserved log area in the SSD. Unlike previous approaches, SHRD can restore the sequentially written data to the original location without requiring explicit copy operations by utilizing the address mapping scheme of the FTL.We implement SHRD in a real SSD device and demonstrate the improved performance resulting from SHRD for various workloads. 3. Graphene: Fine-Grained IO Management for Graph Computing. Source Abstract As graphs continue to grow, external memory graph processing systems serve as a promising alternative to inmemory solutions for low cost and high scalability. Unfortunately, not only does this approach require considerable efforts in programming and IO management, but its performance also lags behind, in some cases by an order of magnitude. In this work, we strive to achieve an ambitious goal of achieving ease of programming and high IO performance (as in-memory processing) while maintaining graph data on disks (as external memory processing). To this end, we have designed and developed Faster Faster 1. vNFS: Maximizing NFS Performance with Compounds and Vectorized I/O. Source Abstract Modern systems use networks extensively, accessing both services and storage across local and remote networks. Latency is a key performance challenge, and packing multiple small operations into fewer large ones is an effective way to amortize that cost, especially after years of significant improvement in bandwidth but not latency. To this end, the NFSv4 protocol supports a We propose 2. On the Accuracy and Scalability of Intensive I/O Workload Replay. Source Abstract We introduce a replay tool that can be used to replay captured I/O workloads for performance evaluation of high-performance storage systems. We study several sources in the stock operating system that introduce the uncertainty of replaying a workload. Based on the remedies of these findings, we design and develop a new replay tool called 3. On the Performance Variation in Modern Storage Stacks. Source Abstract Ensuring stable performance for storage stacks is important, especially with the growth in popularity of hosted services where customers expect QoS guarantees. The same requirement arises from benchmarking settings as well. One would expect that repeated, carefully controlled experiments might yield nearly identical performance results—but we found otherwise. We therefore undertook a study to characterize the amount of variability in benchmarking modern storage stacks. In this paper we report on the techniques used and the results of this study. We conducted many experiments using several popular workloads, file systems, and storage devices—and varied many parameters across the entire storage stack. In over 25% of the sampled configurations, we uncovered variations higher than 10% in storage performance between runs. We analyzed these variations and found that there was no single root cause: it often changed with the workload, hardware, or software configuration in the storage stack. In several of those cases we were able to fix the cause of variation and reduce it to acceptable levels. We believe our observations in benchmarking will also shed some light on addressing stability issues in production systems. 4. Enlightening the I/O Path: A Holistic Approach for Application Performance. Source Abstract In data-intensive applications, such as databases and keyvalue stores, reducing the request handling latency is important for providing better data services. In such applications, I/O-intensive background tasks, such as checkpointing, are the major culprit in worsening the latency due to the contention in shared I/O stack and storage. To minimize the contention, properly prioritizing I/Os is crucial but the effectiveness of existing approaches is limited for two reasons. First, statically deciding the priority of an I/O is insufficient since high-priority tasks can wait for low-priority I/Os due to In this paper, we propose a ","link":"https://blog.shunzi.tech/post/fast-2017-2019/"},{"title":"WiscKey: Separating Keys from Values in SSD-Conscious Storage","content":" 文章题目：WiscKey: Separating Keys from Values in SSD-Conscious Storage。FAST 16 上的文章，该论文的延申发表在 TOS17。 结合之前阅读的 SLM DB 关于 LSM KV 存储的优化方案，想要对 LSM 有更深入的理解 该篇论文主要基于 LSM 的持久化 KV 存储提出了一种数据布局方式来进行优化，尤其是针对 SSD 设备 BTW, 一篇会议转期刊的典型范例....（逃 Abstract WiscKey，一种基于 LSM Tree 的持久化 KV 存储且具有面向性能的数据布局的优化方案 WiscKey 的设计是高度 SSD 优化的，利用了设备的顺序和随机性能特性 相比于 LevelDB 和 RocksDB 都有较大的性能提升 Introduction 相关概念引入 持久化 KV 存储：由于其高效的插入、点查找和范围查询等特性，持久化存储成为了众多现代应用存储的基础。 LSM-Trees KV Store： 针对写敏感型的负载，基于 LSM Tree 的 KV 存储方案已经成为了行业的主流解决方案。 LSM Tree 由于其保证了顺序性，同时采用批量写的方式，相比于 B Tree引入的随机小写，是更利于硬件设备的 IO，因为无论是 SSD 还是 HDD，顺序写的性能都好于随机写 要解决的问题 LSM Tree 为了保证顺序性，存在较为严峻的写放大问题（具体成因请参考 LSM Tree 相关介绍） LSM Tree 的相关特性对于现有的 SSD 不是特别友好（未能充分利用 SSD 的特性） SSD 的随机写和顺序写性能差距相对较小，原始 LSM Tree 得不偿失，因此为了减少随机写而使用大量顺序写可能会对存储带宽造成不必要的浪费（HDD 由于顺序写和随机写的性能差距较大，即便 LSM Tree 存在严峻的写放大问题，但保证了顺序写的特性也使得 LSM 在 HDD 上的实现能显著提升性能） SSD 内部的并行机制未能得到充分利用，原始的 LSM Tree 未考虑到 SSD 内部的并行性； SSD 寿命会受写放大问题的严重影响，原始的 LSM Tree 严峻的写放大问题会导致大量的重复写 提出的方案简介 WiscKey：基于 LSM 的通用实现 LevelDB，提出的针对 SSD 进行优化的持久化 KV 存储方案 核心思想：将 Key 和 Value 分离，Key 存储在 LSM Tree 中，Value 存储在 Log 中。（解耦了键排序和垃圾回收） 实现的效果：保留了 LSM Tree的优势，减小了写放大问题 解决问题的方式 排序时，避免 Value 不必要的迁移来减小写放大 LSM Tree 只存储 Key，数据量变小，从而减小对存储设备的访问次数，更利于使用缓存 该方案面临的挑战和解决方案 分离 KV Value 不再有序存储，会影响 range query（范围查询）的性能 -&gt; 利用 SSD 内部的并行性来提升范围查询的性能（弥补措施） 需要对 Value 进行垃圾回收，回收无效的空间 -&gt; 提出了一种轻量级的 online 垃圾回收机制，它只涉及顺序I/Os 崩溃一致性如何实现 -&gt; 利用了现代文件系统中的一个特性，即不会在崩溃时附加垃圾数据，设计了一致性保证机制来实现和 LSM Tree 一样的效果 方案测试效果简介 比较对象：LevelDB、RocksDB 测试负载：LevelDB’s microbenchmark，YCSB macrobenchmarks 测试结果： LevelDB’s microbenchmark： Load Database：WiscKey 比 LevelDB 快 2.5x - 111x Random lookups：WiscKey 比 LevelDB 快 1.6x - 14x Write Small Values in Random Order：Worse Large Dataset is ranged-quried sequentially：Worse YCSB macrobenchmarks： 六种负载下，WiscKey 表现均更好 Background 该章节主要针对 LSM Tree、LevelDB、读写放大问题 和 新型存储器件 四部分进行叙述。 LSM Tree &amp; LevelDB LSM Tree 和 LevelDB 的原理和读写流程以及一致性的保证方式，此处不再赘述。 可参考我的另外一篇读书报告中的笔记 知乎：SLM-DB KV Store with Persistent Memory 读写放大问题 以 LevelDB 为例，分析其存在的读写放大问题 根本原因：保证磁盘上的数据严格全局有序 主要原因 写： 磁盘上的层级之间的大小限制通常有 10 倍之差，意味着从上层 L(i) 往下一层 L(i+1) 压缩时，最坏的情况下可能需要读取 L(i+1) 层的所有数据来和 L(i) 层的数据进行归并排序，排序完成后将新的数据结果写入到 L(i+1) 层，故在简单的两层数据压缩过程中最坏的情况下就可能出现十次的 SSTable file 数据迁移。 如果数据需要从 L0 层迁移到 L6 层这种极端情况时，数据迁移次数则将达到 50 次。 读： 造成读放大的原因之一：多层结构导致查询操作需要对多层进行数据查询。最坏的情况，查询操作扫描了 L0 层的8个 SSTable File，未找到继续在下一层寻找，直到 L6 层，即总计查询 8+6 = 14 个文件。（L0 层的每一个 SSTable 文件中存储的 Key 是可能重叠的，所以需要遍历 L0 层的所有 SSTable 文件，而 L1-L6 层中的 Key 是严格有序的，故只需要读取一个文件） 造成读放大的原因之二：为了在 SSTable 文件中找到 Key 对应的 Value 往往需要读取多个元数据块。（索引块、布隆过滤器块和数据块）。如读取一个 1KB 的 KV 对，需要读取 16KB 的索引块、4KB 的布隆过滤器块和 4KB 的数据块，即读放大 24 倍 所以最坏的情况下，读放大将达到 24*14 = 336 倍，小 IO 甚至可能造成更严峻的读放大问题。 新型存储器件 LSM 适用于 SSD 的原因 SSD 同 HDD 类似，随机 IO 的性能比起顺序 IO 的性能较差。 随机写对于 SSD 的损害极大，由于 SSD 本身的擦除写机制和垃圾回收机制。 故需要使用 LSM 来保证写入数据的顺序性 SSD 其他特性 SSD 的随机 IO 性能和顺序 IO 性能差距较小，相比于 HDD SSD 的随机 IO 性能要好于 HDD 的随机性能 测试发现 SSD 的并发随机写性能在某些场景下接近 SSD 的顺序写性能 WiscKey 四个核心思想： 分离 Key 和 Value，Key 存储在 LSM Tree 中，Value 存储在单独的日志文件中 针对存储在日志文件中的无序 Value 的处理，（范围查询时需要对硬盘进行随机访问），WiscKey 利用 SSD 内部的并行性提升随机读性能 WiscKey 使用专门的崩溃一致性保障机制和垃圾回收机制来高效管理 Value 对应的 log WiscKey 在保证一致性的前提下移除了 LSM-tree 日志来提升性能，尤其是减小了小IO的开销 Key-Value 分离 该方法解决的问题：传统的 LSM-Tree 中性能开销最大的任务是后台压缩，压缩是为了保证顺序 IO 进行的操作，主要是在磁盘上的多层级结构之间对 SSTable 文件进行归并排序，所以涉及到大量的文件的读取迁移等操作。 理论基础 WiscKey 发现排序只需要对 Key 排序，Value 以 SSD 友好的形式存储在其他地方，Key 和 Value 之间使用地址进行关联，故在 LSM Tree 中存储的是 Key-Address。该方式可以极大地减小排序过程中的数据量，从而减小写放大问题。16B Key - 1024B Value，传统的两层之间的写放大系数为 10 倍，采用 WiscKey 后，（16*10 + 1024）/ （16 + 1024） = 1.14 。注意此处放大系数的计算不准确，放大过程中还有存储的地址的大小。 减小了写放大问题，也就对应地减少了 SSD 上的擦除写，从而延长了 SSD 的使用寿命 是否引入了新的读放大问题？存在，但较小的读放大带来的是整体性能的提升。由于 LSM Tree 中存储的数据量显著减少，相比于传统的从 LSM Tree 中读取 KV 对的数据信息，WiscKey 这种先读取 Key，再根据对应的地址去读 Value 对应的读取的速度在数据量相对较大的情况下是更快的。除此以外还可以针对数据量较小的 LSM Tree 使用内存缓存来加速对 Key 的检索。 架构和流程 组成： LSM Tree - 存储 &lt;Key, Address&gt; Value Log File - 存储 Value 流程 Insert：首先将 Value 追加写到 Value Log 中，然后将 Key 和 Value 对应的地址信息（地址信息主要包含对应的偏移量 vlog-offset 和 Value 的数据大小 value-size）组成的键值对插入到 LSM Tree 中 Delete：直接在 LSM Tree 中删除对应的 Key，不涉及 Value Log，对应的 Value 由垃圾回收机制统一地进行处理。 Point/Range Query： 先去 LSM Tree 中检索对应的 Key，拿到其对应的 Value Address 信息后对应地从 Value Log File 中去读取对应的 Value。 KV 分离对应的挑战（存在的问题） 并行范围查询 Parallel Range Query Range Query 的重要性以及应用场景不再赘述，简单介绍 LevelDB 为了实现 Range Qurey 对外提供的 API。 Seek(key), Next(), Prev(), Key(), Value() 首先 seek 到起始 Key，然后调用 Next 或者 Prev 一个接一个地检索 Key，真正需要获取该指针对应的 Key 时调用 Key(), Value 则调用 Value() 存在的问题 传统的 LSM Tree 如 LevelDB，在进行 Range Query 操作时，由于其 KV 对是顺序地保存在 LSM Tree 中的，故可以进行顺序的 IO 就可以读取出来；但是针对 WiscKey 将 KV 分离了的方案，不可避免地就造成了对 Value 的随机写问题。 关于随机写以及 SSD 的随机写的特性在前述章节中已经介绍到，性能相对较差。 解决思路和实现 解决思路：前述章节有测试过 SSD 内部并行随机 IO 的性能，在一定条件下是可以达到顺序 IO 的速度的，故考虑提高并行性来减小随机 IO 带来的影响。 实现方式：针对 Range Qurey 的操作， 一旦请求了一个连续的键-值对序列，WiscKey 就开始按顺序从 LSM 树中读取大量的后续键，将对应的 Key Address 信息插入到一个队列中，然后启动多线程去从该队列中获取地址信息再并发地去 Value Log File 中读取对应的 Value 信息。 垃圾回收机制的实现 Garbage Collection 传统的 LSM Tree 中垃圾回收的方式是针对要删除的数据或者过时的数据进行标记，在执行压缩过程中对垃圾数据进行回收。 由于 WiscKey 没有对 Value 的压缩过程，所以需要单独设计一套垃圾回收的机制对 Value 进行垃圾回收。 垃圾回收实现思路 最简单的方式（最直接的方式）：扫描 LSM Tree 中存储的所有有效 Key 对应的有效地址信息，将 Value Log 文件中存储的无对应 Key 的 Value 标记为了垃圾数据，再进行回收 更高效的方式：调整 WiscKey 的数据布局。 实现方式 数据布局：在 Value Log 中除了存取 Value 对应的信息外，同时存取 Key 对应的数据信息。数据单元成为 Tuple，其中包含 key size, value size, key, value。 设置头尾指针来进行垃圾回收操作，尾指针只会在垃圾回收时被移动，头指针只会在有数据追加写入到 Value Log File 中时才会移动。 垃圾回收的过程：WiscKey 从尾指针指向的位置读取一个 Chunk 大小的 KV 键值对的集合，大约数 MB，通过查询 LSM Tree 中是否有该集合内的键信息来判断当前 Value 是否有被覆盖写或者删除。判断为有效状态的 Value 将追加写到头指针指向的位置，然后释放尾指针对应的一个 Chunk 的存储空间，并更新尾指针的位置。 垃圾回收过程中一致性的保证 为了避免出现数据丢失（如在垃圾回收过程中发生了故障），需要保证在释放对应的存储空间之前追加写入的新的有效 Value 和新的尾指针持久化到了设备上。 WiscKey 实现垃圾回收过程中的一致性的步骤： 追加写入新的 Value 值到 Value Log 中 调用 fsync()，将 Value Log File 进行同步 同步地将新的 Value 地址和尾指针地址写入到 LSM Tree 中。（尾指针的存储形式为 Key:tail - Value:tail-vlog-offset） 最后进行相应的垃圾回收，释放对应的空间 垃圾回收的时机 WiscKey 可以根据实际需求进行配置，主要有以下两种方式： 周期性地进行垃圾回收 到达一定阈值时触发垃圾回收 崩溃一致性的保证 Crash Consistency 传统的 LSM Tree 的崩溃一致性保证使用了 WAL 来保证 KV 对的原子性插入，恢复时也能严格按照插入的顺序进行恢复，但 LSM Tree 由于采用了 KV 分离的方式使得一致性的保证机制实现变得更为复杂。 WiscKey 一致性机制的实现利用了文件系统的一个特性：即一旦出现了系统故障，发生数据丢失时，针对顺序写入的数据序列 X1 X2 X3...，一旦 X1 丢失，那么 X1 之后的数据都将丢失。具体的讲解请参考 OSDI 14 的一篇文章 All File Systems Are Not Created Equal: On the Complexity of Crafting Crash-Consistent Applications。 实现方式/处理流程 由于数据造成的数据丢失，如果未能在 LSM Tree 中找到对应的丢失的 Key，那么处理方式和传统的 LSM Tree 一样，返回空或者 Key 不存在。（即便其 Value 已经写入到了 Vlog 文件中，会对其进行垃圾回收） 如果 LSM Tree 中存在要查找的 Key，那么将会比传统的 LSM Tree 多一次校验的操作，该校验操作主要校验从 LSM Tree 中查询到的 Value 地址信息是否在有效的 Vlog File 的范围内；其次校验该地址对应的 Value 上存取的 Key 和要查询的 Key 是否一致。如果校验失败，则对应地删除 LSM Tree 中的 Key，并返回给客户端 Key Not Found。 对 Vlog 上查询到的 Value 校验，可以利用对应的 Key 信息，也可以引入 Magic Number 或者校验和的机制让校验更简单。 当用户请求同步插入数据时，LSM Tree 还保证了键值对在系统崩溃后的持久性；而 WiscKey 也可以通过在向 LSM Tree 中同步插入对应的 Key 之前将 Vlog 上的数据同步刷回来实现同步的数据插入。 优化方案 KV 分离对应地引发了新的思考，关于 Value Log 的写操作和原有 LSM Tree Log 写操作，优化方案将主要针对提升相应的日志写的性能展开。 Value-Log Write Buffer 针对每次对 Value Log 的写入操作，都将进行系统调用 write()，对于某些插入敏感型的负载，会对文件系统发起大量的小写操作，针对不同的设备的带宽，选择合适大小的写入单元就显得十分必要。 写 为了减小开销，WiscKey 利用了用户空间的缓冲机制，只有当缓冲池中数据达到设置的阈值或者用户使用同步插入操作时，才将数据进行刷回，从而减小 write() 调用的次数。 读 查询操作，WiscKey 首先搜索 VLog Buffer 中是否存在要查询的键，如果不存在再从 Vlog File 中去读取。 存在的问题 当系统故障时，可能导致缓冲区中的数据丢失，其崩溃一致性的保证同 LevelDB 实现相同。 疑问？？？？？ 优化 LSM Tree Log LSM Tree Log 设计之初就是为了保证保证插入 LSM Tree 中的键值对的一致性，当出现故障时，能够从日志文件中按照插入顺序正确地恢复出来。 WiscKey 由于在垃圾回收机制的设计中，将 Vlog 上存储的数据结构中加入了 Ket 的信息，就已经起到了只存取 Key 的 LSM Tree Log 的作用。 故障恢复 故障恢复时可以直接通过扫描 Vlog 上的数据来对 LSM 中的 Key 进行恢复。 数据恢复的方式，即 Vlog 扫描的方式还是分为两种： （最简单但最低效）扫描整个 Vlog 文件来进行数据恢复 更高效的方式：为了只扫描部分数据，又因为垃圾回收机制中引入了 HEAD 指针，该指针只会在有数据写入的时候移动，所以 WiscKey 通过将该 HEAD 指针的信息以 &lt;head, head-vlog-offset&gt; 的方式插入到 LSM Tree 中。 数据恢复的流程： 当打开数据库时，WiscKey 根据存储在 LSM Tree 中的最近的 HEAD 信息来对 VLog 文件进行扫描，直到扫描到尾指针队对应的位置为止。 针对大量的小写操作，省略了 LSM Tree Log 的写操纵将显著提升性能。 实现 由于 Vlog 会被多种组件访问，故采用 posix_fadvise() 来预定义 vlog 的不同的访问模式，例如 允许查询操作对 vlog 进行随机访问，垃圾回收器只能顺序地从尾指针的位置读取，并顺序地追加写入到头指针对应的地址。 范围查询使用了线程池来对线程进行管理，默认32线程，当有确定数量的地址插入到查询队列中时，则并发地执行查询操作，并将这些值对应地自动缓存到缓冲区中， 为了实现高效的垃圾回收，采用了现代文件系统常用的 hole-punching 机制，从而保证 WiscKey 弹性地使用存储空间。 Evaluation 测试环境 Processors: 2 x Intel(R) Xeon(R) CPU E5-2667 v2 @ 3.30GHz Memory：64-GB Operating System：64-bit Linux 3.14 File System：ext4 Storage Device：500-GB Samsung 840 EVO SSD (SeqR 500MB/S, SeqW 400MB, RandRead 如前图测试SSD内部并行性) Workload：Microbenchmarks，YCSB Benchmarks Microbenchmarks 使用了 LevelDB 默认的 Microbenchmarks， 即 db_bench Key Size 固定 16B， Value Size 变化 禁用了数据压缩 Load Performance 加载性能 分为了 Seq Load 和 Rand Load。（区别在于构造数据库时插入数据的顺序，一个有序插入，一个随机插入） 图表解释 Figure 7 &amp; Figure 9 都分别展示了顺序加载和随机加载数据随着 Value 大小的变化的性能变化，不难发现 WiscKey 方案充分利用了设备的带宽，而 LevelDB 的性能表现和设备带宽相去甚远，尤其是针对随机加载数据的过程。 Figure 8 展示了 LevelDB 在顺序加载数据的时候，针对不同 Value 大小的负载的加载时间分布。主要有三个部分的时间消耗：写日志文件、写 MemTable，等 Memtable 刷回到磁盘 Figure 10 更直观地统计了写放大系数 原因分析 顺序加载和随机加载的性能，WiscKey 表现较好主要是没有了 LSM Tree 中较大的日志开销，且 WiscKey 省略了 LSM Tree 中的日志，采用了 Vlog 的方式，直接进行追加写；LevelDB 性能较差主要是因为后台的压缩线程占用了磁盘的大量带宽，从而影响了 LevelDB 在处理来自客户端的请求时的吞吐量有所下降。WiscKey 由于在 LSM 上只存取了少量的 Key 数据，所以开销较小，使得带宽能够更好地被利用。 Figure 8 中，Value 较小时，时间主要花费在日志文件的写入上（耗时较长的原因前文有叙述，主要是因为小 IO 对应了多次的 fsync() 的调用）；针对大 IO，日志的写入和 Memtable 的排序是相对高效，而 Memtable 的刷回成为了此时的瓶颈 WiscKey 写放大系数特别小的原因主要是 WiscKey 中的 LSM Tree 数据量较小 Query Performance 查询性能 分析 针对 Figure 11，即便 WiscKey 需要同时查询 LSM Tree 和 Value Log，吞吐量仍然比 LevelDB 要好，主要是因为 LevelDB 存在前文提到的严峻的读放大问题，而 WiscKey 由于 LSM Tree 本身的体量较小，对应的读放大问题没那么严峻，除此以外，WiscKey 中的 LSM Tree 不会存在太多的压缩过程带来的 IO 影响。 针对 Figure 12，LevelDB 针对无论是随机还是顺序的数据加载的 range query，在 Value 为 4KB 之前吞吐量都呈上升趋势，超过 4KB 之后，因为一个 SSTable File 只能存储更少的数量的 KV 键值对，开销将主要集中在打开大量的 SSTable 文件上以及读取索引数据块和布隆过滤器数据，所以吞吐量开始呈下降趋势，而 WiscKey 在这方面的开销要小的多，所以针对 Value 大于 4KB 时，WiscKey 表现更好。 对于 Value 大小为 64-B，WiscKey的性能要比 LevelDB 差 12 倍，这是由于设备对于小请求大小的并行随机读吞吐量有限; WiscKey 在具有更高并行随机读吞吐量的高端 SSD 上的相对性能更好。此外，这种工作负载是最坏的情况，即数据库是随机填充的，而 vLog 中的数据是未排序的 对于顺序加载的负载，Figure 12 中显示的数据趋势与随机加载的负载是相似的。针对 64B 大小的 Value， WiscKey 的性能表现还是差于 LSM Tree。但相比于随机加载的数据，差异相对较小，故可以采用对随机加载到 vlog 的数据进行排序来提高 WiscKey 的性能，以减小 64B 场景下同 LevelDB 的差距。 其他测试 垃圾回收机制 使用随机加载的负载进行垃圾回收性能的测试，因为该负载下垃圾回收的影响将会显著影响整个方案的性能。 测试方式：先随机加载数据，然后删除一定比例的数据，触发垃圾回收，此时再随机加载数据，测试此时的性能表现。 该测试下，Value Size 为 4KB，测试不同比例的空闲空间的情况下的性能。 分析：当垃圾回收器发现数据 100% 无效时，即所有的数据都是垃圾数据，此时由于没有拷贝写的开销，性能只下降了约 10%；而针对部分数据有效的情况，下降约为 35%，因为需要后台垃圾回收线程进行额外的写操作。但即便下降了 35%，性能仍然比 LevelDB 高至少 70%。 崩溃一致性测试 测试工具：ALICE，该工具选择并模拟一组多方位的系统崩溃，这些崩溃都很有可能导致数据的不一致性 测试用例：触发少量的同步和异步的 PUT 操作，在 ext4, xfs, btrfs 上都进行了测试，无数据不一致的问题发生 同时还测试了故障恢复时间，主要是测试最长恢复时间。LevelDB 的最长恢复时间取决于 Log File 的大小，而该文件在 Memtable 写入磁盘之前最大，WiscKey 也是在该情况下恢复需要的时间最长。针对 1KB Value，LevelDB 需要大约 0.7s 进行恢复，而 WiscKey 需要大约 2.6s. Space Amplification 空间放大 之前的研究只关注了读写放大的问题，很少有人关注到空间放大问题。空间放大系数是指数据库在磁盘上的占用的实际大小和逻辑意义上的数据库大小的比值。例如 1KB 的 KV 键值对，在磁盘上可能占用了 4KB 的空间，那么空间放大系数就是 4。 传统的 LSM Tree 中的压缩过程则就减小了相应的空间放大问题，从而提高磁盘空间的利用率。对于顺序加载的负载，空间放大系数通常接近 1，而对于随机加载的负载或者覆盖写的负载，空间放大系数通常大于 1（当无效的垃圾数据还没来得及被回收的时候）。 Figure 14 中则主要显示了随机加载 100GB 的数据集时各自方案在存储空间上的开销。其中 WiscKey-GC 在存储空间上的开销随着 Value 大小的不断增大，即元数据信息相比于 Value 足够小的时候，其 WiscKey-GC 的空间开销接近用户真实数据的大小，即元数据的大小相对较小。但 WiscKey 的空间开销相对于 LevelDB 更大。 没有一个存储系统可以实现同时减小写放大、读放大和空间放大。存储系统往往在这三项放大系数之间做了相应的权衡。LevelDB 为了降低空间放大系数，引入了压缩机制，对应地也就引入了严峻的写放大问题。而 WiscKey 中为了减小负载运行过程中的 IO 放大，不可避免地引入了空间放大问题。 CPU Usage - CPU 利用率 针对顺序写负载，由于 LevelDB 消耗了大量的时间在 WAL 的开销上，每一个 KV 对都需要进行编码后存储在对应的日志文件上，对应 CPU 的开销也就相对更大。而 WiscKey 的优化方案中由于省略了写前日志的开销，使得 CPU 的占用率也相对较低。 针对 RangeQuery，由于 WiscKey 中采用了多线程进行并发地读写，使得其 CPU 占用率高于 LevelDB。 由于 LevelDB 中大量的操作（single writer, background compress）都是使用的单线程技术，所以其实 CPU 不是限制 LevelDB 的性能瓶颈，而 RocksDB 则是有针对多核系统进行了涉及，使用了多线程进行压缩。 YCSB Benchmarks YCSB Benchmarks 的介绍不在此处进行赘述，主要是利用不同读写比例的操作来进行真实场景的模拟，对 KV 存储系统进行测试。 值得注意的一点是，对于 WiscKey 的垃圾回收机制，当使用小 Value 时，由于垃圾回收的单元默认大小为一个 Chunk，也就是 4MB，所以 Value 较小对应的键值对的数量就较多，从而需要去 LSM Tree 中检查该 Key 是否有效的次数也越多，使得性能会相对地下降。 Related Work 其他方案介绍 基于 Hash Table 的 KV Store FAWN：将 KV 对保存在 SSD 上只进行追加写的日志上，在内存里维护哈希表存储索引来加速查询。 FlashStore 和 SkimpyStash：遵循了 FAWN 的设计，但优化了内存中的哈希索引。FlashStore 使用 cuckoo 哈希并对 Key 进行压缩签名；SkimpyStash 使用线性链接将哈希表的一部分移动到 SSD BufferHash：使用多个内存 HASH 表，并使用布隆过滤器来选择对应的 HASH 表进行查询操作 SILT：主要面向内存进行优化，组合使用了日志结构化树、哈希表和有序表的数据结构 优化原始 LSM Tree 的 KV Store bLSM：提供了一个新的合并调度器来限制写延迟，从而保持稳定的写吞吐量，并且还使用了布隆过滤器来提高性能。 VT-tree：通过使用间接层，VT-tree[50]避免了在压缩过程中对任何之前已排序的键-值对进行排序 LOCS：将内部flash通道暴露给LSM-tree键值存储，该存储可以利用丰富的并行性实现更有效的压缩。 Atlas：一个基于ARM处理器和擦除编码的分布式键值存储，它将键和值存储在不同的硬盘上。 LSM-trie：使用 trie 结构来组织键，并提出了一种基于 trie 的更有效的压缩方法;但是，这种设计牺牲了 LSM-tree 的一些特性，比如对范围查询的有效支持。 RocksDB：由于RocksDB的设计与LevelDB在本质上相似，所以它的写入放大率仍然很高，但相比于 LevelDB 使用了多线程进行压缩。 Walnut：一个混合对象存储，它在 LSM 树中存储小对象，并将大对象直接写入文件系统。 IndexFS：将其元数据存储在 具有列式模式的 LSM-tree，以加快插入的吞吐量 Purity：通过仅对索引进行排序并按时间顺序存储元组来将其索引与数据元组分离 基于其他数据结构的 KV 存储引擎 TokuDB：基于分形树 fractal tree 索引的，它缓冲内部节点中的更新;键没有排序，为了获得良好的性能，必须在内存中维护一个大索引。 ForestDB：使用 HB+-trie 有效地索引长键，提高了性能，减少了内部节点的空间开销 NVMKV：一个支持 FTL 的键值存储，它使用本地 FTL 功能，比如稀疏寻址和事务支持，还为键值存储提出了将多个请求分组到单个操作的向量接口 克服内存中键值存储的可伸缩性瓶颈的优化方案 Mastree MemC3 Memcache MICA cLSM 以上方案可能用于之后在 WiscKey 基础上再进行优化 References Patrick ONeil, Edward Cheng, Dieter Gawlick, and Elizabeth ONeil. The Log-Structured MergeTree (LSM-tree). Acta Informatica, 33(4):351–385,1996. Problems Extend 参考链接 [1] ChinaUnix：LevelDb之七:根据Key读取记录 [2] 知乎：LevelDb 源码阅读--读操作 ","link":"https://blog.shunzi.tech/post/wisckey-separating-keys-from-values-in-ssd-conscious-storage/"},{"title":"文献阅读和写作技巧","content":" 总结文献阅读和写作过程中的相关技巧 根据列举的相关要点，实现文献的高效阅读 写作部分还有所欠缺，还缺乏一定的实践经验 ... 阅读 根据文献基本都有的框架和结构来对文章内容进行梳理。 泛读则主要阅读：Abstract 摘要、Introduction 引言和Conclusion 结论。（其余章节大致阅览标题） 精读是在泛读的基础上针对具体方案和测试进行深入阅读 泛读 Abstract 摘要 摘要绝对是重中之重，因为它本身就是对全篇内容的概括。一篇论文的核心往往是一两句话，而论文的其他部分都是在用不同的方式支持它的核心。 笔记要点 作者想解决什么问题？question 作者通过什么理论/模型来解决这个问题？method 作者给出的答案/效果是什么？answer/result Introduction 引言 重要性仅次于 Abstract 这一部分其实是对于 文章的研究背景和研究意义 的介绍，有些论文也会将文献综述的简化版加在里面。 相比于摘要，做了更详细的解释，内容更加易懂 在Introduction的最后，作者会介绍文章的架构，让你清楚文章的每一部分都在做什么。 笔记要点 志在解决什么样的问题？ 作者为什么研究这个课题？ 目前这个课题的研究进行到了哪一阶段？ 作者使用的理论是基于哪些假设/理论？ 作者使用的方法或者方案或者思想简介 效果简要描述（测试结果简单介绍） Tips 如果有时间的话，可以把introduction读完。没时间的话粗略看一遍，着重读前两段。 Conclusion 结论 Conclusion通常会把整个文章的主要内容复述一遍，帮助读者回顾+理清思路，然后在此基础上深入自己的研究。 在Abstract或者Introduction中，作者就已经给出了课题的结论。而在Conclusion中会评价自己的结论，并且在这个课题上的基础上进一步探讨，比如开脑洞畅想这个课题未来还有哪些研究方向，可以用来解决哪些问题，等等。 笔记要点 这篇文章存在哪些缺陷？ 这篇文章未来的拓展方向？ 作者关于这个课题的构思有哪几点？ 精读 Background and Motivation 背景 该章节主要介绍了文章所致力于解决的问题的背景。即问题是如何出现的，问题产生的原因，通常会对问题产生的原因追本溯源，找到问题的源头 同时会对现有的一些方案进行介绍和评价，主要分析现有方案的优缺点 介绍自己文章里的研究方案提出的背景，除了解决现有的问题，通常会结合一些业界的新动向和其他技术的发展来介绍 笔记要点 为什么会出现文章中提到的问题？ 针对要解决的问题，现有的解决方案又存在哪些问题？ 文章提出该方案基于什么样的背景和理论基础？ 具体的方案作为独立章节 该章节主要介绍本文提出的方案，从方案架构设计到关键问题的解决办法再到最终的具体实现进行全面的介绍。 会讲述该方案面临的主要挑战和关键问题，以及对应的该方案中的具体解决办法 笔记要点 方案的整体设计是什么样的？ 方案面临哪些关键问题和挑战？ 方案针对要解决的问题的具体解决方案是什么？ 方案的相关实现细节 Evaluation 测试 该章节主要介绍为了验证方案的优越性采用了什么样的测试方案来对该方案进行测试，同时根据测试结果和其他同类方案进行对比，得出该方案的优化效果。 该章节主要涉及到各类测试图表的阅读 笔记要点 测试使用了什么样的测试环境？ 测试使用了什么工作负载？ 测试使用的方法或者模型 测试对比的基准或参考系 测试的结论 Related work 相关工作 内容可能涉及： 为何前人工作没有这么做？ 前人工作的存在怎样的局限性？ 这个领域工作大致发展现状是怎样的？ 当前工作和前人工作有何联系？有何异同？ 参考文献 该章节主要列举了该篇文章引用到的论文。读者往往可以通过一篇论文的参考文献找到很多同类问题对应的Paper，以及一些高水平论文。 与本篇 Paper 相似的工作一般也会列举到参考文献中 参考文献中优秀论文一般表现为较高的影响因子 写作 仍然根据文献基本都有的框架和结构来对文章内容进行梳理。 在掌握了相关阅读技巧之后，针对各个章节需要重点讲述的内容也就有了一定的理解，逆向思维... Abstract 摘要 Introduction 引言 写在前面，分享一则知乎上的有趣回答，珵cici：硕士论文你有哪些经验与收获？ 写论文最重要的是写Introduction。写Introduction就和写童话一样。 有一条巨龙抓走了公主 (介绍你的问题为什么值得研究) 巨龙是多么多么多么难打（强调你的研究的重要性） 王子提着一把金光闪闪的剑而不是破斧子烂长矛登场（你的方法好在哪里，别人sui在哪里） 王子是如何打败巨龙（你的方法简介） 从此王子和公主幸福的生活在一起。（解决了问题） 参考资料 微信文章：如何10分钟读完一篇英文文献 知乎：如何读一篇优秀的计算机论文？ 知乎：计算机专业学生如何搜索和阅读一篇论文 (How to Search&amp;Read a Paper) Quora：What is the best way to read CS research papers? 知乎：科研大牛们怎么读文献？ ","link":"https://blog.shunzi.tech/post/paper-read-and-write/"},{"title":"基于RDMA的RPC实现","content":" 项目起源于分布式系统与中间件的课程作业，自己实现一个分布式系统相关的项目。 由于课程内容中有大量涉及 RPC 的内容，考虑自己实现一个 RPC 的框架。 现如今有很多优秀的 RPC 框架，此次主要基于 RDMA 来进行实现，参考相关论文。 后续会基于 Netty 实现 RPC，顺便了解 Netty 通信框架的基础知识。 背景介绍 在数据中心运行的新型大规模分布式系统的出现给RPC系统增加了额外的压力。例如，HDFS，Zookeeper或OpenFlowcontain等系统要求每秒处理大量RPC请求的集中式RPC服务（例如，名称，调度程序，控制器）。 此外，一些提供低延迟数据访问的新系统，如RAM-Cloud 或Tango，要求超低RPC延迟，同时仍然要求RPC系统扩展到高容量的请求。 不幸的是，当今基于云的系统中使用的RPC实现很难满足这些要求。 例如，HDFS和Zookeeper中的RPC服务通常可以在200到500μs之间的延迟时间内每秒处理100-200K操作。 其他系统（如Tango中使用的系统）表现更好（60-70μs时600K op / s），但在所有这些情况下，RPC系统的性能远低于硬件（CPU，网络）所能提供的性能。 实际上，我们发现这些系统既不会使网络饱和，也不会使CPU饱和。 最近，已经在网络堆栈级别讨论了类似的低效率。 为了克服这些问题，已经建议在用户空间中实现网络堆栈，同时减少开销。 RDMA 的出现，就是为了解决网络传输中客户端与服务器端数据处理的延迟而产生的。它将数据直接从一台计算机的内存传输到另一台计算机，无需双方操作系统的介入。这允许高吞吐、低延迟的网络通信，尤其适合在大规模并行计算机集群中使用。RDMA通过网络把资料直接传入计算机的内存中，将数据从一个系统快速移动到远程系统内存中，而不对操作系统造成任何影响，这样就不需要用到多少计算机的处理能力。它消除了数据包在用户空间和内核空间复制移动和上下文切换的开销，因而能解放内存带宽和CPU周期用于改进应用系统性能。 想法 通过使用远程直接内存访问（RDMA）将RPC处理与用户空间中的网络处理集成在一起。 我们提供DaRPC，这是一种高吞吐量低延迟RPC框架，专为提高数据中心大规模分布式系统的性能而量身定制。 DaRPC中使用的关键思想是协调多核系统中RPC处理和网络的计算和内存资源分配。 这种方法与构建RPC系统的传统方法形成对比，其中RPC消息处理是独立于内核中的网络处理实现的。通过将RPC处理和网络视为联合优化问题，我们可以避免上下文切换，缓存未命中并实现高度并行性以及超低延迟。 核心思想：将曾经在内核态实现的网络堆栈移植到用户态进行实现，不经过内核态的处理直接和硬件模块（CPU，NIC，Memory）进行通信. 前提背景知识 为了使用 RDMA 协议进行数据直取，也就是为了利用设备的 RDMA 特性，在应用层定义了一个抽象概念 ：verbs。 verbs 不是 API，libibverbs 定义了调用支持 RDMA 设备的函数，被称作为 IB verbs API。因此，出现了 verbs 编程，其实也就是一般理解的 RDMA 编程。 核心概念 Memory Registration(MR) | 内存注册 前提要求： 在数据传输过程中，应用程序不能修改数据所在的内存。 操作系统不能对数据所在的内存进行page out操作 – 物理地址和虚拟地址的映射必须是固定不变的。 内存注册方式： 创建两个key (local和remote)指向需要操作的内存区域 注册的keys是数据传输请求的一部分 注册完成以后，获得相应的属性： context : RDMA操作上下文 addr : MR被注册的Buffer地址 length : MR被注册的Buffer长度 lkey：MR被注册的本地key rkey：MR被注册的远程key Memory Registration只是RDMA中对内存保护的一种措施，只有将要操作的内存注册到RDMA Memory Region中，这快操作的内存就交给RDMA 保护域来操作了。这个时候我们就可以对这快内存进行操作，至于操作的起始地址、操作Buffer的长度，可以根据程序的具体需求进行操作。我们只要保证接受方的Buffer 接受的长度大于等于发送的Buffer长度。 Queues | 队列 发送队列(SQ)和接收队列(RQ)，完成队列(CQ)。其中，SQ和RQ通常成对创建，被称为Queue Pairs(QP)。 RDMA是基于消息的传输协议，数据传输都是异步操作。 RDMA操作其实很简单，操作流程大致如下： Host提交工作请求(WR)到工作队列(WQ): 工作队列包括发送队列(SQ)和接收队列(RQ)。工作队列的每一个元素叫做WQE, 也就是WR。 Host从完成队列(CQ）中获取工作完成(WC): 完成队列里的每一个叫做CQE, 也就是WC。 具有RDMA引擎的硬件(hardware)就是一个队列元素处理器。 RDMA硬件不断地从工作队列(WQ)中去取工作请求(WR)来执行，执行完了就给完成队列(CQ)中放置工作完成(WC)。 使用生产者消费者模型来理解就是： Host生产WR, 把WR放到WQ中去 RDMA硬件消费WR（执行请求处理） RDMA硬件生产WC, 把WC放到CQ中去 Host消费WC RDMA 支持的操作 RDMA Send | RDMA发送(/接收)操作 （Send/Recv） 跟TCP/IP的send/recv是类似的，不同的是RDMA是基于消息的数据传输协议（而不是基于字节流的传输协议），所有数据包的组装都在RDMA硬件上完成的，也就是说OSI模型中的下面4层(传输层，网络层，数据链路层，物理层)都在RDMA硬件上完成。 以 SEND 的具体流程为例： 第1步：系统A和B都创建了他们各自的QP的完成队列(CQ), 并为即将进行的RDMA传输注册了相应的内存区域(MR)。 系统A识别了一段缓冲区，该缓冲区的数据将被搬运到系统B上。系统B分配了一段空的缓冲区，用来存放来自系统A发送的数据。 第二步：系统B创建一个WQE并放置到它的接收队列(RQ)中。这个WQE包含了一个指针，该指针指向的内存缓冲区用来存放接收到的数据。系统A也创建一个WQE并放置到它的发送队列(SQ)中去，该WQE中的指针执行一段内存缓冲区，该缓冲区的数据将要被传送。 第三步：系统A上的HCA总是在硬件上干活，看看发送队列里有没有WQE。HCA将消费掉来自系统A的WQE, 然后将内存区域里的数据变成数据流发送给系统B。当数据流开始到达系统B的时候，系统B上的HCA就消费来自系统B的WQE，然后将数据放到该放的缓冲区上去。在高速通道上传输的数据流完全绕过了操作系统内核。 第四步：当数据搬运完成的时候，HCA会创建一个CQE。 这个CQE被放置到完成队列(CQ)中，表明数据传输已经完成。HCA每消费掉一个WQE, 都会生成一个CQE。因此，在系统A的完成队列中放置一个CQE,意味着对应的WQE的发送操作已经完成。同理，在系统B的完成队列中也会放置一个CQE，表明对应的WQE的接收操作已经完成。如果发生错误，HCA依然会创建一个CQE。在CQE中，包含了一个用来记录传输状态的字段。 RDMA Read | RDMA读操作 (Pull) RDMA读操作本质上就是Pull操作, 把远程系统内存里的数据拉回到本地系统的内存里。 RDMA Write | RDMA写操作 (Push) RDMA写操作本质上就是Push操作，把本地系统内存里的数据推送到远程系统的内存里。 其他核心概念 SGE RDMA编程中，SGL(Scatter/Gather List)是最基本的数据组织形式。 SGL是一个数组，该数组中的元素被称之为SGE(Scatter/Gather Element)，每一个SGE就是一个Data Segment(数据段)。RDMA支持Scatter/Gather操作，具体来讲就是RDMA可以支持一个连续的Buffer空间，进行Scatter分散到多个目的主机的不连续的Buffer空间。Gather指的就是多个不连续的Buffer空间，可以Gather到目的主机的一段连续的Buffer空间。 数据结构 struct ibv_sge { uint64_t addr; uint32_t length; uint32_t lkey; }; - addr: 数据段所在的虚拟内存的起始地址 (Virtual Address of the Data Segment (i.e. Buffer)) - length: 数据段长度(Length of the Data Segment) - lkey: 该数据段对应的L_Key (Key of the local Memory Region) ivc_post_send接口 ibv_post_send() - post a list of work requests (WRs) to a send queue 将一个WR列表放置到发送队列中 ibv_post_recv() - post a list of work requests (WRs) to a receive queue 将一个WR列表放置到接收队列中 #include &lt;infiniband/verbs.h&gt; int ibv_post_send(struct ibv_qp *qp, struct ibv_send_wr *wr, struct ibv_send_wr **bad_wr); ibv_post_send（）将以send_wr开头的工作请求（WR）的列表发布到Queue Pair的Send Queue。 它会在第一次失败时停止处理此列表中的WR（可以在发布请求时立即检测到），并通过bad_wr返回此失败的WR。 在调用ibv_post_send()之前，必须填充好数据结构wr。 wr是一个链表，每一个结点包含了一个sg_list(i.e. SGL: 由一个或多个SGE构成的数组), sg_list的长度为num_sge。 RDMA 提交 WR 流程 第一步：创建SGL。wr链表中的每一个结点都包含了一个SGL，SGL是一个数组，包含一个或多个SGE。通过ibv_post_send提交一个RDMA SEND 请求。这个WR请求中，包括一个sg_list的元素。它是一个SGE链表，SGE指向具体需要发送数据的Buffer。 第二步：使用PD进行内存保护。发送一段内存地址的时候，我们需要将这段内存地址通过Memory Registration注册到RDMA中。也就是说注册到PD内存保护域当中。一个SGL至少被一个MR保护, 多个MR存在同一个PD中。如图所示一段内存MR可以保护多个SGE元素。 第三步：一个SGL数组包含了3个SGE, 长度分别为N1, N2, N3字节。我们可以看到，这3个buffer并不连续，它们Scatter(分散)在内存中的各个地方。调用ibv_post_send()将SGL发送到wire上去。RDMA硬件读取到SGL后，进行Gather(聚合)操作，于是在RDMA硬件的Wire上看到的就是N3+N2+N1个连续的字节。换句话说，通过使用SGL, 我们可以把分散(Scatter)在内存中的多个数据段(不连续)交给RDMA硬件去聚合(Gather)成连续的数据段。 RDMA 语义 RDMA有两种基本操作： Memory verbs: 包括RDMA read、write和atomic操作。这些操作指定远程地址进行操作并且绕过接收者的CPU。单边操作（只需要一边进行主动操作） Messaging verbs:包括RDMA send、receive操作。这些动作涉及的接收方CPU，发送的数据被写入由接收方的CPU先前发布的接受所指定的地址。双边操作（发送方发送消息，接收方需要预先发布应用程序缓冲区，指示在何处想要接收数据） 与Socket模型相比，RDMA完全将数据传输操作与控制操作分开。 这有利于预分配通信资源（例如，固定DMA的存储器）并实现数据传输而无需操作系统参与，这对于实现超低延迟是关键。 jVerb 使用jVerbs作为本机RDMA API，我们决定既不牺牲可用的通信语义，也不牺牲最小的网络延迟。 jVerbs提供对所有独有RDMA功能的访问。 为了实现jVerbs中最低的延迟，我们在访问网络硬件时采用与Natvie C用户库相同的技术。对于所有性能关键的操作，Native C Verbs库通过三个队列与RDMA网络设备交互：发送队列，接收队列和完成队列。这些队列代表硬件资源，但是被映射到用户空间以避免内核参与访问它们。 jVerbs充分利用Java的堆外内存直接从JVM中访问这些设备队列。堆外内存分配在垃圾收集器控制之外的单独区域中，但可以通过常规Java内存API（ByteBuffer）访问。在jVerbs中，我们使用标准内存映射I / O将设备硬件资源映射到堆外内存。 postSend（）或postRecv（）等快速路径操作是通过将工作请求直接序列化到映射队列来实现的。所有操作都完全用Java实现，避免了昂贵的JNI调用或对JVM的修改。 SVC (Stateful Verb Calls) 即使jVerbs通过直接访问设备硬件来避免JNI的开销，一个剩余的开销源来自将工作请求序列化到映射队列中。 考虑到现代互连的单位数网络延迟，序列化的成本很容易达到几微秒。 为了解决这个问题，jVerbs采用了一种称为有状态动词调用（SVC）的机制。 此机制直接在API级别上显示：jVerbs不是执行Verbs调用，而是返回一个有状态对象，该对象表示对给定参数值集的Verbs调用。 应用程序使用exec（）执行SVC对象，使用result（）来检索调用结果。 Verbs v = Verbs.open(); /* post the work requests */ v.postSend(sendQueue, workRequestList).exec().free(); /* check if operation has completed */ while (v.pollCQ(cq, pList).exec().result() == 0); SVC的一个关键优势是可以多次缓存和重新执行它们。 但是，在执行之前，应用程序必须使用valid（）函数验证SVC对象是否仍处于有效状态。 从语义上讲，SVC对象的每次执行都与针对SVC对象的当前参数状态评估的jVerbs调用相同。 但是，只有在第一次执行对象时才需要创建执行SVC对象时所需的任何序列化状态。 后续调用使用已建立的序列化状态，因此执行速度会快得多。 一旦不再需要SVC对象，就可以使用free（）API调用释放资源。 某些SVC对象允许在创建对象后更改参数状态。 例如，如果需要，应用程序可以更改postSend（）和postRecv（）返回的SVC对象的地址和偏移量。 在内部，这些对象以递增方式更新其序列化状态。 只有在不扩展序列化状态的情况下，才允许对SVC对象进行修改。 因此，不允许向SVC postSend（）对象添加新的工作请求或Scatter/Gather元素。 Stateful Verb Calls为应用程序提供了一个缓解序列化成本的句柄。 在许多情况下，应用程序可能只需要创建少量SVC对象，以匹配他们打算使用的不同类型的动词调用。 重新使用这些对象有效地将序列化成本降低到几乎为零 Verbs v = Verbs.open(); /* create SVCs */ RdmaSVC post = v.postSend(sq, wrlist); RdmaSVC poll = v.pollCQ(cq, plist); post.exec(); while (poll.exec().result() == 0); /* modify the work requests */ post.getWR(0).getSG(0).setOffset(32); /* post again */ post.exec(); while (poll.exec().result() == 0); 参考文献 [1] DaRPC：DaRPC: Data Center RPC [2] 知乎：一周一论文(翻译 总结) [SOCC 14] DaRPC [3] Github: Repo DaRPC - IBM [4] CSDN：深入浅出全面解析RDMA [5] 知乎：基于Java的RDMA高性能通信库（三）：DiSNI [6] CSDN： RDMA技术详解（一）：RDMA概述 [7] CSDN：RDMA技术详解（二）：RDMA Send Receive操作 [8] CSDN：RDMA技术详解（三）：理解RDMA Scatter Gather List ","link":"https://blog.shunzi.tech/post/rbc-based-on-rdma/"},{"title":"GEM5 & NVMain","content":" 该篇主要记录 gem5 的安装和简单使用过程 计算机系统设计课程实验环境需要，部分论文的实验仿真环境为 gem5 结合部分 Paper，简要介绍 gem5 主要模拟的场景和测试方法 gem5 What is gem5? A modular platform for computer-system architecture research About The gem5 simulator is a modular platform for computer-system architecture research, encompassing system-level architecture as well as processor microarchitecture. Install And Test Install dependencies firstly sudo apt-get install mercurial scons swig gcc m4 python python-dev libgoogle-perftools-dev g++ libprotobuf-dev sudo apt-get install build-essential Notice：apt get is for Ubuntu Download the source code hg clone http://repo.gem5.org/gem5 git clone https://gem5.googlesource.com/public/gem5 git clone git@github.com:gem5/gem5.git github repo(read only) Build the source code cd {gem5-source-code-dir} scons build/X86/gem5.opt Simple Test cd {gem5-source-code-dir} Check and edit the hello.c：vim tests/test-progs/hello/src/hello.c Run the hello.c：build/X86/gem5.opt configs/example/se.py -c tests/test-progs/hello/bin/x86/linux/hello Check the result if contains &quot;Hello world!&quot; Check the output cd m5out/ ls config.ini and config.json describe the configuration info of the simulator. stats.txt describes the info of this test. Notice：This is in SE - System-call Emulation mode (which is designed for running the test program separately). The other mode FS - Full System mode is designed for running the whole linux os. Use Run：&lt;gem5 binary&gt; [gem5 options] &lt;simulation script&gt; [script options] SE mode Prepare Edit the source code and save： vim Test.c #include&lt;stdio.h&gt; int main() { printf(&quot;Hello Test!\\n&quot;); return 0; } Build：gcc -o {Output-File-Name} {Source-File-Name} -static Test：./{Output-File-Name} gcc -o Test Test.c -static ./Test Notice：If gcc return error info : /usr/bin/ld: cannot find -lc, try to install sudo yum install glibc-static Build and Run Enter gem5 directory Build the arch if never build ：scons build/X86/gem5.opt Run the code：build/X86/gem5.opt configs/example/se.py -c {Output-File-Name} build/X86/gem5.opt configs/example/se.py -c /shunzi-test/Test Check the result if contains &quot;Hello Test!&quot;. Notice In order to pass command line arguments to a binary you can use --options=&quot;arg1 arg2 ...&quot; to specify them as a script option in your simulation command. SE mode : Static Compile &amp; Single Thread FS mode Use x86 Arch as an example. Prepare Download and install the full-system binary and disk image files. For Alpha full system files：wget http://www.m5sim.org/dist/current/m5_system_2.0b3.tar.bz2. (We will use one file in this tar in next step.) For x86 full system files：wget http://www.m5sim.org/dist/current/x86/x86-system.tar.bz2 Create fs-image directory and unzip the x86 system files to here：tar -xjf /shunzi-test/m5/system/x86-system.tar.bz2 Create new directory such as alpha to store the data unziped from Alpha full system files：tar -xjf /shunzi-test/m5/system/m5_system_2.0b3.tar.bz2 Copy the file linux-bigswap2.img in Alpha full system files to ../fs-image/disk directory： cp linux-bigswap2.img /shunzi-test/gem5-master/fs-image/ Change the environment varibles in file .bashrc.And add path export M5_PATH=$M5_PATH:/shunzi-test/gem5-master/fs-image/. Make the change work：source .bashrc. Run In gem5 directory, run the py program：build/X86/gem5.opt configs/example/fs.py. And it may throw exceptions and errors.You can viee details in Notice part as first two tips. Rerun with command given kernel and disk image as params：build/X86/gem5.opt configs/example/fs.py --kernel=x86_64-vmlinux-2.6.22.9 --disk-image=linux-x86.img.And check the output if contains following msgs: gem5 Simulator System. http://gem5.org gem5 is copyrighted software; use the --copyright option for details. gem5 compiled Nov 11 2019 18:23:53 gem5 started Nov 14 2019 15:31:57 gem5 executing on ceph-node1.localdomain, pid 729112 command line: build/X86/gem5.opt configs/example/fs.py --kernel=x86_64-vmlinux-2.6.22.9 --disk-image=linux-x86.img Global frequency set at 1000000000000 ticks per second warn: DRAM device capacity (8192 Mbytes) does not match the address range assigned (512 Mbytes) info: kernel located at: /shunzi-test/gem5-master/fs-image/binaries/x86_64-vmlinux-2.6.22.9 system.pc.com_1.device: Listening for connections on port 3456 0: system.pc.south_bridge.cmos.rtc: Real-time clock set to Sun Jan 1 00:00:00 2012 0: system.remote_gdb: listening for remote gdb on port 7000 warn: Reading current count from inactive timer. **** REAL SIMULATION **** info: Entering event queue @ 0. Starting simulation... warn: Don't know what interrupt to clear for console. warn: x86 cpuid: unknown family 0x8086 Mount the image file to /mnt：mount -o,loop,offset=32256 /shunzi-test/m5/system/linux-x86.img /mnt Mount the example code：cp tests/test-progs/hello/bin/x86/linux/hello /mntor mount code wrote by yourself. cp Test /mnt Umount the mnt：umount /mnt Rerun the gem5 with FS mode：build/X86/gem5.opt configs/example/fs.py --kernel=x86_64-vmlinux-2.6.22.9 --disk-image=linux-x86.img In another session, run the /util/term/m5term：m5term 127.0.0.1 3456 Notice Change path in SysPaths.py ：vim ./configs/common/SysPaths.py. And adjust the code. // Old Version except KeyError: path = [ '/dist/m5/system', '/n/poolfs/z/dist/m5/system' ] // New Version except KeyError: path = [ '/dist/m5/system', '/shunzi-test/gem5-master/fs-image' ] Change the x86 image name in Benchmarks.py：vim configs/common/Benchmarks.py // Old Version elif buildEnv['TARGET_ISA'] == 'x86': return env.get('LINUX_IMAGE', disk('x86root.img')) // New Version elif buildEnv['TARGET_ISA'] == 'x86': return env.get('LINUX_IMAGE', disk('linux-x86.img')) If you run the code written by yourself, please confirm the version of gcc between the image and the compiled version. Otherwise, you will have to face the problem of too old kenerl version. NVMain Install And Build Download Source hg clone https://bitbucket.org/mrp5060/nvmain or wget https://bitbucket.org/mrp5060/nvmain/get/9c0e87b164bc.zip or download to local system and upload (recommand) or download from github https://github.com/cyjseagull/gem5-nvmain-hybrid-simulator. Build scons –build-type=fast may throw error messages. If success, the file nvmain.fast will be generated in the nvmain directory. Use Test Use inner test program and test data to test：./nvmain.fast Config/PCM_ISSCC_2012_4GB.config Tests/Traces/hello_world.nvt 10000000 DataType Check Define 4 types write-unit 8 bytes：00000000，0000XXXX，00XX00XX，OtherType Implementation Define four counter in nvmain.h. ncounter_t total00000000; ncounter_t total0000xxxx; ncounter_t total00xx00xx; ncounter_t totalOtherType; Init the counter in constructor. total00000000 = 0; total0000xxxx = 0; total00xx00xx = 0; totalOtherType = 0; Count the amount of different data type. { totalWriteRequests++; NVMDataBlock recvData; recvData = request-&gt;data; recvData.SetSize(64); int index = 0; uint8_t newByte; for (index = 0; index &lt; 64; index+=8) { //uint8_t byteOne, byteTwo, byteThree, byteFour, byteFive, byteSix, byteEight; uint8_t byteData[8] = {1,1,1,1,1,1,1,1}; for (int i = 0; i &lt; 8; i++) { newByte = recvData.GetByte(index + i); if (!(newByte &amp; 0xff)) { byteData[i] = 0; } } if (!(byteData[0] | byteData[1] | byteData[2] | byteData[3] | byteData[4] | byteData[5] | byteData[6] | byteData[7])) { total00000000++; } else if (!(byteData[0] | byteData[1] | byteData[2] | byteData[3])) { total0000xxxx++; } else if (!(byteData[0] | byteData[1] | byteData[4] | byteData[5])) { total00xx00xx++; } else { totalOtherType++; } } } Print the result void NVMain::RegisterStats( ) { AddStat(totalReadRequests); AddStat(totalWriteRequests); AddStat(successfulPrefetches); AddStat(unsuccessfulPrefetches); AddStat(total00000000); AddStat(total0000xxxx); AddStat(total00xx00xx); AddStat(totalOtherType); AddStat(totalWriteBytes); AddStat(totalCompressWriteBytes); } Gem5 &amp; NVMain Build Build Alpha Arch：scons EXTRAS=/shunzi-test/nvmain/gem5-nvmain-hybrid-simulator/nvmain-gem5/nvmain ./build/ALPHA/gem5.opt -j8 Test Run hello-world：./build/ALPHA/gem5.opt configs/example/se.py -c tests/test-progs/hello/bin/alpha/linux/hello --cpu-type=detailed --caches --l2cache --mem-type=NVMainMemory --nvmain-config=../nvmain/Config/PCM_ISSCC_2012_4GB.config Parsec2.1 Download neccessary dependencies wget http://www.cs.utexas.edu/~parsec_m5/vmlinux_2.6.27-gcc_4.3.4 wget http://www.cs.utexas.edu/~parsec_m5/tsb_osfpal wget http://www.cs.utexas.edu/~parsec_m5/linux-parsec-2-1-m5-with-test-inputs.img.bz2 wget http://www.cs.utexas.edu/~parsec_m5/linux-parsec-2-1-m5-with-test-inputs.img.bz2 Use Parsec2.1 Use util to generate shell：./writescripts.pl blackscholes 4 Rebuild the gem5：scons build/ALPHA/gem5.opt -j8 Run with blackscholes_4c_simmedium.rcS Script without nvmain：./build/ALPHA/gem5.opt ./configs/example/fs.py -n 2 --script=/shunzi-test/nvmain/gem5-nvmain-hybrid-simulator/nvmain-gem5/gem5/TR-09-32-parsec-2.1-alpha-files/blackscholes_4c_simmedium.rcS -F 5000000000 Run with nvmain：./build/ALPHA/gem5.opt ./configs/example/fs.py -n 2 --script=/shunzi-test/nvmain/gem5-nvmain-hybrid-simulator/nvmain-gem5/gem5/TR-09-32-parsec-2.1-alpha-files/blackscholes_4c_simsmall.rcS --mem-type=NVMainMemory --nvmain-config=../nvmain/Config/PCM_ISSCC_2012_4GB.config -F 500000000 References [1] gem5 official page [2] CSDN：Gem5的FS（全系统）模拟 ","link":"https://blog.shunzi.tech/post/gem5-and-nvmain/"},{"title":"Ceph Tiring Cache 调优","content":" 主要介绍 Ceph 缓存机制涉及到的相关参数 提供了一些参数的经验值参考 后续针对具体的IO设计更为细致的优化方案 Ceph Tiring 调优 Ceph Tiring 功能需要仔细配置其各种参数以确保良好的性能。需要对工作负载I/O有基本的了解;只有当您的数据只有一小部分是热数据时，Tiring 才能很好地工作。均匀随机或涉及大量顺序访问模式的工作负载的情况下，要么没有任何改进，要么在某些情况下可能比无Tiring 更慢。 Flushing &amp; Eviction target_max_bytes / target_max_objects 该值设定了缓冲池中最大的数据字节数或对象个数，当缓冲池中数据大小或者对象个数到达该值时将根据缓存策略进行数据的刷回或者淘汰。 该值需要根据缓存池的容量大小以及副本个数来设置，以三副本为例，target_max_bytes 不应该超过容量的 1/3，如果实际的负载使得存储池中的数据大小达到了容量的 1/3，后续的 IO 将被阻塞，所以需要设置别的参数来避免池中的数据到达该阈值。 该类参数的设计目的： 作为刷回淘汰操作的触发条件，避免 OSD 被数据撑满。 为什么不直接使用存储池的容量作为该参数，是为了考虑另外一种场景，存在多个缓存池，使用相同的磁盘。 cache_target_full_ratio 该参数的设置是为了防止存储池中的数据大小达到 target_max_bytes/objects。 当数据到达 cache_target_full_ratio 时，将淘汰缓存池中的对象数据 淘汰数据本质就是删除数据，其开销相比于 flush/promote 小的多 需要注意的是，target_max_bytes 和 cache_target_full_ratio 虽然是对存储池的配置，但 Ceph 内部将这些参数用于每个 PG 的 limit 计算。 // 计算脏数据的比率和数据满的比率，单位为百万分之一 // get dirty, full ratios uint64_t dirty_micro = 0; uint64_t full_micro = 0; // 如果设置了 target_max_bytes，就按照字节数算 if (pool.info.target_max_bytes &amp;&amp; num_user_objects &gt; 0) { // 首先计算每个对象的平均大小 avg_size uint64_t avg_size = num_user_bytes / num_user_objects; // 脏数据率 = 100w * 脏数据对象数目 * 每个对象的平均大小 / 每个PG的平均字节数 dirty_micro = num_dirty * avg_size * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_bytes / divisor, 1); // 满数据率 = 100w * 用户对象数目 * 每个对象的平均大小 / 每个PG的平均字节数 full_micro = num_user_objects * avg_size * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_bytes / divisor, 1); } // 如果设置了 target_max_objects，就按照对象个数算 if (pool.info.target_max_objects &gt; 0) { // 脏数据率 = 100w * 脏数据对象数目 / 每个 PG 的平均对象数目 uint64_t dirty_objects_micro = num_dirty * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_objects / divisor, 1); // 取两种计算方式中的最大值 if (dirty_objects_micro &gt; dirty_micro) dirty_micro = dirty_objects_micro; // 满数据率 = 100w * 用户对象数目 / 每个 PG 的平均对象数目 uint64_t full_objects_micro = num_user_objects * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_objects / divisor, 1); if (full_objects_micro &gt; full_micro) full_micro = full_objects_micro; } dout(20) &lt;&lt; __func__ &lt;&lt; &quot; dirty &quot; &lt;&lt; ((float)dirty_micro / 1000000.0) &lt;&lt; &quot; full &quot; &lt;&lt; ((float)full_micro / 1000000.0) &lt;&lt; dendl; 每个 PG 计算出来的结果会因为该 PG 内的数据分布情况不一致，有的 PG 可能大于 full_ratio，有的可能小于 full_ratio，所以为了避免出现故障，不能将该数据设置为 1. 经验表明，不能将 cache_target_full_ratio 设置的太高，需要预留一定的空间，经验值 0.8 通常能够很好低保证系统运行。 cache_target_dirty_ratio / cache_target_high_dirty_ratio 这两个参数主要用于控制刷回操作的时机，区别在于刷回的速度不同 刷回操作是指 cache tier -&gt; base tier 的数据写入过程，涉及到了一次完整的数据写入过程，开销相对较大。所以通常情况下，对象的刷回速度小于淘汰速度，因为淘汰操作开销更小 刷回操作是异步的，不直接影响客户端发来的 IO 请求处理 刷回速度不同的原因是刷回的并发线程数不同，高速刷回使用的并发线程更多。并发线程数一般由 OSD 的相关配置参数指定：osd_agent_max_ops, osd_agent_max_high_ops. 默认低速为 2 个线程，高速为 4 个线程 // flush mode // 获取 flush_target 和 flush_high_target 参数，以及计算 flush_slop uint64_t flush_target = pool.info.cache_target_dirty_ratio_micro; uint64_t flush_high_target = pool.info.cache_target_dirty_high_ratio_micro; uint64_t flush_slop = (float)flush_target * cct-&gt;_conf-&gt;osd_agent_slop; // 根据传入的参数和 flush_mode 对 target 做修正 if (restart || agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_IDLE) { flush_target += flush_slop; flush_high_target += flush_slop; } else { flush_target -= std::min(flush_target, flush_slop); flush_high_target -= std::min(flush_high_target, flush_slop); } // 根据脏数据的比例，设置 flush_mode if (dirty_micro &gt; flush_high_target) { flush_mode = TierAgentState::FLUSH_MODE_HIGH; } else if (dirty_micro &gt; flush_target || (!flush_target &amp;&amp; num_dirty &gt; 0)) { flush_mode = TierAgentState::FLUSH_MODE_LOW; } ... skip_calc: bool old_idle = agent_state-&gt;is_idle(); // 设置新的 flush_mode，并更新统计信息 if (flush_mode != agent_state-&gt;flush_mode) { dout(5) &lt;&lt; __func__ &lt;&lt; &quot; flush_mode &quot; &lt;&lt; TierAgentState::get_flush_mode_name(agent_state-&gt;flush_mode) &lt;&lt; &quot; -&gt; &quot; &lt;&lt; TierAgentState::get_flush_mode_name(flush_mode) &lt;&lt; dendl; recovery_state.update_stats( [=](auto &amp;history, auto &amp;stats) { if (flush_mode == TierAgentState::FLUSH_MODE_HIGH) { osd-&gt;agent_inc_high_count(); stats.stats.sum.num_flush_mode_high = 1; } else if (flush_mode == TierAgentState::FLUSH_MODE_LOW) { stats.stats.sum.num_flush_mode_low = 1; } if (agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_HIGH) { osd-&gt;agent_dec_high_count(); stats.stats.sum.num_flush_mode_high = 0; } else if (agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_LOW) { stats.stats.sum.num_flush_mode_low = 0; } return false; }); agent_state-&gt;flush_mode = flush_mode; } 理论上，集群在正常的使用情况下，脏对象的百分比一般在低脏数据率的附近徘徊，部分对象数据将被刷回到存储层，使用的刷回线程较少，尽可能地减小集群的响应延迟；但写如果发生突增，脏数据的比例将会小幅度上升，但通常不会持续太长一段时间，后台线程仍然进行刷回。但如果出现了持续写，且速度大于低速刷回的速度，那么脏数据的比率将会持续上升，如果持续时间较短则保持低俗刷回，脏数据比例会逐渐下降；但如果持续时间较长，到达高速刷回的阈值，将开启更多的线程进行高速刷回，避免脏数据率进一步增长，一旦写流量减少，脏数据率又会重新回到低阈值。 脏数据率的低阈值和高阈值之间需要存在一定的差距，从而才能够较好地吸收正常写操作，而不需要引入高阈值。高阈值应被视为一种紧急限制。经验值表明：一个好的初始值是低阈值为0.4，高阈值为0.6。 osd_agent_max_ops 的配置，需要根据实际情况进行设置，以便在正常操作条件下，脏对象的数量在低脏比率上下浮动。该参数无准确的经验值，因为该值在很大程度上取决于顶层与底层的大小和性能之比。但是，首先将osd_agent_max_ops设置为1，然后根据需要增加，将 osd_agent_max_high_ops 设置为 osd_agent_max_ops 的至少两倍。 Ceph Dashboard 可以观察刷回情况，如果正在进行高速刷新，可以考虑增加osd_agent_max_ops。如果发现缓存层被填满并阻塞了1/O，那么需要考虑降低 cache_target_dirty_high_ratio，或者增加osd agent_max_high_ops 线程数来阻止层被脏对象填满 Promotions hitset_count &amp; hitset_period hitset_count 控制在最老的 hitset 开始被清除之前可以存在多少 hitset（即最多多少个 hitset）。hitset_period 控制创建 HitSet 的频率。 如果在实验室环境中测试分层，应该注意，为了创建 HitSet，需要对 PG 进行 I/O;在空闲集群上，不会创建或清除hitset。 拥有正确的数量并控制创建 hitset 的频率是能够可靠地控制何时提升对象的关键。 hitset 只包含对象是否被访问过的数据，不包含访问对象的次数计数 如果 hitset_period 太大，那么即使访问相对较少的对象也会出现在大多数 hitset 中。例如，对于hitset_period为2分钟的情况，一个包含磁盘块的RBD对象(日志文件每分钟更新一次)与一个每秒访问100次的对象处于相同的hitset中 相反，如果 hitset_period 过短，即使是热数据对象也可能无法出现在足够多的 hitset 中，从而无法使热数据成为 Promote 的候选对象，则缓存层得不到充分利用。 通过找到正确的 HitSet 创建周期，就能够捕捉到 I/O 的对应视图，从而设置适当大小的热数据比例来优化 Promotion min_read_recency_for_promote &amp; min_write_recency_for_promote 这两个参数定义一个对象必须出现多少最近的 hitset 才能 Promote。 由于概率的影响，半热数据 与 recency 设定的关系不是线性的。一旦 recency 设置超过 3或4，需要 Promote 的对象的数量将指数级下降 虽然可以分别对读或写分别设置Promeote策略，但它们都引用相同的HitSet数据，因此无法确定访问是读还是写。 如果您将 recency 设置为高于 hitset_count 的值，那么它将永远不会 Promote。例如，可以通过将 min_write_recency_for_promote 设置为高于 hitset_count 的值来确保写 I/O 不会导致对象提升。 Promotion 瓶颈 Promotion 是一个开销较大的操作，所以尽量只在必须进行 Promote 的场景下进行该操作。 通常通过配置 HitSet 和 recency 来限制 Promote 操作，但为了限制 Promotion 带来的影响，有额外的参数来限制 Promotion 的速度。 osd_tier_promote_max_bytes_sec &amp; osd_tier_promote_max_objects_sec 默认限制是 4 Mbps 或每秒 5 个对象。虽然这些数字可能看起来很低，特别是与最新ssd的性能相比，他们的主要目标是尽量减少 Promote 对延迟的影响，所以需要根据实际的负载和硬件条件应该进行仔细的调优，以便在集群中找到更好的平衡值。 需要注意的是，这个值是根据每个OSD配置的，因此总的 Promote 速度将是所有 OSD 的总和。 hit_set_grade_search_last_n 该配置选项允许配置刷回对象的选择过程（即选择哪些对象进行刷回） 该参数控制多少个 hitset 被用于查询对象的访问热度，而对象的访问热度反映了访问它的频率。 经验值表明一般将该参数设置为 recency 对应的值 hit_set_grade_decay_rate 两个连续的hit_set之间的温度衰减率 该参数与 hit_set_grade_search_last_n 协同生效，Hitset 的相关结果会随着时间变得不太具有时效性，所以为了确保更频繁访问的对象没有被错误地刷新，比其他对象更频繁访问的对象具有更热的评级。 需要注意的是，min_flush 和 evict_age 的设置可能会覆盖被刷新或被驱逐对象的访问热度 cache_min_flush_age &amp; cache_min_evict_age cache_min_evict_age 和 cache_min_flush_age 参数简单地定义了一个对象在允许被刷新或被驱逐之前必须多长时间没有被修改。 可以用来阻止那些仅仅不够 Promote 的对象，使它们不再陷入在层之间移动的循环中。 经验值将它们设置为10到30分钟，但是需要注意，在没有需要刷新或清除的对象的情况下，要保证缓存层不能被填满。 ","link":"https://blog.shunzi.tech/post/ceph-tiring-cache-optimization/"},{"title":"Ceph Cache Tiering","content":" 主要介绍 Ceph 中的缓存机制和缓存相关实现 介绍 Tiring 的相关模式并结合部分代码 调研业界对于 Ceph 缓存的性能评价和优化方案 Cache Tiering Architecture Data Structure 由于 Tier cache 在 Ceph 中的存在形式是存储池，所以首先了解存储池的相关属性。 src/osd/osd_type.h/struct pg_pool_t struct pg_pool_t { ... // cache_mode typedef enum { CACHEMODE_NONE = 0, ///&lt; no caching CACHEMODE_WRITEBACK = 1, ///&lt; write to cache, flush later CACHEMODE_FORWARD = 2, ///&lt; forward if not in cache CACHEMODE_READONLY = 3, ///&lt; handle reads, forward writes [not strongly consistent] CACHEMODE_READFORWARD = 4, ///&lt; forward reads, write to cache flush later CACHEMODE_READPROXY = 5, ///&lt; proxy reads, write to cache flush later CACHEMODE_PROXY = 6, ///&lt; proxy if not in cache } cache_mode_t; ... // Tier cache : Base Storage = N : 1 // ceph osd tier add {data_pool} {cache pool} std::set&lt;uint64_t&gt; tiers; ///&lt; pools that are tiers of us int64_t tier_of = -1; /// pool for which we are a tier（-1为没有tier） // Note that write wins for read+write ops // WriteBack mode, read_tier is same as write_tier. Both are cache pool. // Diret mode. cache pool is read_tier, not write_tier. // ceph osd tier set-overlay {data_pool} {cache_pool} int64_t read_tier = -1; /// pool/tier for objecter to direct reads（-1为没有tier） int64_t write_tier = -1; /// pool/tier for objecter to direct write（-1为没有tier） // Set cache mode // ceph osd tier cache-mode {cache-pool} {cache-mode} cache_mode_t cache_mode = CACHEMODE_NONE; /// cache pool mode // Cache pool max bytes and objects uint64_t target_max_bytes = 0; ///&lt; tiering: target max pool size uint64_t target_max_objects = 0; ///&lt; tiering: target max pool size // 目标脏数据率：当脏数据比例达到这个值，后台 agent 开始 flush 数据 uint32_t cache_target_dirty_ratio_micro = 0; ///&lt; cache: fraction of target to leave dirty // 高目标脏数据率：当脏数据比例达到这个值，后台 agent 开始高速 flush 数据 uint32_t cache_target_dirty_high_ratio_micro = 0; ///&lt; cache: fraction of target to flush with high speed // 数据满的比率：当数据达到这个比例时，认为数据已满，需要进行缓存淘汰 uint32_t cache_target_full_ratio_micro = 0; ///&lt; cache: fraction of target to fill before we evict in earnest // 对象在 cache 中被刷入到 storage 层的最小时间 uint32_t cache_min_flush_age = 0; ///&lt; minimum age (seconds) before we can flush // 对象在 cache 中被淘汰的最小时间 uint32_t cache_min_evict_age = 0; ///&lt; minimum age (seconds) before we can evict // HitSet 相关参数 HitSet::Params hit_set_params; ///&lt; The HitSet params to use on this pool // 每间隔 hit_set_period 一段时间，系统重新产生一个新的 hit_set 对象来记录对象的h缓存统计信息 uint32_t hit_set_period = 0; ///&lt; periodicity of HitSet segments (seconds) // 记录系统保存最近的多少个 hit_set 记录 uint32_t hit_set_count = 0; ///&lt; number of periods to retain // hitset archive 对象的命名规则 bool use_gmt_hitset = true; ///&lt; use gmt to name the hitset archive object uint32_t min_read_recency_for_promote = 0; ///&lt; minimum number of HitSet to check before promote on read uint32_t min_write_recency_for_promote = 0; ///&lt; minimum number of HitSet to check before promote on write uint32_t hit_set_grade_decay_rate = 0; ///&lt; current hit_set has highest priority on objects ///&lt; temperature count,the follow hit_set's priority decay ///&lt; by this params than pre hit_set uint32_t hit_set_search_last_n = 0; ///&lt; accumulate atmost N hit_sets for temperature bool is_tier() const { return tier_of &gt;= 0; } bool has_tiers() const { return !tiers.empty(); } void clear_tier() { tier_of = -1; clear_read_tier(); clear_write_tier(); clear_tier_tunables(); } bool has_read_tier() const { return read_tier &gt;= 0; } void clear_read_tier() { read_tier = -1; } bool has_write_tier() const { return write_tier &gt;= 0; } void clear_write_tier() { write_tier = -1; } void clear_tier_tunables() { if (cache_mode != CACHEMODE_NONE) flags |= FLAG_INCOMPLETE_CLONES; cache_mode = CACHEMODE_NONE; target_max_bytes = 0; target_max_objects = 0; cache_target_dirty_ratio_micro = 0; cache_target_dirty_high_ratio_micro = 0; cache_target_full_ratio_micro = 0; hit_set_params = HitSet::Params(); hit_set_period = 0; hit_set_count = 0; hit_set_grade_decay_rate = 0; hit_set_search_last_n = 0; grade_table.resize(0); } ... } Cache Mode Write Back Ceph客户端将数据写入缓存层并从缓存层接收ACK。随着时间的流逝，写入缓存层的数据将迁移到存储层，并从缓存层中清除。从概念上讲，缓存层覆盖在后备存储层的“前面”。当Ceph客户端需要驻留在存储层中的数据时，缓存分层代理在读取时将数据迁移到缓存层，然后将其发送到Ceph客户端。此后，Ceph客户端可以使用缓存层执行I / O，直到数据变为非活动状态为止。 该模式适合于大量修改数据的应用场景（例如，照片/视频编辑，交易数据等）。 Read Forward client 发起读请求，对象不在 cache pool 中，出现 cache miss 状态，就返回 redirect 信息给客户端，客户端再根据返回的信息再次直接向 base pool 发起读请求。 Read Proxy 此模式将使用缓存层中已经存在的任何对象，但是如果一个对象不在缓存中，则请求将代理到基层。这对于将writeback模式转换为禁用的缓存非常有用，因为它允许工作负载在缓存耗尽时正常工作，而不需要向缓存添加任何新对象。 Proxy 针对读写请求都会执行proxy，也就是作为一个代理向后端存储池发起请求并返回给客户端，除非强制要求先进行promote操作。 Forward FORWARD模式表示所有到达cache tier存储池的请求都不会处理，直接将它的后端存储池的ID回复给请求方，并返回-ENOENT的错误号，具体实现比较简单。 该模式的用途是在删除WRITEBACK模式的cache tier时，需将其cache mode先设置为FORWARD，并主动调用cache tier的flush和evict操作，确保cache tier存储池的对象全部evict和flush到后端存储池，保证这个过程中不会有新的数据写入。 Read Only 仅在读取操作时将对象提升到缓存中；写操作被转发到基本层。该种模式下，cache pool 设置成单副本，极大减少缓存空间的占用，当cache pool层失效时，也不会有数据丢失。 此模式适用于不需要存储系统强制一致性的只读工作负载。适合一次写入多次读取的场景(警告:当基本层中的对象更新时，Ceph不会尝试将这些更新同步到缓存中的相应对象。因为这种模式被认为是实验性的，所以必须通过一个yes-i-really-mean-it的选项来启用它。) HitSet 在 write back/read forward/read proxy 模式下需要 HitSet 来记录缓存命中。read only 不需要 HitSet 用于跟踪和统计对象的访问行为，记录对象是否存在缓存中。定义了一个缓存查找到抽象接口，目前提供了三种实现方式：ExplicitHashHitSet，ExplicitObjectHitSet，BloomHitSet ceph/src/osd/HitSet.h 定义了抽象接口，同时该头文件中包含了具体的 HitSet 实现 ... /// abstract interface for a HitSet implementation class Impl { public: virtual impl_type_t get_type() const = 0; virtual bool is_full() const = 0; virtual void insert(const hobject_t&amp; o) = 0; virtual bool contains(const hobject_t&amp; o) const = 0; virtual unsigned insert_count() const = 0; virtual unsigned approx_unique_insert_count() const = 0; virtual void encode(ceph::buffer::list &amp;bl) const = 0; virtual void decode(ceph::buffer::list::const_iterator&amp; p) = 0; virtual void dump(ceph::Formatter *f) const = 0; virtual Impl* clone() const = 0; virtual void seal() {} virtual ~Impl() {} }; ... ExplicitHashHitSet ceph/src/osd/HitSet.h/class ExplicitHashHitSet 基于对象的 32 位 HASH 值的 set 来记录对象的命中，每个对象占用 4 bytes 内存空间 优点：空间占用相对较少，但需要根据 HASH 进行全局的扫描遍历比较 /** * explicitly enumerate hash hits in the set */ class ExplicitHashHitSet : public HitSet::Impl { uint64_t count; // Data Structure ceph::unordered_set&lt;uint32_t&gt; hits; public: class Params : public HitSet::Params::Impl { public: HitSet::impl_type_t get_type() const override { return HitSet::TYPE_EXPLICIT_HASH; } HitSet::Impl *get_new_impl() const override { return new ExplicitHashHitSet; } static void generate_test_instances(std::list&lt;Params*&gt;&amp; o) { o.push_back(new Params); } }; ExplicitHashHitSet() : count(0) {} explicit ExplicitHashHitSet(const ExplicitHashHitSet::Params *p) : count(0) {} ExplicitHashHitSet(const ExplicitHashHitSet &amp;o) : count(o.count), hits(o.hits) {} HitSet::Impl *clone() const override { return new ExplicitHashHitSet(*this); } HitSet::impl_type_t get_type() const override { return HitSet::TYPE_EXPLICIT_HASH; } bool is_full() const override { return false; } void insert(const hobject_t&amp; o) override { hits.insert(o.get_hash()); ++count; } bool contains(const hobject_t&amp; o) const override { return hits.count(o.get_hash()); } unsigned insert_count() const override { return count; } unsigned approx_unique_insert_count() const override { return hits.size(); } void encode(ceph::buffer::list &amp;bl) const override { ENCODE_START(1, 1, bl); encode(count, bl); encode(hits, bl); ENCODE_FINISH(bl); } void decode(ceph::buffer::list::const_iterator &amp;bl) override { DECODE_START(1, bl); decode(count, bl); decode(hits, bl); DECODE_FINISH(bl); } void dump(ceph::Formatter *f) const override; static void generate_test_instances(std::list&lt;ExplicitHashHitSet*&gt;&amp; o) { o.push_back(new ExplicitHashHitSet); o.push_back(new ExplicitHashHitSet); o.back()-&gt;insert(hobject_t()); o.back()-&gt;insert(hobject_t(&quot;asdf&quot;, &quot;&quot;, CEPH_NOSNAP, 123, 1, &quot;&quot;)); o.back()-&gt;insert(hobject_t(&quot;qwer&quot;, &quot;&quot;, CEPH_NOSNAP, 456, 1, &quot;&quot;)); } }; WRITE_CLASS_ENCODER(ExplicitHashHitSet) ExplicitObjectHitSet ceph/src/osd/HitSet.h/class ExplicitObjectHitSet 使用一个基于 ceph/src/common/hobject 的 set 来记录对象的命中，占用的内存取决于对象的关键信息的大小 struct hobject_t { ... public: object_t oid; snapid_t snap; private: uint32_t hash; bool max; uint32_t nibblewise_key_cache; uint32_t hash_reverse_bits; public: int64_t pool; std::string nspace; private: std::string key; class hobject_t_max {}; ... 使用内存中缓存数据结构来进行判断带来的优点就是实现相对简单直观，但占用的内存空间相对较大。 /** * explicitly enumerate objects in the set */ class ExplicitObjectHitSet : public HitSet::Impl { uint64_t count; // Data Structure ceph::unordered_set&lt;hobject_t&gt; hits; public: class Params : public HitSet::Params::Impl { public: HitSet::impl_type_t get_type() const override { return HitSet::TYPE_EXPLICIT_OBJECT; } HitSet::Impl *get_new_impl() const override { return new ExplicitObjectHitSet; } static void generate_test_instances(std::list&lt;Params*&gt;&amp; o) { o.push_back(new Params); } }; ExplicitObjectHitSet() : count(0) {} explicit ExplicitObjectHitSet(const ExplicitObjectHitSet::Params *p) : count(0) {} ExplicitObjectHitSet(const ExplicitObjectHitSet &amp;o) : count(o.count), hits(o.hits) {} HitSet::Impl *clone() const override { return new ExplicitObjectHitSet(*this); } HitSet::impl_type_t get_type() const override { return HitSet::TYPE_EXPLICIT_OBJECT; } bool is_full() const override { return false; } void insert(const hobject_t&amp; o) override { hits.insert(o); ++count; } bool contains(const hobject_t&amp; o) const override { return hits.count(o); } unsigned insert_count() const override { return count; } unsigned approx_unique_insert_count() const override { return hits.size(); } void encode(ceph::buffer::list &amp;bl) const override { ENCODE_START(1, 1, bl); encode(count, bl); encode(hits, bl); ENCODE_FINISH(bl); } void decode(ceph::buffer::list::const_iterator&amp; bl) override { DECODE_START(1, bl); decode(count, bl); decode(hits, bl); DECODE_FINISH(bl); } void dump(ceph::Formatter *f) const override; static void generate_test_instances(std::list&lt;ExplicitObjectHitSet*&gt;&amp; o) { o.push_back(new ExplicitObjectHitSet); o.push_back(new ExplicitObjectHitSet); o.back()-&gt;insert(hobject_t()); o.back()-&gt;insert(hobject_t(&quot;asdf&quot;, &quot;&quot;, CEPH_NOSNAP, 123, 1, &quot;&quot;)); o.back()-&gt;insert(hobject_t(&quot;qwer&quot;, &quot;&quot;, CEPH_NOSNAP, 456, 1, &quot;&quot;)); } }; WRITE_CLASS_ENCODER(ExplicitObjectHitSet) BloomHitSet ceph/src/osd/HitSet.h/class BloomHitSet 采用了压缩的 Bloom Filter 的方式来记录对象是否在缓存中，进一步减少了内存占用空间。 /** * use a bloom_filter to track hits to the set */ class BloomHitSet : public HitSet::Impl { compressible_bloom_filter bloom; public: HitSet::impl_type_t get_type() const override { return HitSet::TYPE_BLOOM; } class Params : public HitSet::Params::Impl { public: HitSet::impl_type_t get_type() const override { return HitSet::TYPE_BLOOM; } HitSet::Impl *get_new_impl() const override { return new BloomHitSet; } uint32_t fpp_micro; ///&lt; false positive probability / 1M uint64_t target_size; ///&lt; number of unique insertions we expect to this HitSet uint64_t seed; ///&lt; seed to use when initializing the bloom filter Params() : fpp_micro(0), target_size(0), seed(0) {} Params(double fpp, uint64_t t, uint64_t s) : fpp_micro(fpp * 1000000.0), target_size(t), seed(s) {} Params(const Params &amp;o) : fpp_micro(o.fpp_micro), target_size(o.target_size), seed(o.seed) {} ~Params() override {} double get_fpp() const { return (double)fpp_micro / 1000000.0; } void set_fpp(double f) { fpp_micro = (unsigned)(llrintl(f * 1000000.0)); } void encode(ceph::buffer::list&amp; bl) const override { ENCODE_START(1, 1, bl); encode(fpp_micro, bl); encode(target_size, bl); encode(seed, bl); ENCODE_FINISH(bl); } void decode(ceph::buffer::list::const_iterator&amp; bl) override { DECODE_START(1, bl); decode(fpp_micro, bl); decode(target_size, bl); decode(seed, bl); DECODE_FINISH(bl); } void dump(ceph::Formatter *f) const override; void dump_stream(std::ostream&amp; o) const override { o &lt;&lt; &quot;false_positive_probability: &quot; &lt;&lt; get_fpp() &lt;&lt; &quot;, target_size: &quot; &lt;&lt; target_size &lt;&lt; &quot;, seed: &quot; &lt;&lt; seed; } static void generate_test_instances(std::list&lt;Params*&gt;&amp; o) { o.push_back(new Params); o.push_back(new Params); (*o.rbegin())-&gt;fpp_micro = 123456; (*o.rbegin())-&gt;target_size = 300; (*o.rbegin())-&gt;seed = 99; } }; BloomHitSet() {} BloomHitSet(unsigned inserts, double fpp, int seed) : bloom(inserts, fpp, seed) {} explicit BloomHitSet(const BloomHitSet::Params *p) : bloom(p-&gt;target_size, p-&gt;get_fpp(), p-&gt;seed){} BloomHitSet(const BloomHitSet &amp;o) { // oh god ceph::buffer::list bl; o.encode(bl); auto bli = std::cbegin(bl); this-&gt;decode(bli); } HitSet::Impl *clone() const override { return new BloomHitSet(*this); } bool is_full() const override { return bloom.is_full(); } void insert(const hobject_t&amp; o) override { bloom.insert(o.get_hash()); } bool contains(const hobject_t&amp; o) const override { return bloom.contains(o.get_hash()); } unsigned insert_count() const override { return bloom.element_count(); } unsigned approx_unique_insert_count() const override { return bloom.approx_unique_element_count(); } void seal() override { // aim for a density of .5 (50% of bit set) double pc = bloom.density() * 2.0; if (pc &lt; 1.0) bloom.compress(pc); } void encode(ceph::buffer::list &amp;bl) const override { ENCODE_START(1, 1, bl); encode(bloom, bl); ENCODE_FINISH(bl); } void decode(ceph::buffer::list::const_iterator&amp; bl) override { DECODE_START(1, bl); decode(bloom, bl); DECODE_FINISH(bl); } void dump(ceph::Formatter *f) const override; static void generate_test_instances(std::list&lt;BloomHitSet*&gt;&amp; o) { o.push_back(new BloomHitSet); o.push_back(new BloomHitSet(10, .1, 1)); o.back()-&gt;insert(hobject_t()); o.back()-&gt;insert(hobject_t(&quot;asdf&quot;, &quot;&quot;, CEPH_NOSNAP, 123, 1, &quot;&quot;)); o.back()-&gt;insert(hobject_t(&quot;qwer&quot;, &quot;&quot;, CEPH_NOSNAP, 456, 1, &quot;&quot;)); } }; WRITE_CLASS_ENCODER(BloomHitSet) ceph/src/osd/HitSet.cc HitSet::HitSet(const HitSet::Params&amp; params) : sealed(false) { switch (params.get_type()) { case TYPE_BLOOM: { BloomHitSet::Params *p = static_cast&lt;BloomHitSet::Params*&gt;(params.impl.get()); impl.reset(new BloomHitSet(p)); } break; case TYPE_EXPLICIT_HASH: impl.reset(new ExplicitHashHitSet(static_cast&lt;ExplicitHashHitSet::Params*&gt;(params.impl.get()))); break; case TYPE_EXPLICIT_OBJECT: impl.reset(new ExplicitObjectHitSet(static_cast&lt;ExplicitObjectHitSet::Params*&gt;(params.impl.get()))); break; default: assert (0 == &quot;unknown HitSet type&quot;); } } IO Add Cache 在 ceph/src/mon/OSDMonitor.cc 中实现了 add-cache 命令，从命令行中获取对应的参数并绑定 Tier 关系 else if (prefix == &quot;osd tier add-cache&quot;) { ... // go // 分别获取 base_pool 和 cache_pool 信息 pg_pool_t *np = pending_inc.get_new_pool(pool_id, p); pg_pool_t *ntp = pending_inc.get_new_pool(tierpool_id, tp); // 检查 Pool 之间的 Tier 关系 if (np-&gt;tiers.count(tierpool_id) || ntp-&gt;is_tier()) { wait_for_finished_proposal(op, new C_RetryMessage(this, op)); return true; } // 将缓存 Pool 添加到 base_pool 的 tiers 中去 np-&gt;tiers.insert(tierpool_id); // 设置 read_tier/write_tier = cache_tire np-&gt;read_tier = np-&gt;write_tier = tierpool_id; np-&gt;set_snap_epoch(pending_inc.epoch); // tier will update to our snap info np-&gt;set_last_force_op_resend(pending_inc.epoch); ntp-&gt;set_last_force_op_resend(pending_inc.epoch); ntp-&gt;tier_of = pool_id; // 设置缓存策略，默认 write-back ntp-&gt;cache_mode = mode; // 设置 flush evict 相关参数 ntp-&gt;hit_set_count = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_hit_set_count&quot;); ntp-&gt;hit_set_period = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_hit_set_period&quot;); ntp-&gt;min_read_recency_for_promote = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_min_read_recency_for_promote&quot;); ntp-&gt;min_write_recency_for_promote = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_min_write_recency_for_promote&quot;); ntp-&gt;hit_set_grade_decay_rate = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_hit_set_grade_decay_rate&quot;); ntp-&gt;hit_set_search_last_n = g_conf().get_val&lt;uint64_t&gt;(&quot;osd_tier_default_cache_hit_set_search_last_n&quot;); ntp-&gt;hit_set_params = hsp; ntp-&gt;target_max_bytes = size; ss &lt;&lt; &quot;pool '&quot; &lt;&lt; tierpoolstr &lt;&lt; &quot;' is now (or already was) a cache tier of '&quot; &lt;&lt; poolstr &lt;&lt; &quot;'&quot;; wait_for_finished_proposal(op, new Monitor::C_Command(mon, op, 0, ss.str(), get_last_committed() + 1)); return true; } 选择 Cache Pool 在 ceph/src/osdc/Objecter.cc 中指定目标存储池为 Cache Pool，设置之后由后续的代码在该 Pool 中执行 Crush 算法。 int Objecter::_calc_target(op_target_t *t, Connection *con, bool any_change) { ... // 根据读写操作，分别设置需要操作的 tier // apply tiering t-&gt;target_oid = t-&gt;base_oid; t-&gt;target_oloc = t-&gt;base_oloc; if ((t-&gt;flags &amp; CEPH_OSD_FLAG_IGNORE_OVERLAY) == 0) { if (is_read &amp;&amp; pi-&gt;has_read_tier()) t-&gt;target_oloc.pool = pi-&gt;read_tier; if (is_write &amp;&amp; pi-&gt;has_write_tier()) t-&gt;target_oloc.pool = pi-&gt;write_tier; pi = osdmap-&gt;get_pg_pool(t-&gt;target_oloc.pool); if (!pi) { t-&gt;osd = -1; return RECALC_OP_TARGET_POOL_DNE; } } } Cache Pool 请求处理 Cache 的相关请求处理可以通过主流程进行梳理。 主流程中主要包含了 agent_choose_mode 和 maybe_handle_cache_detail 两个主要方法。 主流程 PrimaryLogPG.cc/do_op(OpRequestRef &amp;) /** do_op - do an op * pg lock will be held (if multithreaded) * osd_lock NOT held. */ void PrimaryLogPG::do_op(OpRequestRef&amp; op) { ... // 预处理 ObjectContextRef obc; bool can_create = op-&gt;may_write(); hobject_t missing_oid; // kludge around the fact that LIST_SNAPS sets CEPH_SNAPDIR for LIST_SNAPS const hobject_t&amp; oid = m-&gt;get_snapid() == CEPH_SNAPDIR ? head : m-&gt;get_hobj(); ... // io blocked on obc? if (!m-&gt;has_flag(CEPH_OSD_FLAG_FLUSH) &amp;&amp; maybe_await_blocked_head(oid, op)) { return; } // 获取上下文信息 obc int r = find_object_context(oid, &amp;obc, can_create, m-&gt;has_flag(CEPH_OSD_FLAG_MAP_SNAP_CLONE), &amp;missing_oid); // bool in_hit_set = false; // 如果有 hit_set if (hit_set) { if (obc.get()) { if (obc-&gt;obs.oi.soid != hobject_t() &amp;&amp; hit_set-&gt;contains(obc-&gt;obs.oi.soid)) in_hit_set = true; } else { // 如果是读操作，且要读的object在当前的cachepool中不存在，但是在hit_set中记录了该object 刚被访问过。 if (missing_oid != hobject_t() &amp;&amp; hit_set-&gt;contains(missing_oid)) in_hit_set = true; } if (!op-&gt;hitset_inserted) { // hitset 统计这次访问 object 的操作 hit_set-&gt;insert(oid); op-&gt;hitset_inserted = true; // 如果这hit_set 满了，或者时间间隔到了，则需要持久化这个hit_set信息。 if (hit_set-&gt;is_full() || hit_set_start_stamp + pool.info.hit_set_period &lt;= m-&gt;get_recv_stamp()) { // 持久化 hit_set hit_set_persist(); } } } // 如果这个pg存在agent_state if (agent_state) { // 对相应的 PG 设置 flush 模式和 evict 模式 if (agent_choose_mode(false, op)) return; } if (obc.get() &amp;&amp; obc-&gt;obs.exists &amp;&amp; obc-&gt;obs.oi.has_manifest()) { if (maybe_handle_manifest(op, write_ordered, obc)) return; } // 如果maybe_handle_cache 处理成功了则直接return，否则继续进行后面的操作。 if (maybe_handle_cache(op, write_ordered, obc, r, missing_oid, false, in_hit_set)) return; ... // 如果读的话，则直接读本osd，如果写的话，就分发到其他replicate osd上 } agent_choose_mode(bool restart, OpRequestRef op) PrimaryLogPG.cc/agent_choose_mode 该函数主要计算一个 PG 的 flush_mode 和 evic_mode 的参数值。 返回值如果为 True，表明该请求 Op 被重新加入请求队列（由于 EvictMode 为 Full），其他情况返回 false。 bool PrimaryLogPG::agent_choose_mode(bool restart, OpRequestRef op) { bool requeued = false; // Let delay play out if (agent_state-&gt;delaying) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; this &lt;&lt; &quot; delaying, ignored&quot; &lt;&lt; dendl; return requeued; } TierAgentState::flush_mode_t flush_mode = TierAgentState::FLUSH_MODE_IDLE; TierAgentState::evict_mode_t evict_mode = TierAgentState::EVICT_MODE_IDLE; unsigned evict_effort = 0; // 如果当前统计的信息无效，暂时跳过计算过程，暂时不计算 flush_mode 和 evic_mode 的值 if (info.stats.stats_invalid) { // idle; stats can't be trusted until we scrub. dout(20) &lt;&lt; __func__ &lt;&lt; &quot; stats invalid (post-split), idle&quot; &lt;&lt; dendl; goto skip_calc; } { // 计算 divisor 的值， 也就是 cache_pool 中 PG 的数量 uint64_t divisor = pool.info.get_pg_num_divisor(info.pgid.pgid); ceph_assert(divisor &gt; 0); // 基于 HitSet 对象的数量计算 unflushable，不能刷回的对象 // adjust (effective) user objects down based on the number // of HitSet objects, which should not count toward our total since // they cannot be flushed. uint64_t unflushable = info.stats.stats.sum.num_objects_hit_set_archive; // also exclude omap objects if ec backing pool const pg_pool_t *base_pool = get_osdmap()-&gt;get_pg_pool(pool.info.tier_of); ceph_assert(base_pool); // 如果 base_pool 是 ec pool，不支持 omap，去掉所有需要 omap 支持的对象 if (!base_pool-&gt;supports_omap()) unflushable += info.stats.stats.sum.num_objects_omap; // 计算 num_user_objects uint64_t num_user_objects = info.stats.stats.sum.num_objects; // 其值为统计的对象数目减去 unflushable 对象数 if (num_user_objects &gt; unflushable) num_user_objects -= unflushable; else num_user_objects = 0; uint64_t num_user_bytes = info.stats.stats.sum.num_bytes; uint64_t unflushable_bytes = info.stats.stats.sum.num_bytes_hit_set_archive; // 计算 num_user_bytes，其值为统计信息的字节数减去 unflushable 字节数 num_user_bytes -= unflushable_bytes; uint64_t num_overhead_bytes = osd-&gt;store-&gt;estimate_objects_overhead(num_user_objects); num_user_bytes += num_overhead_bytes; // 计算脏对象的数目 num_dirty 值 // also reduce the num_dirty by num_objects_omap int64_t num_dirty = info.stats.stats.sum.num_objects_dirty; // 如果 base_pool 不支持 omap，去掉带 omap 的对象 if (!base_pool-&gt;supports_omap()) { if (num_dirty &gt; info.stats.stats.sum.num_objects_omap) num_dirty -= info.stats.stats.sum.num_objects_omap; else num_dirty = 0; } dout(10) &lt;&lt; __func__ &lt;&lt; &quot; flush_mode: &quot; &lt;&lt; TierAgentState::get_flush_mode_name(agent_state-&gt;flush_mode) &lt;&lt; &quot; evict_mode: &quot; &lt;&lt; TierAgentState::get_evict_mode_name(agent_state-&gt;evict_mode) &lt;&lt; &quot; num_objects: &quot; &lt;&lt; info.stats.stats.sum.num_objects &lt;&lt; &quot; num_bytes: &quot; &lt;&lt; info.stats.stats.sum.num_bytes &lt;&lt; &quot; num_objects_dirty: &quot; &lt;&lt; info.stats.stats.sum.num_objects_dirty &lt;&lt; &quot; num_objects_omap: &quot; &lt;&lt; info.stats.stats.sum.num_objects_omap &lt;&lt; &quot; num_dirty: &quot; &lt;&lt; num_dirty &lt;&lt; &quot; num_user_objects: &quot; &lt;&lt; num_user_objects &lt;&lt; &quot; num_user_bytes: &quot; &lt;&lt; num_user_bytes &lt;&lt; &quot; num_overhead_bytes: &quot; &lt;&lt; num_overhead_bytes &lt;&lt; &quot; pool.info.target_max_bytes: &quot; &lt;&lt; pool.info.target_max_bytes &lt;&lt; &quot; pool.info.target_max_objects: &quot; &lt;&lt; pool.info.target_max_objects &lt;&lt; dendl; // 计算脏数据的比率和数据满的比率，单位为百万分之一 // get dirty, full ratios uint64_t dirty_micro = 0; uint64_t full_micro = 0; // 如果设置了 target_max_bytes，就按照字节数算 if (pool.info.target_max_bytes &amp;&amp; num_user_objects &gt; 0) { // 首先计算每个对象的平均大小 avg_size uint64_t avg_size = num_user_bytes / num_user_objects; // 脏数据率 = 100w * 脏数据对象数目 * 每个对象的平均大小 / 每个PG的平均字节数 dirty_micro = num_dirty * avg_size * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_bytes / divisor, 1); // 满数据率 = 100w * 用户对象数目 * 每个对象的平均大小 / 每个PG的平均字节数 full_micro = num_user_objects * avg_size * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_bytes / divisor, 1); } // 如果设置了 target_max_objects，就按照对象个数算 if (pool.info.target_max_objects &gt; 0) { // 脏数据率 = 100w * 脏数据对象数目 / 每个 PG 的平均对象数目 uint64_t dirty_objects_micro = num_dirty * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_objects / divisor, 1); // 取两种计算方式中的最大值 if (dirty_objects_micro &gt; dirty_micro) dirty_micro = dirty_objects_micro; // 满数据率 = 100w * 用户对象数目 / 每个 PG 的平均对象数目 uint64_t full_objects_micro = num_user_objects * 1000000 / std::max&lt;uint64_t&gt;(pool.info.target_max_objects / divisor, 1); if (full_objects_micro &gt; full_micro) full_micro = full_objects_micro; } dout(20) &lt;&lt; __func__ &lt;&lt; &quot; dirty &quot; &lt;&lt; ((float)dirty_micro / 1000000.0) &lt;&lt; &quot; full &quot; &lt;&lt; ((float)full_micro / 1000000.0) &lt;&lt; dendl; // flush mode // 获取 flush_target 和 flush_high_target 参数，以及计算 flush_slop uint64_t flush_target = pool.info.cache_target_dirty_ratio_micro; uint64_t flush_high_target = pool.info.cache_target_dirty_high_ratio_micro; uint64_t flush_slop = (float)flush_target * cct-&gt;_conf-&gt;osd_agent_slop; // 根据传入的参数和 flush_mode 对 target 做修正 if (restart || agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_IDLE) { flush_target += flush_slop; flush_high_target += flush_slop; } else { flush_target -= std::min(flush_target, flush_slop); flush_high_target -= std::min(flush_high_target, flush_slop); } // 根据脏数据的比例，设置 flush_mode if (dirty_micro &gt; flush_high_target) { flush_mode = TierAgentState::FLUSH_MODE_HIGH; } else if (dirty_micro &gt; flush_target || (!flush_target &amp;&amp; num_dirty &gt; 0)) { flush_mode = TierAgentState::FLUSH_MODE_LOW; } // evict mode // 获取 evict_target 的值，用 evict_slop 做修正 uint64_t evict_target = pool.info.cache_target_full_ratio_micro; uint64_t evict_slop = (float)evict_target * cct-&gt;_conf-&gt;osd_agent_slop; if (restart || agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_IDLE) evict_target += evict_slop; else evict_target -= std::min(evict_target, evict_slop); // 判定缓存满 // 如果 full_micro &gt; 100w，设置为EVICT_MODE_FULL，evict_effort为100w if (full_micro &gt; 1000000) { // evict anything clean evict_mode = TierAgentState::EVICT_MODE_FULL; evict_effort = 1000000; } else if (full_micro &gt; evict_target) { // 设置为 EVICT_MODE_SOME，部分满，需要计算 evic_effort(PG 在 agent 队列中的优先级 ) // set effort in [0..1] range based on where we are between evict_mode = TierAgentState::EVICT_MODE_SOME; uint64_t over = full_micro - evict_target; uint64_t span = 1000000 - evict_target; evict_effort = std::max(over * 1000000 / span, uint64_t(1000000.0 * cct-&gt;_conf-&gt;osd_agent_min_evict_effort)); // 通过 osd_agent_quantize_effort 进行修正，使得优先级级别不会太多 // quantize effort to avoid too much reordering in the agent_queue. uint64_t inc = cct-&gt;_conf-&gt;osd_agent_quantize_effort * 1000000; ceph_assert(inc &gt; 0); uint64_t was = evict_effort; evict_effort -= evict_effort % inc; if (evict_effort &lt; inc) evict_effort = inc; ceph_assert(evict_effort &gt;= inc &amp;&amp; evict_effort &lt;= 1000000); dout(30) &lt;&lt; __func__ &lt;&lt; &quot; evict_effort &quot; &lt;&lt; was &lt;&lt; &quot; quantized by &quot; &lt;&lt; inc &lt;&lt; &quot; to &quot; &lt;&lt; evict_effort &lt;&lt; dendl; } } skip_calc: bool old_idle = agent_state-&gt;is_idle(); // 设置新的 flush_mode，并更新统计信息 if (flush_mode != agent_state-&gt;flush_mode) { dout(5) &lt;&lt; __func__ &lt;&lt; &quot; flush_mode &quot; &lt;&lt; TierAgentState::get_flush_mode_name(agent_state-&gt;flush_mode) &lt;&lt; &quot; -&gt; &quot; &lt;&lt; TierAgentState::get_flush_mode_name(flush_mode) &lt;&lt; dendl; recovery_state.update_stats( [=](auto &amp;history, auto &amp;stats) { if (flush_mode == TierAgentState::FLUSH_MODE_HIGH) { osd-&gt;agent_inc_high_count(); stats.stats.sum.num_flush_mode_high = 1; } else if (flush_mode == TierAgentState::FLUSH_MODE_LOW) { stats.stats.sum.num_flush_mode_low = 1; } if (agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_HIGH) { osd-&gt;agent_dec_high_count(); stats.stats.sum.num_flush_mode_high = 0; } else if (agent_state-&gt;flush_mode == TierAgentState::FLUSH_MODE_LOW) { stats.stats.sum.num_flush_mode_low = 0; } return false; }); agent_state-&gt;flush_mode = flush_mode; } // 设置新的 evict_mode if (evict_mode != agent_state-&gt;evict_mode) { dout(5) &lt;&lt; __func__ &lt;&lt; &quot; evict_mode &quot; &lt;&lt; TierAgentState::get_evict_mode_name(agent_state-&gt;evict_mode) &lt;&lt; &quot; -&gt; &quot; &lt;&lt; TierAgentState::get_evict_mode_name(evict_mode) &lt;&lt; dendl; // 如果evict_mode由 FULL 变为其他类型，并且 PG 的状态 Active，需要把当前的 op 以及因 cache full 而等待的操作都重新加入请求队列，设置返回值为 true if (agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL &amp;&amp; is_active()) { // 把当前的 op 以及因 cache full 而等待的操作都重新加入请求队列 if (op) requeue_op(op); // 各个操作的等待队列 requeue_ops(waiting_for_flush); requeue_ops(waiting_for_active); requeue_ops(waiting_for_readable); requeue_ops(waiting_for_scrub); requeue_ops(waiting_for_cache_not_full); objects_blocked_on_cache_full.clear(); // 设置返回值为 true requeued = true; } recovery_state.update_stats( [=](auto &amp;history, auto &amp;stats) { if (evict_mode == TierAgentState::EVICT_MODE_SOME) { stats.stats.sum.num_evict_mode_some = 1; } else if (evict_mode == TierAgentState::EVICT_MODE_FULL) { stats.stats.sum.num_evict_mode_full = 1; } if (agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_SOME) { stats.stats.sum.num_evict_mode_some = 0; } else if (agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL) { stats.stats.sum.num_evict_mode_full = 0; } return false; }); agent_state-&gt;evict_mode = evict_mode; } uint64_t old_effort = agent_state-&gt;evict_effort; if (evict_effort != agent_state-&gt;evict_effort) { dout(5) &lt;&lt; __func__ &lt;&lt; &quot; evict_effort &quot; &lt;&lt; ((float)agent_state-&gt;evict_effort / 1000000.0) &lt;&lt; &quot; -&gt; &quot; &lt;&lt; ((float)evict_effort / 1000000.0) &lt;&lt; dendl; agent_state-&gt;evict_effort = evict_effort; } // 根据 MODE 做相应的处理 // NOTE: we are using evict_effort as a proxy for *all* agent effort // (including flush). This is probably fine (they should be // correlated) but it is not precisely correct. if (agent_state-&gt;is_idle()) { if (!restart &amp;&amp; !old_idle) { // 把该 PG 从 agent_queue 中删除 osd-&gt;agent_disable_pg(this, old_effort); } } else { if (restart || old_idle) { // 把该 PG 重新加入 agent_queue 处理队列 osd-&gt;agent_enable_pg(this, agent_state-&gt;evict_effort); } else if (old_effort != agent_state-&gt;evict_effort) { // 已经存在与队列中，调整 evict_effort 队列中的优先级 osd-&gt;agent_adjust_pg(this, old_effort, agent_state-&gt;evict_effort); } } return requeued; } maybe_handle_cache_detail PrimaryLogPG.cc/maybe_handle_cache_detail PrimaryLogPG::cache_result_t PrimaryLogPG::maybe_handle_cache_detail( OpRequestRef op, bool write_ordered, ObjectContextRef obc, int r, hobject_t missing_oid, bool must_promote, bool in_hit_set, ObjectContextRef *promote_obc) { // return quickly if caching is not enabled if (pool.info.cache_mode == pg_pool_t::CACHEMODE_NONE) return cache_result_t::NOOP; if (op &amp;&amp; op-&gt;get_req() &amp;&amp; op-&gt;get_req()-&gt;get_type() == CEPH_MSG_OSD_OP &amp;&amp; (op-&gt;get_req&lt;MOSDOp&gt;()-&gt;get_flags() &amp; CEPH_OSD_FLAG_IGNORE_CACHE)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot;: ignoring cache due to flag&quot; &lt;&lt; dendl; return cache_result_t::NOOP; } must_promote = must_promote || op-&gt;need_promote(); if (obc) dout(25) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; &quot; &quot; &lt;&lt; (obc-&gt;obs.exists ? &quot;exists&quot; : &quot;DNE&quot;) &lt;&lt; &quot; missing_oid &quot; &lt;&lt; missing_oid &lt;&lt; &quot; must_promote &quot; &lt;&lt; (int)must_promote &lt;&lt; &quot; in_hit_set &quot; &lt;&lt; (int)in_hit_set &lt;&lt; dendl; else dout(25) &lt;&lt; __func__ &lt;&lt; &quot; (no obc)&quot; &lt;&lt; &quot; missing_oid &quot; &lt;&lt; missing_oid &lt;&lt; &quot; must_promote &quot; &lt;&lt; (int)must_promote &lt;&lt; &quot; in_hit_set &quot; &lt;&lt; (int)in_hit_set &lt;&lt; dendl; // if it is write-ordered and blocked, stop now if (obc.get() &amp;&amp; obc-&gt;is_blocked() &amp;&amp; write_ordered) { // we're already doing something with this object dout(20) &lt;&lt; __func__ &lt;&lt; &quot; blocked on &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; dendl; return cache_result_t::NOOP; } if (r == -ENOENT &amp;&amp; missing_oid == hobject_t()) { // we know this object is logically absent (e.g., an undefined clone) return cache_result_t::NOOP; } // 判断该object是否在cache pool中是否命中。如果在cachepool中命中，则直接return false，然后在do_op中会直接操作cache pool后面的流程。 if (obc.get() &amp;&amp; obc-&gt;obs.exists) { osd-&gt;logger-&gt;inc(l_osd_op_cache_hit); return cache_result_t::NOOP; } if (!is_primary()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; cache miss; ask the primary&quot; &lt;&lt; dendl; osd-&gt;reply_op_error(op, -EAGAIN); return cache_result_t::REPLIED_WITH_EAGAIN; } // 如果缓存未能命中，则获取对应的 oid 信息 if (missing_oid == hobject_t() &amp;&amp; obc.get()) { missing_oid = obc-&gt;obs.oi.soid; } auto m = op-&gt;get_req&lt;MOSDOp&gt;(); const object_locator_t oloc = m-&gt;get_object_locator(); if (op-&gt;need_skip_handle_cache()) { return cache_result_t::NOOP; } OpRequestRef promote_op; // 根据不同的缓存策略分别处理 switch (pool.info.cache_mode) { // Write Back 策略 case pg_pool_t::CACHEMODE_WRITEBACK: // Cache 满 if (agent_state &amp;&amp; agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL) { // 读操作 Proxy Read if (!op-&gt;may_write() &amp;&amp; !op-&gt;may_cache() &amp;&amp; !write_ordered &amp;&amp; !must_promote) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; cache pool full, proxying read&quot; &lt;&lt; dendl; do_proxy_read(op); return cache_result_t::HANDLED_PROXY; } // 写操作，等待缓存淘汰 dout(20) &lt;&lt; __func__ &lt;&lt; &quot; cache pool full, waiting&quot; &lt;&lt; dendl; block_write_on_full_cache(missing_oid, op); return cache_result_t::BLOCKED_FULL; } // Cache 未满 if (must_promote || (!hit_set &amp;&amp; !op-&gt;need_skip_promote())) { promote_object(obc, missing_oid, oloc, op, promote_obc); return cache_result_t::BLOCKED_PROMOTE; } // 写操作 代理写 if (op-&gt;may_write() || op-&gt;may_cache()) { do_proxy_write(op); // Promote too? 判断是否需要从 Base 层取数据到 Cache if (!op-&gt;need_skip_promote() &amp;&amp; maybe_promote(obc, missing_oid, oloc, in_hit_set, pool.info.min_write_recency_for_promote, OpRequestRef(), promote_obc)) { return cache_result_t::BLOCKED_PROMOTE; } return cache_result_t::HANDLED_PROXY; } else { // 读操作 代理读 do_proxy_read(op); // Avoid duplicate promotion if (obc.get() &amp;&amp; obc-&gt;is_blocked()) { if (promote_obc) *promote_obc = obc; return cache_result_t::BLOCKED_PROMOTE; } // Promote too? if (!op-&gt;need_skip_promote()) { (void)maybe_promote(obc, missing_oid, oloc, in_hit_set, pool.info.min_read_recency_for_promote, promote_op, promote_obc); } return cache_result_t::HANDLED_PROXY; } // 异常信息输出 ceph_abort_msg(&quot;unreachable&quot;); return cache_result_t::NOOP; // Read Only 策略 case pg_pool_t::CACHEMODE_READONLY: // TODO: clean this case up // 缓存中无该对象，对应地去从 Base 层 Promote 对象 if (!obc.get() &amp;&amp; r == -ENOENT) { // we don't have the object and op's a read promote_object(obc, missing_oid, oloc, op, promote_obc); return cache_result_t::BLOCKED_PROMOTE; } // 非读操作 - 写操作 重定向写操作 if (!r) { // it must be a write do_cache_redirect(op); return cache_result_t::HANDLED_REDIRECT; } // 异常处理 // crap, there was a failure of some kind return cache_result_t::NOOP; // Forward 策略 已过期，由 Proxy 取代 case pg_pool_t::CACHEMODE_FORWARD: // this mode is deprecated; proxy instead // Proxy 策略 case pg_pool_t::CACHEMODE_PROXY: // 可以不从 Base 层 Promote 的情况 if (!must_promote) { if (op-&gt;may_write() || op-&gt;may_cache() || write_ordered) { // 写操作 - 代理写 do_proxy_write(op); return cache_result_t::HANDLED_PROXY; } else { // 读操作 - 代理读 do_proxy_read(op); return cache_result_t::HANDLED_PROXY; } } // ugh, we're forced to promote. // 写操作 缓存已满 进入等待队列 if (agent_state &amp;&amp; agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; cache pool full, waiting&quot; &lt;&lt; dendl; block_write_on_full_cache(missing_oid, op); return cache_result_t::BLOCKED_FULL; } // 缓存未满，进行 Promote promote_object(obc, missing_oid, oloc, op, promote_obc); return cache_result_t::BLOCKED_PROMOTE; // READ-FORWARD 模式，已过期，由 Proxy 模式处理 case pg_pool_t::CACHEMODE_READFORWARD: // this mode is deprecated; proxy instead // READ-PROXY 模式 case pg_pool_t::CACHEMODE_READPROXY: // Do writeback to the cache tier for writes if (op-&gt;may_write() || write_ordered || must_promote) { if (agent_state &amp;&amp; agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; cache pool full, waiting&quot; &lt;&lt; dendl; block_write_on_full_cache(missing_oid, op); return cache_result_t::BLOCKED_FULL; } promote_object(obc, missing_oid, oloc, op, promote_obc); return cache_result_t::BLOCKED_PROMOTE; } // If it is a read, we can read, we need to proxy it do_proxy_read(op); return cache_result_t::HANDLED_PROXY; default: ceph_abort_msg(&quot;unrecognized cache_mode&quot;); } return cache_result_t::NOOP; } 图解缓存策略 将以上缓存策略的处理流程转换为流程图如下所示（注：流程细节随着Ceph版本的迭代已经有锁改变，此处重点关注最终的调用）： WriteBack 策略： 针对其中涉及到的几个封装好的方法的操作： do_cache_redirect， do_proxy_read， do_proxy_write, promote_object do_cache_redirect ：客户端请求cache pool，cache pool告诉客户端你应该去base pool中请求，客户端收到应答后，再次发送请求到base pool中请求数据，由base pool告诉客户端请求完成。 void PrimaryLogPG::do_cache_redirect(OpRequestRef op) { // Cast the request to MOSDOp auto m = op-&gt;get_req&lt;MOSDOp&gt;(); int flags = m-&gt;get_flags() &amp; (CEPH_OSD_FLAG_ACK|CEPH_OSD_FLAG_ONDISK); // 构造 MOSDOpReply 对象 MOSDOpReply *reply = new MOSDOpReply(m, -ENOENT, get_osdmap_epoch(), flags, false); // 将请求重定向到指定的存储池 Pool request_redirect_t redir(m-&gt;get_object_locator(), pool.info.tier_of); reply-&gt;set_redirect(redir); dout(10) &lt;&lt; &quot;sending redirect to pool &quot; &lt;&lt; pool.info.tier_of &lt;&lt; &quot; for op &quot; &lt;&lt; op &lt;&lt; dendl; // 发送响应信息（包含重定向目标存储池的信息和对象的相关信息） m-&gt;get_connection()-&gt;send_message(reply); return; } do_proxy_read：客户端发送读请求到cache pool，但是未命中，则cache pool自己会发送请求到base pool中，获取数据后，由cache pool将数据发送给客户端，完成读请求。但是值得注意的是，虽然cache pool读取到了该object，但不会保存在cache pool中，下次请求仍然需要重新向basepool请求。 void PrimaryLogPG::do_proxy_read(OpRequestRef op, ObjectContextRef obc) { // NOTE: non-const here because the ProxyReadOp needs mutable refs to // stash the result in the request's OSDOp vector MOSDOp *m = static_cast&lt;MOSDOp*&gt;(op-&gt;get_nonconst_req()); object_locator_t oloc; hobject_t soid; /* extensible tier */ // 获取对应的需要查询的对象的信息 // 判断是否包含 manifest if (obc &amp;&amp; obc-&gt;obs.exists &amp;&amp; obc-&gt;obs.oi.has_manifest()) { switch (obc-&gt;obs.oi.manifest.type) { // 如果为 redirect 类型，获取对应的重定向 Target case object_manifest_t::TYPE_REDIRECT: oloc = object_locator_t(obc-&gt;obs.oi.manifest.redirect_target); // 获取重定向 target 对应的信息 soid = obc-&gt;obs.oi.manifest.redirect_target; break; default: ceph_abort_msg(&quot;unrecognized manifest type&quot;); } } else { // 不包含 manifest /* proxy */ soid = m-&gt;get_hobj(); oloc = object_locator_t(m-&gt;get_object_locator()); oloc.pool = pool.info.tier_of; } unsigned flags = CEPH_OSD_FLAG_IGNORE_CACHE | CEPH_OSD_FLAG_IGNORE_OVERLAY; // pass through some original flags that make sense. // - leave out redirection and balancing flags since we are // already proxying through the primary // - leave off read/write/exec flags that are derived from the op flags |= m-&gt;get_flags() &amp; (CEPH_OSD_FLAG_RWORDERED | CEPH_OSD_FLAG_ORDERSNAP | CEPH_OSD_FLAG_ENFORCE_SNAPC | CEPH_OSD_FLAG_MAP_SNAP_CLONE); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; Start proxy read for &quot; &lt;&lt; *m &lt;&lt; dendl; ProxyReadOpRef prdop(std::make_shared&lt;ProxyReadOp&gt;(op, soid, m-&gt;ops)); ObjectOperation obj_op; obj_op.dup(prdop-&gt;ops); // 判断 Cache Mode 和 缓存是否已满 if (pool.info.cache_mode == pg_pool_t::CACHEMODE_WRITEBACK &amp;&amp; (agent_state &amp;&amp; agent_state-&gt;evict_mode != TierAgentState::EVICT_MODE_FULL)) { for (unsigned i = 0; i &lt; obj_op.ops.size(); i++) { ceph_osd_op op = obj_op.ops[i].op; switch (op.op) { case CEPH_OSD_OP_READ: case CEPH_OSD_OP_SYNC_READ: case CEPH_OSD_OP_SPARSE_READ: case CEPH_OSD_OP_CHECKSUM: case CEPH_OSD_OP_CMPEXT: op.flags = (op.flags | CEPH_OSD_OP_FLAG_FADVISE_SEQUENTIAL) &amp; ~(CEPH_OSD_OP_FLAG_FADVISE_DONTNEED | CEPH_OSD_OP_FLAG_FADVISE_NOCACHE); } } } C_ProxyRead *fin = new C_ProxyRead(this, soid, get_last_peering_reset(), prdop); unsigned n = info.pgid.hash_to_shard(osd-&gt;m_objecter_finishers); // 调用 objecter read 方法读取对象数据 ceph_tid_t tid = osd-&gt;objecter-&gt;read( soid.oid, oloc, obj_op, m-&gt;get_snapid(), NULL, flags, new C_OnFinisher(fin, osd-&gt;objecter_finishers[n]), &amp;prdop-&gt;user_version, &amp;prdop-&gt;data_offset, m-&gt;get_features()); fin-&gt;tid = tid; prdop-&gt;objecter_tid = tid; proxyread_ops[tid] = prdop; in_progress_proxy_ops[soid].push_back(op); } do_proxy_write：类似于 do_proxy_read void PrimaryLogPG::do_proxy_write(OpRequestRef op, ObjectContextRef obc) { // NOTE: non-const because ProxyWriteOp takes a mutable ref MOSDOp *m = static_cast&lt;MOSDOp*&gt;(op-&gt;get_nonconst_req()); object_locator_t oloc; SnapContext snapc(m-&gt;get_snap_seq(), m-&gt;get_snaps()); hobject_t soid; /* extensible tier */ if (obc &amp;&amp; obc-&gt;obs.exists &amp;&amp; obc-&gt;obs.oi.has_manifest()) { switch (obc-&gt;obs.oi.manifest.type) { case object_manifest_t::TYPE_REDIRECT: oloc = object_locator_t(obc-&gt;obs.oi.manifest.redirect_target); soid = obc-&gt;obs.oi.manifest.redirect_target; break; default: ceph_abort_msg(&quot;unrecognized manifest type&quot;); } } else { /* proxy */ soid = m-&gt;get_hobj(); oloc = object_locator_t(m-&gt;get_object_locator()); oloc.pool = pool.info.tier_of; } unsigned flags = CEPH_OSD_FLAG_IGNORE_CACHE | CEPH_OSD_FLAG_IGNORE_OVERLAY; if (!(op-&gt;may_write() || op-&gt;may_cache())) { flags |= CEPH_OSD_FLAG_RWORDERED; } if (op-&gt;allows_returnvec()) { flags |= CEPH_OSD_FLAG_RETURNVEC; } dout(10) &lt;&lt; __func__ &lt;&lt; &quot; Start proxy write for &quot; &lt;&lt; *m &lt;&lt; dendl; ProxyWriteOpRef pwop(std::make_shared&lt;ProxyWriteOp&gt;(op, soid, m-&gt;ops, m-&gt;get_reqid())); pwop-&gt;ctx = new OpContext(op, m-&gt;get_reqid(), &amp;pwop-&gt;ops, this); pwop-&gt;mtime = m-&gt;get_mtime(); ObjectOperation obj_op; obj_op.dup(pwop-&gt;ops); C_ProxyWrite_Commit *fin = new C_ProxyWrite_Commit( this, soid, get_last_peering_reset(), pwop); unsigned n = info.pgid.hash_to_shard(osd-&gt;m_objecter_finishers); ceph_tid_t tid = osd-&gt;objecter-&gt;mutate( soid.oid, oloc, obj_op, snapc, ceph::real_clock::from_ceph_timespec(pwop-&gt;mtime), flags, new C_OnFinisher(fin, osd-&gt;objecter_finishers[n]), &amp;pwop-&gt;user_version, pwop-&gt;reqid); fin-&gt;tid = tid; pwop-&gt;objecter_tid = tid; proxywrite_ops[tid] = pwop; in_progress_proxy_ops[soid].push_back(op); } promote_object：当客户端发送请求到cache pool中，但是cache pool未命中，cache pool会选择将该object从base pool中提升到cache pool中，然后在cache pool进行读写操作，操作完成后告知客户端请求完成，在cache pool会缓存该object，下次直接在cache中处理，和proxy_read存在的区别。 void PrimaryLogPG::promote_object(ObjectContextRef obc, const hobject_t&amp; missing_oid, const object_locator_t&amp; oloc, OpRequestRef op, ObjectContextRef *promote_obc) { hobject_t hoid = obc ? obc-&gt;obs.oi.soid : missing_oid; ceph_assert(hoid != hobject_t()); // 等待 Scrub 操作完成 if (write_blocked_by_scrub(hoid)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; hoid &lt;&lt; &quot; blocked by scrub&quot; &lt;&lt; dendl; if (op) { waiting_for_scrub.push_back(op); op-&gt;mark_delayed(&quot;waiting for scrub&quot;); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; hoid &lt;&lt; &quot; placing op in waiting_for_scrub&quot; &lt;&lt; dendl; } else { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; hoid &lt;&lt; &quot; no op, dropping on the floor&quot; &lt;&lt; dendl; } return; } if (op &amp;&amp; !check_laggy_requeue(op)) { return; } // Context为空创建一个新的Context if (!obc) { // we need to create an ObjectContext ceph_assert(missing_oid != hobject_t()); obc = get_object_context(missing_oid, true); } if (promote_obc) *promote_obc = obc; /* * Before promote complete, if there are proxy-reads for the object, * for this case we don't use DONTNEED. */ unsigned src_fadvise_flags = LIBRADOS_OP_FLAG_FADVISE_SEQUENTIAL; // 获取该对象对应的 proxy_read 等待队列的遍历器 map&lt;hobject_t, list&lt;OpRequestRef&gt;&gt;::iterator q = in_progress_proxy_ops.find(obc-&gt;obs.oi.soid); if (q == in_progress_proxy_ops.end()) { src_fadvise_flags |= LIBRADOS_OP_FLAG_FADVISE_DONTNEED; } // 构造 PromoteCallback CopyCallback *cb; object_locator_t my_oloc; hobject_t src_hoid; // 判断是否有 manifest if (!obc-&gt;obs.oi.has_manifest()) { my_oloc = oloc; my_oloc.pool = pool.info.tier_of; src_hoid = obc-&gt;obs.oi.soid; cb = new PromoteCallback(obc, this); } else { // 有manifest，判断类型是否为 chunk_data if (obc-&gt;obs.oi.manifest.is_chunked()) { src_hoid = obc-&gt;obs.oi.soid; cb = new PromoteManifestCallback(obc, this); } else if (obc-&gt;obs.oi.manifest.is_redirect()) { // mainfest 类型为 redirect object_locator_t src_oloc(obc-&gt;obs.oi.manifest.redirect_target); my_oloc = src_oloc; src_hoid = obc-&gt;obs.oi.manifest.redirect_target; cb = new PromoteCallback(obc, this); } else { ceph_abort_msg(&quot;unrecognized manifest type&quot;); } } unsigned flags = CEPH_OSD_COPY_FROM_FLAG_IGNORE_OVERLAY | CEPH_OSD_COPY_FROM_FLAG_IGNORE_CACHE | CEPH_OSD_COPY_FROM_FLAG_MAP_SNAP_CLONE | CEPH_OSD_COPY_FROM_FLAG_RWORDERED; // 复制对象数据 start_copy(cb, obc, src_hoid, my_oloc, 0, flags, obc-&gt;obs.oi.soid.snap == CEPH_NOSNAP, src_fadvise_flags, 0); ceph_assert(obc-&gt;is_blocked()); if (op) wait_for_blocked_object(obc-&gt;obs.oi.soid, op); recovery_state.update_stats( [](auto &amp;history, auto &amp;stats) { stats.stats.sum.num_promote++; return false; }); } void PrimaryLogPG::start_copy(CopyCallback *cb, ObjectContextRef obc, hobject_t src, object_locator_t oloc, version_t version, unsigned flags, bool mirror_snapset, unsigned src_obj_fadvise_flags, unsigned dest_obj_fadvise_flags) { const hobject_t&amp; dest = obc-&gt;obs.oi.soid; dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; dest &lt;&lt; &quot; from &quot; &lt;&lt; src &lt;&lt; &quot; &quot; &lt;&lt; oloc &lt;&lt; &quot; v&quot; &lt;&lt; version &lt;&lt; &quot; flags &quot; &lt;&lt; flags &lt;&lt; (mirror_snapset ? &quot; mirror_snapset&quot; : &quot;&quot;) &lt;&lt; dendl; ceph_assert(!mirror_snapset || src.snap == CEPH_NOSNAP); // cancel a previous in-progress copy? if (copy_ops.count(dest)) { // FIXME: if the src etc match, we could avoid restarting from the // beginning. CopyOpRef cop = copy_ops[dest]; vector&lt;ceph_tid_t&gt; tids; cancel_copy(cop, false, &amp;tids); osd-&gt;objecter-&gt;op_cancel(tids, -ECANCELED); } // 封装 cop 对象 CopyOpRef cop(std::make_shared&lt;CopyOp&gt;(cb, obc, src, oloc, version, flags, mirror_snapset, src_obj_fadvise_flags, dest_obj_fadvise_flags)); copy_ops[dest] = cop; obc-&gt;start_block(); if (!obc-&gt;obs.oi.has_manifest()) { // 执行实际的 copy 操作 _copy_some(obc, cop); } else { if (obc-&gt;obs.oi.manifest.is_redirect()) { _copy_some(obc, cop); } else if (obc-&gt;obs.oi.manifest.is_chunked()) { auto p = obc-&gt;obs.oi.manifest.chunk_map.begin(); _copy_some_manifest(obc, cop, p-&gt;first); } else { ceph_abort_msg(&quot;unrecognized manifest type&quot;); } } } void PrimaryLogPG::_copy_some(ObjectContextRef obc, CopyOpRef cop) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; *obc &lt;&lt; &quot; &quot; &lt;&lt; cop &lt;&lt; dendl; unsigned flags = 0; if (cop-&gt;flags &amp; CEPH_OSD_COPY_FROM_FLAG_FLUSH) flags |= CEPH_OSD_FLAG_FLUSH; if (cop-&gt;flags &amp; CEPH_OSD_COPY_FROM_FLAG_IGNORE_CACHE) flags |= CEPH_OSD_FLAG_IGNORE_CACHE; if (cop-&gt;flags &amp; CEPH_OSD_COPY_FROM_FLAG_IGNORE_OVERLAY) flags |= CEPH_OSD_FLAG_IGNORE_OVERLAY; if (cop-&gt;flags &amp; CEPH_OSD_COPY_FROM_FLAG_MAP_SNAP_CLONE) flags |= CEPH_OSD_FLAG_MAP_SNAP_CLONE; if (cop-&gt;flags &amp; CEPH_OSD_COPY_FROM_FLAG_RWORDERED) flags |= CEPH_OSD_FLAG_RWORDERED; C_GatherBuilder gather(cct); if (cop-&gt;cursor.is_initial() &amp;&amp; cop-&gt;mirror_snapset) { // list snaps too. ceph_assert(cop-&gt;src.snap == CEPH_NOSNAP); ObjectOperation op; op.list_snaps(&amp;cop-&gt;results.snapset, NULL); ceph_tid_t tid = osd-&gt;objecter-&gt;read(cop-&gt;src.oid, cop-&gt;oloc, op, CEPH_SNAPDIR, NULL, flags, gather.new_sub(), NULL); cop-&gt;objecter_tid2 = tid; } ObjectOperation op; if (cop-&gt;results.user_version) { op.assert_version(cop-&gt;results.user_version); } else { // we should learn the version after the first chunk, if we didn't know // it already! ceph_assert(cop-&gt;cursor.is_initial()); } op.copy_get(&amp;cop-&gt;cursor, get_copy_chunk_size(), &amp;cop-&gt;results.object_size, &amp;cop-&gt;results.mtime, &amp;cop-&gt;attrs, &amp;cop-&gt;data, &amp;cop-&gt;omap_header, &amp;cop-&gt;omap_data, &amp;cop-&gt;results.snaps, &amp;cop-&gt;results.snap_seq, &amp;cop-&gt;results.flags, &amp;cop-&gt;results.source_data_digest, &amp;cop-&gt;results.source_omap_digest, &amp;cop-&gt;results.reqids, &amp;cop-&gt;results.reqid_return_codes, &amp;cop-&gt;results.truncate_seq, &amp;cop-&gt;results.truncate_size, &amp;cop-&gt;rval); op.set_last_op_flags(cop-&gt;src_obj_fadvise_flags); C_Copyfrom *fin = new C_Copyfrom(this, obc-&gt;obs.oi.soid, get_last_peering_reset(), cop); unsigned n = info.pgid.hash_to_shard(osd-&gt;m_objecter_finishers); gather.set_finisher(new C_OnFinisher(fin, osd-&gt;objecter_finishers[n])); // 调用 objecter-&gt;read方法进行读取 ceph_tid_t tid = osd-&gt;objecter-&gt;read(cop-&gt;src.oid, cop-&gt;oloc, op, cop-&gt;src.snap, NULL, flags, gather.new_sub(), // discover the object version if we don't know it yet cop-&gt;results.user_version ? NULL : &amp;cop-&gt;results.user_version); fin-&gt;tid = tid; cop-&gt;objecter_tid = tid; gather.activate(); } 无论是 Proxy Read 还是 Promote Object 操作最终都是调用了 objecter 的 read 方法来从base storage层读取对象数据 Cache flush &amp; evict flush cache pool 空间不够时，需要选择一些对象回刷到数据层 evict 将一些 clean 对象从缓存层中剔除。以释放更多的缓存空间 Data Structure OSDServices ：定义了 AgentThread 线程，用于完成 flush 和 evict 操作 class OSDService { .... // -- agent shared state -- // agent 线程锁，保护下面所有数据结构 ceph::mutex agent_lock = ceph::make_mutex(&quot;OSDService::agent_lock&quot;); // 线程相应的条件变量 ceph::condition_variable agent_cond; // 所有淘汰或者回刷所需的 PG 集合，根据 PG 集合的优先级，保存在不同的 map 中 map&lt;uint64_t, set&lt;PGRef&gt; &gt; agent_queue; // 当前在扫描的 PG 集合的一个位置，只有 agent_valid_iterator 为 true 时，这个指针才有效，否则从集合的起始处开始扫描 set&lt;PGRef&gt;::iterator agent_queue_pos; bool agent_valid_iterator; // 所有正在进行的回刷和淘汰操作 int agent_ops; // once have one pg with FLUSH_MODE_HIGH then flush objects with high speed int flush_mode_high_count; // 所有正在进行的 agent 操作（回刷或者淘汰）的对象 set&lt;hobject_t&gt; agent_oids; // agent 是否有效 bool agent_active; // agent 线程 struct AgentThread : public Thread { OSDService *osd; explicit AgentThread(OSDService *o) : osd(o) {} void *entry() override { osd-&gt;agent_entry(); return NULL; } } agent_thread; // agent 停止的标志 bool agent_stop_flag; ceph::mutex agent_timer_lock = ceph::make_mutex(&quot;OSDService::agent_timer_lock&quot;); // agent 相关定时器：当扫描一个 PG 对象时，该对象既没有剔除操作，也没有回刷操作，就停止 PG 的扫描，把该 PG 加入到定时器中，5S 后继续 SafeTimer agent_timer; } TierAgentState：用于保存 PG 相关的 agent 信息 flush/evict 执行入口 agent_entry：agent_entry 是 agent_thread 的入口函数，它在后台完成 flush 操作和 evict 操作 void OSDService::agent_entry() { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; start&quot; &lt;&lt; dendl; // 加锁，保护相关字段 std::unique_lock agent_locker{agent_lock}; while (!agent_stop_flag) { if (agent_queue.empty()) { // 扫描 agent_queue 队列，如果为空则在 agent_cond 上等待 dout(20) &lt;&lt; __func__ &lt;&lt; &quot; empty queue&quot; &lt;&lt; dendl; agent_cond.wait(agent_locker); continue; } uint64_t level = agent_queue.rbegin()-&gt;first; // 从队列中取出优先级最高的 PG 的集合 top set&lt;PGRef&gt;&amp; top = agent_queue.rbegin()-&gt;second; dout(10) &lt;&lt; __func__ &lt;&lt; &quot; tiers &quot; &lt;&lt; agent_queue.size() &lt;&lt; &quot;, top is &quot; &lt;&lt; level &lt;&lt; &quot; with pgs &quot; &lt;&lt; top.size() &lt;&lt; &quot;, ops &quot; &lt;&lt; agent_ops &lt;&lt; &quot;/&quot; &lt;&lt; cct-&gt;_conf-&gt;osd_agent_max_ops &lt;&lt; (agent_active ? &quot; active&quot; : &quot; NOT ACTIVE&quot;) &lt;&lt; dendl; dout(20) &lt;&lt; __func__ &lt;&lt; &quot; oids &quot; &lt;&lt; agent_oids &lt;&lt; dendl; // 获取 agent 操作的最大数目 max 值和 agent_flush_quota int max = cct-&gt;_conf-&gt;osd_agent_max_ops - agent_ops; int agent_flush_quota = max; if (!flush_mode_high_count) agent_flush_quota = cct-&gt;_conf-&gt;osd_agent_max_low_ops - agent_ops; if (agent_flush_quota &lt;= 0 || top.empty() || !agent_active) { agent_cond.wait(agent_locker); continue; } // 迭代器获取 PG if (!agent_valid_iterator || agent_queue_pos == top.end()) { agent_queue_pos = top.begin(); agent_valid_iterator = true; } PGRef pg = *agent_queue_pos; dout(10) &lt;&lt; &quot;high_count &quot; &lt;&lt; flush_mode_high_count &lt;&lt; &quot; agent_ops &quot; &lt;&lt; agent_ops &lt;&lt; &quot; flush_quota &quot; &lt;&lt; agent_flush_quota &lt;&lt; dendl; agent_locker.unlock(); // 调用 pg-&gt;agent_work()，正常返回 true，若返回 false，则处于 delay，需要加入定时器 if (!pg-&gt;agent_work(max, agent_flush_quota)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; pg-&gt;pg_id &lt;&lt; &quot; no agent_work, delay for &quot; &lt;&lt; cct-&gt;_conf-&gt;osd_agent_delay_time &lt;&lt; &quot; seconds&quot; &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_tier_delay); // Queue a timer to call agent_choose_mode for this pg in 5 seconds std::lock_guard timer_locker{agent_timer_lock}; Context *cb = new AgentTimeoutCB(pg); agent_timer.add_event_after(cct-&gt;_conf-&gt;osd_agent_delay_time, cb); } agent_locker.lock(); } dout(10) &lt;&lt; __func__ &lt;&lt; &quot; finish&quot; &lt;&lt; dendl; } agent_work：完成一个 PG 内的 evict 操作和 flush 操作 bool PrimaryLogPG::agent_work(int start_max, int agent_flush_quota) { // 加锁 std::scoped_lock locker{*this}; if (!agent_state) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; no agent state, stopping&quot; &lt;&lt; dendl; return true; } ceph_assert(!recovery_state.is_deleting()); if (agent_state-&gt;is_idle()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; idle, stopping&quot; &lt;&lt; dendl; return true; } osd-&gt;logger-&gt;inc(l_osd_agent_wake); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; max &quot; &lt;&lt; start_max &lt;&lt; &quot;, flush &quot; &lt;&lt; agent_state-&gt;get_flush_mode_name() &lt;&lt; &quot;, evict &quot; &lt;&lt; agent_state-&gt;get_evict_mode_name() &lt;&lt; &quot;, pos &quot; &lt;&lt; agent_state-&gt;position &lt;&lt; dendl; ceph_assert(is_primary()); ceph_assert(is_active()); // 加载 hit_set 历史对象到内存 agent_load_hit_sets(); const pg_pool_t *base_pool = get_osdmap()-&gt;get_pg_pool(pool.info.tier_of); ceph_assert(base_pool); int ls_min = 1; int ls_max = cct-&gt;_conf-&gt;osd_pool_default_cache_max_evict_check_size; // list some objects. this conveniently lists clones (oldest to // newest) before heads... the same order we want to flush in. // // NOTE: do not flush the Sequencer. we will assume that the // listing we get back is imprecise. vector&lt;hobject_t&gt; ls; hobject_t next; // 扫描本 PG 的对象，从 agent_state-&gt;position 开始扫描，结果保存在 ls 中 int r = pgbackend-&gt;objects_list_partial(agent_state-&gt;position, ls_min, ls_max, &amp;ls, &amp;next); ceph_assert(r &gt;= 0); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; got &quot; &lt;&lt; ls.size() &lt;&lt; &quot; objects&quot; &lt;&lt; dendl; int started = 0; // 对扫描的 ls 对象做相应的检查 for (vector&lt;hobject_t&gt;::iterator p = ls.begin(); p != ls.end(); ++p) { // 跳过 hitset if (p-&gt;nspace == cct-&gt;_conf-&gt;osd_hit_set_namespace) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (hit set) &quot; &lt;&lt; *p &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过 degraded 对象 if (is_degraded_or_backfilling_object(*p)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (degraded) &quot; &lt;&lt; *p &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过 missing 对象 if (is_missing_object(p-&gt;get_head())) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (missing head) &quot; &lt;&lt; *p &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过 object_context 不存在的对象 ObjectContextRef obc = get_object_context(*p, false, NULL); if (!obc) { // we didn't flush; we may miss something here. dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (no obc) &quot; &lt;&lt; *p &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过对象的 obs if (!obc-&gt;obs.exists) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (dne) &quot; &lt;&lt; obc-&gt;obs.oi.soid &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过正在进行 scrub 操作的对象 if (range_intersects_scrub(obc-&gt;obs.oi.soid, obc-&gt;obs.oi.soid.get_head())) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (scrubbing) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过已经被阻塞的对象 if (obc-&gt;is_blocked()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (blocked) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 跳过有正在读写请求的对象 if (obc-&gt;is_request_pending()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (request pending) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // 如果不支持 omap，跳过有 omap 的对象 // be careful flushing omap to an EC pool. if (!base_pool-&gt;supports_omap() &amp;&amp; obc-&gt;obs.oi.is_omap()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (omap to EC) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); continue; } // agent_maybe_evict 完成对象的 evict 操作 if (agent_state-&gt;evict_mode != TierAgentState::EVICT_MODE_IDLE &amp;&amp; agent_maybe_evict(obc, false)) ++started; // agent_maybe_flush 完成一个对象的 flush 操作 else if (agent_state-&gt;flush_mode != TierAgentState::FLUSH_MODE_IDLE &amp;&amp; agent_flush_quota &gt; 0 &amp;&amp; agent_maybe_flush(obc)) { ++started; --agent_flush_quota; } if (started &gt;= start_max) { // If finishing early, set &quot;next&quot; to the next object if (++p != ls.end()) next = *p; break; } } if (++agent_state-&gt;hist_age &gt; cct-&gt;_conf-&gt;osd_agent_hist_halflife) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; resetting atime and temp histograms&quot; &lt;&lt; dendl; agent_state-&gt;hist_age = 0; agent_state-&gt;temp_hist.decay(); } // Total objects operated on so far int total_started = agent_state-&gt;started + started; bool need_delay = false; dout(20) &lt;&lt; __func__ &lt;&lt; &quot; start pos &quot; &lt;&lt; agent_state-&gt;position &lt;&lt; &quot; next start pos &quot; &lt;&lt; next &lt;&lt; &quot; started &quot; &lt;&lt; total_started &lt;&lt; dendl; // See if we've made a full pass over the object hash space // This might check at most ls_max objects a second time to notice that // we've checked every objects at least once. if (agent_state-&gt;position &lt; agent_state-&gt;start &amp;&amp; next &gt;= agent_state-&gt;start) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; wrap around &quot; &lt;&lt; agent_state-&gt;start &lt;&lt; dendl; if (total_started == 0) need_delay = true; else total_started = 0; agent_state-&gt;start = next; } agent_state-&gt;started = total_started; // See if we are starting from beginning if (next.is_max()) agent_state-&gt;position = hobject_t(); else agent_state-&gt;position = next; // Discard old in memory HitSets hit_set_in_memory_trim(pool.info.hit_set_count); if (need_delay) { ceph_assert(agent_state-&gt;delaying == false); agent_delay(); return false; } // 重新计算 agent 的 evict 和 flush 值 agent_choose_mode(); return true; } 真正执行操作的方法 evict：agent_maybe_evict bool PrimaryLogPG::agent_maybe_evict(ObjectContextRef&amp; obc, bool after_flush) { const hobject_t&amp; soid = obc-&gt;obs.oi.soid; // 检查对象的状态 if (!after_flush &amp;&amp; obc-&gt;obs.oi.is_dirty()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (dirty) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } // This is already checked by agent_work() which passes after_flush = false if (after_flush &amp;&amp; range_intersects_scrub(soid, soid.get_head())) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (scrubbing) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } if (!obc-&gt;obs.oi.watchers.empty()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (watchers) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } if (obc-&gt;is_blocked()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (blocked) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } if (obc-&gt;obs.oi.is_cache_pinned()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (cache_pinned) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } if (soid.snap == CEPH_NOSNAP) { int result = _verify_no_head_clones(soid, obc-&gt;ssc-&gt;snapset); if (result &lt; 0) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (clones) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } } // 检查 evict 模式是否为 EVICT_MODE_SOME 模式 if (agent_state-&gt;evict_mode != TierAgentState::EVICT_MODE_FULL) { // 检查 clean 的时间是否大于设置的最小淘汰时间 // is this object old than cache_min_evict_age? utime_t now = ceph_clock_now(); utime_t ob_local_mtime; if (obc-&gt;obs.oi.local_mtime != utime_t()) { ob_local_mtime = obc-&gt;obs.oi.local_mtime; } else { ob_local_mtime = obc-&gt;obs.oi.mtime; } if (ob_local_mtime + utime_t(pool.info.cache_min_evict_age, 0) &gt; now) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (too young) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } // 计算对象的热度值 // is this object old and/or cold enough? int temp = 0; uint64_t temp_upper = 0, temp_lower = 0; if (hit_set) agent_estimate_temp(soid, &amp;temp); agent_state-&gt;temp_hist.add(temp); agent_state-&gt;temp_hist.get_position_micro(temp, &amp;temp_lower, &amp;temp_upper); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; temp &quot; &lt;&lt; temp &lt;&lt; &quot; pos &quot; &lt;&lt; temp_lower &lt;&lt; &quot;-&quot; &lt;&lt; temp_upper &lt;&lt; &quot;, evict_effort &quot; &lt;&lt; agent_state-&gt;evict_effort &lt;&lt; dendl; dout(30) &lt;&lt; &quot;agent_state:\\n&quot;; Formatter *f = Formatter::create(&quot;&quot;); f-&gt;open_object_section(&quot;agent_state&quot;); agent_state-&gt;dump(f); f-&gt;close_section(); f-&gt;flush(*_dout); delete f; *_dout &lt;&lt; dendl; if (1000000 - temp_upper &gt;= agent_state-&gt;evict_effort) return false; } // evict_mode 为 FULL 模式，调用函数 _delete_oid 删除该对象 dout(10) &lt;&lt; __func__ &lt;&lt; &quot; evicting &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; OpContextUPtr ctx = simple_opc_create(obc); auto null_op_req = OpRequestRef(); if (!ctx-&gt;lock_manager.get_lock_type( ObjectContext::RWState::RWWRITE, obc-&gt;obs.oi.soid, obc, null_op_req)) { close_op_ctx(ctx.release()); dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (cannot get lock) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; return false; } osd-&gt;agent_start_evict_op(); ctx-&gt;register_on_finish( [this]() { osd-&gt;agent_finish_evict_op(); }); ctx-&gt;at_version = get_next_version(); ceph_assert(ctx-&gt;new_obs.exists); // 删除该对象 int r = _delete_oid(ctx.get(), true, false); if (obc-&gt;obs.oi.is_omap()) ctx-&gt;delta_stats.num_objects_omap--; ctx-&gt;delta_stats.num_evict++; ctx-&gt;delta_stats.num_evict_kb += shift_round_up(obc-&gt;obs.oi.size, 10); if (obc-&gt;obs.oi.is_dirty()) --ctx-&gt;delta_stats.num_objects_dirty; ceph_assert(r == 0); finish_ctx(ctx.get(), pg_log_entry_t::DELETE); // 发起实际的删除请求 simple_opc_submit(std::move(ctx)); osd-&gt;logger-&gt;inc(l_osd_tier_evict); osd-&gt;logger-&gt;inc(l_osd_agent_evict); return true; } flush：该方法完成一个对象的 flush 操作（非最底层的实现） bool PrimaryLogPG::agent_maybe_flush(ObjectContextRef&amp; obc) { // 检查对象是否为脏数据 if (!obc-&gt;obs.oi.is_dirty()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (clean) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } // 检查对象是否为 cache_pinned 状态 if (obc-&gt;obs.oi.is_cache_pinned()) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (cache_pinned) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } // 统计时间 utime_t now = ceph_clock_now(); utime_t ob_local_mtime; if (obc-&gt;obs.oi.local_mtime != utime_t()) { ob_local_mtime = obc-&gt;obs.oi.local_mtime; } else { ob_local_mtime = obc-&gt;obs.oi.mtime; } // 判断当前 evict 状态是否为 full bool evict_mode_full = (agent_state-&gt;evict_mode == TierAgentState::EVICT_MODE_FULL); // 未满则检查该对象作为脏数据的时间，和最短刷回时间进行对比 if (!evict_mode_full &amp;&amp; obc-&gt;obs.oi.soid.snap == CEPH_NOSNAP &amp;&amp; // snaps immutable; don't delay (ob_local_mtime + utime_t(pool.info.cache_min_flush_age, 0) &gt; now)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (too young) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } // 检查对象是否处于 activate 状态 if (osd-&gt;agent_is_active_oid(obc-&gt;obs.oi.soid)) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; skip (flushing) &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } dout(10) &lt;&lt; __func__ &lt;&lt; &quot; flushing &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; dendl; // FIXME: flush anything dirty, regardless of what distribution of // ages we expect. hobject_t oid = obc-&gt;obs.oi.soid; osd-&gt;agent_start_op(oid); // no need to capture a pg ref, can't outlive fop or ctx std::function&lt;void()&gt; on_flush = [this, oid]() { osd-&gt;agent_finish_op(oid); }; // 调用函数 start_flush 完成对象的刷回操作 int result = start_flush( OpRequestRef(), obc, false, NULL, on_flush); if (result != -EINPROGRESS) { on_flush(); dout(10) &lt;&lt; __func__ &lt;&lt; &quot; start_flush() failed &quot; &lt;&lt; obc-&gt;obs.oi &lt;&lt; &quot; with &quot; &lt;&lt; result &lt;&lt; dendl; osd-&gt;logger-&gt;inc(l_osd_agent_skip); return false; } osd-&gt;logger-&gt;inc(l_osd_agent_flush); return true; } start_flush：该函数完成实际的 flush 操作 int PrimaryLogPG::start_flush( OpRequestRef op, ObjectContextRef obc, bool blocking, hobject_t *pmissing, std::optional&lt;std::function&lt;void()&gt;&gt; &amp;&amp;on_flush) { const object_info_t&amp; oi = obc-&gt;obs.oi; const hobject_t&amp; soid = oi.soid; dout(10) &lt;&lt; __func__ &lt;&lt; &quot; &quot; &lt;&lt; soid &lt;&lt; &quot; v&quot; &lt;&lt; oi.version &lt;&lt; &quot; uv&quot; &lt;&lt; oi.user_version &lt;&lt; &quot; &quot; &lt;&lt; (blocking ? &quot;blocking&quot; : &quot;non-blocking/best-effort&quot;) &lt;&lt; dendl; bool preoctopus_compat = get_osdmap()-&gt;require_osd_release &lt; ceph_release_t::octopus; SnapSet snapset; if (preoctopus_compat) { // 过滤掉已经删除的 snap 对象 // for pre-octopus compatibility, filter SnapSet::snaps. not // certain we need this, but let's be conservative. snapset = obc-&gt;ssc-&gt;snapset.get_filtered(pool.info); } else { // NOTE: change this to a const ref when we remove this compat code snapset = obc-&gt;ssc-&gt;snapset; } // 检查比当前 clone 对象更早版本的克隆对象 // verify there are no (older) check for dirty clones { dout(20) &lt;&lt; &quot; snapset &quot; &lt;&lt; snapset &lt;&lt; dendl; vector&lt;snapid_t&gt;::reverse_iterator p = snapset.clones.rbegin(); while (p != snapset.clones.rend() &amp;&amp; *p &gt;= soid.snap) ++p; if (p != snapset.clones.rend()) { hobject_t next = soid; next.snap = *p; ceph_assert(next.snap &lt; soid.snap); if (recovery_state.get_pg_log().get_missing().is_missing(next)) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; missing clone is &quot; &lt;&lt; next &lt;&lt; dendl; if (pmissing) *pmissing = next; return -ENOENT; } ObjectContextRef older_obc = get_object_context(next, false); if (older_obc) { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; next oldest clone is &quot; &lt;&lt; older_obc-&gt;obs.oi &lt;&lt; dendl; if (older_obc-&gt;obs.oi.is_dirty()) { dout(10) &lt;&lt; __func__ &lt;&lt; &quot; next oldest clone is dirty: &quot; &lt;&lt; older_obc-&gt;obs.oi &lt;&lt; dendl; return -EBUSY; } } else { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; next oldest clone &quot; &lt;&lt; next &lt;&lt; &quot; is not present; implicitly clean&quot; &lt;&lt; dendl; } } else { dout(20) &lt;&lt; __func__ &lt;&lt; &quot; no older clones&quot; &lt;&lt; dendl; } } // 设置对应的对象为 blocked 状态 if (blocking) obc-&gt;start_block(); // 检查该对象是否在 flush_ops 中，也就是该对象是否已经在 flush map&lt;hobject_t,FlushOpRef&gt;::iterator p = flush_ops.find(soid); if (p != flush_ops.end()) { FlushOpRef fop = p-&gt;second; if (fop-&gt;op == op) { // we couldn't take the write lock on a cache-try-flush before; // now we are trying again for the lock. return try_flush_mark_clean(fop); } if (fop-&gt;flushed_version == obc-&gt;obs.oi.user_version &amp;&amp; (fop-&gt;blocking || !blocking)) { // nonblocking can join anything // blocking can only join a blocking flush dout(20) &lt;&lt; __func__ &lt;&lt; &quot; piggybacking on existing flush &quot; &lt;&lt; dendl; if (op) fop-&gt;dup_ops.push_back(op); return -EAGAIN; // clean up this ctx; op will retry later } // cancel current flush since it will fail anyway, or because we // are blocking and the existing flush is nonblocking. dout(20) &lt;&lt; __func__ &lt;&lt; &quot; canceling previous flush; it will fail&quot; &lt;&lt; dendl; if (fop-&gt;op) osd-&gt;reply_op_error(fop-&gt;op, -EBUSY); while (!fop-&gt;dup_ops.empty()) { osd-&gt;reply_op_error(fop-&gt;dup_ops.front(), -EBUSY); fop-&gt;dup_ops.pop_front(); } vector&lt;ceph_tid_t&gt; tids; cancel_flush(fop, false, &amp;tids); osd-&gt;objecter-&gt;op_cancel(tids, -ECANCELED); } if (obc-&gt;obs.oi.has_manifest() &amp;&amp; obc-&gt;obs.oi.manifest.is_chunked()) { // 执行对应的 flush 操作 int r = start_manifest_flush(op, obc, blocking, std::move(on_flush)); if (r != -EINPROGRESS) { if (blocking) obc-&gt;stop_block(); } return r; } start_manifest_flush：真正刷回数据之前的数据准备 int PrimaryLogPG::start_manifest_flush(OpRequestRef op, ObjectContextRef obc, bool blocking, std::optional&lt;std::function&lt;void()&gt;&gt; &amp;&amp;on_flush) { auto p = obc-&gt;obs.oi.manifest.chunk_map.begin(); FlushOpRef manifest_fop(std::make_shared&lt;FlushOp&gt;()); manifest_fop-&gt;op = op; manifest_fop-&gt;obc = obc; manifest_fop-&gt;flushed_version = obc-&gt;obs.oi.user_version; manifest_fop-&gt;blocking = blocking; manifest_fop-&gt;on_flush = std::move(on_flush); int r = do_manifest_flush(op, obc, manifest_fop, p-&gt;first, blocking); if (r &lt; 0) { return r; } flush_ops[obc-&gt;obs.oi.soid] = manifest_fop; return -EINPROGRESS; } do_manifest_flush：真正刷回数据的过程 int PrimaryLogPG::do_manifest_flush(OpRequestRef op, ObjectContextRef obc, FlushOpRef manifest_fop, uint64_t start_offset, bool block) { // 获取 manifest 和 实际的对象数据 struct object_manifest_t &amp;manifest = obc-&gt;obs.oi.manifest; hobject_t soid = obc-&gt;obs.oi.soid; ceph_tid_t tid; SnapContext snapc; uint64_t max_copy_size = 0, last_offset = 0; // 遍历 manifest 统计要复制刷回的数据大小 map&lt;uint64_t, chunk_info_t&gt;::iterator iter = manifest.chunk_map.find(start_offset); ceph_assert(iter != manifest.chunk_map.end()); for (;iter != manifest.chunk_map.end(); ++iter) { if (iter-&gt;second.is_dirty()) { last_offset = iter-&gt;first; max_copy_size += iter-&gt;second.length; } if (get_copy_chunk_size() &lt; max_copy_size) { break; } } iter = manifest.chunk_map.find(start_offset); for (;iter != manifest.chunk_map.end(); ++iter) { // 如果数据 clean 则跳过 if (!iter-&gt;second.is_dirty()) { continue; } uint64_t tgt_length = iter-&gt;second.length; uint64_t tgt_offset= iter-&gt;second.offset; hobject_t tgt_soid = iter-&gt;second.oid; object_locator_t oloc(tgt_soid); ObjectOperation obj_op; bufferlist chunk_data; // 先读取数据到 chunk_data 中 int r = pgbackend-&gt;objects_read_sync(soid, iter-&gt;first, tgt_length, 0, &amp;chunk_data); if (r &lt; 0) { dout(0) &lt;&lt; __func__ &lt;&lt; &quot; read fail &quot; &lt;&lt; &quot; offset: &quot; &lt;&lt; tgt_offset &lt;&lt; &quot; len: &quot; &lt;&lt; tgt_length &lt;&lt; &quot; r: &quot; &lt;&lt; r &lt;&lt; dendl; return r; } if (!chunk_data.length()) { return -ENODATA; } // 判断刷回的模式 unsigned flags = CEPH_OSD_FLAG_IGNORE_CACHE | CEPH_OSD_FLAG_IGNORE_OVERLAY | CEPH_OSD_FLAG_RWORDERED; tgt_length = chunk_data.length(); // 根据不同的存储池指纹信息，选择对应的摘要算法获取 chunk_data 对应的 hash 值 if (pg_pool_t::fingerprint_t fp_algo = pool.info.get_fingerprint_type(); iter-&gt;second.has_reference() &amp;&amp; fp_algo != pg_pool_t::TYPE_FINGERPRINT_NONE) { object_t fp_oid = [fp_algo, &amp;chunk_data]() -&gt; string { switch (fp_algo) { case pg_pool_t::TYPE_FINGERPRINT_SHA1: return crypto::digest&lt;crypto::SHA1&gt;(chunk_data).to_str(); case pg_pool_t::TYPE_FINGERPRINT_SHA256: return crypto::digest&lt;crypto::SHA256&gt;(chunk_data).to_str(); case pg_pool_t::TYPE_FINGERPRINT_SHA512: return crypto::digest&lt;crypto::SHA512&gt;(chunk_data).to_str(); default: assert(0 == &quot;unrecognized fingerprint type&quot;); return {}; }}(); bufferlist in; // 如果 oid 不一致 if (fp_oid != tgt_soid.oid) { // 减小旧块的引用计数 // decrement old chunk's reference count ObjectOperation dec_op; cls_chunk_refcount_put_op put_call; put_call.source = soid; ::encode(put_call, in); // 对 chunk 的计数执行修改 PUT 操作 dec_op.call(&quot;cas&quot;, &quot;chunk_put&quot;, in); // 执行 objecter_mutate 方法会将请求转化为 Op 请求，再进行请求的提交，写入到后端存储池 dec_op chunk_put // we don't care dec_op's completion. scrub for dedup will fix this. tid = osd-&gt;objecter-&gt;mutate( tgt_soid.oid, oloc, dec_op, snapc, ceph::real_clock::from_ceph_timespec(obc-&gt;obs.oi.mtime), flags, NULL); in.clear(); } tgt_soid.oid = fp_oid; iter-&gt;second.oid = tgt_soid; // 编码实际操作的关键数据（偏移量和数据长度） // add data op ceph_osd_op osd_op; osd_op.extent.offset = 0; osd_op.extent.length = chunk_data.length(); // 将数据编码 encode(osd_op, in); encode(soid, in); in.append(chunk_data); obj_op.call(&quot;cas&quot;, &quot;cas_write_or_get&quot;, in); } else { obj_op.add_data(CEPH_OSD_OP_WRITE, tgt_offset, tgt_length, chunk_data); } C_ManifestFlush *fin = new C_ManifestFlush(this, soid, get_last_peering_reset()); fin-&gt;offset = iter-&gt;first; fin-&gt;last_offset = last_offset; manifest_fop-&gt;chunks++; unsigned n = info.pgid.hash_to_shard(osd-&gt;m_objecter_finishers); // 封装写请求写入到后端存储池 obj_op cas_write_or_get tid = osd-&gt;objecter-&gt;mutate( tgt_soid.oid, oloc, obj_op, snapc, ceph::real_clock::from_ceph_timespec(obc-&gt;obs.oi.mtime), flags, new C_OnFinisher(fin, osd-&gt;objecter_finishers[n])); fin-&gt;tid = tid; manifest_fop-&gt;io_tids[iter-&gt;first] = tid; dout(20) &lt;&lt; __func__ &lt;&lt; &quot; offset: &quot; &lt;&lt; tgt_offset &lt;&lt; &quot; len: &quot; &lt;&lt; tgt_length &lt;&lt; &quot; oid: &quot; &lt;&lt; tgt_soid.oid &lt;&lt; &quot; ori oid: &quot; &lt;&lt; soid.oid.name &lt;&lt; &quot; tid: &quot; &lt;&lt; tid &lt;&lt; dendl; if (last_offset &lt; iter-&gt;first) { break; } } return 0; } 通过源码分析我们不难看出，flush 操作最终是以 Op 请求的方式传递到底层存储层的，也就意味着需要再执行一次 Ceph 存储池写数据的相关逻辑。 Command ceph osd tier add {data_pool} {cache pool} // Bind cache pool to storage pool ceph osd tier cache-mode {cache-pool} {cache-mode} // Set cache mode for cache pool ceph osd tier cache-mode {cache-pool} {cache-mode} // Set read tier/write tier according to the cache mode References [1] Ceph IO：CACHE TIERING [2] ceph的数据存储之路(12)----- cache tier 曾经有过的YY [3] Ceph 进阶系列（三）：谈谈 Ceph Cache Tier(Cache Pool) 的配置 、原理 和 源码分析 [4] CSDN：ceph的 cache tier实现分析 [5] CNBlogs：Ceph分层存储分析 [6] 柴少鹏：Ceph跟着官网学一波（四） [7] Ceph冷知识 | Cache Tier 的抉择与使用 [8] Ceph cache pool tiering: a scalable and distributed cache [9] 杉岩数据智能缓存技术AgileCache亮相2018 Ceph亚太峰会 [10] 在Ceph中用对Flashcache了吗？（一） [11] 在Ceph中用对Flashcache了吗？（二） [12] ceph的 cache tier实现分析 [13] CSDN：ceph的 cache tier实现分析 ","link":"https://blog.shunzi.tech/post/ceph-cache-tiering/"},{"title":"File Systems Unit as Distributed Storage:Lessons from 10 Years of Ceph Evolution","content":" 该篇文章原文是两年一届的 SOSP2019 《File Systems Unit as Distributed Storage:Lessons from 10 Years of Ceph Evolution》 该篇文章原文是两年一届的 SOSP2019 《File Systems Unit as Distributed Storage:Lessons from 10 Years of Ceph Evolution》 由于项目涉及到了 Ceph 相关，以及课程选择了分布式存储系统 Ceph 的后端存储引擎研究，故拜读了这篇 Paper 这篇讲主要总结 Ceph 后端存储引擎的发展历程，以及各种后端存储的优劣，得出如标题的结论 还将参考 MSST17 上的一篇文章 《Understanding Write Behaviors of Storage Backends in Ceph Object Store》 关于 Ceph 的其他组件和基础介绍，请参考 Ceph 相关博文，此处之总结后端存储引擎相关。 Abstract 过去十年，Ceph 同其他分布式存储系统一样，没能落俗，将本地文件系统作为分布式系统的后端存储，之所以文件存储能在分布式后端存储中大行其道，是因为文件系统的易用性以及技术和代码发展相对成熟。但从 Ceph 发展的经验来看，文件系统带来了极大的开销。 开发零开销的事务机制极具挑战性 元数据在本地级别的性能会显著提高!严重影响在分布式的层次的性能表现 支持新兴存储硬件的速度极其缓慢 Ceph 开发了新的基于裸存储设备的后端存储引擎 BlueStore 来解决上述问题，BlueStore 现在也已经成了绝大多数用户的使用方案。BlueStore 在用户态运行并完全控制 IO 栈，让元数据更节省空间，并引入了数据校验和，实现了编码数据的快速覆盖写和内联压缩，降低了性能的多变性，避免了本地文件系统的性能缺陷，同时对新兴存储器件提供了较好的支持。 Introduction 文件系统的优劣 文件系统作为分布式存储系统的后端存储引擎的优势： 能够将数据持久性和数据块的分配等复杂问题委托给久经历练且性能较好的文件系统处理； 文件系统提供了类 POSIX 的接口和抽象（文件和目录） 文件系统支持一些标准工具的使用（ls,find），较好地查找磁盘上的内容 文件系统作为后端存储引擎的局限： 文件系统带来的性能损失； 文件系统对于现如今多样化的存储器件缺乏支持 BlueStore 诞生的原因 基于现有的文件系统难以实现高效的事务机制 虽然已经有大量工作致力于向文件系统中引入事务的机制，但往往由于其高额的开销或者功能的局限性或者接口本身以及实现的复杂度，导致这些方法很难投入实际应用。 Ceph 采用了一种方式：利用文件系统有限的内部事务机制，在用户态实现 WAL；或者采用事务型的 KV 存储机制。但在性能上的表现都差强人意 本地文件系统的元数据性能可能严重影响分布式系统的整体性能 Ceph 面对的一个很大的挑战就是“如何快速地枚举文件夹中数百万项的内容，如何保证返回的结果有序”。 基于 Btrfs 和 XFS 的后端存储往往都会有这样的问题，同时用于分配元数据负载的目录分割操作与系统策略其实是有一定冲突的，整个系统的性能会受到元数据性能的影响。 新型存储器件向文件系统提出了挑战 文件系统日趋成熟带来的影响就是显得更加的保守和死板，不能较好地适配现在很多摒弃了块接口的新型存储器件。面向数据中心的新型存储器件往往都需要在原有应用程序接口层面做较大的修改。 诸如为了提升容量， HDD 正在向 SMR 过渡，同时支持 Zone Interface；为了减小 SSD 中由于 FTL 造成的 IO 尾延迟，引入了 Zoned Namespace SSD 技术，支持 Zone Interface；云计算和云存储供应商也在调整他们的软件栈来适配 Zone 设备。分布式文件系统在这方面目前缺乏较好的支持。 BlueStore 的新特性 使用 KV 存储低级别的文件系统元数据（如bitmap），从而避免磁盘格式的变化，同时减小了实现的复杂度； 通过精细的接口设计，优化克隆操作并减小范围引用计数的开销； 引入自定义的 BlueFS 文件系统使 RocksDB 运行的更快，引入一个空间分配器，使得磁盘上每 1 TB 的数据只需要使用约 35MB 的内存； Background 分布式存储系统后端存储概要 分布式存储系统对于后端存储的共同需求： 高效的事务机制 快速的元数据操作 （可选）对新兴存储器件的支持 事务机制 后端存储支持事务相关的操作的话将会简化强一致性的实现过程 如果底层文件烯烃支持事务操作的话，作为后端存储也能无缝地提供事务的相关功能，但大多数文件系统实现了 POSIX 标准，就缺乏了事务的概念。 分布式文件系统的开发者只好采用低效或者复杂的机制来实现事务，例如在文件系统的基础上实现 WAL，或者利用文件系统内部的事务机制。 元数据管理 作为分布式文件系统中的一大痛点，主要表现为不能较好地枚举大目录中的内容或者无法较好地处理大规模的小文件，该类场景下对于无论是集中式还是分布式的元数据管理都严重影响性能上的表现。 为了解决这个问题，开发者采用了诸如 元数据缓存，使用 HASH 进行深度的目录层次结构排序，或者使用自定义开发的数据库来题文件系统进行管理。 新型硬件的支持 SMR 提升 HDD 容量大于 25%，且预计 2023年将有超过半数的数据中心使用 SMR ZNS SSDs 摒弃了 FTL，不会再收到不可控的垃圾回收延迟的影响，减小了 IO 尾延迟。 Ceph 架构 不再赘述，请参考其他博文。 Ceph 后端存储的演变 FileStore Object Collection 对应的为 目录；Object Data 对应地存储在文件上；Object Attributes 一开始存储在 POSIX 扩展文件属性（xattrs）中，但如果超过xattrs的限制将移植到 LevelDB Btrfs 实现了事务机制，数据去重，校验和和透明压缩，但受到数据和元数据碎片的严重影响。 XFS 具有更好的扩展性和更为高效的元数据管理，但仍然受元数据的碎片影响，且无法发挥硬件的最大性能，由于自身缺乏事务的支持，在用户态实现 WAL 又严重限制了 Ceph 的性能；除此以外，不支持 CoW，导致克隆操作被过度使用，也带了较大的开销。 NewStore 一开始是为了解决在文件系统作为后端过程中存在的元数据管理问题，NewStore 将对象元数据存储在 RocksDB（有序的KV存储） 中，对象数据依然保存在文件中。RocksDB 也被用于实现 WAL； 用文件的形式存储数据，并在日志文件系统上运行 RocksDB 也相应地引入了较高的一致性开销。 于是产生了 BlueStore。 Building Storage Backends on Local File Systems is Hard 文件系统不适用于作为分布式存储系统后端，文件系统面临以下几个方面的挑战： 高效的事务机制 事务的作用：通过将一系列操作封装成一个单独的原子工作单元来简化应用程序的开发。 基于文件系统实现事务的机制大致可以通过以下三种方式： 利用文件系统内部的事务机制 在用户态实现 WAL 机制 使用具有事务功能的 KV 数据库作为 WAL 文件系统内部的事务机制 许多文件系统实现了内核态的事务机制，以便原子性地执行复合操作。由于该种事务框架是为了保证内部文件系统的一致性的，功能上存在一定的限制，对于用户而言是不可用的，譬如缺乏回滚的机制，因为在文件系统内部为了保证一致性没有必要使用回滚机制。 Btrfs 通过一组系统调用将操作原子性地从用户态的操作应用到文件系统内部的事务机制，从而向用户提供事务机制。起初基于 Btrfs 的 FileStore 引擎就依赖了这些系统调用，但仍然缺乏回滚的机制。对于一些在事务执行过程中的致命错误，如软件 CRASH 或者被 KILL，Btrfs将提交部分事务出现不一致的状态。 Ceph 最终通过引入一个单独的系统调用(transaction system calls)来指定整个事务，并使用快照来实现回滚机制。但事实证明此种方式造成的开销很大，所以 Btrfs 的作者在后来决定标识该系统调用为过期。 经验表明，通过使用文件系统内部的事务机制来提供用户态的事务功能是很困难的。 用户态实现 WAL 通过在用户态实现逻辑上的 WAL 能够提供对应的事务机制，但同样也会引入以下几个重要的问题： Slow Read-Mddify-Write 缓慢的读-修改-写操作 Non-Idempotent Operations 非幂等性操作 Double Writes 双写问题 Slow Read-Modify-Write 首先明确什么是 Read-Modify-Write？RMW 操作在 Ceph 系统内的表现其实就是在准备生成一个事务的时候，往往需要依赖前一个事务，这时候需要读取前一个事务的数据，然后对应的修改当前的事务数据，再把修改过后的事务数据最终写入进行提交。 用户态的 WAL 的实现是如何提供事务机制的呢？首先，将事务的全部数据序列化并写入到 log 中去；然后调用 fsync 操作将事务提交到磁盘；最后将事务中执行的具体操作应用到文件系统中。 为什么RMW会比较慢？因为事务的机制表明了前一个事务在事务进行第三步操作之前是无法被后一个事务读取的，也就是说只有在前一个事务的操作真正应用到文件系统上之后才能读取该事务的数据。所以每一次 RMW 操作都需要等待一次完整的 WAL 提交，将受到 WAL 延迟的影响，无法进行高效的流水线操作。 Non-Idempotent Operations 幂等性操作是指在 WAL 的机制下，当系统 CRASH 之后需要重放 WAL 的记录来恢复数据，此时可能是非幂等操作。 由于 WAL 日志是定期进行修改的，那么就会存在一个时间窗口，该时间窗口内事务已经提交且应用到了文件系统中，但仍然保留在 WAL 日志中。 考虑以下场景： 事务A包含以下三个操作，如果第二个操作之后 CRASH，回放 WAL 日志，b 的值将会被错误地修改 1. clone a-&gt;b 2. updata a 3. update c 事务B包含以下四个操作，如果 CRASH 发生在第三个操作完成之后，回放 WAL 将错误地修改 a 1. update b 2. rename b-&gt;c 3. rename a-&gt;b 4. update d 为了解决非幂等性的问题。Btrfs FileStore 通过定期进行持久化快照操作，并在 WAL 日志中标记快照的时间；恢复时将基于最近的一次快照，然后将 WAL 日志中对应该快照的标记之后的操作进行回放。 使用 XFS 代替 Btrfs 之后，由于缺乏有效的快照机制，造成了两个问题： XFS 中只支持开销特别大的 sync 系统调用。会把所有文件系统数据同步到所有驱动设备上，后来出现了 syncfs 系统调用解决了该问题。 XFS 不能将文件系统恢复到某一个具体的状态再回放 WAL 中的操作。所以需要引入 哨兵（序列号）机制来避免非幂等操作的，但验证序列号的正确性是一项非常复杂的操作。 Double Writes 使用 WAL 日志机制对应的引入了双写问题，一次需要写入 WAL ，后一次需要写入文件系统，平分了磁盘的带宽，该种方式又称作 Data Journaling。所以很多文件熊只是在 WAL 中记录元数据的变更信息，在 CRASH 时允许数据丢失，从而避免数据的双写问题，该种方式又称作 Metadata Journaling。 Data Journaling 和 Metadata Journaling 的区别在于是否将 user data 的数据写入到日志文件中，以及 Metadata Journaling 的实现方式时先写入用户数据到文件系统，再写元数据日志，Data Journaling 是把所有数据先写入日志提交之后，再写入文件系统。 参考链接：日志文件系统是怎样工作的 使用键值存储作为 WAL NewStore 存储引擎中，元数据被保存在有序的键值存储 RocksDB 中，对象的数据仍然以文件的形式保存在文件系统中，因此元数据的相关操作可以原子性地执行，数据的覆盖些则是先记录到 RocksDB，然后执行写操作。 KV 存储结局了逻辑 WAL 存在的三个问题，但也引入了新的一致性开销。 如何解决Logical WAL存在的问题 KV 存储提供的接口允许读取对象的最新状态值，不用等待事务的提交。 避免了非幂等操作重放的问题，因为此类操作的读端是在事务准备时解决的。例如 clone a-&gt;b 操作的处理，如果对象较小，将被复制并插入到事务中，如果对象较大，将会使用 CoW 机制。 避免了新对象的双写问题，因为对象命名空间于文件系统状态实现了解耦。新对象的数据，先写入文件系统，然后创建一个对应的应用并原子性地添加到数据库中去。 键值存储带来的弊端 RocksDB 和 日志文件系统的结合引入了较高的一致性开销，类似于 journaling of journal 问题。 在 NewStore 中创建一个对象的步骤： 1.数据写入到一个文件，调用 fsync 2.将对象元数据同步写入到 RocksDB，同样调用 fsync 因此对于 KV 存储，大约需要执行两次 fsync，而每一次 fsync 命令又会发起一次 FLUSH CACHE 的命令，但又因为是基于日志型文件系统的操作，每次 fsync 调用会额外触发一次对日志数据的 FLUSH CACHE命令。故一个对象的创建需要触发四次 FLUSH。 使用 Benchmark 模拟存储后端创建大量对象进行测试，循环创建 数据大小为0.5MiB，元数据大小为 500bytes 的对象，一种基于 XFS 进行创建，一种基于裸盘和基于裸盘和预先分配的WAL文件构建的RocksDB。 快速的元数据操作 Ceph 使用 FileStore 作为后端面临的问题也包含了大目录结构的枚举操作较慢以及返回的结果无序。 存在的问题 什么时候进行枚举操作 针对 Ceph 中的 Scrubbing、Recovery 以及 RADOS 提供的 List 接口操作，都需要进行目录内容的枚举。 为什么结果无序 RADOS中的对象根据其名称的散列映射到PG，并按散列顺序枚举。 FileStore 的解决方案 FileStore遵循一种通常采用的解决慢枚举问题的解决方案:创建具有大扇出的目录层次结构，将对象分布在目录中，然后在读取后对所选目录的内容进行排序 为了快速地对它们进行排序，并限制潜在的stat调用的开销，在目录中的条目数量增长时对它们进行拆分，从而使目录保持较小(只有几百个条目) 拆分条目存在的问题 随着规模的的增大，拆分操作将会是一个高开销的操作 一次处理数百万个inode降低了dentry缓存的可靠性，导致许多小的I/Os到磁盘。 XFS将子目录放在不同的分配组中，以确保有空间让将来的目录条目紧密地放在一起 随着对象数量的增长，目录内容会分散，由于查找而导致分割操作需要更长的时间。因此，当所有Ceph OSDs开始一致地分裂时，性能就会受到影响。 关于 Split 操作对磁盘吞吐量的影响的测试数据，请参见原文。 对新型存储器件的支持 HDD 逐渐转向 SMR，为了利用额外的容量并同时实现可预测的性能，应该使用具有向后兼容 zone 接口的主机管理 SMR 驱动器。另一方面，zone接口将磁盘管理为256个必须按顺序写入的MiB区域的序列，这鼓励了一种 日志结构的、CoW 的设计，与大多数成熟的系统所遵循的 Overwrite 设计是完全相反的。 SSD 也逐渐转变为 OpenChannel SSD，该类 SSD 省略了 FTL，直接由 host 来管理裸盘资源，但由于缺乏统一的标准，各大厂商分别做了自己的实现，为此，主流厂商引入了一种新的 NVMe 标准 ZNS（Zoned Namespaces）,重新定定义了 OpenChannel SSD的标准。消除 FTL 带来了很多优势：减小了写放大问题，改善了延迟异常和吞吐量，一定程度上也减小了成本。 其他挑战 页缓存中的 Write-Back 机制 导致基于文件系统的存储后端出现高差异请求延迟的原因之一是操作系统页面缓存。大多数操作系统使用回写 write-back 策略来实现页面缓存，在这种策略中，一旦数据被存入内存，相应的页面被标记为脏，写操作就完成了。在几乎没有I/O活动的系统上，脏页定期写回磁盘，从而同步磁盘上和内存中的数据副本。另一方面，在繁忙的系统中，回写行为由一组复杂的策略控制，这些策略可以在任意时间触发写操作。 尽管写回策略为系统负载较轻的用户生成响应系统，但它会使繁忙的存储后端实现可预测的延迟变得复杂。即使定期使用fsync, FileStore也无法限制延迟的inode元数据回写的数量，从而导致性能不一致 无法利用 CoW 高效地实现一些操作 针对部分操作，例如快照，利用 CoW 可以十分高效地进行实现。不支持 CoW，导致在FileStore中快照和覆盖擦除编码数据的成本高得令人望而却步，因为需要对对象进行全量拷贝 但如果引入了 CoW 机制，文件系统也会存在其他缺陷，例如 Btrfs 的碎片问题。 BlueStore - 全新的后端存储引擎 BlueStore 主要设计目的就是为了解决本地文件系统所面临的诸多问题。其中几个主要目标为： 快速的元数据操作 写入对象时无一致性开销 支持 CoW clone 操作 没有日志带来的双写问题 为 HDD 和 SSD 做 IO 整形优化 Architecture BlueStore 直接运行在裸设备上，其中由内置的 Space Allocator 决定新数据的存放位置，内部的元数据信息和对象的元数据信息存储在 RocksDB 中，RocksDB 又运行在定制化的小型文件系统 BlueFS 上。 BlueFS and RocksDB BlueStore 通过将元数据保存在 RocksDB 中来实现快速的元数据管理；通过直接将数据写入裸盘，实现只执行一次 FLUSH，重用 WAL 文件作为环形缓冲来构建 RocksDB 实现元数据写操作也只需执行一次 FLUSH。 BlueFS Metadata Organization RocksDB 中保留有多个 Namespaces，每一个 Namespace 存储不同种类的元数据。例如对象元数据存储在 O Namespace中，块分配元数据存储在 B Namespace中，集合元数据存储在 C Namespace中，每一个对象集合映射到 Ceph 中的一个 PG 并代表一个存储池 Namespace 的分片。集合名通常也包括存储池的标识符以及该集合中对象所共享的前缀字符串。 例如一个 KV 对 C12.e4-6 标识了在 12 号存储池中的 HASH 值以 e4 为起始的集合，使用 6 个有效位。故对象O12.e532属于该集合，对象O12.e832则不属于。可以通过调整有效位数来对集合进行拆分。 在Ceph的OSD集群中，往往需要进行数据均衡分布的操作。相比于 FileStore 基于目录 rename 的拆分，此种方式开销更小。 Data Path and Space Allocation BlueStore 支持 CoW 机制，针对写入的数据（大小大于最小分配的空间大小 HDD 64KiB，SSD 16KiB）先写入预先分配的区域，数据持久化之后，再向 RocksDB 中插入元数据信息。该机制提供了高效的克隆操作，也同时避免了双写问题，针对 HDD 和 SSD 分别进行了优化（最小分配空间大小的不同） Space Allocator BlueStore 使用 FreeList Manager 和 Allocator 来进行空间分配。 FreeList Manager FreeList manager 负责表示磁盘中已经持久化的区域，同其他元数据一样保存在 RocksDB，Manager 有两种实现： 一种是 将磁盘中已经使用的区域以键值对的形式表示，同时存储偏移量和长度信息。该种方式实现的坏处是事务必须序列化：为了避免FreeList不一致的情况，在插入新的 Key 之前需要删除旧的 Key。 一种是基于位图进行实现，分配和回收操作都使用 RocksDB 合并操作来将相关数据块进行位翻转，消除排序的约束，且 RocksDB 的合并操作执行了一个延迟的原子读-修改-写操作，该操作不改变语义，并且避免了点查询的成本 Allocator Allocator 主要负责为新数据分配对应的空间，它将空闲列表的副本保存在内存中，并在进行分配时通知FreeList manager。 Allocator 对应也有两种实现： 第一个实现是基于区段的，将空闲区段划分为两个power-of- size的区域。随着磁盘使用量的增加，这种设计容易产生碎片。 第二个实现使用索引层次结构来跟踪块的整个区域。通过分别查询较高和较低的索引，可以方便地找到较大和较小的区段。这个实现的单位内存使用为35MiB/TB Cache 由于 BlueStore 实现在用户态，且以 Direct IO的模式访问磁盘，无法省略操作系统页缓存，所以 BlueStore 使用2Q算法实现了自己的写穿缓存 缓存实现被分片以实现并行。它使用与Ceph OSD相同的分片方案 它将请求分片到跨多个核心的集合。 BlueStore 引入的新特性 节省空间的校验和 为什么需要校验和？ 尽管在 Ceph 中有 Scrub/Deep Scrub 操作来扫描副本间数据的一致性，当发现不一致时仍然很难判断哪一个副本被损坏了。因此，校验和就显得不可或缺了，特别是在分布式存储系统中处理经常发生位翻转的 PB 级别的数据时。 文件系统校验和 大多数文件系统不支持校验和，部分支持校验和的文件系统，如 Btrfs，每一个数据块计算出的校验和超过 4KiB，10TB 的数据校验和数据的大小将超过 10GB，此时将很难将校验和缓存在内存中进行快速的验证。 BlueStore 校验和 在分布式文件系统中的大多数数据位只读数据，所以可以以更大的规模去做校验和相关的操作。 BlueStore 为每次写操作计算校验和，为每次读操作验证校验和，也支持多种校验和算法。CRC32c 由于其在 x86 和 ARM 架构上都有比较好的优化被用作默认校验和算法，在检测随机Bit位错误时也很高效。 由于BlueStore 完全控制了整个 IO 栈，BlueStore 可以根据 IO 的情况决定计算校验和的大小。 纠删码数据的覆盖写 Ceph 支持使用纠删码存储池来保证数据的一致性，但是在 BlueStore 之前，EC pools 只是支持对象的追加写和删除操作，覆盖写操作效率太低可能导致系统不可用，所以只有 RGW 模块能够使用 EC 池，RBD 和 CephFS 都使用了多副本存储池 为了避免多步骤写可能造成的数据不一致问题，Ceph 在 EC 池中使用了两阶段提交算法来进行覆盖写。 所有存储EC对象块的osd都会复制该块，以便在发生故障时可以回滚。 在所有osd接收到新内容并覆盖其块之后，旧的副本将被丢弃 和 基于 XFS 的FileStore 的区别在于，BlueStore 的 CoW 机制避免了第一步全量物理拷贝，FileStore 需要做全量物理拷贝。 透明压缩 针对多副本场景，BlueStore 实现了数据被存储之前自动进行透明压缩。 当压缩的对象大大小超过 128KiB chunks 时压缩就能起到很好的效果，所以如果对象被完整地写入，压缩就能发挥很大的效益。 对于部分写操作，BlueStore 将新数据放置到一个单独的区域并更新元数据然后指向它，当该对象被多次覆盖写之后碎片化严重，BlueStore 通过读取和重写来压缩对象。 新型接口的支持 相比于传统的本地文件系统缺乏对新型存储器件的支持，BlueStore 不再拘泥于基于块进行设计，现有的 BlueStore 运行的 BlueFS 和 RocksDB 已经可以在 SMR 盘的基础上运行。同时也在位新型 NVMe 接口的设备开发新的存储引擎。 Evaluation 测试主要将 FileStore 和 BlueStore 进行性能上的对比。将比较 RADOS 的对象写的吞吐量，端对端的吞吐量（基于RBD的随机写、顺序写、顺序读）以及基于EC池构建的 RBD 的随机写吞吐量。 实验环境 Cisco Nexus 3264-Q 64-port QSFP+ 40GbE switch 每个节点： 16-core Intel E5-2698Bv3 Xeon 2GHz CPU 64GiB RAM 400GB Intel P3600 NVMe SSD 4TB 7200RPM Seagate ST4000NM0023 HDD a Mellanox MCX314A-BCCT 40GbE NIC Linux Kernel 4.15 on Ubuntu 18.04 Ceph Luminous(v12.2.11) Bare RADOS Benchmark RADOS Block Device Benchmarks EC Pool RBD Challenges of Building Effcient Storage Backends on Raw Storage 缓存大小的调整和写回策略 文件系统继承了操作系统页缓存的优势，会根据应用程序的使用情况动态调整缓存的大小；而对于类似于 BlueStore 穿过内核的存储后端需要自己实现类似的机制。 BlueStore 中需要自己手动设置缓存大小相关的参数，如何在用户态构建一个如操作系统页缓存动态调整大小的机制时很多存储系统都面临的问题，如 PostgreSQl，RocksDB。同时面对已经出现的 NVMe SSD，缓存需要更加高效，才能减小 SSD 的写负载，同时也是当前页缓存面临的问题。 KV 存储的效率问题 Ceph 的经验表明把所有元数据给移植到有序的 KV 存储（如 RocksDB）上能够显著提高元数据操作的性能，然而同时也发现嵌入 RocksDB 带来的一些问题： 在 OSD 节点上使用 NVMe SSD 时，RocksDB 的压缩机制和严峻的写放大问题已经成为了主要的性能限制； 由于RockDB被视为一个黑盒，因此数据被序列化，并在其中来回复制，消耗数据 CPU 时间 RocksDB 有自己的线程模型，这限制了自定义分片的能力。 CPU 和 内存的效率问题 Ceph 中复杂的数据结构，且生命周期较长，采用默认的布局一定程度上会造成内存的浪费。因为现代编译器在内存中对齐和填充基本数据类型，这样CPU就可以方便地获取数据，从而提高了性能。跨越了内核的存储后端控制几乎机器的所有的内存。 如上提到的 KV 存储的问题，在使用 NVMe SSD 时，其工作负载会被 CPU 限制，涉及到大量的序列化和反序列化操作。故 Ceph 开发团队试图减小 CPU 的消耗，通过减小序列化和反序列化数据的大小，尝试使用 SeaStar 框架的共享模型来避免由于锁导致的上下文切换。 Extend ","link":"https://blog.shunzi.tech/post/file-systems-unit-as-distributed-storagelessons-from-10-years-of-ceph-evolution/"},{"title":"SLM-DB Single-Level Key-Value Store with Persistent Memory","content":" FAST 2019 的文章《SLM-DB Single-Level Key-Value Store with Persistent Memory》 这篇论文主要在新型存储器件Persistent Memory上针对传统的 LSM 树进行了优化。 本篇论文用于课程论文阅读汇报，故在此总结便于演讲。同时学习 NVM 相关基础知识。 摘要 这篇 Paper 主要研究了如何利用新型的字节型寻址器件 PM 来提升 KV 存储系统的性能。提出了一种新型的 KV 存储系统的架构设计 SLM-DB。 SLM-DB 充分利用 B+ Tree 和 LSM Tree 的优势充分发挥 PM 的特性，实现了较高的读写性能，同时降低了写放大，实现了接近最优的读放大。 Introduction 两种传统的 KV 存储类型：一种是基于 B-Tree 的存储引擎，一种是基于 LSM 的存储引擎。 B-Tree 特性：在读操作和范围查询操作方面表现较好，在写操作的性能方面表现较差，特别是针对大量的随机小写，并且由于需要动态地维护平衡结构往往就会有很严峻的写放大问题。 B-Tree 类型的 KV 存储更适用于 读密集型的工作负载。 实际的应用产品：KyotoCabinet LSM Tree LSM-Tree 有较高的写性能，通过将相关 KV 缓冲在内存中并批量地将其顺序写入磁盘中。但该结构由于需要维护多层的结构来进行合并压缩归并排序等操作，导致了具有很严重的读写放大问题，在读性能方面表现较差 LSM Tree 类型的 KV 存储更适用于 写密集型的工作负载。 实际的应用产品：BigTable，LevelDB，RocksDB，Cassandra。 Persistent Memory 一种新型的存储器件，具有与 DRAM 相当的读写性能，带宽相对 DRAM 较低，但也比 DRAM 拥有更大的容量，最重要的是具有持久性的相关功能 优化方式 读优化 利用 PM 的特性维护 B+ Tree 作为索引，加速数据的检索 写优化 利用 LSM 的特性来进行 KV 的写入操作，使用 PM 进行缓冲写入，省略了 WAL 的写开销 采用单层的 LSM 结构来减小写放大问题带来的影响，但同时需要设计新的压缩算法来保证单层结构中的数据部分有序，从而保证范围查询的性能。 Background &amp; Motivation LevelDB Introduction 组成部分 Memory：使用传统的 DRAM 存储和操作 MemTable 和 Immutable Memtable Disk：使用磁盘存储和操作多级的 SSTables 核心机制 写流程 写流程可以具体划分为插入和更新操作。当有记录要写入时，按照 Key 在 Memtable 中的对应的位置，判断该 Key 是否已经存在，如果存在则进行更新。不存在则相应地写入该 Key 和 Value。 Memtable 是内存中存数据的地方，这些数据按 Key值排序，为了快速查找，会有红黑树之类的数据结构作为索引。当 Memtable 大小到一定阈值，先状态改变，变成 Immutable Memtable，然后新建一个 Memtable，启动一个后台线程就把 Immutable Memtable 存到一个文件，这个文件就是一个SSTable，相应地将文件刷入磁盘。 随着 SSTable File的数量越来越多，触发对应的 Compaction 操作，将对应的 SSTable 文件进行合并，合并到新的数据层上。 读流程 首先在memtable中查找，没找到的话，再到 Immutable Memtable 上进行查找，还是没找到的话，再按时间顺序在SSTable文件中查找。当读数据的时候，如果某个记录不存在，需要读取所有的SSTable才能确定。所以一般每个SSTable文件生成的时候会带一个bloom filter，对这一点进行优化。 一致性保证 在内存中的写操作采用了 WAL 的机制来进行一致性的保证，统一地先进行日志写的操作，再进行实际的数据插入操作。 在磁盘上的数据的一致性保证，采用了 MANIFEST 日志进行保证，针对来自内存的刷会操作和压缩操作都会进行对应的日志记录，主要记录 LSM Tree的结构变化数据。 延申参考链接 [1] 掘金：看图轻松理解数据结构与算法系列(NoSQL存储-LSM树) [2] 掘金：一文带你了解 LSM Compaction Limitation of LevelDB LevelDB 的几个核心问题：较低的读性能 和 严峻的读写放大问题 Slow Read Operation 最少两次读块的操作：从 LevelDB 中的读流程中我们不难发现，针对在内存中未能命中的数据往往都意味着在磁盘上的每一个 Level 上都会有进行二分查找的操作，首先二分查找定位数据所在的文件 SSTable File；找到对应的文件以后，基于 SSTable File 中存储的数据索引再次进行二分查找，找到对应的数据块。一次读索引块，一次读数据块 如果数据块中不包含，则需要进入下一级重复该操作 引入 Bloom Filter，一定程度上减小了读操作的开销。 High write and read amplification Write Amplification LSM 通过维护垂直的多级有序数据结构 SSTables 来保证顺序写，从而提高写的速度，但对应的也就引入了维护该结构的开销。 需要在不同的层级之间进行归并排序操作，同时需要常常进行数据压缩(Compaction)。 Read Amplification 由于 LSM 树的原因，读数据时往往需要在多个层级进行检索 读取数据时，往往不仅需要读取该 Key 所对应的数据所在的数据块，还要读取对应的索引块和 Bloom Filter 块，索引块和 Bloom Filter块的数据和可能比 KV 本身大得多。 Persistent Memory 特性 非易失，字节寻址 PM 将通过内存总线进行连接，而不是块接口，所以针对PM的原子写操作应该为8byte大小，相比于传统的存储设备较小。 为了保证一致性，需要保证内存操作的顺序 一致性保证 为了让内存的带宽最大化，现代的处理器可能会将内存写操作进行指令重排序。所以为了保证内存中的有序写，可能需要显式地使用如 CFLUSH 和 MFENCE等缓存刷新和内存隔离指令。 针对一些在 PM 上的写操作，如果数据量超过了 8bytes，在系统故障的情况下可能出现数据部分更新成功的情况，因为 PM 只保证 8bytes 的原子性，为了解决这样的不一致问题，考虑使用日志或者 CoW（Copy on Write）等技术来解决。 基于 LSM 树的优化 提高并行性 RockesDB(Facebook)：利用数据的特性（无重复Key），进行多线程压缩 HyperLevelDB：通过更为细致的锁管理机制增加了写线程的并行性。 减小写放大 PebblesDB (SOSP'17)：借鉴跳表的思想，弱化全局有序的约束，将每一层进行分段，段之间保证有序，段内无序。 基于硬件的优化（SSD） VT-Tree (FAST'13) WiscKey (FAST'16)：在 LSM-Tree 上做 Key 和 Value 的分离存储 Single-Level Merge DB 主要贡献 使用 PM 存储 MemTable、Immutable MemTable 来提高更强的一致性保证，同时减小一致性的开销 在 PM 中引入 B+-Tree 作为索引，提高查询速度 将磁盘中的多级数据结构转为单级数据结构，并设计了一种选择性压缩机制 Persistent MemTable MemTable 的核心数据结构为 跳表 SkipList 跳表的操作，如插入、更新、删除均可用 8bytes 的原子操作完成 采用 PM 存储 MemTable，不再需要 WAL 来保证一致性 针对跳表的索引部分不需要提供一致性保证，因为可以从原始数据中快速重建 Insert Insert(key, value, prevNode) 1: curNode := NewNode(key, value); // Create Node 2: curNode.next := prevNode.next; // Modify currentNode.next pointer 3: mfence(); // 插入内存屏障（读屏障），让高速缓存中的数据失效，重新从主内存加载数据 4: clflush(curNode); // 缓存刷新， 持久化 currentNode 数据 5: mfence(); // 插入内存屏障（写屏障），内存隔离，让写入缓存的最新数据写回到主内存 6: prevNode.next := curNode; // Modify preNode.next pointer 7: mfence(); // 插入内存屏障（读屏障），让高速缓存中的数据失效，重新从主内存加载数据 8: clflush(prevNode.next); // 缓存刷新，持久化 preNode 的修改 9: mfence(); // 插入内存屏障（写屏障），内存隔离，让写入缓存的最新数据写回到主内存 延申链接 [1] 博客园：内存屏障 B+-Tree Index in PM When to Build the Tree And Some Key Info 当把 KV 键值对从 Immutable MemTable 中往 SSTable 中刷入时，把 Key 相关的索引信息插入到 B+ 树中，作为叶子节点。 叶子节点包含的信息有：SSTable File 的标识符，文件内部对应的该 Key 的块的偏移量 和 数据块的大小。 B+ Tree 的更新操作：更新已有的 KV 对时，首先创建对应该 Key 的索引对象，然后修改原有的在 B+ 树中的指针，指向新的索引对象。旧的索引信息将会被视为垃圾数据进行统一的垃圾回收。 B+ Tree 的删除操作：当需要删除指定 Key 的数据时，则将该 Key 在 B+ 树中对应的索引信息删除。 垃圾回收时使用 PM 管理器如 PMDK 来进行垃圾回收。 Build the Tree 当 KV 从 Immutable MemTable 中往 SSTable 刷入时（即 Flush操作），SLD-DB 创建两个后台线程：一个线程用于创建新的 SSTable File，一个用于向 B+ 树中插入索引信息。 File Creation Thread 该线程创建一个新的 SSTable 文件，把 Immutable MemTable 中的数据（K-V）写入到该文件中。 将该文件执行文件系统相关同步操作，如 fsync，同步到磁盘 同步之后，该线程将刚刚存储的 KV 对添加到一个队列中。（由向 B+ 树插入索引的线程创建的队列） B+ Tree insertion thread 创建相应的 KV 索引信息队列 待数据写入线程将该队列填入数据之后，依次处理队列中的每个数据，构建 B+ 树 一致性保证 待 B+ Tree构建完成以后，将 LSM 树中组织结构的变化（例如 SSTable File 的元数据信息）以追加写的方式写入到 MANIFEST 文件中 其他 待完成一致性的日志追加写操作之后，对应的删除PM中的 Immutable Memtable. Scanning a B+-tree SLM-DB 提供了一个迭代器，主要用于扫描存储在磁盘中的 KV 键值对。支持 seek()、value()、next()等操作。 seek(k)：让迭代器指向 key 为 k 的键值对或者当 k 不存在时，指向比 k 稍大的 Key next()：将迭代器指针移动到下一个 Key value()：返回该 Key 对应的值 在 SLM-DB 中，实现了 B+ Tree 的迭代器，主要用于扫描存储在 SSTables 中的 Key。 Selective Compaction 选择性压缩 Why need Compaction? 为了回收部分旧数据。（旧数据是由 Update 操作产生的，更新 Key 对应的值时，之前存储的该 Key 的数据就变成了老数据） 提升存储在 SSTables 中的 KV 键值对的顺序性，从而提升 Range Query的性能，以免要查询的 Key 分布散落在多个 SSTable File上 What is Selective Compaction? 有选择性地选出一些 SSTables 进行压缩 SLM-DB 维护一个 候选压缩对象列表。（候选对象为具有一定特性的 SSTables） When execute? 通过启动一个后台的压缩线程来执行： 当 SSTable Files的一些结构性数据发生变化时 针对一个确定的 SSTable File 出现了多次 seek 操作时 当压缩对象候选列表里的文件数量大于一定的阈值时 执行时，从候选列表中根据一定的计算规则选出子集进行压缩。该规则主要是计算 SSTable Files 之间的重叠率，通过计算出和候选列表中的其他 SSTable Files 重叠率最高的 SSTable Files，然后进行压缩合并。 How to compact? 同上文一样使用两个线程，一个用于文件创建，一个用于索引的插入。 合并多个 SSTable Files时，需要检查该文件中的每一个键值对是有效数据还是老旧数据。通过查询 B+树中的索引信息来决定是否有效。如果有效，则需要和其他有效的数据进行归并排序，如果是老旧的数据则对应将其删除。 How to select? SLM-DB 实现了三种选择算法： Live-Key Ratio Select：有效 Key 比例选择 Leaf Node Scan：B+树叶子节点扫描选择 Degree of Sequentially per range query：根据顺序程度进行选择 Live-Key Ratio Select SLM-DB 统计每个 SSTable File 的有效 Key 比例，并且设定一个阈值，一旦比例低于该阈值，就会被添加到候选压缩对象列表中。 Live-Key Ratio 是指每个 SSTable File 中有效 Key 的比例。有效 Key 是指仍然维护在 B+ Tree 中的 Key，也就是仍为该 Key 对应的最新值。 Leaf Node Scan 在执行压缩时，将触发一次 B+ Tree 叶子节点的扫描操作，通过循环扫描的方式，统计出存储了 B+ Tree 上有指定 Key 所对应的 SSTable Files 的文件数量，如果文件数量大于阈值，将这些文件添加到候选队列中进行压缩。 文件数量意味着在 LSM 结构中存在多个数据文件存储旧值。 Degree of Sequentially per range 将 range 操作拆分为多个 子范围查询操作，每个子范围查询对应的 Key 数量相等，然后追踪每个子范围查询访问的 SSTable File 的数量，一旦操作完成，找到访问最多文件的子范围查询，如果对应的文件数大于阈值，把这些对应的文件添加到候选人队列中等待压缩。 该方法主要是为了提高数据分布式的顺序性。 存在的问题？ 1. B+ 树的引入带来的写放大问题何如？ 2. B+ 树索引结构的维护的一致性如何保证？ 3. 针对大量小写的情况，B+ 树是否会成为对应的性能瓶颈？ 论文延申 Bloom Filter 布隆过滤器 本质上布隆过滤器是一种数据结构，比较巧妙的概率型数据结构（probabilistic data structure），特点是高效地插入和查询，可以用来告诉你 “某样东西一定不存在或者可能存在”。 特点 相比于传统的 List、Set、Map 等数据结构，它更高效、占用空间更少，但是缺点是其返回的结果是概率性的，而不是确切的。 如果布隆过滤器判断的结果为不存在，那么事实上一定不存在；但如果布隆过滤器判断的结果为存在，那么事实上不一定存在。 原理 数据结构：bit 向量/数组（初始化全为0） 支持的操作：添加（add）、判断是否存在（ifExsit） 不支持的操作：删除（delete） 添加操作 使用多个 Hash 函数将要添加的值进行 Hash，得到多个对应的结果，将 bit 向量上的对应位置置为1。 每一次新的值都进行相同的操作，将对应位置置为0，但可能置0的位置已经为0了，所以仍然保持为1。 判断操作 判断某个值是否已经存在，利用设定好的 Hash 函数对应的求出相应的 Hash 值，看这多个值对应的 bit 向量上是否全为1。如果不全为1，则一定不存在；如果全为1，则可能存在。（原因：不同的值可能产生哈希冲突同时使得某个位置上的值为1） 如何支持删除操作？(是个可以研究的问题) 对应的对每一位 bit 进行计数，采用计数删除的方式。但对应的会给每一位增加一个数值的开销。 然而要保证安全地删除元素并非如此简单。首先我们必须保证删除的元素的确在布隆过滤器里面。这一点单凭这个过滤器是无法保证的。另外计数器回绕也会造成问题。 如何选择哈希函数个数 k 和布隆过滤器长度 m k 次哈希函数某一 bit 位未被置为 1 的概率为： (1 - \\frac {1}{m})^k 插入n个元素后依旧为 1 的概率： 1 - (1 - \\frac {1}{m})^{kn} 标明某个元素是否在集合中所需的 k 个位置都按照如上的方法设置为 1，但是该方法可能会使算法错误的认为某一原本不在集合中的元素却被检测为在该集合中（False Positives） (1 - (1 - \\frac {1}{m})^{kn})^k \\approx (1 - e^{-kn/m})^k 选择合适的 k 和 m：n 为插入的元素个数，p 为误报率 m = - n\\ln p / (\\ln 2)^2 k = \\frac {m}{n} \\ln2 参考链接 [1] 知乎：详解布隆过滤器的原理，使用场景和注意事项 [2] CodeBear的园子：大白话布隆过滤器 ","link":"https://blog.shunzi.tech/post/slm-db-single-level-key-value-store-with-persistent-memory/"},{"title":"Exploiting Commutativity For Practical Fast Replication","content":" NSDI 2019 的文章《Exploiting Commutativity For Practical Fast Replication》 NSDI 2019 的文章《Exploiting Commutativity For Practical Fast Replication》 这篇论文主要针对分布式系统中的主从复制场景提供了一种利用执行操作的可交换性快速同步的方案 恰好最近有在学习分布式系统中的一致性以及组会需要讲解论文，故细读了这篇 Paper 并进行总结 由于是研究生期间第一篇细读的论文，想以总结的形式顺便了解到学术论文的相关写作技巧和规范 结合组会交流过程中遇到的几个问题，进行仔细地思考和相关文献资料的查阅，未完待续~ 摘要 传统的数据强一致性场景下的主从复制需要 2RTT（Round-Trip Time）来完成一次操作，而本文提出了一种新的方案 CURP （Consistent Unordered Replication Protocal）实现在 1RTT 内完成一次操作。通过在 Redis 和 RAMCloud 上进行测试：RAMCloud 性能有较大提升，写延迟从 14 微秒降低到 7.1微秒，吞吐量提升约为 4 倍； Redis 的持久化开销也相应地被减小。 编者注：1RTT 只是针对少量场景能达到的最好效果，一般场景可能仍然需要 2RTT 甚至恶化到 3RTT，所以 1RTT 的说法稍有“标题党”的嫌疑。测试场景主要针对 K-V 型的存储，也是该方案的局限所在，后文具体介绍。 Introduction 背景 许多分布式系统，特别是分布式存储系统，对可用性和一致性要求都非常高。为了保证服务的高可用，业界往往采用了副本的方式来作为当前服务主节点的“备胎”，但针对部分一致性要求高的场景，需要尽量保证副本的数据和主节点上的数据保持一致。而为了保证一致性，常常都会采用复制的策略，但复制的策略又主体划分为： 1.树形复制，也称基于强 Leader 的数据一致性复制协议，诸如 Raft/Multi-Paxos 2.链式复制，链式复制也有相应的优化方案，可参考链接 Github：复制模型 3.分发复制：Client直接向各个节点直接进行分发写入，节点之间并不进行通信复制，只要写入多数节点成功，就判为写入成功。 对比以上几种模型，树形复制作为吞吐量和延迟上的折中方案，被很多系统应用到了实际的生产环境中。树形复制流程如图所示，论文主要针对树形复制的 2RTT 进行优化，将其降低为 1RTT，在此基础上提出优化方案。 现有的一些方案 复制协议的核心思想： 要保证操作执行顺序的一致性，特别是一些具有因果关系的操作。 需要提供相应的持久化功能，保证能够从故障情况中进行恢复。 主从复制 - 2RTT 传统意义上的主从复制方案，即上文提到的树形复制方案，实现了相应的核心思想，但产生了 2RTT 。 1RTT 来自于 Client 和 Master 之间的交互， 1RTT 来自于 Master 和 Backup 。 该方案中主要依赖 Master 节点对操作进行相应的排序和序列化。 Raft/Multi-Paxos 等基于强 Leader 的数据一致性复制协议，和普通的主从复制的区别仅仅在于复制过程中同步的数据有所不同，Raft/Multi-Paxos 主要针对操作日志进行同步复制。 Fast Paxos and Generalized Paxos - 1.5RTT 该类型方案中不是 Leader 节点来完成对应的序列化操作。 根据推测的执行顺序来进行推断，如果多数节点响应成功，则对应的执行操作。 1RTT 来自于客户端和主节点之间的交互， 0.5RTT 主要来自于等待多数节点同意该顺序。 编者注： 此处介绍的很抽象模糊，后续会针对这几种 Paxos 算法进行具体的分析比较。 Network-Ordered Paxos and Speculative Paxos - Near 1RTT 利用了特殊的网络硬件设施来保证操作的执行顺序。 由于硬件设施的复杂性以及延迟较高，该类型目前而言不太适用于实际生产环境。 CURP Key Idea Source of Idea：为了满足一致性复制协议的两个核心思想，性能上往往都差强人意。从需求方面而言，关于持久性的要求为了保证高可用不能做丝毫妥协，则只有在操作的执行顺序上下手进行优化。前有 multi-paxos 基于类似 pipeline 复制的乱序 commit，但是并不能乱序 apply，所以后来又有了 Parallel Raft 通过让 raft 协议感知具体应用而实现乱序 Apply，其实都是想在执行顺序上做些突破。 CURP 采用了一种延迟确定顺序的方案，通过判断操作之间是否有顺序依赖，来区分可以乱序执行的操作和顺序执行的操作，从而实现 1RTT 的延迟。 Architecture 组成 Client：并发地向 Primary 和 Witnesses 节点发送 op request，等到 Primary 和 Witnesses 都返回成功了，则认为该请求对应的执行成功。针对某些失败的操作，需要 Client 端针对返回的信息进行相应的处理，同时也有重试的机制。 Primary：数据操作执行的主节点，也就是副本的 Leader 节点，主节点收到 op request，立马返回执行结果，然后异步地复制给 Backups。其中主节点把持久化的操作划分为了两个区域（或者以状态进行标识），同步区（Synced，图中绿色表示）和未同步区（Unsynced，图中白色表示），便于同步操作的执行。 Backups：其他副本节点，主要接受来自 Primary 同步或者异步的复制操作。 Witnesses：节点集群，接收客户端的 op request，将对接收到的请求和节点中已经持久化的 op request 进行交换律的验证，如果满足交换律，则对应的将该操作持久化到该节点中，否则返回 reject，由客户端处理该响应后向 Primary 节点发起 SYNC RPC，主动地触发 Primary 节点向 Backups 节点的复制，大多数副本节点返回成功后对应地向客户端返回成功。 Witnesses 集群 关于 Witness 节点数量的设置，取决于该模型中副本的个数（除去主副本），从而保证故障恢复情况下，整个系统能对应地表现出相应的容错率。在实际部署过程中，可以考虑 Witness 节点是否和 Backup 节点在一台物理机或者虚拟机上。 为了避免掉电时数据丢失， Witnesses 节点一般部署在 NVM 上，由于是临时数据，数据量不大，且对 IO 执行的速度有一定的要求（响应客户端的请求以及故障恢复时需要较快地读取数据进行恢复），故选择了 NVM 作为存储器件。 执行情况描述 正常执行（可交换的操作执行）： Client 端的表现：并发地发出请求，等待 Primary 和所有的 Witnesses 节点返回成功，则对应的确认该操作成功执行。这种机制也相应地带来了性能瓶颈：虽说是 1RTT 完成了操作的执行，但是具体的时间可能将取决于 Witenesses 以及 Primary 中最后执行成功的节点，同时还要考虑 Witnesses 节点部分执行成功的情况。 如果 Witnesses 集群中仅有部分节点执行成功，则客户端需要向 Primary 中发起 SYNC RPC调用来进行同步。同接收到 Reject 的响应的结果一样。 Witnesses 节点的操作：处理来自客户端的 Recored 请求，判断该请求和当前持久化在节点中的请求是否有冲突，没有则继续持久化到节点中，相应地返回 Accepted。 Primary 节点的操作：执行对应的操作，持久化到节点中，并发送执行成功的响应给客户端，异步地进行主从的复制，并修改该节点中每个操作的同步状态，定期地向 Witnesses 发送 GC 请求。 与之前的操作存在冲突时执行： Client 端的表现：在接收到来自 Witnesses 节点的 Reject 之后，发起 SYNC RPC，让 Primary 节点把其中 Unsynced 的操作进行对应的同步操作，并等待接收同步成功的请求。 在处理 Reject 的响应中，整个模型的流程将退化为 2RTT，因为多了一次主从之间的同步操作；但在最坏的场景（发送 SYNC RPC 之后发现 Primary 节点宕机）下可能恶化为 3RTT。 Witnesses 节点的操作：在判断操作与现有的冲突之后，对应的发送 Reject 的响应，并等待 Primary 节点发起 GC 请求，把 Witnesses 中已经同步了的操作进行清除。 Primary 节点的操作：先执行对应的操作，持久化到节点中，并发送执行成功的响应给客户端，当收到来自客户端的 SYNC RPC 时，对应地将节点中 Unsynced 的数据同步到副本节点中，并向客户端发送 Synced 响应。 读操作的执行 读操作和写操作的流程大致相同，核心思想都是要先检查读操作和已有的未同步 Unsynced 操作之间是否存在冲突，如果存在则需要 Witnesses 拒绝掉 Record 请求，客户端发送 SYNC RPC来及时进行数据的同步。 基于副本数据的一致读实现 背景：主从复制的集群中，客户端的读操作往往都是在单节点上进行处理的，由主节点接管对应的读写请求来保证数据的一致性。但往往出于负载的考虑，部分分布式系统允许客户端从副本节点上去读取数据，从而缓解主节点上的压力，以及针对多数据中心的情况，一定程度上减少读取操作的开销，提升其响应操作。 CURP 方案中的问题：由于 CURP 方案为了提升读写操作的响应速度缩减到 1RTT，所以允许读写操作在完全同步到副本节点之间就完成。这样的机制虽然提高了响应速度，但与此同时也破坏了线性性。 CURP方案中的实现：为了避免客户端读取到老旧的数据，客户端利用了就近的 Witness 节点（一般为和副本节点在同一台物理机或者虚拟机上的 Witness 节点）来保证数据是最新的。 大致流程： 客户端向副本节点发起读操作之前，需要先向 Witness 节点发起请求询问是否和现有的 Unsynced 操作冲突； 如果不冲突，即此时副本节点上要读取的该数据是最新的，即可以直接向副本节点发起读请求； 如果冲突了，即存在最新的更新还未来得及同步到所有副本节点，故只能从主节点获取最新的数据。 故障恢复流程 Primary 节点宕机 故障恢复大致可以分为三个阶段： (1) restoration from backups：首先从 Backups 中选举出新的 Primary 节点，恢复对应的数据 (2) replay from witnesses：从 Witnesses 节点中选取一个节点，要求该节点停止接收客户端请求，进行相应操作的回放，保证数据的一致性 (3) Sync backups and reset witnesses：完成恢复以后，向其他副本节点异步地进行同步，同时重置当前这组 Witnesses 集群，或者要求管理节点分配一组新的 Witnesses 集群。 副本节点宕机 副本节点宕机和其他非 CURP 模型的处理方式一样，相应地需要将其踢出副本集群，并针对该节点执行后续的故障恢复操作。 Witnesses 节点宕机 Witness 节点宕机后，将由 系统管理节点 将该节点踢出集群，并分配一个新的 Witness 节点给节点数为 f-1 的 Witnesses 集群，对应的通知 Primary 节点更新其维护的所对应的 Witnesses 集群列表，Primary 节点将还未同步的数据同步到副本节点，相应地触发 GC，告知系统管理节点现在可以正常服务之后，恢复 Witnesses 节点的服务功能。 为了保证客户端维护的 Witnesses 集群信息缓存为最新，使用递增的版本号 WitnessListVersion 来进行控制。每次 Witnesses 集群信息发生改变时，相应地增加版本号，并通知到 Primary 节点。每次客户端向 Master 发起请求时，携带并发访问的 Witnesses 集群版本号，由 Primary 节点来判断是否为最新的 Witnesses 信息，如果不是最新，相应地返回客户端需要更新 Witnesses 集群信息的响应，再由客户端主动拉取 Witnesses 集群信息更新本地缓存。 数据迁移 出于负载均衡上的考虑，主节点可能将自己的数据拆分成两个分区上的数据，并把部分数据迁移到另外一个主节点上。 迁移的流程 在处理相关服务请求的同时z准备拷贝数据； 暂停该节点的相关服务进行数据迁移。 其他节点 为了简化数据迁移的场景以及减小数据迁移带来的不可靠因素的影响，在进行最后的数据迁移操作之前，主节点需要先将该节点上未同步的数据及时进行同步，确保副本节点上数据最新。 针对客户端可能访问到原有的主节点的情况，此时原主节点拒绝一切来自客户端的请求，并获取最新的主节点信息返回给客户端，告知其最新的集群信息，然后客户端向新的集群发起该次操作的重试。 核心操作的实现 Witnesses 节点如何判断操作之间是否有冲突？ 本篇 Paper 提出的模型只能针对 能够进行顺序依赖判断的简单操作 进行优化，如 KV 型的 IO 操作，可以简单地通过比较每次操作的 KEY 是否相同来进行判断，使用简单的 HASH 函数即可实现。而针对部分需要先进行计算之后才能判断是否具有依赖关系的操作不能进行很好的判断。例如 SQL 语句使用了不同的限定条件（“UPDATE T SET rate = 40 WHERE level = 3” and “UPDATE T SET rate = rate + 10 WHERE dept = SDE”） Witnesses 节点在出现数据不一致的情况下如何进行处理？ Client 需要等待所有的 Witnesses 节点 Record 成功之后才会进行下一步操作，如果其中接收到了任何一个 Reject 操作，都会调用 SYNC RPC 来进行主从同步复制。 在故障恢复过程中，只会从 Witenesses 节点中选择一个节点来进行操作的回放操作，所以也避免了不一致的情况。 故障恢复过程中如何避免操作重复执行？ CURP 利用了 RIFL 提供的保证只执行一次的机制。在 RIFL 中，客户端会为每一次 RPC 调用分配一个全局唯一标识 ID，在 Server 端对应的记录已经完成的 RPC 调用的 ID，从而便于之后恢复过程中对该操作是否执行过进行判断。 GC 垃圾回收的时机 Primary 节点会根据 Synced 的操作请求的 RPC ID，向 Witnesses 节点发起 GC 的请求，批量地进行垃圾回收以减小 Witnesses 中冲突的可能性。 在 NoSQL 中的实现 Witness 的生命周期和需要执行的操作 // CLIENT TO WITNESS: record(masterID, list of keyHash, rpcId, request) -&gt; ! {ACCEPTED or REJECTED} // Saves the client request (with rpcId) of an update on keyHashes. // Returns whether the witness could accomodate and save the request. // MASTER TO WITNESS: gc(list of {keyHash, rpcId}) -&gt; list of request // Drops the saved requests with the given keyHashes and rpcIds. // Returns stale requests that haven’t been garbage collected for a long time. getRecoveryData() -&gt; list of request // Returns all requests saved for a particular crashed master. // CLUSTER COORDINATOR TO WITNESS: start(masterId) -&gt; {SUCCESS or FAIL} // Start a witness instance for the given master, and return SUCCESS. // If the server fails to create the instance, FAIL is returned. end() -&gt; NULL // This witness is decommissioned. Destruct itself Witness 的数据结构 Witness 的写入操作类似于缓存的级联操作，一次 Witness 的写入请求，会对应地将该请求中操纵的对象的主键进行 HASH（64位哈希值），然后根据哈希值计算当前 Witness 节点中是否已存在 Slot（比较哈希值），即是否存在操作同一个对象的请求。如果有说明操作冲突相应地拒绝掉该请求，没有则对应地写入该操作。 主节点上的操作可交换性检查（冲突检测） 由于 CURP 中的主节点存储的数据存在两种状态，SYNCED 和 UNSYNCED。故主节点可以通过判断操作的的数据对象的状态是否已同步来判断出该操作是否会冲突。 如果操作的对象的实际值是以日志的形式存储的，主节点可以通过比较日志中上次同步的位置和对象存储的值的现在的位置来比较。 如果操作的对象的实际值不是以日志的形式存储的，主节点可以利用单调递增的时间戳来进行比较判断。当主节点更新对象的值后，对应地更新一条元数据信息，类似于最近修改时间一样的时间戳属性，同时主节点也要记录最近一次同步的开始时间戳，通过比较这两个时间戳来判断是否已同步。 CURP 提高主节点的吞吐量方式 由于主从复制集群中，主节点往往是整个集群的性能瓶颈，故为了提高主节点的吞吐量，CURP 采用了以下几种方式： 进行批量同步操作 CURP 中采用了延迟同步的策略，所以不用每次操作之后都进行相应的同步操作。在无冲突异步复制的场景下，设定一个阈值，当主节点中 Unsynced 的数据个数达到该阈值时才对应地触发同步操作；在存在冲突主动复制的场景下，将此时主节点中未同步的数据全部进行同步（一般小于异步复制时设定的阈值）。 无论哪种场景，都采用了批量同步的情况，避免了频繁的 RPC 调用，从而提高了主节点的吞吐量。 注意：异步复制时的阈值设定，对主节点吞吐量可能造成比较大的影响。当阈值较大时，每一次操作的冲突可能性越大，也就相应地增大了触发同步复制的可能性。所以需要根据具体的IO访问情况，进行阈值相关的测试，寻找到合适的阈值设定来保证主节点吞吐量最大化。 非阻塞IO 相比于其他主从同步的方案，主节点往往都需要等待复制完成后才进行后续的操作，而在CURP的方案中，主节点无需等待复制操作的完成就可以直接进行下一个请求的处理，一定程度上也提升了主节点的吞吐量。 GC 垃圾回收的实现 使用操作对象主键对应的 64 位哈希值和 RIFL 分配的 RpcID 来定位需要回收的垃圾数据，在收到对应的 GC 命令后，使用 keyHash 定位对象所属的 Slot，然后再根据 RpcID 来删除掉匹配的请求数据。 尽管简单的 GC 机制已经可以清除掉绝大多数垃圾，但是针对一些特殊情况如网络上的延迟以及客户端的一些宕机情况导致 Witness 节点存在未能正确清楚的垃圾数据，为了避免造成不一致问题，Witness 提供了相应的检测机制。 由于存在垃圾数据，故新请求被 Reject 的概率增大，如果发现针对某给对象的请求多次遇到冲突被拒绝，Witness 节点就将该对象标记为可能未正确回收的垃圾数据，在下一次主节点发送垃圾回收命令时相应地将这些未回收的数据信息返回给主节点，在下一次垃圾回收时，主节点将一并处理之前未回收的垃圾数据。 问题讨论 1. 可不可以让Witnesses 节点存储全量的数据，不区分操作是否冲突，直接异步地进行同步。 2. Witness 节点是以什么样的形式记录操作的？为何不合并相关写操作，记录最终的写数据？ 3. 从设计角度来讲，CURP方案应该是有很大的尾延迟，为什么测试出来的尾延迟反而有提升？ 4. 读过程是否可以进行优化，直接读取witness上的数据？ 5. 多并发写相同数据时，系统的运行情况 ","link":"https://blog.shunzi.tech/post/exploiting-commutativity-for-practical-fast-replication/"},{"title":"重拾Docker","content":" 本篇博文主要是针对 Docker 相关进行一次系统地总结以及一些实战过程中的记录 主要参考 Docker 官方文档，会结合一些简单的实战例子讲解部署的相关流程。 后续可能针对 Docker 和 K8S 容器编排的结合进行讲解，包括一些自动化平台的集成。 Overview Edition Docker Engine - Community is ideal for individual developers and small teams looking to get started with Docker and experimenting with container-based apps. Docker Engine - Enterprise is designed for enterprise development of a container runtime with security and an enterprise grade SLA in mind. Docker Enterprise is designed for enterprise development and IT teams who build, ship, and run business critical applications in production at scale. This post is built based on Docker Engine - Community Docker Engine - Community Docker Engine - Community is ideal for developers and small teams looking to get started with Docker and experimenting with container-based apps. Docker Engine - Community has three types of update channels, stable, test, and nightly. Supported platforms: Linux(CentOS/Debian/Fedora/Ubuntu)、Windows、MacOS How to get it(Docker Engine - Community) for CentOS? Prerequisites To install Docker Engine - Community, you need a maintained version of CentOS 7. Archived versions aren’t supported or tested. The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it. The overlay2 storage driver is recommended. Uninstall Old Versions $ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine Install You can install Docker Engine - Community in different ways, depending on your needs: (In this post, only introduce the way of using th repository) Most users set up Docker’s repositories and install from them, for ease of installation and upgrade tasks. This is the recommended approach. Some users download the RPM package and install it manually and manage upgrades completely manually. This is useful in situations such as installing Docker on air-gapped systems with no access to the internet. In testing and development environments, some users choose to use automated convenience scripts to install Docker. Step 1. Install required packages. $ sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 Step 2. Set up the stable repository. (yum-config-manager is provided by yum-utils) $ sudo yum-config-manager \\ --add-repo \\ https://download.docker.com/linux/centos/docker-ce.repo Step 3. Install Docker Engine # the latest version of Docker Engine - Community and containerd. $ sudo yum install docker-ce docker-ce-cli containerd.io ------------------------------------------------------------------- # install a specific version of Docker Engine - Community, list the available versions in the repo, then select and install $ yum list docker-ce --showduplicates | sort -r docker-ce.x86_64 3:18.09.1-3.el7 docker-ce-stable docker-ce.x86_64 3:18.09.0-3.el7 docker-ce-stable docker-ce.x86_64 18.06.1.ce-3.el7 docker-ce-stable docker-ce.x86_64 18.06.0.ce-3.el7 docker-ce-stable $ sudo yum install docker-ce-&lt;VERSION_STRING&gt; docker-ce-cli-&lt;VERSION_STRING&gt; containerd.io Step 4. Run and Verify # Start docker $ sudo systemctl start docker # Run hello-world demo $ sudo docker run hello-world Another Step: Uninstall Docker Engine - Community sudo yum remove docker-ce # Images, containers, volumes, or customized configuration files on your host are not automatically removed. To delete all images, containers, and volumes: sudo rm -rf /var/lib/docker How to get it for Windows And MacOS? Install desktop application and run demo repo. Get Started Docker Concepts Docker is a platform for developers and sysadmins to build, share, and run applications with containers. The use of containers to deploy applications is called containerization. Containers are not new, but their use for easily deploying applications is. Images and Containers Fundamentally, a container is nothing but a running process, with some added encapsulation features applied to it in order to keep it isolated from the host and from other containers. One of the most important aspects of container isolation is that each container interacts with its own, private filesystem; this filesystem is provided by a Docker image. An image includes everything needed to run an application -- the code or binary, runtimes, dependencies, and any other filesystem objects required. Differences between Docker And VM A container runs natively on Linux and shares the kernel of the host machine with other containers. It runs a discrete process, taking no more memory than any other executable, making it lightweight. A virtual machine (VM) runs a full-blown “guest” operating system with virtual access to host resources through a hypervisor. In general, VMs incur a lot of overhead beyond what is being consumed by your application logic. Enable K8S Direct Way: In docker-desktop settings, enable the K8S. And you may meet some problems when enable K8S. You can visit Github:k8s-for-docker-desktop for more help info. Since the network problem, you may need to download K8S images mannually. And this script may be useful for you. #!/bin/bash set -e # Check version in https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ # Search &quot;Running kubeadm without an internet connection&quot; # For running kubeadm without an internet connection you have to pre-pull the required master images for the version of choice: KUBE_VERSION=v1.14.6 KUBE_DASHBOARD_VERSION=v1.10.1 KUBE_PAUSE_VERSION=3.1 ETCD_VERSION=3.1.12 DNS_VERSION=1.14.8 GCR_URL=k8s.gcr.io ALIYUN_URL=registry.cn-hangzhou.aliyuncs.com/google_containers images=(kube-proxy-amd64:${KUBE_VERSION} kube-scheduler-amd64:${KUBE_VERSION} kube-controller-manager-amd64:${KUBE_VERSION} kube-apiserver-amd64:${KUBE_VERSION} pause-amd64:${KUBE_PAUSE_VERSION} etcd-amd64:${ETCD_VERSION} k8s-dns-sidecar-amd64:${DNS_VERSION} k8s-dns-kube-dns-amd64:${DNS_VERSION} k8s-dns-dnsmasq-nanny-amd64:${DNS_VERSION} kubernetes-dashboard-amd64:${KUBE_DASHBOARD_VERSION}) for imageName in ${images[@]} ; do docker pull $ALIYUN_URL/$imageName docker tag $ALIYUN_URL/$imageName $GCR_URL/$imageName docker rmi $ALIYUN_URL/$imageName done docker images Manage Docker Service service docker start # Start docker service，daemon process service docker stop # stop docker service service docker status # check docker running status chkconfig docker on # Enable auto-start with boot Container Management # list all running conatiners docker container ls # list all containers including running and stopped docker container ls --all docker start [containerID/Names] # Start container docker stop [containerID/Names] # Stop container docker rm [containerID/Names] # Remove container docker logs [containerID/Names] # View the log of container docker exec -it [containerID/Names] /bin/bash # Enter in container with bash # Copy file from remote path of running container to local path(. means cuurent directory) docker container cp [containID]:[/path/to/file] . # Exec `echo &quot;hello world&quot;` or other command in container[eg.centos] docker run centos echo &quot;hello world&quot; docker run centos yum install -y wget # List all containers docker ps docker ps -a docker run -i -t centos /bin/bash # 启动一个容器 docker inspect centos # 检查运行中的镜像 docker commit 8bd centos # 保存对容器的修改 docker commit -m &quot;n changed&quot; my-nginx my-nginx-image # 使用已经存在的容器创建一个镜像 # Get the pid with id 44fc0f0582d9 docker inspect -f {{.State.Pid}} 44fc0f0582d9 # Pull the image with given name docker pull gitlab/gitlab-ce:11.2.3-ce.0 ","link":"https://blog.shunzi.tech/post/Docker-Notes/"},{"title":"Ceph中的数据一致性","content":" Ceph 作为分布式存储系统，在可用性和一致性方面有极高的要求。 Ceph 区别于其他分布式数据库等产品，更多的是作为云计算基础设施提供服务。 本文主要介绍 Ceph 中的数据一致性的实现方案，以多副本为例，后续介绍纠删码。 Ceph IO RBD IO 首先以 RBD 为例，回顾一下 Ceph RBD 的整个 IO 过程。 客户端创建对应的存储池 Pool，指定相应的 PG 个数以及 PGP 个数（用于 PG 中的数据均衡） 创建 pool/image rbd设备进行挂载 用户写入的数据进行切块，每个块有默认大小，并且每个块都有一个 Key，Key 就是 object+序号 将每个 object 通过 pg 进行副本位置的分配 PG 根据 cursh 算法会寻找指定个数的 osd（主从个数），把这个 object 分别保存在这些 osd 上 osd 上实际是把底层的 disk 进行了格式化操作，一般部署工具会将它格式化为 xfs 文件系统 object 的存储就变成了存储一个文件 rbd0.object1.file 一致性实现方式 Ceph 中的数据一致性实现主要依赖底层对象存储系统中的数据一致性保证。在 Ceph 中对应的则是 RADOS 在一致性上的实现，RADOS 作为 Ceph 存储的核心，主要提供了多副本和纠删码的方式来保证一致性， 主要分类 多副本一致性协议（以 PG 为逻辑存储单元的 OSD 多副本） EC 纠删码一致性协议 多副本一致性协议 RADOS 提供的数据复制策略：主拷贝、链式复制、Splay树复制 其中在 Ceph 中主要采用的为 主拷贝 方式。 主要流程 librados 向主 OSD 写入分好块的二进制数据块 (先建立TCP/IP连接，然后发送消息给 OSD，OSD 接收后写入其磁盘) 主 OSD 负责同时向一个或者多个次 OSD 写入副本。注意这里是写到日志（Journal）就返回，因此，使用SSD作为Journal的话，可以提高响应速度，做到服务器端对客户端的快速同步返回写结果（ack） 当主次OSD都写入完成后，主 OSD 向客户端返回写入成功。 当一段时间（也许得几秒钟）后Journal 中的数据向磁盘写入成功后，Ceph通过事件通知客户端数据写入磁盘成功（commit），此时，客户端可以将写缓存中的数据彻底清除掉了。 默认地，Ceph 客户端会缓存写入的数据直到收到集群的commit通知。如果此阶段内（在写方法返回到收到commit通知之间）OSD 出故障导致数据写入文件系统失败，Ceph 将会允许客户端重做尚未提交的操作（replay） 实现方式 Ceph 使用 PGLog 来保证多副本之间的一致性。PGLog是由PG来维护，记录了该PG的所有操作，其作用类似于数据库里的undo log。PGLog通常只保存近千条的操作记录（默认是3000条），但是当PG处于降级状态时，就会保存更多的日志（默认是10000条），这样就可以在故障的PG重现上线后用来恢复PG的数据。 数据结构 pglog的示意图如下：pglog主要是用来记录做了什么操作，比如修改，删除等，而每一条记录里包含了对象信息，还有版本。 ceph使用版本控制的方式来标记一个PG内的每一次更新，每个版本包括一个(epoch，version)来组成：其中epoch是osdmap的版本，每当有OSD状态变化如增加删除等时，epoch就递增；version是PG内每次更新操作的版本号，递增的，由PG内的Primary 属性： last_complete：在该指针之前的版本都已经在所有的OSD上完成更新（只表示内存更新完成）； last_update：PG内最近一次更新的对象的版本，还没有在所有OSD上完成更新，在last_update与last_complete之间的操作表示该操作已在部分OSD上完成但是还没有全部完成； log_tail：指向pg log最老的那条记录； head：最新的pg log记录； tail：指向最老的pg log记录的前一个； log：存放实际的pglog记录的list； 存储方式 在ceph的实现里，对于写I/O的处理，都是先封装成一个transaction，然后将这个transaction写到journal里，在journal写完成后，触发回调流程，经过多个线程及回调的处理后再进行写数据到buffer cache的操作，从而完成整个写journal和写本地缓存的流程。 总体来说，PGLog也是封装到transaction中，在写journal的时候一起写到日志盘上，最后在写本地缓存的时候遍历transaction里的内容，将PGLog相关的东西写到Leveldb里，从而完成该OSD上PGLog的更新操作。 写I/O和PGLog都会序列化到transaction里的bufferlist里，这里就对这个bufferlist里的主要内容以图的形式展示出来。transaction的bufflist里就是按照操作类型op来序列化不同的内容，如OP_WRITE表示写I/O，而OP_OMAPSETKEYS就表示设置对象的omap，其中的attrset就是一个kv的map。 注意这里面的oid，对于pglog来说，每个pg在创建的时候就会生成一个logoid，会加上pglog构造的一个对象，对于pginfo来说，是pginfo_构造的一个对象，而对于真正的数据对象来说，attrset就是其属性。 流程细化 写流程：以 FileStore 后端存储为例 1）client把写请求发到Primary OSD上，Primary OSD上将写请求序列化到一个事务中（在内存里），然后构造一条pglog记录，也序列化到这个事务中，然后将这个事务以directIO的方式异步写入journal，同时Primary OSD把写请求和pglog(pglog_entry是由primary生成)发送到Replicas上； 2）在Primary OSD将事务写到journal上后，会通过一系列的线程和回调处理，然后将这个事务里的数据写入filesystem（只是写到文件系统的缓存里，会有线程定期刷数据），这个事务里的pglog记录（也包括pginfo的last_complete和last_update）会写到leveldb，还有一些扩展属性相关的也在这个事务里，在遍历这个事务时也会写到leveldb； 3）在Replicas上，也是进行类似于Primary的动作，先写journal，写成功会给Primary发送一个committed ack，然后将这个事务里的数据写到filesystem，pglog与pginfo写到leveldb里，写完后会给Primary发送另外一个applied ack； 4）Primary在自己完成journal的写入时，以及在收到Replica的committed ack时都会检查是否多个副本都写入journal成功了，如果是则向client端发送ack通知写完成；Primary在自己完成事务写到文件系统和leveldb后，以及在收到replica的applied ack时都会检查是否多个副本都写文件系统成功，如果是则向client端发送ack通知数据可读； 读流程：由于实现了强一致性，主节点和从节点的数据基本完全一致，故在读取时采用了随机的方式进行OSD的选取，然后读取对应的数据。 PGLog封装到transaction里面和journal一起写到盘上的好处：如果osd异常崩溃时，journal写完成了，但是数据有可能没有写到磁盘上，相应的pg log也没有写到leveldb里，这样在osd再启动起来时，就会进行journal replay，这样从journal里就能读出完整的transaction，然后再进行事务的处理，也就是将数据写到盘上，pglog写到leveldb里。 故障恢复 基于pglog的一致性协议包含两种恢复过程，一个是Primary挂掉后又起来的恢复，一种是Replica挂掉后又起来的恢复。 Primary 故障恢复 1）正常情况下，都是由Primary处理client端的I/O请求，这时，Primary和Replicas上的 last_update 和 last_complete 都会指向pglog最新记录； 2）当Primary挂掉后，会选出一个Replica作为“临时主”，这个“临时主”负责处理新的读写请求，并且这个时候“临时主”和剩下的Replicas上的 last_complete 和 last_update 都更新到该副本上的pglog的最新记录； 3）当原来的Primary又重启时，会从本地读出pginfo和pglog，当发现 last_complete &lt; last_update 时，last_complete 和 last_update 之间就可能存在丢失的对象，遍历 last_complete 到 last_update 之间的pglog记录，对于每一条记录，从本地读出该记录里对象的属性（包含本地持久化过的版本），对比pglog记录里的对象版本与读出来的版本，如果读出来的对象版本小于pglog记录里的版本，说明该对象不是最新的，需要进行恢复，因此将该对象加到missing列表里； 4）Primary发起peering过程，即“抢回原来的主”，选出权威日志，一般就是“临时主”的pglog，将该权威日志获取过来，与自己的pglog进行 merge_log 的步骤，构建出missing列表，并且更新自己的 last_update 为最新的pglog记录（与各个副本一致），这个时候 last_complete 与 last_update 之间的就会加到missing列表，并且peering完成后会持久化 last_complete 和 last_update； 5）当有新的写入时，仍然是由Primary负责处理，会更新 last_update，副本上会同时更新 last_complete，与此同时，Primary会进行恢复，就是从其他副本上拉取对象数据到自己这里进行恢复，每当恢复完一个时，就会更新自己的 last_complete（会持久化的），当所有对象都恢复完成后，last_complete 就会追上 last_update 了。 6）当恢复过程中，Primary又挂了再起来恢复时，先读出本地pglog时就会根据自己的 last_complete 和 last_update 构建出missing列表，而在peering的时候对比权威日志和本地的pglog发现权威与自己的 last_update 都一样，peering的过程中就没有新的对象加到missing列表里，总的来说，missing列表就是由两个地方进行构建的：一个是osd启动的时候 read_log 里构建的，另一个是peering的时候对比权威日志构建的； Replica 故障恢复 与Primary的恢复类似，peering都是由Primary发起的，Replica起来后也会根据pglog的 last_complete 和 last_update 构建出replica自己的missing，然后Primary进行peering的时候对比权威日志（即自身）与故障replica的日志，结合replica的missing，构建出 peer_missing，然后就遍历 peer_missing 来恢复对象。然后新的写入时会在各个副本上更新 last_complete和 last_update，其中故障replica上只更新 last_update，恢复过程中，每恢复完一个对象，故障replica会更新 last_complete，这样所有对象都恢复完成后，replica的 last_complete 就会追上 last_update。 如果恢复过程中，故障replica又挂掉，然后重启后进行恢复的时候，也是先读出本地log，对比 last_complete 与 last_update之间的pglog记录里的对象版本与本地读出来的该对象版本，如果本地不是最新的，就会加到missing列表里，然后Primary发起peering的时候发现replica的 last_update 是最新的，peering过程就没有新的对象加到 peer_missing 列表里，peer_missing 里就是replica自己的missing里的对象。 参考文献 [1] System Notes：ceph解读之PGLog [2] System Notes：ceph基于pglog的一致性协议 [3] Ceph IO：LOG BASED PG ","link":"https://blog.shunzi.tech/post/ceph-consistency/"},{"title":"Tail Latency","content":" 在相关课程以及论文阅读过程中都遇到了尾延迟问题，借此机会深入学习一下 国内尾延迟资料相对较少，针对部分参考资料进行翻译搬运 埋坑，看一些尾延迟相关研究和论文，未完待续。 btw 针对部分论文和实验中关于尾延迟的测试进行阐述 Preventing Long Tail Latency（译） 引言 最近，我们有一个客户跟我们求助说，他和之前的一个CDN提供商一直在努力解决延迟时间中的长尾问题。 他们有一小部分客户经历了长达30秒的加载时间，这对他们而言是“完全不能接受的”。 对于这个特殊的客户，降低总体平均延迟是很重要，但对他们来说更重要的是控制异常值。 什么是长尾延迟？以及是什么原因导致了长尾延迟？ 尾部延迟总是用百分数来表示；长尾延迟指的是与平均延迟时间相比延迟的更高百分位数(例如 第98、第99)。当查看您的分析仪表板时，您可能会注意到它写着类似“每个请求，1% 的用户平均会经历一秒钟的延迟”。 对于服务提供商来说，缩短延迟分布的尾部可能是一个挑战，特别是对于大规模的交互式服务。由谷歌杰出工程师 Luiz André Barroso 领导的一项研究发现，系统规模越大，延迟可变性越大。当系统的规模和复杂性扩大时，服务提供商更难提供一致性。这不仅仅是随着总体使用量的增加而扩大规模的问题。譬如，当并行处理请求时，并行操作的长尾分布会立即成为一个问题，并占据整个响应时间。每个响应必须具有一致且低的延迟，否则整个操作响应时间将非常慢。高性能导致高的容忍度，这意味着您的整个系统需要按照严格的标准进行设计。 长尾延迟的其他原因包括: 共享资源：如果机器由不同的应用程序共享，所有这些应用程序都在同一应用程序内争夺相同的共享资源(例如内存或网络带宽、处理器缓存、中央处理器内核)，不同的请求可能最终会争夺相同的资源。 全局资源共享：运行在不同机器上的应用程序可能会争夺全局资源(例如共享文件系统、网络交换机)。 后台守护程序：后台守护程序通常会使用特定数量的资源；然而，如果计划好了，它还是们可能会出现几毫秒的不可用状态。即使这种情况很少，但也是可能影响大规模分布式系统中的大量请求的。 长尾延迟的影响 为什么尾延迟比平均延迟更重要？Gil Tene 在东南电台的一个关于尾部延迟的播客中简洁地说:这是“因为人类不会感知到事件的平均结果或者平均影响；并不会因为平均水平比较高而忽略掉自己在该事件中痛苦和糟糕的经历”。 能够快速响应用户操作(100毫秒以内)的系统对用户来说比耗时更长的系统更自然更容易让人接受。响应是问题的关键。对于用户而言，系统响应速度的平均值较高是不够的，不足以提供好的用户体验。亚马逊开展的一项研究涉及到在账户测试中以100毫秒为增量延迟页面，即使微小的延迟也会导致收入大幅下降，代价高昂。同样，谷歌运行的一项研究显示，加载时间延迟半秒钟导致流量下降20%。 在 Barroso 的研究中，他用一个例子来说明尾延迟会有多长: 想象一个场景，一个客户端向一个单独的网络服务器发出请求。百分之九十九的请求将在合理的时间内都会接受到对应的响应。然而，百分之一的请求的响应时间可能会很慢。如果您检查延迟的分布情况，可以发现大多数请求收到响应的延迟是小的，但是在请求分布的末尾部分可能有一个大的响应延迟。这其实倒没有什么太大的影响。这仅仅意味着每隔一段时间就会有一个客户收到稍微慢一点的回复。 但是，假设您有数百万个对多个服务器的请求。现在，10K受到影响，而不是只有一个客户的响应速度较慢，这极大地改变了尾部延迟的影响。 使用相同的组件并扩大相应的规模会导预料不到的结果。这是可扩展系统的基本特性:高性能等于高公差。在大的规模情况下，不能忽视尾部延迟带来的影响。 如何解决长延迟？ 完全消除导致延迟可变性问题的所有根源在大规模场景或共享环境中是不切实际的；然而，可以实现尾部延迟容忍软件技术，以帮助从不太可预测的部分中形成可预测的整体（实现尾延迟的预测）。 多种尾部容忍技术来解决长尾部延迟问题，包括: 针对不同环境的隔离：使用容器 所有容器的内存限制：防止给整个系统带来压力 K8S-管理数据项的多个副本的部署，并提供高可用性体系结构 广播域名解析：允许域名系统查询路由到最近的数据中心，以实现快速连接域名系统连接并改进网站性能 更高层次的解决方案 Section使用的一个更高级别的独特解决方案是以一种缩短首字节的获取时间的方式来缓存HTML。我们的竞争对手不使用同样的方法，倾向于更多地关注事物的静态资产方面。当试图消除长尾延迟时，缓存超文本标记语言是必不可少的一步。我们发现，让用户尽可能快地获得HTML是减少页面加载时间的最好方法。 另一种减少首字节获取时间的方法是我们通过全球边缘网络在任何地方加速 PoPs 。我们的网络目前包括北美和南美、欧洲、亚洲和澳大拉西亚的60 多个 PoPs，我们都能够按需创造新的 PoPs。 第三，缓慢的加载时间可以归因于第三方的资源。在 Section，我们致力于提供关于如何在页面加载事件后延迟这些资源的建议或者将第三方资源带到客户端域并缓存它们，从而保证更快的交互。 最后，我们始终通过部门的综合RUM数据提供关于长尾延迟的出色可视化功能，这为我们的客户提供了帮助控制长尾异常值所需的洞察力。 参考文献 [1] 翻译原文链接：Section IO : Preventing Long Tail Latency [2] the morning paper：RobinHood: tail latency aware caching [3] Google: Taming The Long Latency Tail - When More Machines Equals Worse Results [4] Microsoft：Managing Tail Latency in Datacenter-Scale FileSystems Under Production Constraints ","link":"https://blog.shunzi.tech/post/tail-latency/"},{"title":"分布式系统中的一致性","content":" 源于分布式存储系统中对一致性的严格要求，该篇针对 CAP 理论中的一致性做简单总结。 源于分布式存储系统中对一致性的严格要求，该篇针对 CAP 理论中的一致性做简单总结。 结合行业中为了保证一致性所采用的方案进行实例讲解，诸如分布式数据库等产品 同 ACID 中的事务一致性进行对比总结，进一步理解分布式系统 一致性问题前情提要 并发性和一致性 并发性：指同一个时间段内哪些任务可以同时执行，并且都没有执行结束。例如，任务1和任务2同时执行 一致性：确保执行完毕后每个计算节点（或用户）都能看到一致的数据视图。例如，任务3和任务5的不同执行顺序可能导致完全不同的计算结果 并发时序 分布式解决了单节点性能和容量的问题，但也带来了节点间时序关系的难题 时钟不一致：分布式系统中没有一个绝对的全局时钟，各物理设备上的本地时钟也是不准确，即使设置了时间同步机制，也会存在毫秒级别的偏差，这在金融分布式事务中是不可接受的。在分布式事务中，不精确的并发时序关系可能会导致结果的不一致 Lamport逻辑时钟 与物理时钟的区别：在实际应用中，只要所有机器有相同的时间就够了，这个时间不一定要跟物理时间相同。 如果两个节点之间不进行交互，那么它们的时间甚至都不需要同步。因此问题的关键点在于节点间要在事件发生顺序上达成一致，而不是对时间达成一致 定义（ Leslie Lamport 在1978年提出逻辑时钟的概念）：逻辑时钟指的是分布式系统中用于区分事件发生顺序的时间机制。从某种意义上讲，现实世界中的物理时间其实是逻辑时钟的一个特例 机制 节点（或进程）ni维护一个逻辑时钟Li，由单调递增的软件计数器维护 逻辑时钟可用于为事件添加一个时间戳 事件时间戳大小可表示事件间的时序关系 向量时钟 设计目标：克服Lamport时钟不能进行因果推导的缺点 基本原理：每个事件维护一个向量时钟V(a)=[c1, c2, …, cn] 其中，ci为节点i中因果关系上发在事件e之间的事件数量 什么是一致性问题 背景：今天的业务场景越来越复杂，规模越来越大。在面向大规模复杂任务场景时，单点的服务往往难以解决可扩展（Scalability）和容错（Fault-tolerance）两方面的需求，就需要多台服务器来组成集群系统，虚拟为更加强大和稳定的“超级服务器”。集群的规模越大，处理能力越强，管理的复杂度也就越高。目前在运行的大规模集群包括谷歌公司的搜索系统，通过数十万台服务器支持了对整个互联网内容的搜索服务。 宏观层面：通常情况下，集群系统中的不同节点可能处于不同的状态，随时收到不同的请求，要时刻保持对外响应的“一致性”。 微观层面：针对某些具体的业务场景，往往需要多种业务操作按照一定的顺序在一个完整的事务中执行，要么全部成功要么全部失败，如简单数据库事务或者分布式数据库事务场景。或者在多个服务节点以相同的角色对外提供服务时，面对相同时间的相同请求需要确保该角色系统中每一个服务节点的执行情况一致，如主从节点之间的数据一致性。 产生一致性问题的原因 系统层面的原因：分布式系统的引入 分布式系统在带来了相应的处理性能优势（通过将计算资源和存储资源协同，从系统层面解决了数据的“海量”问题以及“快”响应的问题）以及架构上的优势（实现了软件工程领域中高内聚低耦合的思想）的同时，引入了多节点之间的协同工作，相比于单机情况下，也相应地引入了节点之间如何保持业务场景中分布式事务的一致性问题（节点协作），以及多个节点中的数据一致性问题（服务分治）。 根本原因 分布式事务场景：某个业务场景下的功能事务，往往涉及到跨库跨表的数据操作，跨库跨表则相应地依赖各项操作执行的结果，对应地受到所在节点的硬件条件和网络的影响。 主从数据一致性：分布式系统为了保证服务的高可用，往往提供了数据副本，而从主数据复制到副本则是产生不一致性的根本原因，复制操作的策对应地影响系统的一致性表现。 一致性基本概念 定义 一致性（Consistency），早期也叫（Agreement），在分布式系统领域中是指对于多个服务节点，给定一系列操作，在约定协议的保障下，使得它们对处理结果达成“某种程度”的协同。 理想情况（不考虑节点故障）下，如果各个服务节点严格遵循相同的处理协议（即构成相同的状态机逻辑），则在给定相同的初始状态和输入序列时，可以确保处理过程中的每个步骤的执行结果都相同。因此，传统分布式系统中讨论一致性，往往是指在外部任意发起请求（如向多个节点发送不同请求）的情况下，确保系统内大部分节点实际处理请求序列的一致，即对请求进行全局排序。 一致性模型：一致性模型本质上是进程与数据存储的约定：如果进程遵循某些规则，那么进程对数据的读写操作都是可预期的。其中数据存储是指在分布式系统中指分布式共享数据库、分布式文件系统等。 分类 一致性 在互联网与计算机领域中按照业务场景主要有两大分支：分布式系统中 CAP 理论所介绍的数据一致性 和 数据库事务 ACID 中所提及的一致性。 分布式事务一致性，指的是“操作序列在多个服务节点中执行的顺序是一致的”。 分布式数据一致性，指的是“数据在多份副本中存储时，各副本中的数据是一致的”。 一致性 根据不同的要求等级又可以划分为 强一致性 和 弱一致性。 套用一致性模型的定义的话，一致性模型 主要可以分为两类：能够保证所有进程对数据的读写顺序都保持一致的一致性模型称为 强一致性模型，而不能保证的一致性模型称为 弱一致性模型。 强一致性（模型） 强一致性（模型）又可以分为 线性一致性（模型） 和 顺序一致性（模型）。 线性一致性（模型）Linearizability Consistency 线性一致性 也叫 严格一致性（Strict Consistency）或者 原子一致性（Atomic Consistency） Maurice P. Herlihy 与 Jeannette M. Wing 在 1990 年经典论文《Linearizability: A Correctness Condition for Concurrent Objects》中共同提出，在顺序一致性前提下加强了进程间的操作排序，形成唯一的全局顺序（系统等价于是顺序执行，所有进程看到的所有操作的序列顺序都一致，并且跟实际发生顺序一致），是很强的原子性保证。但是比较难实现，目前基本上要么依赖于全局的时钟或锁，要么通过一些复杂算法实现，性能往往不高。 具体要求 任何一次读都能读取到某个数据最近的一次写的数据。 所有进程看到的操作顺序都跟全局时钟下的顺序一致。 顺序一致性（模型）Sequential Consistency Leslie Lamport 1979 年经典论文《How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs》中提出，是一种比较强的约束，保证所有进程看到的全局执行顺序（total order）一致，并且每个进程看自身的执行顺序（local order）跟实际发生顺序一致。例如，某进程先执行 A，后执行 B，则实际得到的全局结果中就应该为 A 在 B 前面，而不能反过来。同时所有其它进程在全局上也应该看到这个顺序。顺序一致性实际上限制了各进程内指令的偏序关系，但不在进程间按照物理时间进行全局排序。 具体要求 任何一次读写操作都是按照某种特定的顺序。 所有进程看到的读写操作顺序都保持一致。 线性一致性和顺序一致性的对比 相同点 都能够保证所有进程对数据的读写顺序保持一致。 不同点 实现保证读写顺序一致的方式不同： 线性一致性的实现很简单，就按照全局时钟（可以简单理解为物理时钟）为参考系，所有进程都按照全局时钟的时间戳来区分事件的先后 顺序一致性使用的是 逻辑时钟 来作为分布式系统中的全局时钟，进而所有进程也有了一个统一的参考系对读写操作进行排序 和实际上发生的顺序的一致性不同： 顺序一致性虽然通过逻辑时钟保证所有进程保持一致的读写操作顺序，但这些读写操作的顺序跟实际上发生的顺序并不一定一致。 线性一致性是严格保证跟实际发生的顺序一致的。 弱一致性（模型） 弱一致性（模型）又可以分为 因果一致性（Causal Consistency）、最终一致性（Eventual Consistency）和以客户端为中心的一致性（Client-centric Consistency）。 因果一致性 Causal Consistency 需要有因果关系的多种操作能够保持顺序一致，其本质是一种弱化的顺序一致性模型。 因果关系 是指操作之间存在一定的依赖关系，譬如自增操作中需要先读取对应的值，再进行自增，此时增加操作和读取操作就存在了因果关系。 具体要求 所有进程必须以相同的顺序看到具有因果关系的读写操作。 不同进程可以以不同的顺序看到并发的读写操作。 与顺序一致性的区别 顺序一致性虽然不保证事件发生的顺序跟实际发生的保持一致，但是它能够保证所有进程看到的读写操作顺序是一样的。而 因果一致性更进一步弱化了顺序一致性中对读写操作顺序的约束，仅保证有因果关系的读写操作有序，没有因果关系的读写操作（并发事件）则不做保证。 也就是说如果是无因果关系的数据操作不同进程看到的值是有可能是不一样，而有因果关系的数据操作不同进程看到的值保证是一样的。 最终一致性 Eventual Consistency 只保证所有副本的数据最终在某个时刻会保持一致。 具体要求 “最终”到底是多久？通常来说，实际运行的系统需要能够保证提供一个有下限的时间范围。 多副本之间对数据更新采用什么样的策略？一段时间内可能数据可能多次更新，到底以哪个数据为准？一个常用的数据更新策略就是以时间戳最新的数据为准。 以客户端为中心的一致性 Client-centric Consistency 以客户端为中心的一致性为单一客户端提供一致性保证，保证该客户端对数据存储的访问的一致性，但是它不为不同客户端的并发访问提供任何一致性保证。 实现方式：客户端缓存。举个例子：客户端 A 在副本 M 上读取 x 的最新值为 1，假设副本 M 挂了，客户端 A 连接到副本 N 上，此时副本 N 上面的 x 值为旧版本的 0，那么一致性模型会保证客户端 A 读取到的 x 的值为 1，而不是旧版本的 0。一种可行的方案就是给数据 x 加版本标记，同时客户端 A 会缓存 x 的值，通过比较版本来识别数据的新旧，保证客户端不会读取到旧的值。 以客户端为中心的一致性包含了四种子模型： 单调读一致性（Monotonic-read Consistency），单调写一致性（Monotonic-write Consistency），读写一致性（Read-your-writes Consistency），写读一致性（Writes-follow-reads Consistency） 单调读一致性（Monotonic-read Consistency） 如果一个进程读取数据项 x 的值，那么该进程对于 x 后续的所有读操作要么读取到第一次读取的值要么读取到更新的值。即保证客户端不会读取到旧值。单调读比强一致性更弱，比最终一致性更强。 实现方式：确保每个用户总是从同一个节点进行读取（不同的用户可以从不同的节点读取），比如可以基于用户ID的哈希值来选择节点，而不是随机选择节点。 单调写一致性（Monotonic-write Consistency） 一个进程对数据项 x 的写操作必须在该进程对 x 执行任何后续写操作之前完成。即保证客户端的写操作是串行的。 读写一致性（Read-your-writes Consistency） 一个进程对数据项 x 执行一次写操作的结果总是会被该进程对 x 执行的后续读操作看见。即保证客户端能读到自己最新写入的值。 实现方式： 最简单的方案，对于某些特定的内容，都从主库读。举个例子，知乎个人主页信息只能由用户本人编辑，而不能由其他人编辑。因此，永远从主库读取用户自己的个人主页，从从库读取其他用户的个人主页。 客户端可以在本地记住最近一次写入的时间戳，发起请求时带着此时间戳。从库提供任何查询服务前，需确保该时间戳前的变更都已经同步到了本从库中。如果当前从库不够新，则可以从另一个从库读，或者等待从库追赶上来。 写读一致性（Writes-follow-reads Consistency） 同一个进程对数据项 x 执行的读操作之后的写操作，保证发生在与 x 读取值相同或比之更新的值上。即保证客户端对一个数据项的写操作是基于该客户端最新读取的值。 一致性的实现方式 基本实现思路 分布式事务的一致性 为了保证分布式事务的一致性，主要采用了 两阶段提交协议、三阶段提交协议和 TCC 补偿协议。 现如今也已有带事务功能的消息中间件来辅助实现分布式事务，如 RocketMQ 强一致性 强一致性的实现方式最主要分为两种：一种是主从同步复制，另外一种是多数派的方式。 主从同步复制 流程：主节点接收写请求，主节点复制日志到从节点，主节点等待，直到所有的库都返回相应的执行结果。 问题：一个节点失败， 主节点阻塞，导致整个主从节点对应的集群不可使用，保证了强一致性，但牺牲了可用性。 多数派 流程：每次写入保证写入大于N/2个节点，每次读保证从大于N/2个节点读。 问题：在并发环境下，无法保证系统的正确性，顺序很重要。 最终一致性 为了实现数据的最终一致性，往往需要结合共识算法 Paxos\\Raft 等算法以及 Gossip 去中心化协议等来进行实现。 分布式存储系统中的实现 Azure Cosmos DB Azure Cosmos DB 是一个支持多地部署的分布式NoSQL数据库服务。它提供了丰富的可配置的一致性级别。以下五种一致性级别，从前向后可以提供更低的读写延迟，更高的可用性，更好的读扩展性。 一致性级别 强一致性 保证读操作总是可以读到最新版本的数据（即可线性化） 写操作需要同步到多数派副本后才能成功提交。读操作需要多数派副本应答后才返回给客户端。读操作不会看到未提交的或者部分写操作的结果，并且总是可以读到最近的写操作的结果。 保证了全局的（会话间）单调读，读自己所写，单调写，读后写 读操作的代价比其他一致性级别都要高，读延迟最高 有界旧一致性(bounded staleness) 保证读到的数据最多和最新版本差K个版本 通过维护一个滑动窗口，在窗口之外，有界旧一致性保证了操作的全局序。此外，在一个地域内，保证了单调读。 会话一致性 在一个会话内保证单调读，单调写，和读自己所写，会话之间不保证 会话一致性能够提供把读写操作的版本信息维护在客户端会话中，在多个副本之间传递 会话一致性的读写延迟都很低 前缀一致性 前缀一致保证，在没有更多写操作的情况下，所有的副本最终会一致 前缀一致保证，读操作不会看到乱序的写操作。例如，写操作执行的顺序是A, B, C，那么一个客户端只能看到A, A, B, 或者A, B, C，不会读到A, C，或者B, A, C等。 在每个会话内保证了单调读 最终一致性 最终一致性保证，在没有更多写操作的情况下，所有的副本最终会一致 最终一致性是很弱的一致性保证，客户端可以读到比之前发生的读更旧的数据 最终一致性可以提供最低的读写延迟和最高的可用性，因为它可以选择读取任意一个副本 Cassandra Cassandra 是一个使用多数派协议的NoSQL存储系统，通过控制读写操作访问的副本数和副本的位置，可以实现不同的一致性级别。注意，作为NoSQL系统，Cassandra只提供单行操作的原子性，多行操作不是原子的。下面的读写操作，都是指单行操作。 对于NoSQL系统，一般支持的写操作叫做PUT（有些系统叫做UPSERT）。这个操作的含义是，如果这行存在（通过唯一主键查找），则修改它；如果这行不存在，则插入。这个语义，可以近似（在不考虑二级索引的时候）等价于关系数据库的INSERT ON DUPLICATE KEY UPDATE语句，类比于 MySQL 中的 REPLACE 原语。本文前面所讲的“写操作”也是泛指这种语义。这个语义有什么特殊之处呢？ 第一， 它是幂等的 。所以PUT操作可以重复执行，不怕消息重传。第二， 它是覆盖（overwrite）语义 。所以，NoSQL系统的最终一致性，允许对于同一行数据的写操作可以乱序，只要写操作不断，最终各个副本会一致。而关系数据库的insert和update等修改语句，内部实现都是即需要读也需要写。所以，关系数据库的多副本一致性，假设简单地把SQL修改语句同步到多个副本的方式来实现，必须要以相同的顺序执行才能保证结果一致（当然，实际系统不能这么实现）。 写操作配置 写操作一致性配置定义了对于写操作在哪些副本上成功之后，才能返回给客户端。 ALL: 写操作需要同步到所有副本并应用到内存中。提供了最强的一致性保证，但是单点故障会引起写入失败，造成系统不可用。 EACH_QUORUM: 在每个机房（数据中心）中，写操作同步到多数派副本节点中。在多数据中心部署的集群中，可以在每个数据中心提供QUORUM一致性保证。 QUORUM: 写操作同步到多数派副本节点中。当少数副本宕机的时候，写操作可以持续服务。 LOCAL_QUORUM: 写操作必须同步到协调者节点所在数据中心的多数派副本中。这种模式可以避免多数据中心部署时，跨机房同步引起的高延迟。在单机房内，可以容忍少数派宕机。 ONE: 写操作必须写入最少一个副本中。 TWO: 写操作必须写入至少两个副本中。 THREE: 写操作必须写入至少三个副本中。 LOCAL_ONE: 写操作必须写入本地数据中心至少一个副本中。在多机房部署的集群中，可以达到和ONE相同的容灾效果，并且把写操作限制在本地机房。 读操作配置 每个读操作可以设定如下不同的一致性配置。 ALL: 读操作在全部副本节点应答后才返回给客户端。单点单机会引起写操作失败，造成系统不可用。 QUORUM: 在任何数据中心的一定数量的副本已响应后，返回记录。读操作在多数派副本返回应答后返回给客户端。 LOCAL_QUORUM: 读操作在本机房多数派副本返回应答后返回给客户端。可以避免跨机房访问的高延迟。 ONE: 最近的一个副本节点应答后即返回给客户端。可能返回旧数据。 TWO: 两个副本节点应答后即返回给客户端。 THREE: 三个副本节点应答后返回给客户端。 LOCAL_ONE: 本机房最近的一个副本节点应答后返回客户端。 系统一致性级别 从系统层面来看，Cassandra提供了强一致性和最终一致性两种一致性级别。不考虑多机房因素，通过设置上述读写操作的一致性配置，当写入副本数与读取副本数之和大于总副本数的时候，可以保证读操作总是可以读取最新被写入的数据，即强一致性保证。如果写入副本数与读取副本数之和小于总副本数的时候，读操作可能无法读到最新的数据，而且读操作可能读到比之前发生的读操作更旧的数据，所以这种情况下是最终一致性。 而副本位置是选择整个集群、每个机房还是本地机房等因素，是为了在不同的容灾场景下，对跨机房通讯引入的高延迟进行优化，固有的一致性级别并不受影响。例如，写操作用EACH_QUORUM，读操作用LOCAL_QUORUM，还是提供了强一致性保证，但是不同机房的读操作都变成本地的了，读延迟较低。但是，和写操作用QUORUM模式相比，某个机房发生了多数派宕机（总副本数还是少数派），就会导致写操作失败。再如，读写操作都用LOCAL_QUORUM，那么协调者节点所在机房内是强一致性的，与协调者节点不在一个机房的读操作则可能读到旧数据。 OceanBase OceanBase是一个支持海量数据的高性能分布式数据库系统，实现了数千亿条记录、数百TB数据上的跨行跨表事务，由淘宝核心系统研发、运维、DBA、广告、应用研发等部门共同完成。在设计和实现OceanBase的时候暂时摒弃了不紧急的DBMS的功能，例如临时表，视图(view)，研发团队把有限的资源集中到关键点上，当前 OceanBase主要解决数据更新一致性、高性能的跨表读事务、范围查询、join、数据全量及增量dump、批量数据导入。 一致性级别 强一致性： OceanBase使用Multi-Paxos分布式共识算法在多个数据副本之间同步事务提交日志，每个修改事务，要在多数派副本应答以后才认为提交成功。多个副本之间，通过自主投票的机制，选出其中一个副本为主副本（leader），它负责所有修改语句的执行，特别的，达成多数派的事务提交日志要求包含主副本自己。在通常情况下，数据库需要保证强一致性语义（和单机数据库类比），我们的做法是，读写语句都在主副本上执行。当主副本宕机的时候，其余的多数派副本会选出新的主副本。此时，已经完成的每一个事务一定有至少一个副本记录了提交日志的。新的主副本通过和其他副本的通信可以获得所有已提交事务的日志，进而完成恢复，恢复以后继续提供服务。通过这种机制，OceanBase可以保证在少数派宕机的情况下不会丢失任何数据，而强一致性读写服务的宕机恢复时间小于一分钟。 如果一个语句的执行涉及到多个表的分区，在OceanBase中这些分区的主副本可能位于不同的服务节点上。严格的数据库隔离级别要求涉及多个分区的读请求看到的是一个“快照”，也就是说，不允许看到部分事务。这要求维护某种形式的全局读版本号，开销较大。如果应用允许，可以调整读一致性级别，系统保证读到最新写入的数据，但是不同分区上的数据不是一个快照。从一致性级别来看，这也是强一致性级别，但是打破了数据库事务的ACID属性。 最终一致性：在最弱的级别下，我们可以利用所有副本提供读服务。在OceanBase的实现中，多副本同步协议只保证日志落盘，并不要求日志在多数派副本上完成回放（写入存储引擎的memtable中）。所以，利用任意副本提供读服务时，即使对于同一个分区的多个副本，每个副本完成回放的数据版本也是不同的，这样可能会导致读操作读到比之前发生的读更旧的数据。也就是说，这种情况下提供的是最终一致性。当任意副本宕机的时候，客户端可以迅速重试其他副本，甚至当多数派副本宕机的时候还可以提供这种读服务。 前缀一致性：它可以在每个数据库连接内，保证单调读。这种模式，一般用于OceanBase集群内读库的访问，业务本身是读写分离的架构。 有界旧一致性：在多地部署OceanBase的时候，跨地域副本数据之间的延迟是固有的。比如，用户配置允许读到30秒内的数据，那么只要本地副本的延迟小于30秒，则读操作可以读取本地副本。如果不能满足要求，则读取主副本所在地的其他副本。如果还不能满足，则会读取主副本。这样的方式可以获得最小的读延迟，以及比强一致性读更好的可用性。这样，在同时保证会话级单调读的条件下，我们提供了有界旧一致性级别。 参考链接 [1] 知乎专栏：通俗易懂 强一致性、弱一致性、最终一致性、读写一致性、单调读、因果一致性 的区别与联系 [2] 维基百科：Consistency Model [3] 关于分布式一致性的探究 [4] 腾讯云：分布式系统一致性分类 [5] 分布式系统的一致性与共识性 [6] InfoQ：分布式系统事务一致性解决方案 [7] 知乎：分布式存储系统的一致性是什么？ [8] 掘金：分布式系统：一致性模型 [9] 掘金：这或许是最通俗易懂的数据一致性问题解读 [10] 区块链技术指南：一致性问题 [11] 掘金：分布式系统：Lamport 逻辑时钟 [12] CSDN：线性一致性理论 [13] 简书：测试分布式系统的线性一致性 [14] 掘金：分布式系统-向量时钟 [15] 分布式中数据一致性探索 ","link":"https://blog.shunzi.tech/post/distributed-system-consistency/"},{"title":"Ceph-RBD 源码阅读","content":" RBD 是 Ceph 分布式存储系统中提供的块存储服务 该篇主要针对 RBD 中的整体架构以及 IO 流程进行介绍 针对 librbd 中提供的接口进行简单介绍，后续将在此基础上进行实战 Ceph RBD RBD：RADOS Block Devices. Ceph block devices are thin-provisioned, resizable and store data striped over multiple OSDs in a Ceph cluster. 整体介绍 Ceph RBD 模块主要提供了两种对外接口： 一种是基于 librados 的用户态接口库 librbd，支持 C/C++ 接口以及 Python 等高级语言的绑定； 另外一种是通过 kernel Module 的方式（一个叫 krbd 的内核模块），通过用户态的 rbd 命令行工具，将 RBD 块设备映射为本地的一个块设备文件。 RDB 的块设备由于元数据信息少而且访问不频繁，故 RBD 在 Ceph 集群中不需要单独的守护进程讲元数据加载到内存进行元数据访问加速，所有的元数据和数据操作直接与集群中的 Monitor 服务和 OSD 服务进行交互。 RBD IO 流 RBD 模块 IO 流图 几个重要的存储组织 Pool：存储资源池。IO 之前，需要先创建一个存储池，存储池统一地对逻辑存储单元进行管理，并对其进行初始化。同时指定一个 Pool 中的 PG 数量。是 Ceph 存储数据时的逻辑分区，类似于 HDFS 中的 namespace ceph osd pool create rbd 32 rbd pool init rbd RBD：块设备镜像。在创建好 Pool 的基础之上，对应的创建块设备镜像并和存储池进行映射绑定 Object：按照数据切片的大小，将所有数据切片为一个个对象，进行相应的对象存储操作。其中 Key 需要根据序号进行生成从而进行区分。 rbd create --size {megabytes} {pool-name}/{image-name} rbd create --size 1024 swimmingpool/bar PG：Placement Group，用于放置标准大小的 Object 的载体。其数量的计算公式：Total PGs = (Total_number_of_OSD * 100) / max_replication_count 再对结果向上取 2 的 N 次方作为最终的数量。PG 同时作为数据均衡和迁移的最小单位，PG 也有相应的主从之分。 OSD：OSD 是负责物理存储的进程，也可以理解为最终的对象存储节点。一般情况下，一块磁盘启动一个 OSD 进程，一组 PG （多副本）分布在不同的 OSD 上。 IO 流程 客户端创建对应的存储池 Pool，指定相应的 PG 个数以及 PGP 个数（用于 PG 中的数据均衡） 创建 pool/image rbd设备进行挂载 用户写入的数据进行切块，每个块有默认大小，并且每个块都有一个 Key，Key 就是 object+序号 将每个 object 通过 pg 进行副本位置的分配 PG 根据 cursh 算法会寻找指定个数的 osd（主从个数），把这个 object 分别保存在这些 osd 上 osd 上实际是把底层的 disk 进行了格式化操作，一般部署工具会将它格式化为 xfs 文件系统 object 的存储就变成了存储一个文件 rbd0.object1.file RBD IO 框架 客户端写数据osd过程： 采用的是 librbd 的形式，使用 librbd 创建一个块设备，向这个块设备中写入数据 在客户端本地同过调用 librados 接口，然后经过 pool，rbd，object，pg 进行层层映射（CRUSH 算法）,在 PG 这一层中，可以知道数据保存在哪几个 OSD 上，这几个 OSD 分为主从的关系 客户端与 primary OSD 建立 SOCKET 通信，将要写入的数据传给 primary OSD，由 primary OSD 再将数据发送给其他 replica OSD 数据节点。 librbd librbd 到 OSD 的数据流向如下： 模块介绍 librbd：Librbd 是Ceph提供的块存储接口的抽象，它提供C/C++、Python等多种接口。对于C++，最主要的两个类就是RBD 和 Image。 RBD 主要负责创建、删除、克隆映像等操作，而Image 类负责映像的读写等操作。 cls_rbd：cls_rbd是Cls的一个扩展模块，Cls允许用户自定义对象的操作接口和实现方法，为用户提供了一种比较直接的接口扩展方式。通过动态链接的形式加入 osd 中，在 osd 上直接执行。 librados：librados 提供客户端访问 Ceph 集群的原生态统一接口。其它接口或者命令行工具都基于该动态库实现。在 librados 中实现了 Crush 算法和网络通信等公共功能，数据请求操作在 librados 计算完成后可以直接与对应的 OSD 交互进行数据传输。 OSDC：该模块是客户端模块比较底层的模块，用于封装操作数据，计算对象的地址、发送请求和处理超时。 OSD：部署在每一个硬盘上的 OSD 进程，主要功能是存储数据、复制数据、平衡数据、恢复数据等，与其它OSD间进行心跳检查等，并将一些变化情况上报给Ceph Monitor OS：操作系统，在此处则主要是 OSD 的 IO 请求下发到对应的硬盘上的文件系统，由文件系统来完成后续的 IO 操作。 librbd 详细介绍 功能模块 核心机制 librbd 是一个将 block io （[off, len]）转换成 rados object io （[oid, off, len]）的中间层。为了支持高性能 io 处理，其内部维护了一个 io 队列，一个异步回调队列，以及对这两个队列中的请求进行处理的线程池，如下图所示。 IO 时序图 librbd 提供了针对 image 的数据读写和管理操作两种访问接口，其中数据读写请求入 io_work_queue，然后由线程池中的线程将 io 请求以 object 粒度切分并分别调用 rados 层的 aio 接口（IoCtxImpl）下发，当所有的 object 请求完成时，调用 librbd io 回调（librbd::io::AioCompletion）完成用户层的数据 io。而对 image 的管理操作通常需要涉及单个或多个对象的多次访问以及对内部状态的多次更新，其第一次访问将从用户线程调用至 rados 层 aio 接口或更新状态后入 op_work_queue 队列进行异步调用，当 rados aio 层回调或 Context 完成时再根据实现逻辑调用新的 rados aio 或构造 Context 回调，如此反复，最后调用应用层的回调完成管理操作请求。 此外为了支持多客户端共享访问 image，librbd 提供了构建于 rados watch/notify 之上的通知、远程执行以及 exclusive lock 分布式锁机制。每个 librbd 客户端在打开 image 时（以非只读方式打开）都会 watch image 的 header 对象，从远程发往本地客户端的通知消息或者内部的 watch 错误消息会通过 RadosClient 的 Finisher 线程入 op_work_queue 队列进行异步处理。 组成元素 image 主要由 rbd_header 元数据 rados 对象及 rbd_data 数据 rados 对象组成，随着特性的增加会增加其它一些元数据对象，但 librbd 内部的运行机制并不会有大的变化，一切都以异步 io、事件（请求）驱动为基础。 相关接口声明 此处以 librbd 的 C++ 库 librbd.hpp 为例对 librbd 提供的相关功能 API 进行介绍（除此以外还提供了 C 语言的相关库 librbd.h） librbd 提供的接口导图如下： namespace librbd { // 库在librbd名字空间中 using librados::IoCtx; // librados 库对外提供的接口 class Image; class ImageOptions; class PoolStats; typedef void *image_ctx_t; typedef void *completion_t; typedef void (*callback_t)(completion_t cb, void *arg); // 异步操作回调接口 ... class CEPH_RBD_API RBD { public: RBD(); ~RBD(); // This must be dynamically allocated with new, and // must be released with release(). // Do not use delete. struct AioCompletion { void *pc; AioCompletion(void *cb_arg, callback_t complete_cb); bool is_complete(); int wait_for_complete(); ssize_t get_return_value(); void release(); }; // 接下来一些API: open/create/clone/remove/rename/list/migration 等 // RBD groups support functions create/remove/list/rename private: /* We don't allow assignment or copying */ RBD(const RBD&amp; rhs); const RBD&amp; operator=(const RBD&amp; rhs); }; // Image 参数设置 class CEPH_RBD_API ImageOptions { public: ImageOptions(); ImageOptions(rbd_image_options_t opts); ImageOptions(const ImageOptions &amp;imgopts); ~ImageOptions(); int set(int optname, const std::string&amp; optval); int set(int optname, uint64_t optval); int get(int optname, std::string* optval) const; int get(int optname, uint64_t* optval) const; int is_set(int optname, bool* is_set); int unset(int optname); void clear(); bool empty() const; private: friend class RBD; friend class Image; rbd_image_options_t opts; }; // 存储池 Pool 状态 class CEPH_RBD_API PoolStats { public: PoolStats(); ~PoolStats(); PoolStats(const PoolStats&amp;) = delete; PoolStats&amp; operator=(const PoolStats&amp;) = delete; int add(rbd_pool_stat_option_t option, uint64_t* opt_val); private: friend class RBD; rbd_pool_stats_t pool_stats; }; class CEPH_RBD_API UpdateWatchCtx { public: virtual ~UpdateWatchCtx() {} /** * Callback activated when we receive a notify event. */ virtual void handle_notify() = 0; }; class CEPH_RBD_API Image { public: Image(); ~Image(); // 镜像的读写，flatten，trim等操作 private: friend class RBD; Image(const Image&amp; rhs); const Image&amp; operator=(const Image&amp; rhs); image_ctx_t ctx; // viod*, 实际指向具体实现的类 }; } class CEPH_RBD_API RBD RBD 主要负责 Image 的创建、删除、重命名、克隆映像等操作，包括对存储池的元数据的管理 针对部分操作提供异步接口 class CEPH_RBD_API RBD { public: RBD(); ~RBD(); // This must be dynamically allocated with new, and // must be released with release(). // Do not use delete. struct AioCompletion { void *pc; AioCompletion(void *cb_arg, callback_t complete_cb); bool is_complete(); int wait_for_complete(); ssize_t get_return_value(); void *get_arg(); void release(); }; void version(int *major, int *minor, int *extra); int open(IoCtx&amp; io_ctx, Image&amp; image, const char *name); int open(IoCtx&amp; io_ctx, Image&amp; image, const char *name, const char *snapname); int open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id); int open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id, const char *snapname); int aio_open(IoCtx&amp; io_ctx, Image&amp; image, const char *name, const char *snapname, RBD::AioCompletion *c); int aio_open_by_id(IoCtx&amp; io_ctx, Image&amp; image, const char *id, const char *snapname, RBD::AioCompletion *c); // see librbd.h int open_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *name, const char *snapname); int open_by_id_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *id, const char *snapname); int aio_open_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *name, const char *snapname, RBD::AioCompletion *c); int aio_open_by_id_read_only(IoCtx&amp; io_ctx, Image&amp; image, const char *id, const char *snapname, RBD::AioCompletion *c); int list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt;&amp; names) __attribute__((deprecated)); int list2(IoCtx&amp; io_ctx, std::vector&lt;image_spec_t&gt;* images); int create(IoCtx&amp; io_ctx, const char *name, uint64_t size, int *order); int create2(IoCtx&amp; io_ctx, const char *name, uint64_t size, uint64_t features, int *order); int create3(IoCtx&amp; io_ctx, const char *name, uint64_t size, uint64_t features, int *order, uint64_t stripe_unit, uint64_t stripe_count); int create4(IoCtx&amp; io_ctx, const char *name, uint64_t size, ImageOptions&amp; opts); int clone(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname, IoCtx&amp; c_ioctx, const char *c_name, uint64_t features, int *c_order); int clone2(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname, IoCtx&amp; c_ioctx, const char *c_name, uint64_t features, int *c_order, uint64_t stripe_unit, int stripe_count); int clone3(IoCtx&amp; p_ioctx, const char *p_name, const char *p_snapname, IoCtx&amp; c_ioctx, const char *c_name, ImageOptions&amp; opts); int remove(IoCtx&amp; io_ctx, const char *name); int remove_with_progress(IoCtx&amp; io_ctx, const char *name, ProgressContext&amp; pctx); int rename(IoCtx&amp; src_io_ctx, const char *srcname, const char *destname); int trash_move(IoCtx &amp;io_ctx, const char *name, uint64_t delay); int trash_get(IoCtx &amp;io_ctx, const char *id, trash_image_info_t *info); int trash_list(IoCtx &amp;io_ctx, std::vector&lt;trash_image_info_t&gt; &amp;entries); int trash_purge(IoCtx &amp;io_ctx, time_t expire_ts, float threshold); int trash_purge_with_progress(IoCtx &amp;io_ctx, time_t expire_ts, float threshold, ProgressContext &amp;pctx); int trash_remove(IoCtx &amp;io_ctx, const char *image_id, bool force); int trash_remove_with_progress(IoCtx &amp;io_ctx, const char *image_id, bool force, ProgressContext &amp;pctx); int trash_restore(IoCtx &amp;io_ctx, const char *id, const char *name); // Migration int migration_prepare(IoCtx&amp; io_ctx, const char *image_name, IoCtx&amp; dest_io_ctx, const char *dest_image_name, ImageOptions&amp; opts); int migration_execute(IoCtx&amp; io_ctx, const char *image_name); int migration_execute_with_progress(IoCtx&amp; io_ctx, const char *image_name, ProgressContext &amp;prog_ctx); int migration_abort(IoCtx&amp; io_ctx, const char *image_name); int migration_abort_with_progress(IoCtx&amp; io_ctx, const char *image_name, ProgressContext &amp;prog_ctx); int migration_commit(IoCtx&amp; io_ctx, const char *image_name); int migration_commit_with_progress(IoCtx&amp; io_ctx, const char *image_name, ProgressContext &amp;prog_ctx); int migration_status(IoCtx&amp; io_ctx, const char *image_name, image_migration_status_t *status, size_t status_size); // RBD pool mirroring support functions int mirror_mode_get(IoCtx&amp; io_ctx, rbd_mirror_mode_t *mirror_mode); int mirror_mode_set(IoCtx&amp; io_ctx, rbd_mirror_mode_t mirror_mode); int mirror_peer_add(IoCtx&amp; io_ctx, std::string *uuid, const std::string &amp;cluster_name, const std::string &amp;client_name); int mirror_peer_remove(IoCtx&amp; io_ctx, const std::string &amp;uuid); int mirror_peer_list(IoCtx&amp; io_ctx, std::vector&lt;mirror_peer_t&gt; *peers); int mirror_peer_set_client(IoCtx&amp; io_ctx, const std::string &amp;uuid, const std::string &amp;client_name); int mirror_peer_set_cluster(IoCtx&amp; io_ctx, const std::string &amp;uuid, const std::string &amp;cluster_name); int mirror_peer_get_attributes( IoCtx&amp; io_ctx, const std::string &amp;uuid, std::map&lt;std::string, std::string&gt; *key_vals); int mirror_peer_set_attributes( IoCtx&amp; io_ctx, const std::string &amp;uuid, const std::map&lt;std::string, std::string&gt;&amp; key_vals); int mirror_image_status_list(IoCtx&amp; io_ctx, const std::string &amp;start_id, size_t max, std::map&lt;std::string, mirror_image_status_t&gt; *images); int mirror_image_status_summary(IoCtx&amp; io_ctx, std::map&lt;mirror_image_status_state_t, int&gt; *states); int mirror_image_instance_id_list(IoCtx&amp; io_ctx, const std::string &amp;start_id, size_t max, std::map&lt;std::string, std::string&gt; *sevice_ids); // RBD groups support functions int group_create(IoCtx&amp; io_ctx, const char *group_name); int group_remove(IoCtx&amp; io_ctx, const char *group_name); int group_list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt; *names); int group_rename(IoCtx&amp; io_ctx, const char *src_group_name, const char *dest_group_name); int group_image_add(IoCtx&amp; io_ctx, const char *group_name, IoCtx&amp; image_io_ctx, const char *image_name); int group_image_remove(IoCtx&amp; io_ctx, const char *group_name, IoCtx&amp; image_io_ctx, const char *image_name); int group_image_remove_by_id(IoCtx&amp; io_ctx, const char *group_name, IoCtx&amp; image_io_ctx, const char *image_id); int group_image_list(IoCtx&amp; io_ctx, const char *group_name, std::vector&lt;group_image_info_t&gt; *images, size_t group_image_info_size); int group_snap_create(IoCtx&amp; io_ctx, const char *group_name, const char *snap_name); int group_snap_remove(IoCtx&amp; io_ctx, const char *group_name, const char *snap_name); int group_snap_rename(IoCtx&amp; group_ioctx, const char *group_name, const char *old_snap_name, const char *new_snap_name); int group_snap_list(IoCtx&amp; group_ioctx, const char *group_name, std::vector&lt;group_snap_info_t&gt; *snaps, size_t group_snap_info_size); int group_snap_rollback(IoCtx&amp; io_ctx, const char *group_name, const char *snap_name); int group_snap_rollback_with_progress(IoCtx&amp; io_ctx, const char *group_name, const char *snap_name, ProgressContext&amp; pctx); int namespace_create(IoCtx&amp; ioctx, const char *namespace_name); int namespace_remove(IoCtx&amp; ioctx, const char *namespace_name); int namespace_list(IoCtx&amp; io_ctx, std::vector&lt;std::string&gt;* namespace_names); int namespace_exists(IoCtx&amp; io_ctx, const char *namespace_name, bool *exists); int pool_init(IoCtx&amp; io_ctx, bool force); int pool_stats_get(IoCtx&amp; io_ctx, PoolStats *pool_stats); int pool_metadata_get(IoCtx &amp;io_ctx, const std::string &amp;key, std::string *value); int pool_metadata_set(IoCtx &amp;io_ctx, const std::string &amp;key, const std::string &amp;value); int pool_metadata_remove(IoCtx &amp;io_ctx, const std::string &amp;key); int pool_metadata_list(IoCtx &amp;io_ctx, const std::string &amp;start, uint64_t max, std::map&lt;std::string, ceph::bufferlist&gt; *pairs); int config_list(IoCtx&amp; io_ctx, std::vector&lt;config_option_t&gt; *options); private: /* We don't allow assignment or copying */ RBD(const RBD&amp; rhs); const RBD&amp; operator=(const RBD&amp; rhs); }; class CEPH_RBD_API Image Image 类负责镜像的读写(read/write)，以及快照相关的操作等等。 同时提供了相关异步操作的接口。 class CEPH_RBD_API Image { public: Image(); ~Image(); // 镜像的读写，resize, flush, flatten，trim等操作 int close(); int aio_close(RBD::AioCompletion *c); int resize(uint64_t size); int resize2(uint64_t size, bool allow_shrink, ProgressContext&amp; pctx); int resize_with_progress(uint64_t size, ProgressContext&amp; pctx); int stat(image_info_t &amp;info, size_t infosize); int get_name(std::string *name); int get_id(std::string *id); std::string get_block_name_prefix(); int64_t get_data_pool_id(); int parent_info(std::string *parent_poolname, std::string *parent_name, std::string *parent_snapname) __attribute__((deprecated)); int parent_info2(std::string *parent_poolname, std::string *parent_name, std::string *parent_id, std::string *parent_snapname) __attribute__((deprecated)); int get_parent(linked_image_spec_t *parent_image, snap_spec_t *parent_snap); int old_format(uint8_t *old); int size(uint64_t *size); int get_group(group_info_t *group_info, size_t group_info_size); int features(uint64_t *features); int update_features(uint64_t features, bool enabled); int get_op_features(uint64_t *op_features); int overlap(uint64_t *overlap); int get_flags(uint64_t *flags); int set_image_notification(int fd, int type); /* exclusive lock feature */ int is_exclusive_lock_owner(bool *is_owner); int lock_acquire(rbd_lock_mode_t lock_mode); int lock_release(); int lock_get_owners(rbd_lock_mode_t *lock_mode, std::list&lt;std::string&gt; *lock_owners); int lock_break(rbd_lock_mode_t lock_mode, const std::string &amp;lock_owner); /* object map feature */ int rebuild_object_map(ProgressContext &amp;prog_ctx); int check_object_map(ProgressContext &amp;prog_ctx); int copy(IoCtx&amp; dest_io_ctx, const char *destname); int copy2(Image&amp; dest); int copy3(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts); int copy4(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts, size_t sparse_size); int copy_with_progress(IoCtx&amp; dest_io_ctx, const char *destname, ProgressContext &amp;prog_ctx); int copy_with_progress2(Image&amp; dest, ProgressContext &amp;prog_ctx); int copy_with_progress3(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts, ProgressContext &amp;prog_ctx); int copy_with_progress4(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts, ProgressContext &amp;prog_ctx, size_t sparse_size); /* deep copy */ int deep_copy(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts); int deep_copy_with_progress(IoCtx&amp; dest_io_ctx, const char *destname, ImageOptions&amp; opts, ProgressContext &amp;prog_ctx); /* striping */ uint64_t get_stripe_unit() const; uint64_t get_stripe_count() const; int get_create_timestamp(struct timespec *timestamp); int get_access_timestamp(struct timespec *timestamp); int get_modify_timestamp(struct timespec *timestamp); int flatten(); int flatten_with_progress(ProgressContext &amp;prog_ctx); int sparsify(size_t sparse_size); int sparsify_with_progress(size_t sparse_size, ProgressContext &amp;prog_ctx); /** * Returns a pair of poolname, imagename for each clone * of this image at the currently set snapshot. */ int list_children(std::set&lt;std::pair&lt;std::string, std::string&gt; &gt; *children) __attribute__((deprecated)); /** * Returns a structure of poolname, imagename, imageid and trash flag * for each clone of this image at the currently set snapshot. */ int list_children2(std::vector&lt;librbd::child_info_t&gt; *children) __attribute__((deprecated)); int list_children3(std::vector&lt;linked_image_spec_t&gt; *images); int list_descendants(std::vector&lt;linked_image_spec_t&gt; *images); /* advisory locking (see librbd.h for details) */ int list_lockers(std::list&lt;locker_t&gt; *lockers, bool *exclusive, std::string *tag); int lock_exclusive(const std::string&amp; cookie); int lock_shared(const std::string&amp; cookie, const std::string&amp; tag); int unlock(const std::string&amp; cookie); int break_lock(const std::string&amp; client, const std::string&amp; cookie); /* snapshots */ int snap_list(std::vector&lt;snap_info_t&gt;&amp; snaps); /* DEPRECATED; use snap_exists2 */ bool snap_exists(const char *snapname) __attribute__ ((deprecated)); int snap_exists2(const char *snapname, bool *exists); int snap_create(const char *snapname); int snap_remove(const char *snapname); int snap_remove2(const char *snapname, uint32_t flags, ProgressContext&amp; pctx); int snap_remove_by_id(uint64_t snap_id); int snap_rollback(const char *snap_name); int snap_rollback_with_progress(const char *snap_name, ProgressContext&amp; pctx); int snap_protect(const char *snap_name); int snap_unprotect(const char *snap_name); int snap_is_protected(const char *snap_name, bool *is_protected); int snap_set(const char *snap_name); int snap_set_by_id(uint64_t snap_id); int snap_rename(const char *srcname, const char *dstname); int snap_get_limit(uint64_t *limit); int snap_set_limit(uint64_t limit); int snap_get_timestamp(uint64_t snap_id, struct timespec *timestamp); int snap_get_namespace_type(uint64_t snap_id, snap_namespace_type_t *namespace_type); int snap_get_group_namespace(uint64_t snap_id, snap_group_namespace_t *group_namespace, size_t snap_group_namespace_size); int snap_get_trash_namespace(uint64_t snap_id, std::string* original_name); /* I/O */ ssize_t read(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl); /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */ ssize_t read2(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl, int op_flags); int64_t read_iterate(uint64_t ofs, size_t len, int (*cb)(uint64_t, size_t, const char *, void *), void *arg); int read_iterate2(uint64_t ofs, uint64_t len, int (*cb)(uint64_t, size_t, const char *, void *), void *arg); /** * get difference between two versions of an image * * This will return the differences between two versions of an image * via a callback, which gets the offset and length and a flag * indicating whether the extent exists (1), or is known/defined to * be zeros (a hole, 0). If the source snapshot name is NULL, we * interpret that as the beginning of time and return all allocated * regions of the image. The end version is whatever is currently * selected for the image handle (either a snapshot or the writeable * head). * * @param fromsnapname start snapshot name, or NULL * @param ofs start offset * @param len len in bytes of region to report on * @param include_parent true if full history diff should include parent * @param whole_object 1 if diff extents should cover whole object * @param cb callback to call for each allocated region * @param arg argument to pass to the callback * @returns 0 on success, or negative error code on error */ int diff_iterate(const char *fromsnapname, uint64_t ofs, uint64_t len, int (*cb)(uint64_t, size_t, int, void *), void *arg); int diff_iterate2(const char *fromsnapname, uint64_t ofs, uint64_t len, bool include_parent, bool whole_object, int (*cb)(uint64_t, size_t, int, void *), void *arg); ssize_t write(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl); /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */ ssize_t write2(uint64_t ofs, size_t len, ceph::bufferlist&amp; bl, int op_flags); int discard(uint64_t ofs, uint64_t len); ssize_t writesame(uint64_t ofs, size_t len, ceph::bufferlist &amp;bl, int op_flags); ssize_t compare_and_write(uint64_t ofs, size_t len, ceph::bufferlist &amp;cmp_bl, ceph::bufferlist&amp; bl, uint64_t *mismatch_off, int op_flags); int aio_write(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c); /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */ int aio_write2(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c, int op_flags); int aio_writesame(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c, int op_flags); int aio_compare_and_write(uint64_t off, size_t len, ceph::bufferlist&amp; cmp_bl, ceph::bufferlist&amp; bl, RBD::AioCompletion *c, uint64_t *mismatch_off, int op_flags); /** * read async from image * * The target bufferlist is populated with references to buffers * that contain the data for the given extent of the image. * * NOTE: If caching is enabled, the bufferlist will directly * reference buffers in the cache to avoid an unnecessary data copy. * As a result, if the user intends to modify the buffer contents * directly, they should make a copy first (unconditionally, or when * the reference count on ther underlying buffer is more than 1). * * @param off offset in image * @param len length of read * @param bl bufferlist to read into * @param c aio completion to notify when read is complete */ int aio_read(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c); /* @param op_flags see librados.h constants beginning with LIBRADOS_OP_FLAG */ int aio_read2(uint64_t off, size_t len, ceph::bufferlist&amp; bl, RBD::AioCompletion *c, int op_flags); int aio_discard(uint64_t off, uint64_t len, RBD::AioCompletion *c); int flush(); /** * Start a flush if caching is enabled. Get a callback when * the currently pending writes are on disk. * * @param image the image to flush writes to * @param c what to call when flushing is complete * @returns 0 on success, negative error code on failure */ int aio_flush(RBD::AioCompletion *c); /** * Drop any cached data for this image * * @returns 0 on success, negative error code on failure */ int invalidate_cache(); int poll_io_events(RBD::AioCompletion **comps, int numcomp); int metadata_get(const std::string &amp;key, std::string *value); int metadata_set(const std::string &amp;key, const std::string &amp;value); int metadata_remove(const std::string &amp;key); /** * Returns a pair of key/value for this image */ int metadata_list(const std::string &amp;start, uint64_t max, std::map&lt;std::string, ceph::bufferlist&gt; *pairs); // RBD image mirroring support functions int mirror_image_enable(); int mirror_image_disable(bool force); int mirror_image_promote(bool force); int mirror_image_demote(); int mirror_image_resync(); int mirror_image_get_info(mirror_image_info_t *mirror_image_info, size_t info_size); int mirror_image_get_status(mirror_image_status_t *mirror_image_status, size_t status_size); int mirror_image_get_instance_id(std::string *instance_id); int aio_mirror_image_promote(bool force, RBD::AioCompletion *c); int aio_mirror_image_demote(RBD::AioCompletion *c); int aio_mirror_image_get_info(mirror_image_info_t *mirror_image_info, size_t info_size, RBD::AioCompletion *c); int aio_mirror_image_get_status(mirror_image_status_t *mirror_image_status, size_t status_size, RBD::AioCompletion *c); int update_watch(UpdateWatchCtx *ctx, uint64_t *handle); int update_unwatch(uint64_t handle); int list_watchers(std::list&lt;image_watcher_t&gt; &amp;watchers); int config_list(std::vector&lt;config_option_t&gt; *options); private: friend class RBD; Image(const Image&amp; rhs); const Image&amp; operator=(const Image&amp; rhs); image_ctx_t ctx; // void*, 实际指向具体实现的类 }; 具体实现 librbd.cc 主要实现了 I/O 相关接口 read/write。 internal.cc 主要实现了定义在头文件中的相关函数接口 Cls cls_rbd是Cls的一个扩展模块，Cls允许用户自定义对象的操作接口和实现方法，为用户提供了一种比较直接的接口扩展方式。通过动态链接的形式加入 osd 中，在 osd 上直接执行。 Client cls_rbd_client.h/cc 该文件中主要定义了客户端上运行的接口，将函数参数封装后发送给服务端 OSD ，然后做后续处理. cls_rbd_client.h/cc 定义了通过客户端访问osd注册的cls函数的方法。以 snapshot_add 函数和 create_image 函数为例，这个函数将参数封装进 bufferlist ，通过 ioctx-&gt;exec 方法，把操作发送给osd处理。 void snapshot_add(librados::ObjectWriteOperation *op, snapid_t snap_id, const std::string &amp;snap_name, const cls::rbd::SnapshotNamespace &amp;snap_namespace) { bufferlist bl; ::encode(snap_name, bl); ::encode(snap_id, bl); ::encode(cls::rbd::SnapshotNamespaceOnDisk(snap_namespace), bl); op-&gt;exec(&quot;rbd&quot;, &quot;snapshot_add&quot;, bl); } void create_image(librados::ObjectWriteOperation *op, uint64_t size, uint8_t order, uint64_t features, const std::string &amp;object_prefix, int64_t data_pool_id) { bufferlist bl; ::encode(size, bl); ::encode(order, bl); ::encode(features, bl); ::encode(object_prefix, bl); ::encode(data_pool_id, bl); op-&gt;exec(&quot;rbd&quot;, &quot;create&quot;, bl); } int create_image(librados::IoCtx *ioctx, const std::string &amp;oid, uint64_t size, uint8_t order, uint64_t features, const std::string &amp;object_prefix, int64_t data_pool_id) { librados::ObjectWriteOperation op; create_image(&amp;op, size, order, features, object_prefix, data_pool_id); return ioctx-&gt;operate(oid, &amp;op); } Server cls_rbd.h/cc 该类中主要定义了服务端上（OSD）执行的函数，响应客户端的请求。在 cls_rbd.cc 函数中，对函数进行定义和注册。 例如，下面的代码注册了rbd模块，以及 snapshot_add 和 create 函数。 cls_register(&quot;rbd&quot;, &amp;h_class); cls_register_cxx_method(h_class, &quot;snapshot_add&quot;, CLS_METHOD_RD | CLS_METHOD_WR, snapshot_add, &amp;h_snapshot_add); cls_register_cxx_method(h_class, &quot;create&quot;, CLS_METHOD_RD | CLS_METHOD_WR, create, &amp;h_create); cls_rbd.cc定义了方法在服务端的实现，其一般流程是：从bufferlist将客户端传入的参数解析出来，调用对应的方法实现，然后将结果返回客户端。 /** * Adds a snapshot to an rbd header. Ensures the id and name are unique. */ int snapshot_add(cls_method_context_t hctx, bufferlist *in, bufferlist *out) { bufferlist snap_namebl, snap_idbl; cls_rbd_snap snap_meta; uint64_t snap_limit; // 从bl中解析参数 try { bufferlist::iterator iter = in-&gt;begin(); ::decode(snap_meta.name, iter); ::decode(snap_meta.id, iter); if (!iter.end()) { ::decode(snap_meta.snapshot_namespace, iter); } } catch (const buffer::error &amp;err) { return -EINVAL; } // 判断参数合法性，略 ...... // 完成操作，在rbd_header对象中增加新的snapshot元数据，并更新sanp_seq。 map&lt;string, bufferlist&gt; vals; vals[&quot;snap_seq&quot;] = snap_seqbl; vals[snapshot_key] = snap_metabl; r = cls_cxx_map_set_vals(hctx, &amp;vals); if (r &lt; 0) { CLS_ERR(&quot;error writing snapshot metadata: %s&quot;, cpp_strerror(r).c_str()); return r; } return 0; } /** * Initialize the header with basic metadata. * Extra features may initialize more fields in the future. * Everything is stored as key/value pairs as omaps in the header object. * * If features the OSD does not understand are requested, -ENOSYS is * returned. * * Input: * @param size number of bytes in the image (uint64_t) * @param order bits to shift to determine the size of data objects (uint8_t) * @param features what optional things this image will use (uint64_t) * @param object_prefix a prefix for all the data objects * @param data_pool_id pool id where data objects is stored (int64_t) * * Output: * @return 0 on success, negative error code on failure */ int create(cls_method_context_t hctx, bufferlist *in, bufferlist *out) { string object_prefix; uint64_t features, size; uint8_t order; int64_t data_pool_id = -1; // 从 buffer 里解析参数 try { auto iter = in-&gt;cbegin(); decode(size, iter); decode(order, iter); decode(features, iter); decode(object_prefix, iter); if (!iter.end()) { decode(data_pool_id, iter); } } catch (const buffer::error &amp;err) { return -EINVAL; } CLS_LOG(20, &quot;create object_prefix=%s size=%llu order=%u features=%llu&quot;, object_prefix.c_str(), (unsigned long long)size, order, (unsigned long long)features); if (features &amp; ~RBD_FEATURES_ALL) { return -ENOSYS; } if (!object_prefix.size()) { return -EINVAL; } bufferlist stored_prefixbl; // 从 cls context 里获取 object_prefix int r = cls_cxx_map_get_val(hctx, &quot;object_prefix&quot;, &amp;stored_prefixbl); if (r != -ENOENT) { CLS_ERR(&quot;reading object_prefix returned %d&quot;, r); return -EEXIST; } bufferlist sizebl; bufferlist orderbl; bufferlist featuresbl; bufferlist object_prefixbl; bufferlist snap_seqbl; bufferlist timestampbl; uint64_t snap_seq = 0; utime_t timestamp = ceph_clock_now(); encode(size, sizebl); encode(order, orderbl); encode(features, featuresbl); encode(object_prefix, object_prefixbl); encode(snap_seq, snap_seqbl); encode(timestamp, timestampbl); // 更新 rbd_header omap map&lt;string, bufferlist&gt; omap_vals; omap_vals[&quot;size&quot;] = sizebl; omap_vals[&quot;order&quot;] = orderbl; omap_vals[&quot;features&quot;] = featuresbl; omap_vals[&quot;object_prefix&quot;] = object_prefixbl; omap_vals[&quot;snap_seq&quot;] = snap_seqbl; omap_vals[&quot;create_timestamp&quot;] = timestampbl; omap_vals[&quot;access_timestamp&quot;] = timestampbl; omap_vals[&quot;modify_timestamp&quot;] = timestampbl; if ((features &amp; RBD_FEATURE_OPERATIONS) != 0ULL) { CLS_ERR(&quot;Attempting to set internal feature: operations&quot;); return -EINVAL; } if (features &amp; RBD_FEATURE_DATA_POOL) { if (data_pool_id == -1) { CLS_ERR(&quot;data pool not provided with feature enabled&quot;); return -EINVAL; } bufferlist data_pool_id_bl; encode(data_pool_id, data_pool_id_bl); omap_vals[&quot;data_pool_id&quot;] = data_pool_id_bl; } else if (data_pool_id != -1) { CLS_ERR(&quot;data pool provided with feature disabled&quot;); return -EINVAL; } // 更新 OMAP r = cls_cxx_map_set_vals(hctx, &amp;omap_vals); if (r &lt; 0) return r; return 0; } Others 分片 Striper Ceph RBD 默认分片到了许多对象上，这些对象最终会存储在 RADOS 中，对 RBD Image 的读写请求会分布在集群中的很多个节点上，从而避免当 RBD Image 特别大或者繁忙的时候，单个节点不会成为瓶颈。 Ceph RBD 的分片由三个参数控制 object-size，代码中常常简写为 os，通常为 2 的幂指数，默认的对象大小是4 MB，最小的是4K，最大的是32M stripe_unit，代码中常常简写为 su，一个对象中存储了连续该大小的分片。默认和对象大小相等。 stripe_count，代码中常常简写为 sc，在向 [stripe_count] 对象写入 [stripe_unit] 字节后，循环到初始对象并写入另一个条带，直到对象达到其最大大小。此时，将继续处理下一个 [stripe_count] 对象。 分片的组织形式类似于 RAID0，看一个来自官网的 例子。此处只举例一个 Object Set，对象大小在如下图示中即为纵向的一个对象大小，由整数个分片单元组成，而一个分片是指指定 stripe_count 个 stripe_unit，图示中的 stripe_count 即为 5，假设 stripe_unit 为 64KB，那么对应的分片大小即为 320KB，对象大小如果设置为 64GB，那么一个对象对应的就会有 64GB/64KB = 1048576 个 stripe_unit. _________ _________ _________ _________ _________ /object 0\\ /object 1\\ /object 2\\ /object 3\\ /object 4\\ +=========+ +=========+ +=========+ +=========+ +=========+ | stripe | | stripe | | stripe | | stripe | | stripe | o | unit | | unit | | unit | | unit | | unit | stripe 0 b | 0 | | 1 | | 2 | | 3 | | 4 | j |---------| |---------| |---------| |---------| |---------| e | stripe | | stripe | | stripe | | stripe | | stripe | c | unit | | unit | | unit | | unit | | unit | stripe 1 t | 5 | | 6 | | 7 | | 8 | | 9 | |---------| |---------| |---------| |---------| |---------| s | . | | . | | . | | . | | . | e . . . . . t | . | | . | | . | | . | | . | |---------| |---------| |---------| |---------| |---------| 0 | stripe | | stripe | | stripe | | stripe | | stripe | stripe | unit | | unit | | unit | | unit | | unit | 1048575 | 5242875 | | 5242876 | | 5242877 | | 5242878 | | 5242879 | \\=========/ \\=========/ \\=========/ \\=========/ \\=========/ ceph/src/osdc/Striper.h class Striper { public: static void file_to_extents( CephContext *cct, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, uint64_t buffer_offset, striper::LightweightObjectExtents* object_extents); /* * std::map (ino, layout, offset, len) to a (list of) ObjectExtents (byte * ranges in objects on (primary) osds) */ static void file_to_extents(CephContext *cct, const char *object_format, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, std::map&lt;object_t, std::vector&lt;ObjectExtent&gt; &gt;&amp; extents, uint64_t buffer_offset=0); ... 分片大小对应的数据结构： struct file_layout_t { // file -&gt; object mapping uint32_t stripe_unit; ///&lt; stripe unit, in bytes, uint32_t stripe_count; ///&lt; over this many objects uint32_t object_size; ///&lt; until objects are this big int64_t pool_id; ///&lt; rados pool id std::string pool_ns; ///&lt; rados pool namespace file_layout_t(uint32_t su=0, uint32_t sc=0, uint32_t os=0) : stripe_unit(su), stripe_count(sc), object_size(os), pool_id(-1) { } // 默认分片大小 4MB static file_layout_t get_default() { return file_layout_t(1&lt;&lt;22, 1, 1&lt;&lt;22); } uint64_t get_period() const { return static_cast&lt;uint64_t&gt;(stripe_count) * object_size; } void from_legacy(const ceph_file_layout&amp; fl); void to_legacy(ceph_file_layout *fl) const; bool is_valid() const; void encode(ceph::buffer::list&amp; bl, uint64_t features) const; void decode(ceph::buffer::list::const_iterator&amp; p); void dump(ceph::Formatter *f) const; void decode_json(JSONObj *obj); static void generate_test_instances(std::list&lt;file_layout_t*&gt;&amp; o); }; 查看分片类对应的实现：ceph/src/osdc/Striper.cc void Striper::file_to_extents(CephContext *cct, const char *object_format, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, std::vector&lt;ObjectExtent&gt;&amp; extents, uint64_t buffer_offset) { striper::LightweightObjectExtents lightweight_object_extents; file_to_extents(cct, layout, offset, len, trunc_size, buffer_offset, &amp;lightweight_object_extents); // convert lightweight object extents to heavyweight version extents.reserve(lightweight_object_extents.size()); for (auto&amp; lightweight_object_extent : lightweight_object_extents) { auto&amp; object_extent = extents.emplace_back( object_t(format_oid(object_format, lightweight_object_extent.object_no)), lightweight_object_extent.object_no, lightweight_object_extent.offset, lightweight_object_extent.length, lightweight_object_extent.truncate_size); object_extent.oloc = OSDMap::file_to_object_locator(*layout); object_extent.buffer_extents.reserve( lightweight_object_extent.buffer_extents.size()); object_extent.buffer_extents.insert( object_extent.buffer_extents.end(), lightweight_object_extent.buffer_extents.begin(), lightweight_object_extent.buffer_extents.end()); } } void Striper::file_to_extents( CephContext *cct, const file_layout_t *layout, uint64_t offset, uint64_t len, uint64_t trunc_size, uint64_t buffer_offset, striper::LightweightObjectExtents* object_extents) { ldout(cct, 10) &lt;&lt; &quot;file_to_extents &quot; &lt;&lt; offset &lt;&lt; &quot;~&quot; &lt;&lt; len &lt;&lt; dendl; ceph_assert(len &gt; 0); /* * we want only one extent per object! this means that each extent * we read may map into different bits of the final read * buffer.. hence buffer_extents */ __u32 object_size = layout-&gt;object_size; __u32 su = layout-&gt;stripe_unit; __u32 stripe_count = layout-&gt;stripe_count; ceph_assert(object_size &gt;= su); if (stripe_count == 1) { ldout(cct, 20) &lt;&lt; &quot; sc is one, reset su to os&quot; &lt;&lt; dendl; su = object_size; } uint64_t stripes_per_object = object_size / su; ldout(cct, 20) &lt;&lt; &quot; su &quot; &lt;&lt; su &lt;&lt; &quot; sc &quot; &lt;&lt; stripe_count &lt;&lt; &quot; os &quot; &lt;&lt; object_size &lt;&lt; &quot; stripes_per_object &quot; &lt;&lt; stripes_per_object &lt;&lt; dendl; uint64_t cur = offset; uint64_t left = len; while (left &gt; 0) { // layout into objects uint64_t blockno = cur / su; // which block // which horizontal stripe (Y) uint64_t stripeno = blockno / stripe_count; // which object in the object set (X) uint64_t stripepos = blockno % stripe_count; // which object set uint64_t objectsetno = stripeno / stripes_per_object; // object id uint64_t objectno = objectsetno * stripe_count + stripepos; // map range into object uint64_t block_start = (stripeno % stripes_per_object) * su; uint64_t block_off = cur % su; uint64_t max = su - block_off; uint64_t x_offset = block_start + block_off; uint64_t x_len; if (left &gt; max) x_len = max; else x_len = left; ldout(cct, 20) &lt;&lt; &quot; off &quot; &lt;&lt; cur &lt;&lt; &quot; blockno &quot; &lt;&lt; blockno &lt;&lt; &quot; stripeno &quot; &lt;&lt; stripeno &lt;&lt; &quot; stripepos &quot; &lt;&lt; stripepos &lt;&lt; &quot; objectsetno &quot; &lt;&lt; objectsetno &lt;&lt; &quot; objectno &quot; &lt;&lt; objectno &lt;&lt; &quot; block_start &quot; &lt;&lt; block_start &lt;&lt; &quot; block_off &quot; &lt;&lt; block_off &lt;&lt; &quot; &quot; &lt;&lt; x_offset &lt;&lt; &quot;~&quot; &lt;&lt; x_len &lt;&lt; dendl; striper::LightweightObjectExtent* ex = nullptr; auto it = std::upper_bound(object_extents-&gt;begin(), object_extents-&gt;end(), objectno, OrderByObject()); striper::LightweightObjectExtents::reverse_iterator rev_it(it); if (rev_it == object_extents-&gt;rend() || rev_it-&gt;object_no != objectno || rev_it-&gt;offset + rev_it-&gt;length != x_offset) { // expect up to &quot;stripe-width - 1&quot; vector shifts in the worst-case ex = &amp;(*object_extents-&gt;emplace( it, objectno, x_offset, x_len, object_truncate_size(cct, layout, objectno, trunc_size))); ldout(cct, 20) &lt;&lt; &quot; added new &quot; &lt;&lt; *ex &lt;&lt; dendl; } else { ex = &amp;(*rev_it); ceph_assert(ex-&gt;offset + ex-&gt;length == x_offset); ldout(cct, 20) &lt;&lt; &quot; adding in to &quot; &lt;&lt; *ex &lt;&lt; dendl; ex-&gt;length += x_len; } ex-&gt;buffer_extents.emplace_back(cur - offset + buffer_offset, x_len); ldout(cct, 15) &lt;&lt; &quot;file_to_extents &quot; &lt;&lt; *ex &lt;&lt; dendl; // ldout(cct, 0) &lt;&lt; &quot;map: ino &quot; &lt;&lt; ino &lt;&lt; &quot; oid &quot; &lt;&lt; ex.oid &lt;&lt; &quot; osd &quot; // &lt;&lt; ex.osd &lt;&lt; &quot; offset &quot; &lt;&lt; ex.offset &lt;&lt; &quot; len &quot; &lt;&lt; ex.len // &lt;&lt; &quot; ... left &quot; &lt;&lt; left &lt;&lt; dendl; left -= x_len; cur += x_len; } } 数据 IO 主要对应于 ImageCtx::io_work_queue 成员变量。librbd::io::ImageRequestWQ 派生自 ThreadPool::PointerWQ&lt;ImageRequest&gt;（&lt;= Luminous） / ThreadPool::PointerWQ&lt;ImageDispatchSpec&gt;（&gt;= Mimic）。 librbd 支持两种类型的 aio，一种是普通的 aio，一种是非阻塞 aio。前者的行为相对简单，直接在用户线程的上下文进行 io 处理，而后者将用户的 io 直接入 io_work_queue 队列，然后 io 由队列的工作线程出队并在工作线程上下文进行后续的处理。这两种 aio 的行为由配置参数 rbd_non_blocking_aio 决定，默认为 true，因此默认为非阻塞 aio，但需要注意的是，即使默认不是非阻塞 aio，在某些场景下 aio 仍然会需要入 io_work_queue 队列，总结如下: read ImageRequestWQ::writes_blocked() 为 true，即已调用 ImageRequestWQ::block_writes，当前已禁止 write io 下发至 rados 层； ImageRequestWQ::writes_empty() 为 false，即前面已经有 write io 入了 io_work_queue 队列； ImageRequestWQ::require_lock_on_read() 为 true，这里的 lock 是指 exclusive lock，表示当前还未拿到，在启用 exclusive lock 特性的前提下，一旦开启克隆 COR (copy on read) 或者启用 journaling 特性，处理 read io 也要求拿锁； write ImageRequestWQ::writes_blocked() 为 true，即已调用 ImageRequestWQ::block_writes，当前已禁止 write io 下发至 rados 层； 对于 write 而言，并没有类似 ImageRequestWQ::require_lock_on_write 的接口，这是因为一旦启用 exclusive lock 特性，在初始化 exclusive lock 时会调用 ImageRequestWQ::block_writes（参考 ExclusiveLock::init），直至拿到锁（参考 ExclusiveLock::handle_post_acquired_lock），因此增加 ImageRequestWQ::require_lock_on_write 接口并没有必要。 需要注意的是，ImageRequestWQ::block_writes 并不只是简单的设置禁止标志，还需要 flush 已下发的 rados io，即等待所有已下发的 rados io 结束才返回。 从上面对 read、write 的分析，似乎 ImageRequestWQ::writes_blocked 改成 ImageRequestWQ::io_blocked 似乎更合理，但实际上这里并没有真的禁止 read io 下发至 rados 层，只是让 read io 先入 io_work_queue 队列。 TCMU-RBD static void tcmu_rbd_image_close(struct tcmu_device *dev) { struct tcmu_rbd_state *state = tcmur_dev_get_private(dev); rbd_close(state-&gt;image); rados_ioctx_destroy(state-&gt;io_ctx); rados_shutdown(state-&gt;cluster); state-&gt;cluster = NULL; state-&gt;io_ctx = NULL; state-&gt;image = NULL; } static int tcmu_rbd_image_open(struct tcmu_device *dev) { struct tcmu_rbd_state *state = tcmur_dev_get_private(dev); int ret; ret = rados_create(&amp;state-&gt;cluster, state-&gt;id); if (ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not create cluster. (Err %d)\\n&quot;, ret); return ret; } /* Try default location when conf_path=NULL, but ignore failure */ ret = rados_conf_read_file(state-&gt;cluster, state-&gt;conf_path); if (state-&gt;conf_path &amp;&amp; ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not read config %s (Err %d)&quot;, state-&gt;conf_path, ret); goto rados_shutdown; } rados_conf_set(state-&gt;cluster, &quot;rbd_cache&quot;, &quot;false&quot;); ret = timer_check_and_set_def(dev); if (ret) tcmu_dev_warn(dev, &quot;Could not set rados osd op timeout to %s (Err %d. Failover may be delayed.)\\n&quot;, state-&gt;osd_op_timeout, ret); ret = rados_connect(state-&gt;cluster); if (ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not connect to cluster. (Err %d)\\n&quot;, ret); goto rados_shutdown; } tcmu_rbd_detect_device_class(dev); ret = rados_ioctx_create(state-&gt;cluster, state-&gt;pool_name, &amp;state-&gt;io_ctx); if (ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not create ioctx for pool %s. (Err %d)\\n&quot;, state-&gt;pool_name, ret); goto rados_shutdown; } ret = rbd_open(state-&gt;io_ctx, state-&gt;image_name, &amp;state-&gt;image, NULL); if (ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not open image %s. (Err %d)\\n&quot;, state-&gt;image_name, ret); goto rados_destroy; } ret = tcmu_rbd_service_register(dev); if (ret &lt; 0) goto rbd_close; return 0; rbd_close: rbd_close(state-&gt;image); state-&gt;image = NULL; rados_destroy: rados_ioctx_destroy(state-&gt;io_ctx); state-&gt;io_ctx = NULL; rados_shutdown: rados_shutdown(state-&gt;cluster); state-&gt;cluster = NULL; return ret; } static int tcmu_rbd_open(struct tcmu_device *dev, bool reopen) { rbd_image_info_t image_info; char *pool, *name, *next_opt; char *config, *dev_cfg_dup; struct tcmu_rbd_state *state; uint32_t max_blocks; int ret; state = calloc(1, sizeof(*state)); if (!state) return -ENOMEM; tcmur_dev_set_private(dev, state); dev_cfg_dup = strdup(tcmu_dev_get_cfgstring(dev)); config = dev_cfg_dup; if (!config) { ret = -ENOMEM; goto free_state; } tcmu_dev_dbg(dev, &quot;tcmu_rbd_open config %s block size %u num lbas %&quot; PRIu64 &quot;.\\n&quot;, config, tcmu_dev_get_block_size(dev), tcmu_dev_get_num_lbas(dev)); config = strchr(config, '/'); if (!config) { tcmu_dev_err(dev, &quot;no configuration found in cfgstring\\n&quot;); ret = -EINVAL; goto free_config; } config += 1; /* get past '/' */ pool = strtok(config, &quot;/&quot;); if (!pool) { tcmu_dev_err(dev, &quot;Could not get pool name\\n&quot;); ret = -EINVAL; goto free_config; } state-&gt;pool_name = strdup(pool); if (!state-&gt;pool_name) { ret = -ENOMEM; tcmu_dev_err(dev, &quot;Could not copy pool name\\n&quot;); goto free_config; } name = strtok(NULL, &quot;;&quot;); if (!name) { tcmu_dev_err(dev, &quot;Could not get image name\\n&quot;); ret = -EINVAL; goto free_config; } state-&gt;image_name = strdup(name); if (!state-&gt;image_name) { ret = -ENOMEM; tcmu_dev_err(dev, &quot;Could not copy image name\\n&quot;); goto free_config; } /* The next options are optional */ next_opt = strtok(NULL, &quot;;&quot;); while (next_opt) { if (!strncmp(next_opt, &quot;osd_op_timeout=&quot;, 15)) { state-&gt;osd_op_timeout = strdup(next_opt + 15); if (!state-&gt;osd_op_timeout || !strlen(state-&gt;osd_op_timeout)) { ret = -ENOMEM; tcmu_dev_err(dev, &quot;Could not copy osd op timeout.\\n&quot;); goto free_config; } } else if (!strncmp(next_opt, &quot;conf=&quot;, 5)) { state-&gt;conf_path = strdup(next_opt + 5); if (!state-&gt;conf_path || !strlen(state-&gt;conf_path)) { ret = -ENOMEM; tcmu_dev_err(dev, &quot;Could not copy conf path.\\n&quot;); goto free_config; } } else if (!strncmp(next_opt, &quot;id=&quot;, 3)) { state-&gt;id = strdup(next_opt + 3); if (!state-&gt;id || !strlen(state-&gt;id)) { ret = -ENOMEM; tcmu_dev_err(dev, &quot;Could not copy id.\\n&quot;); goto free_config; } } next_opt = strtok(NULL, &quot;;&quot;); } ret = tcmu_rbd_image_open(dev); if (ret &lt; 0) { goto free_config; } tcmu_rbd_check_excl_lock_enabled(dev); ret = tcmu_rbd_check_image_size(dev, tcmu_dev_get_block_size(dev) * tcmu_dev_get_num_lbas(dev)); if (ret) { goto stop_image; } ret = rbd_stat(state-&gt;image, &amp;image_info, sizeof(image_info)); if (ret &lt; 0) { tcmu_dev_err(dev, &quot;Could not stat image.\\n&quot;); goto stop_image; } /* * librbd/ceph can better split and align unmaps and internal RWs, so * just have runner pass the entire cmd to us. To try and balance * overflowing the OSD/ceph side queues with discards/RWs limit it to * up to 4. */ max_blocks = (image_info.obj_size * 4) / tcmu_dev_get_block_size(dev); tcmu_dev_set_opt_xcopy_rw_len(dev, max_blocks); tcmu_dev_set_max_unmap_len(dev, max_blocks); tcmu_dev_set_opt_unmap_gran(dev, image_info.obj_size / tcmu_dev_get_block_size(dev), false); tcmu_dev_set_write_cache_enabled(dev, 0); free(dev_cfg_dup); return 0; stop_image: tcmu_rbd_image_close(dev); free_config: free(dev_cfg_dup); free_state: tcmu_rbd_state_free(state); return ret; } static void tcmu_rbd_close(struct tcmu_device *dev) { struct tcmu_rbd_state *state = tcmur_dev_get_private(dev); tcmu_rbd_image_close(dev); tcmu_rbd_state_free(state); } static int tcmu_rbd_aio_read(struct tcmu_device *dev, struct rbd_aio_cb *aio_cb, rbd_completion_t completion, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { struct tcmu_rbd_state *state = tcmur_dev_get_private(dev); int ret; aio_cb-&gt;bounce_buffer = malloc(length); if (!aio_cb-&gt;bounce_buffer) { tcmu_dev_err(dev, &quot;Could not allocate bounce buffer.\\n&quot;); return -ENOMEM; } ret = rbd_aio_read(state-&gt;image, offset, length, aio_cb-&gt;bounce_buffer, completion); if (ret &lt; 0) free(aio_cb-&gt;bounce_buffer); return ret; } static int tcmu_rbd_aio_write(struct tcmu_device *dev, struct rbd_aio_cb *aio_cb, rbd_completion_t completion, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { struct tcmu_rbd_state *state = tcmur_dev_get_private(dev); int ret; aio_cb-&gt;bounce_buffer = malloc(length); if (!aio_cb-&gt;bounce_buffer) { tcmu_dev_err(dev, &quot;Failed to allocate bounce buffer.\\n&quot;); return -ENOMEM;; } tcmu_memcpy_from_iovec(aio_cb-&gt;bounce_buffer, length, iov, iov_cnt); ret = rbd_aio_write(state-&gt;image, offset, length, aio_cb-&gt;bounce_buffer, completion); if (ret &lt; 0) free(aio_cb-&gt;bounce_buffer); return ret; } static int tcmu_rbd_read(struct tcmu_device *dev, struct tcmulib_cmd *cmd, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { struct rbd_aio_cb *aio_cb; rbd_completion_t completion; ssize_t ret; aio_cb = calloc(1, sizeof(*aio_cb)); if (!aio_cb) { tcmu_dev_err(dev, &quot;Could not allocate aio_cb.\\n&quot;); goto out; } aio_cb-&gt;dev = dev; aio_cb-&gt;type = RBD_AIO_TYPE_READ; aio_cb-&gt;read.length = length; aio_cb-&gt;tcmulib_cmd = cmd; aio_cb-&gt;iov = iov; aio_cb-&gt;iov_cnt = iov_cnt; ret = rbd_aio_create_completion (aio_cb, (rbd_callback_t) rbd_finish_aio_generic, &amp;completion); if (ret &lt; 0) { goto out_free_aio_cb; } ret = tcmu_rbd_aio_read(dev, aio_cb, completion, iov, iov_cnt, length, offset); if (ret &lt; 0) goto out_release_tracked_aio; return TCMU_STS_OK; out_release_tracked_aio: rbd_aio_release(completion); out_free_aio_cb: free(aio_cb); out: return TCMU_STS_NO_RESOURCE; } static int tcmu_rbd_write(struct tcmu_device *dev, struct tcmulib_cmd *cmd, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { struct rbd_aio_cb *aio_cb; rbd_completion_t completion; ssize_t ret; aio_cb = calloc(1, sizeof(*aio_cb)); if (!aio_cb) { tcmu_dev_err(dev, &quot;Could not allocate aio_cb.\\n&quot;); goto out; } aio_cb-&gt;dev = dev; aio_cb-&gt;type = RBD_AIO_TYPE_WRITE; aio_cb-&gt;tcmulib_cmd = cmd; ret = rbd_aio_create_completion (aio_cb, (rbd_callback_t) rbd_finish_aio_generic, &amp;completion); if (ret &lt; 0) { goto out_free_aio_cb; } ret = tcmu_rbd_aio_write(dev, aio_cb, completion, iov, iov_cnt, length, offset); if (ret &lt; 0) { goto out_release_tracked_aio; } return TCMU_STS_OK; out_release_tracked_aio: rbd_aio_release(completion); out_free_aio_cb: free(aio_cb); out: return TCMU_STS_NO_RESOURCE; } 参考链接 [1] Ceph RDB 官方文档介绍 [2] 简书 - ceph rbd：总览 [3] 掘金 - Ceph介绍及原理架构分享 [4] CSDN：Ceph学习——Librbd块存储库与RBD读写流程源码分析 [5] CSDN：Ceph RBD编程接口Librbd(C++) -- 映像创建与数据读写 [6] 腾讯云专栏：大话Ceph系列 [7] runsisi.com - librbd 内部运行机制 [8] CSDN：Ceph学习——Librados与Osdc实现源码解析 [9] CSDN：librbd代码目录解读 [10] Ceph: RBD 创建镜像过程以及源码分析 [11] librbd 架构分析 ","link":"https://blog.shunzi.tech/post/ceph-rbd-src/"},{"title":"Git 从入门到掐死","content":" Git 从入门到掐死教程 (简易教程) 主要整理了一下之前实习过程中一些 Git 的操作核问题的处理 并为自己整理的 Git 命令集提供相应的索引 未完待续~ 写在前面 Git 相关的基础教程已经很多了，个人觉得对新人比较友好的教程为 廖雪峰的 Git 教程。 建议在上文提到的教程（一定要动手敲键盘鸭！）基础上再结合本文一起食用更佳。 本文不再对基本操作做过多介绍，简单提及主要流程，主要针对特殊场景（开发常用）进行讲解。 针对一些常用的 Git 高级命令，请参考 BlogIssue 常用Git命令 (长期更新) ... BTW，其实现有的 GUI 工具已经做的很好了 (直观且易用) ，熟练掌握 GUI 的使用，不会命令行也没太大关系。 SourceTree Git GUI：Git 官方 GUI Github Desktop IDE 集成的 Git 工具等等 正文 Git 仓库级别的管理 创建的两种方式 本地初始化：git init 远程仓库克隆/下载：git clone [ssh-url/http-url] 两种创建方式的区别 本地初始化只是在本地建立了一个对应的 Git 仓库，未和远端仓库建立相应的关系，只使用了一些默认的配置； 远程仓库在克隆时已经加载了远程仓库中存储的关于该仓库的 Git 配置； Git 本地仓库建立与远程仓库的联系 Git 仓库可以实现 本地 1：N 远程的关系，从而同步到多个远程代码库 添加新的映射关系：git remote add [name] [url] 查看已有的映射关系： git remote -v Git 的日常操作流程 1.养成好习惯-工作之前保持同步 下拉远程端最新的提交：git pull [remote-name] [remote branch] 2.(可选) 下拉过程的特殊处理 如果本地已经存在部分修改，和远程端新的提交的修改的文件一致，Git 检测到文件版本不一致，拒绝下拉。此时可根据实际情况决定如何操作： 保留本地修改： 使用 git stash 暂存本地修改到一个栈中； 正常下拉远程端最新的提交； git stash pop 弹栈之间暂存的提交，自动合并； 合并中如果出现冲突相应地解决冲突。 不想保留本地修改： 还未添加到缓冲区中：可以直接丢弃掉本地已有的修改 git checkout -- filepathname 已经添加到缓冲区中：使用 git reset HEAD filepathname 已经有了本地的提交：可以使用 git reset --hard HEAD^ 来回退到上一次commit的状态 本地和远程端同时都信了新提交的时候，为了让整个提交的历史记录能够更加的简洁完整，避免一些不必要的合并操作，保证提交树的结构是线性的，可以使用 git pull --rebase 来自动地根据相应的提交时间进行友好的合并。 3.工作时注意文件编码和文件符号 检查修改的文件编码和换行符 4.(可选) 新建一个本地分支用于特定功能 针对部分功能模块或者程序 Bug 的修复，建议本地基于需要迭代的版本新建开发分支进行开发 可参考 Git Branch 的相关命令来进行分支的管理 5.添加修改到缓冲区 可指定文件也可全部添加到缓冲区：git add [..] 记得养成查看工作区状态的好习惯：git status 6.提交缓冲区的带评论的修改 提交存在缓冲区中的修改：git commit -m [comment] 提交时会生成一个全局唯一的 ID，便于后续对本次 commit 进行标识 记得养成查看工作区状态的好习惯：git status 7.推送到远程仓库的某个分支保持一致 将本地提交推送到远程端：git push [remote-name default:origin] [local-branch default master]:[remote-branch] 注意区分远端分支和本地分支，默认情况下保持远端和本地 master 分支同步 (可选) 针对本地新建的分支，可以直接推送到远程端，远程端自动新建新分支，如果已有类似分支，也可进行本地和远程分支之间的绑定。 8.Git 平台提交一个 PR/MR 请求 review 和 合并 平台 GUI 操作， New Pull Request CI/CD 流程相关代码审查和测试来验证本次提交的正确性 Reviewer 进行代码审核 代码审核通过相应地合并到对应的分支 （可选，视业务情况而定）在进行了对应的人工测试和一系列回归测试之后，进行新版本的 Release Git 历史记录追溯 查看 Git 提交记录（日志） 主要信息：commit-id, commit-comment 普通模式查看相关提交记录，退出按 q：git log 简洁模式查看提交记录：git log --pretty=oneline 可以使用某些参数对提交记录进行检索：git log --author=&quot;Elvis Zhang&quot; 比较前后提交的差异 HEAD 是一个指针，指向当前最新的提交 比较最近的一次提交和上一次的提交：git diff HEAD~1 HEAD 可以设置参数来比较缓冲区的内容：--cached 可以查看某个具体文件的修改内容：git diff HEAD~1 HEAD src/tools/rbd_mirror/ImageReplayer.cc 针对不同的比较对象（缓冲区、工作区、已经提交的记录），可以使用如下命令： &gt;&gt; git diff // Compare the cache and working area. &gt;&gt; git diff --cached […] // Compare the cache area and latest commit. [..] file name &gt;&gt; git diff HEAD […] // Compare the working area and latest commit. &gt;&gt; git diff commit-id […] // Compare the workinga area and given commit. &gt;&gt; git diff --cached [] […] // Compare the cache and given commit. &gt;&gt; git diff [] [] // Compare two commits. &gt;&gt; git diff --HEAD &gt; patch-name // Make the differences between the working area and latest commit as a patch. And can use command git apply patch-name to apply patch. 版本回退 版本回退主要有两种模式：hard &amp; soft. hard：能够回退到指定分支的最新提交，也可以根据 commit-id 回退到某个具体的提交，但不保留已有的修改； soft：能够回退到指定分支的最新提交，也可以根据 commit-id 回退到某个具体的提交，保留修改到缓冲区中。 &gt;&gt; git reset --hard origin/master // Reset the cache and the commit &gt;&gt; git reset --soft origin/master // Reset the commit and keep the modification. 查看Git命令执行历史 git reflog // Show the git operation history ","link":"https://blog.shunzi.tech/post/kill-git/"},{"title":"分布式缓存读书笔记（一）","content":" 分布式缓存读书笔记系列之一 从缓存的基本概念以及淘汰算法简单介绍入手 介绍优秀缓存框架的实现（本地缓存和集中式缓存） 缓存基本概念 缓存分类（软件系统中分类） 根据所处位置的不同划分为： 客户端缓存： BS 应用中页面的缓存（LocalStorage）和浏览器缓存，移动应用中APP自身所使用的缓存 服务端缓存： 数据库缓存：例如Mysql 查询缓存：当开启了 Mysql 的 Query Cache，会针对查询的结果 ResultSet 进行缓存，当接收新的查询语句时，判断是否满足Qurey Cache要求，根据预先设定好的 Hash 算法将查询豫军进行 HASH，对应地作为键去查询缓存中的数据。 平台级缓存：一般指带有缓存特性的应用框架，例如 Ehcache, JBoss Cache等 应用级缓存： 面向 Redis 的缓存应用 多级缓存：Nginx 应用服务器的本地缓存解决了热点数据的缓存问题，Redis分布式缓存集群减少了访问回源率，Tomcat集群使用的平台级缓存防止了相关缓存失效/崩溃之后的冲击，数据库缓存提升数据库查询时的效率。 公有云提供的缓存服务：动态扩容，数据多备，自动容灾，成本较低。 网路中的缓存：Web缓存（正向代理）-- Squid ，边缘缓存（反向代理）-- Varnish, SDN 根据规模和部署方式可划分为： 单体缓存 缓存集群 分布式缓存 缓存算法（替代策略） LRU - Least Recently Used 在CPU缓存淘汰和虚拟内存系统中使用的效果很好，浏览器缓存一般也使用了该算法。 LFU - Least Frequently Used 替换访问次数最少的缓存。针对某些一开始可能有很高的使用频率但之后再也不会使用的场景，容易导致缓存污染。 LRU2 - Least Recently Used 2 LRU的一个变种，把被两次访问过的对象放入缓存池，缓存池满后对应的清除两次最少使用的缓存对象去除。因为需要跟踪对象2次，访问负载会随着缓存池的增加而增加。同理 LRUK 2Q - Two Queue LRU的另一个变种。把被访问的数据放在LRU的缓存中，如果这个对象再一次被访问，就把他转移到第二个更大的LRU缓存中，使用了多级缓存的方式。去除缓存对象是为了保持第一个缓存池是第二个缓存池的1/3，当缓存的访问负载是固定的时候，把 LRU换成LRU2，就比增加缓存的容量更好。 SIZE 替换占用空间最大的对象，但针对部分进入缓存的小对象，之后可能永远不再被访问，SIZE策略没有提供淘汰这类对象的机制，也会导致“缓存污染”。 LRU-Threshold 不缓存超过某一size的对象，其他与LRU相同 Log(Size)+LRU 替换 size 最大的对象，当 size 相同时，按照LRU进行替换。 Hyper-G LFU 的改进版，同时考虑上次访问的时间和对象size。 Pitkow/Recker 替换最近最少使用的对象，除非所有对象都是今天访问过的，如果是这样，则替换掉最大的对象。 ARC - Adaptive Replacement Cache ARC 介于 LRU 和 LFU 之间，为了提高效果，由两个 LRU 组成，第一个包含的条目时最近只被使用过一次的，而第二个 LRU 包含的是最近被使用过两次的条目，因此得到了新的对象和常用的对象。ARC能够自我调节并且是低负载的。 MRU - Most Recently Used MRU 与 LRU 是相对的，移除最近最多被使用的对象，针对某些场景，找到最近最少使用的对象是一项时间复杂度较高的任务，此时考虑 MRU。在数据库内存缓存中比较常见。 FIFO - First In First Out 队列先进先出。 Random Cache 随即缓存就是随意地替换缓存数据，比FIFO机制好，某些特殊情况下甚至优于LRU 缓存的原理 缓存的规范 以 JSR (Java Specification Requests) 为例，该规范主要定义了一种对 Java 对象临时在内存中的进行缓存的方法，包括对象的创建、访问、失效、一致性等。 javax.cache 包中有一个 CacheManager 接口负责保存和控制一系列的缓存。主要包括以下特性： 缓存的读写； 缓存具有原子性操作； 具有缓存时间监听器； 具有缓存注解； 保存缓存的KV对类型应该为泛型。 JCache API 核心类 CacheProvider：创建、配置、管理 CacheManager CacheManager：创建、配置、管理 Cache Cache：类似 Map. 存储以 Key 为索引的值 Entry：存储在 Cache 中的 kev-value 对 CachingProvider cachingProvider = Caching. getCachingProvider(); CacheManager cacheManager = cachingProvider. getCacheManager(); MutableConfiguration&lt;String, String&gt; config = new MutableConfiguration(); Cache&lt;String, String&gt; cache = cacheManager. createCache(&quot;JDKCodeNames&quot;,config); cache.put(&quot;JDK1.5&quot;,&quot;Tiger&quot;); cache.put(&quot;JDK1.6&quot;,&quot;Mustang&quot;); cache.put(&quot;JDK1.7&quot;,&quot;Dolphin&quot;); String jdk7CodeName = cache.get(&quot;JDK1.7&quot;); JCache 的配置： JCache API提供了一组标准的接口和实现，通过它们可以以编程方式配置缓存。javax.cache.configuration.MutableConfiguration及其构建器类API有助于配置。可以配置以下高速缓存特性: METHOD JCACHE CONFIGURATION ARTIFACT setTypes Data types to be stored in the Cache setStoreByValue Store by reference or value setExpiryPolicyFactory Expiration Policy setReadThrough, setWriteThrough Read through and Write through policies setCacheLoaderFactory, setCacheWriterFactory Loader and Writer implementation addCacheEntryListenerConfiguration Entry Listener implementation setManagementEnabled, setStatisticsEnabled Management and statistics activation Store-By-Value &amp; Store-By-Reference Value：存储 KV 对时，会将值拷贝一份存入缓存（复制一份），避免外界修改KV时，污染了缓存中的KV对内容； Reference：存储 KV 对时，直接将引用存入缓存。Java常见的堆内缓存，一般使用该方式，从而提升缓存性能（免去了复制以及回收时的开销） 缓存过期策略：这些策略都对应地实现了 ExpirePolicy&lt;K, V&gt; 接口 public interface ExpiryPolicy&lt;K, V&gt; { Duration getExpiryForCreacedBntry(Entry&lt;? extends K, ? extends V&gt;entry)； Duration getExpiryForAccessedEntry(Entry&lt;? extends K, ? extends V&gt;entry)； Duration getExpiryForKodifiedEntry(Entry&lt;? extends K, ? extends V&gt;entry)； } EXPIRATION POLICY IMPLEMENTATION CLASS DESCRIPTION AccessedExpiryPolicy Based on time of last access CreatedExpiryPolicy Based on creation time EternalExpiryPolicy Ensures the cache entries never expire (default expiry policy) ModifiedExpiryPolicy Based on time of last update TouchedExpiryPolicy Based on time of last access OR update MutableConfiguration&lt;String,String&gt; config = new MutableConfiguration(); config.setExpiryPolicyFactory(CreatedExpiryPolicy.factoryOf(Duration.ONE_MINUTE)); 参考链接 [1] CSDN ： Java Caching(缓存)-策略和JCache API 缓存框架的实现 CsCache 源码基于SPI实现并定义了JCache的缓存规范。 参考博客：《深入分布式缓存》之“自己动手写缓存” Ehcache 和 Guava Cache Ehcache 主要特点 快速、简单：对应的线程机制是为大型高并发系统设计的，同时提供了易于使用的 API. 被广泛用于了 Hibernate、Spring、Cocoon等其他开源系统。 多种缓存策略：LRU/LFU/FIFO 缓存数据有两级：内存和磁盘 虚拟机重启过程中缓存数据写入到磁盘：引入了缓存数据持久化存储，可以显示调用将缓存数据刷入磁盘。 可以通过 RMI 和可插入 API等方式进行分布式缓存：Terracotta 缓存集群；RMI、JGroups/JMS 冗余缓存数据。 提供相关监听器接口：可以监听缓存事件和缓存管理器。 提供 Hibernate 的缓存实现 具体使用 关于 EhCache 的一些具体使用，可以参考相关博客和官方示例文档，此处不赘述。 参考链接 [1] CSDN：Spring整合EhCache的Demo [2] CSDN：Ehcache缓存入门实战（附源码） Ehcache 集群 Ehcache 支持多种集群方案，分别是 Terracotta、RMI、JMS、JGroup、Ehcache Server RMI 组播方式 RMI 是 Java 的一种远程方法调用技术，是一种点对点的基于 Java 对象的通讯方式。 采用 RMI 集群模式时，集群中的每个节点都是对等关系，并不存在主节点或者从节点的概念，因此节点间必须有一个机制能够互相认识对方，必须知道其它节点的信息，包括主机地址、端口号等。EhCache 提供两种节点的发现方式：手工配置和自动发现。 手工配置：要求在每个节点中配置其它所有节点的连接信息，一旦集群中的节点发生变化时，需要对缓存进行重新配置。 自动发现：使用 TCP 广播来建立和包含一个广播组，它的特征是最小配置和对成员组的自动添加和管理。没有那个服务器是有优先级的。对等点每一秒中向广播组发送心跳，如果一个对等点在五秒钟内没发送过来，则此对等点将会被删除，如果有新的，则会被加入集群。 缓存改变时，Ehcache 会向组播 IP 地址和端口号发送 RMI UDP 组播包。 JMS 消息方式 JMS 是两个应用程序之间进行异步通信的 API，它为标准消息协议和消息服务提供了一组通用接口，包括创建、发送和读取消息等，同时也支持基于事件的通信机制，通过发布事件机制向所有与服务器保持连接的客户端发送消息，再发送消息的时候，接收者不需要在线，等到客户端上线的时候，能保证接收到服务端发送的消息。 JMS 核心就是一个消息队列，每个应用节点都订阅预先定义好的主题，同时，节点有元素更新时，也会发布更新元素到主题中去。各个应用服务器节点通过侦听MQ获取到最新的数据，然后分别更新自己的Ehcache缓存，Ehcache默认支持ActiveMQ，我们也可以通过自定义组件的方式实现类似Kafka，RabbitMQ。 Cache Server 模式 Ehcache Server 集中存放缓存数据，Server 之间进行数据复制，提供了强大的安全机制和监控功能，以 WAR 包的形式进行部署，提供 RESTful 和 SOAP 类型的 API。 参考链接 [1] IBM : 深入探讨在集群环境中使用 EhCache 缓存系统 [2] CSDN : 集群环境中使用 EhCache 缓存系统 [3] 简书 ：玩转EhCache之最简单的缓存框架 Ehcache 适用场景 比较少的更新数据表的情况下。（针对Hibernate） EhCache 作为 Hibernate 缓存时。在进行修改表数据的时候，会自动把缓存中关于此表的缓存全部删除掉，为了保证数据的一致性，但性能较差。 对并发要求不是很严格的情况下。多台应用服务器中的数据无法进行实时同步。 对一致性要求不高的情况下。因为 EhCache 本地缓存的特性，目前无法很好解决不同服务器之间的缓存同步的问题，在一致性要求高的情况下，建议使用 Redis、Memcached 等集中式缓存。 Ehcache 的瓶颈 缓存漂移：每个应用节点只管理自己的缓存，在更新某个节点的时候，不会影响到其他节点，不能较好地进行数据之间的同步。 对数据库要求高：集群环境中，每一个应用数据为了保持最新，会增大相应的数据库开销。 实际应用 减小缓存击穿的风险 可以使用 EhCache 作为集中式缓存的二级缓存，利用服务器本地的应用缓存在集中式缓存宕机以后承担相应的负载。 但与此同时也相应地引入了 本地缓存 Ehcache 的更新问题以及集群环境下的同步问题 定时轮询：每台应用服务器定时轮询集中式缓存，比较缓存数据和本地缓存的版本号，如果不一致，对应地进行本地缓存的同步更新。 仍然会有数据一致性的问题。每台服务器定是轮询的时间点可能不一样，可能造成缓存数据不一致的问题，故只适用于对一致性要求不高的场景下。 主动通知：引入消息队列，每台应用服务器的本地缓存侦听 MQ 消息，通过 MQ 推送或者拉取的方式，一定程度上保证数据的同步更新。 不同服务器之间的网络速度的不同仍然可能保证达到完全强一致性，基于此原理使用 ZooKeeper 等分布式协调通知组件也是如此。 Guava Cache 使用场景 愿意消耗一些本地内存空间来提升速度 更新锁定：针对缓存不命中的情况，并发量较大的话，可能出现多个请求同时从数据源中请求相同的数据。而Guava Cache 可以在 CacheLoader 的 load 方法中加以控制，针对同一个 Key，只让一个请求去数据源中读取数据，而其他请求阻塞等待结果。同时也支持 refreshAfterWrite 定时刷新数据的配置项，刷新时其他请求会阻塞在一个固定的时间段，如果这段时间内没有获得新值，就直接返回旧值。 具体使用 可参考相关博客例子。 Guava 实现原理也可参考相关博客 参考链接 [1] Segmentfault : Guava Cache用法介绍 [2] CSDN : Guava Cache 使用学习 [3] crossoverjie : Guava 源码分析（Cache 原理） [4] crossoverjie : Guava 源码分析（Cache 原理【二阶段】） [5] GIthub WIKI : Guava CachesExplained 淘汰策略 被动删除 基于数据的大小进行删除：当容量即将达到（接近）指定的大小（最大条数）的时候，删除不常用的键值对 基于过期时间进行删除： expireAfterAccess(long, TimeUnit)：当某个 Key 最后一次访问后，再隔多长时间后删除； expireAfterWrite(long, TimeUnit)：当某个 Key 被创建后，隔多长时间后被删除。 基于引用的删除：基于 Java 的垃圾回收机制，判断缓存的数据引用的关系，如果没有被引用，则 Guava 会将该数据删除 主动删除 Cache.invalidate(key)：单独删除某一个 Key 对应的缓存 Cache.invalidate(keys)：批量删除一批 Key 对应的缓存 Cache.invalidateAll()：删除所有的数据 集中式缓存 Memcached 特性 协议简单：服务端和客户端通信不使用复杂的XML等格式，而是使用简单的基于文本协议或者二进制协议。 基于 libevent 的事件处理：由于 epoll, kqueue, /dev/poll 每个接口都有自己的特点，程序移植困难，故应运而生了将 Linux 的 epoll、BSD 类操作系统的 kqueue 等事件处理功能封装成统一的接口的库 libevent. 故在 Linux、BSD、Solaris 等操作系统上能发挥其高性能。 内置内存存储方式：数据均保存在内置的内存存储空间中从而提高性能，故掉电或者重启之后数据会全部丢失。另外内存达到指定的值后，会自动删除不使用的内存，采用 LRU 算法。 通过客户端实现分布式：服务器实例之间不会通信共享信息，在客户端会实现一些路由选择算法来决定具体定向到哪台 Memcached 缓存服务器，分布式的能力实在客户端代码实现的。 问题 无法备份，重启无法恢复：通过第三方的方案来进行持久化，需要兼容 Memcached。如 MemcachedDB 以及 Tokyo Cabinet （DBM 数据库）和 Tokyo Tyrant （网络协议）配合使用。 不支持高级查询：由于存储机制，不支持范围查询 单点故障 failover：可以通过主从模式解决。应用服务器通过客户端hash，对缓存服务器的主节点和子节点进行双写，同时更新；读取的时候，先读取主节点数据，若返回为空或者无法取得数据的s时候，访问子节点。针对可能存在的一致性问题，统一以Master为准。在进行缓存数据的更新操作的时候，先获取Master中的数据，再对应地进行CAS更新，更新成功后对应地再更新子节点。如果CAS多次后都失败，则对Master和Slave进行删除操作，后续让请求穿透缓存，从数据库中获取数据，再写回缓存。 内存存储 Slab-Allocation 机制 传统的内存分配机制为通过对所有记录简单地进行 malloc 和 free 来进行，但该方式会产生内存碎片，加重操作系统内存管理器的负担。 Slab-Allocation 机制则是按照预先规定的大小，将分配的内存分割成各种尺寸的块（chunk），并把尺寸相同的块分成组 Slab Class（chunk 的集合）。分配的块可以重复利用，不释放到内存中。 核心概念 Page：分配给 Slab 的内存空间，默认 1MB。 Chunk：用于缓存记录的内存空间。 Slab Class：特定大小的 chunk 的组。 Chunk 大小之间的关系由 growth factor 决定。如 88bytes * 1.25 = 112 bytes; 112 bytes * 1.25 = 144 bytes，依此类推。 Slab-Allocation 的机制简单说就是，Memcached 根据收到的数据的大小，选择最合适数据大小的 Slab。Memcached 中保存着 Slab 内空闲的 Chunks 的列表，根据该列表选择 Chunk，然后将数据缓存其中。 该机制解决了内存碎片的问题，但同时也引入了新的问题：无法有效利用分配的内存，内存的利用率不高。如 112bytes 的 chunk 存放 100bytes 的数据。 可以调整 Growth Factor 来进行调优 Item 在内存中保存（key, value）30 秒，数据被打包成一个 Item set key 0 30 5 value 一个完整的 Item 包括以下几个部分： key ：键 nkey：键长 flags：用户定义的flag nbytes：值长，包括换行符 suffix：后缀 Buffer nsuffix：后缀长 完整的 Item 长度是 “键长+值长+后缀长+item结构大小（32字节）”，从而计算 Slab 的 classid. 典型问题解析 容量问题：单一服务节点无法突破单机内存上限 服务高可用 HA：单实例场景下，如果因为网络波动或者机器故障等原因缓存不可用，会出现缓存击穿的情况 扩展问题：单一服务节点无法突破单实例请求峰值上限，比如热点问题，微博热搜，秒杀事件等。 过期机制 Lazy Expiration：在 get 时，查看记录的时间戳，检查记录是否过期，从而实现不会在过期监视上耗费 CPU 时间。 LRU 算法：在 Slab 的范围内执行 LRU ： 最近最少使用 LRU-K ： 最近使用过 K 次 Two Queues ： 两个缓存队列，一个是 FIFO 队列，一个是 LRU 队列 Multi Queues ： 根据访问频率将数据划分为多个队列，不同队列具有不同的优先级。 哈希算法 Memcached 的分布式部署结构以来的核心即为 哈希算法。例如部署了三个 Memcached 实例，简单的哈希算法实现即为 hash(key) mod 3，对应地将 Key 散列化之后取模，从而落到一个具体的实例上。 但如上所示的简单 Hash 算法无法较好地解决实例的增加或减少的问题中去，考虑采用一致性 Hash和哈希环来解决。 热点问题 数据热点访问问题（读热点问题）：例如某些热点商品的访问度非常高，可能造成单个 Memcached 节点的负载特别高，通用的解决思路是：在 Cache 的 Client 端做本地 LocalCache，当发现热点数据时直接缓存到客户端，而不用每次都请求到缓存服务端。 巨热数据的处理：Facebook 的解决办法为通过多个 key_index(key:xxx#N) 来获取热点 Key 的数据，其实质也是把 Key 分散，对于非高一致性要求的高并发读有一定的效果。解决之道：把巨热数据的 key 发布到所有的服务器上，每个服务器给对应的 key 一个别名，比如 get key:xxx =&gt; get key:xxx#N。每次对热点数据访问时对应地进行Hash，从而转换成 子Key，从而分担单个缓存服务器的压力。 缓存与数据库的更新问题 该问题主要是针对更新数据库数据时，对应地需要更新缓存中的数据（也可以选择淘汰缓存中的数据），从而保证数据库和缓存数据的一致性，同时要尽量保证整个过程耗时较短（根据具体的业务需求）。 参考链接 [1] CSDN : 高并发下缓存和数据库一致性问题（更新淘汰缓存不得不注意的细节） [2] 简书 ：缓存与数据库一致性之缓存更新设计 命名空间 如果很多不同的业务系统使用同一套 Memcached 服务，那么很有可能存在 Key 的冲突。为了对不同的业务数据进行隔离，引入命名空间的概念来进行区分。 由于 Memcached 本身不提供命名空间机制，故可使用 Key 的前缀来模拟命名空间。如 orderid_xxx, userid_xxx CAS CAS 主要解决原子性操作问题。操作一个 Key 的过程当中，不允许被其他访问操作，如果被操作过，当前操作则失败。 Memcached 中使用 gets 命令获取 key-value 和对应的令牌（64位版本标识符），然后产生新的 Value，最后附带版本号提交新的 Key-Value；服务端判断 CAS 操作中的版本号是不是最新的，如果不是，则认为 Key 对应的 Value 已经被修改，本次 CAS 操作失败。 参考链接 [1] CSDN : Memcached的CAS机制的实现 [2] 博客园 ：分布式缓存系统 Memcached CAS协议 Memcached 客户端 客户端的功能主要包括：Memcached 协议的封装，连接池的实现， sharding 机制，故障转移，序列化等机制 此处以 Java Client Spymemcached 为例进行介绍。 Spymemcached 设计思想解析 特性 异步：NIO 作为底层通信框架 集群：默认支持服务器集群的 sharding 机制 自动恢复：网络闪断，会进行异步重连，自动恢复 failover 机制，提供可扩展的容错机制 支持批量 get，本地进行数据的聚合 序列化：默认支持 JDK 序列化机制，可自定义扩展 超时机制，支持访问超时等设置 整体设计 API 接口：对外提供同步的接口和异步的接口调用。异步接口实际返回 Future 任务封装：将访问操作和 callback 封装为 Task 路由机制：通过默认的 Sharding 策略（支持 arrayMod 和 Ketama）,选择 Key 对应的连接。 将 Task 放到对应连接的队列 Selector 线程异步获取连接队列的 Task，进行协议的封装和发送 收到 Memcached 返回的包，会找到对应的 Task，调用 callback 进行回调，对返回结果进行协议的解析，并进行序列化。 sharding 机制及容错 路由机制： 实现的 HASH 算法： NATIVE_HASH：默认的哈希算法，计算 hashCode CRC_HASH：使用 crc32 进行 hash KETAMA_HASH：ketama 的一致性 hash 默认支持的两种路由策略： arrayMod：hash 采用的是 NATIVE_HASH，取模选择路由节点。但是取模带来了扩展性问题，导致缓存命中率降低。 hash 采用 KETEMA_HASH，该算法选择路由节点，同时满足平衡性和单调性，有效解决了扩缩容带来的缓存不命中问题。 容错：Key 路由到的节点挂掉了的时候。如何处理？ 自动重连：失败的节点会从正常的队列中摘除，添加到重连队列中，定期对该节点进行重连，重连成功，添加到正常的队列中 failover 处理： Redistribute：若路由到一个失败节点，根据策略选择下一个节点，直到选择到正确的节点，容错性较强；但由于自动 failover 到其他节点，会导致大量回源，并且在启动的时候由于 Memcached 没有固化，导致再次大量回源。 Retry：若路由的是失败节点，仍然用该节点访问，导致大量的访问失败。 Cancel：如果路由的是失败节点，直接抛异常，影响缓存的可用性。 序列化：根据对应的数据类型，按照预先制定好的序列化规则进行序列化，序列化完了以后根据二进制数据的长度是否大于阈值，来决定是否采用 GZIP 压缩。 扩缩容：将对应的节点信息更新到 Zookeeper 上，应用服务器监听 ZK 节点的状态，会收到来自 ZK 节点关于新加入节点信息的通知。集群里的路由策略采用一致性哈希来保证单调性 Redis 数据结构 &quot;article:12345&quot; - 即为 article 这个命名空间（db）下 id 为 12345 的元素的 Key 通用的数据结构 typedef struct redisObject { unsigned type:4; unsigned encoding:4; unsigned lru:REDIS_LRU_BITS; /* lru time (relative to server.lruclock) */ int refcount; void *ptr; } robj; type (4 位)： 2^4 = 8 &gt; 5 可以表示八种数据类型 #define REDIS_STRING 0 #define REDIS_LIST 1 #define REDIS_SET 2 #define REDIS_ZSET 3 #define REDIS_HASH 4 encoding (4 位)： 2^4 = 8 可以表示八种编码方式 #define REDIS_ENCODING_RAW 0 /* Raw representation */ #define REDIS_ENCODING_INT 1 /* Encoded as integer */ #define REDIS_ENCODING_HT 2 /* Encoded as hash table */ #define REDIS_ENCODING_ZIPMAP 3 /* Encoded as zipmap */ #define REDIS_ENCODING_LINKEDLIST 4 /* Encoded as regular linked list */ #define REDIS_ENCODING_ZIPLIST 5 /* Encoded as ziplist */ #define REDIS_ENCODING_INTSET 6 /* Encoded as intset */ #define REDIS_ENCODING_SKIPLIST 7 /* Encoded as skiplist */ lru：表示本对象的空转时长，用于有限内存下长久不访问对象的清理 refcount：应用计数用于对象的垃圾回收 ptr：指向以 encoding 方式实现这个对象的实际承载者的地址 具体的数据结构 对于具体的数据结构以及 Redis 自身提供的一些优化方案，此处不再赘述，后续将专门针对 Redis 的源码和实现进行具体的分析。 客户端与服务器的交互 Redis 实例运行于单独的进程，应用系统和 Redis 通过 Redis 协议进行交互。 客户端/服务器协议 网络交互 Redis 协议位于 TCP 之上，即客户端和 Redis 实例保持双工的连接。 交互的内容为序列化后的相应类型的协议数据，服务器端为每个客户端建立对应的连接，在应用层维护一系列状态保存在连接中，连接之间无相互关联。 序列化协议 Redis 中协议数据分为不同的类型，每种类型均已 CRLF 结束，根据数据的首字符进行区分： Inline command： simple string： bulk string： error： Integer： array： C/S 两端使用的数据协议 请求/响应模式 串行化实现 同一个连接上，客户端接收完第一个请求的响应之后，再发起第二个请求。每一次的请求的发送都依赖于上一次请求的响应结果完全接收，同一个连接每秒的吞吐量低。 单连接的大部分时间都处于网络等待，没有充分利用服务器的处理能力。 pipeline实现 基于本身就支持全双工的 TCP 协议，可以将请求数据批量发送到服务器，再批量地从服务器连接的字节流中依次读取每个响应数据，极大地提高单连接的吞吐量。 主要由客户端来实现，通过发送批量请求或者异步化请求来实现，但非异步化批量发送时需要考虑每个批次的数据量，避免连接的 buffer 满之后的死锁。 事务模式 两阶段保证事务原子性 入队阶段：服务端将客户端发送来的请求暂存到连接对象的对应请求队列中。 执行阶段：发送完一个批次的所有请求后，服务端依次执行连接对象队列中的所有请求。 事务的一致性 严格意义上讲，Redis 的事务并不是一致的。由于自身并不包含回滚机制（执行到一半的批量操作必须继续执行完），在批量执行过程中有一条请求执行失败，后续继续执行，只在返回客户端的 array 类型响应中标记这条出错的结果，客户端决定如何恢复。 事务的只读操作 批量请求在服务端一次性执行，应用程序需要一开始就在入队阶段（未真正执行时）确定每次写操作的值，也就是说每个请求的参数取值不能依赖上一次的执行结果。 由于只读操作放在批量执行中没有任何意义，既不会改变事务的执行行为，也不会改变 Redis 的数据状态，所以入队的请求应该均为写操作。只读操作需要放到事务开启之前的语句执行。 乐观锁的可串行化事务隔离 Redis 通过 WATCH 机制实现乐观锁解决上述一致性问题： 将本次事务所有涉及到的 Key 注册为观察模式，假设此时逻辑时间为 tstart 执行只读操作 根据只读操作的结果组装对应的写操作命令并发送到服务器端入队 发送原子化的批量执行命令 EXEC 试图执行连接的请求队列中的命令，假设此时逻辑时间为 tcommit 如果在 tstart 到 tcommit 的这段时间内，有一个或多个 key 被修改过，那么 EXEC 将直接失败，拒绝执行，否则顺序执行队列中的所有请求。 脚本模式 Redis 允许客户端向服务端提交一个脚本，后者结构化地（分支/循环）编排业务事务中的多个 Redis 操作，脚本还可获取每次操作的结果作为下次操作的入参。使得服务端的逻辑潜入成为可能。 发布/订阅模式 除了上文描述的由客户端主动触发，服务端被动接收的交互模式以外，Redis还提供了一种一个客户端触发，多个客户端被动接收，通过服务器的中转的发布订阅模式。 核心概念 客户端分为发布者和订阅者两种角色 发布者和订阅者通过 Channel 进行关联 发布者和 Redis 的交互仍然是 请求/响应 模式 服务器向订阅者推送数据 时序：推送发生在服务器收到发布消息之后 两类 channel 普通 channel：订阅者将自己绑定或者解绑到某个 channel 上；发布者的 publish 命令将指定某个消息发送到哪个 channel，再由服务器将消息转发给 channel 上绑定的订阅者。 Pattern Channel：：订阅者将自己绑定或者解绑到某个 pattern channel 上；发布者的 publish 命令将指定某个消息发送到哪个 channel，再由服务器通过 channel 的名字和 pattern channel 的名字做匹配，匹配成功则将消息转发到这个 pattern channel 上的绑定的订阅者。 订阅关系的实现 使用了字典的数据结构维护普通 channel 和订阅者的关系，键是 channel 的名字，值是所有订阅者 client 的指针链表。 使用了一个来链表维护 pattern channel 和订阅者的关系 发布者发布消息时，首先从 channel map 中寻找到对应 channel 的所有客户端，再发送消息；再遍历 Pattern Channel 链表，向匹配到的元素的 client 发送消息。 单机处理逻辑 为了处理高吞吐量的访问需求，以及高并发访问，Redis 单线程地处理来自开所有客户端的并发请求来保证 hashtable 的线程安全。 多路复用 首先从多路复用框架中 select 出已经 ready 的 文件描述符，asApiPoll 函数的实现根据实际宿主机器的具体环境，分为四种实现方式：epoll、evport、kqueue，以上实现都找不到时使用 select 这种最通用的方式； ready 的标准是依据每个 fd 的 interestSet，如已有数据到达 kernel（AE_READABLE）,已准备好写入数据； 对于上一步已经 ready 的文件描述符，redis 会分别对每个文件描述符上已 ready 的事件进行处理，处理完相同文件描述符上的所有事件后，再处理下一个 ready 的文件描述符。其事件处理逻辑根据所属场景主要分为3种实现： acceptTcpHandler 处理 redis 的 serverSocket 上来自客户端的连接建立请求 ，会为客户端对应的文件描述符注册其关注的事件（interestSet）：AS_READABLE，以便感知该文件描述符后续发来的数据； readQueryFromClient 处理来自客户端的数据，它会读取每一个完整的命令并执行，再将执行结果暂存，待客户端对应文件描述符准备好写时向客户端写入。所以该方法需要为文件描述符注册 AS_WRITEABLE 事件并以 sendReplyToClient 作为处理器。对于 multi （批处理的事务），需要等到 multi 包含一个全部的命令时才进行执行； sendReplyToClient 将暂存的执行结果写回客户端 处理完来自客户端的命令之后，处理定时任务（processEvent） aeApiPoll 的等待时间取决于定时任务处理（TimeEvents）逻辑； 本次主循环完毕，进入下一次主循环的 beforeSleep 逻辑，后者负责处理数据过期，增量持久化的文件写入等任务。 定时任务处理 持久化 Redis 对外提供数据访问服务时使用的是驻存在内存中的数据，这些数据在 Redis 重启之后将消失。为了让数据在重启之后得以恢复，Redis 具备将数据持久化到本地磁盘的能力。 全量模式 在持久化触发的时刻，将当时的状态（所有 db 的 key-value 值）完全保存下来，形成了一个 snapshot，重启时通过加载最近的一个 snapshot 数据，可将 Redis 恢复至最近一次持久化时的状态上。 故该模式可划分为写入和恢复两个流程。 写入流程 SAVE：可以由客户端显示触发，也可以在 redis shutdown 时触发，无论以哪种形式触发，SAVE 本身都是以普通命令的方式执行——单线程地执行一个一个命令。SAVE的执行过程就是把 Redis 的当前状态写入磁盘作为快照保存的过程，期间其他所有命令不会并发执行。 BGSAVE：客户端通过命令显示触发，可以通过配置由定时任务触发，也可以在 master-slave 的分布式结构下由 slave 节点触发。该命令执行始于 fork 出一个子线程，在子线程启动之后修改一些 redisServer 对象的状态之后执行完毕，Redis 进程的主循环接着处理后续命令。将 Redis 数据状态写入磁盘的工作由子线程并发地完成。子进程写入文件面对的是父进程在 fork 时的数据库状态副本，该副本在写入期间不会发生变更。 BGSAVE 的优势在于可以在持久化期间继续对外提供数据读写服务，而需要付出涉及父进程内存复制的fork操作的代价，相应的会增加服务器内存的开销，当高到不得不使用虚拟内存时，BGSAVE 的 fork 会阻塞服务器运行，造成秒级以上的不可用，故在使用 BGSAVE 时需要确保内存空间足够。 恢复流程 从 Redis 启动到进入前文所述事件处理主循环时，Redis 会从本地磁盘加载之前持久化的文件，将内存置于文件所描述的数据”状态“时，再受理后续来自客户端的数据访问命令。 增量模式（Append-only File） 增量持久化保存的是状态的每一次”变迁“，在初始状态的基础之上，经过相同的”变迁“序列之后也能达到最终的状态。仅对数据的变化进行存储。 写入流程 在主循环中的每次处理完写命令的执行之后，通过 propagate 函数显示触发增量持久化，该方法将当前命令的内容追加到 redisServer 对象的 aof_buf 变量中，在下一次迭代进入多路复用的 select 前，通过 flushAppendOnlyFile 方法将 aof_buf 的内容写到 AOF 对应的文件中，但此时只写到了缓存，需要显示调用 fsync 强制落盘。 同步策略： alaways：在 flushAppendOnlyFile 函数中直接同步触发 fsync 方法，强制落盘。该策略会降低 Redis 吞吐量，使得磁盘的写入成为 Redis 对外写服务的瓶颈，但由于每个命令都在写入磁盘后再返回，这种模式下具有最高的容错能力。 every second：每秒异步触发 fsync, 由 bio 线程池中的某个线程来执行。flushAppendOnlyFile 函数只是作为生产者将 fsync 任务放入队列，由 bio 线程消费执行。 no：不显式调用，由操作系统决定什么时候落盘。该模式下，Redis 无法决定曾玲落地时间，容错能力不可控。 采用 every second 策略，实际吞吐量仍然与磁盘的写入能力有关。Redis 对外写服务的吞吐量仍然可能超过磁盘的写入吞吐量，否则会造成 bio 任务队列积压，为保护内存用量会限制任务队列的长度使得后续提交任务时阻塞。Reids 仍然通过阻塞来处理磁盘吞吐量低的情况，如果发现 bio 执行 fsync 的线程还在执行中的时候，是不会往队列提交任务的，阻塞发生在 write 函数上：当 bio 线程执行 fsync 时，write 方法会自然阻塞。 恢复流程 一旦发现存在 AOF，会选择增量恢复，通过 loadAppendOnlyFile 方法恢复数据，将 AOF 中保存的命令重新执行一遍。 优化 由于采用了追加写的方式，AOF文件会越来越大，占用大量的磁盘空间，同时降低了恢复加载效率，于是 Redis 通过 rewrite 机制合并历史 AOF 记录。 该机制针对增量写过程中 AOF 文件大于某个状态的快照文件大小时，此时使用快照代替增量，以减少磁盘空间占用。快照采用 cmd 形式来承载，即将快照中的所有 KV 键值对用插入命令来表示，从而保证快照文件和普通的 AOF 文件一致，从而复用相同的加载逻辑来统一处理 Redis 启动时的数据恢复。 实现：定时任务定期检查是否满足 rewrite 条件，满足的话 fork 一个子进程，创建完成后获得 Redis 主进程的数据状态，写入 rewrite的AOF文件，子进程运行期间，Redis 主进程继续对外提供服务，新的增量写入到 redisServer 对象的 aof_rewrite_buf_blocks 中，待子进程完成后，将增量内容追加到 rewrite 的快照文件末尾，再后续的增量，会写入新的 AOF 中。 历史 AOF 文件：快照形式保存，仍然使用 CMD 插入命令保存； 快照写入期间的增量：待快照写入完成后追加到快照文件末尾； 后续增量：写入到新的 AOF。 分布式 Redis 单实例节点在实际应用中可能面临的问题： 数据量伸缩：面对存储容量达到瓶颈时，作为缓存可以利用 key 的过期淘汰机制从而控制容量；但作为 NoSQL 时，业务数据长期有效时淘汰机制不再适用。 访问量伸缩：单实例 Redis 单线程运行，吞吐量受限于单次请求处理的平均时耗。 单点故障：持久化机制一定程度上解决了单点故障的问题，但由于单实例部署，在发生不可恢复故障时，如何保证业务数据不丢失以及恢复机制成为了挑战。 解决方案（对于数据存储系统而言通用的解决方案）： 水平拆分：分布式环境下，节点分为不同的分组，每个分组处理业务数据的一个子集，分组之间数据无交集。数据无交集的特性使得水平拆分解决了数据量瓶颈，可以通过增加减少分组来伸缩数据量，同时也解决了访问量瓶颈，吞吐量也会随着分组的增加而增加。 主备复制：同一份业务数据存在多个副本，对数据的每次访问根据一定规则分发到某一个或者多个副本上执行。通过 W + R &gt; N 的读写配置可以做到读取数据内容的实时性，随着N的增加，当读写访问量差不多时，业务的吞吐量相比单实例会提升到逼近2倍。但实际中，读的访问量常常远高于写的量，W=N，R=1，吞吐量会随着读写真的增加而提升。 故障转移：业务数据所在节点发生故障时，这部分业务数据转移到其他节点上进行，使得故障节点在恢复期间，对应的业务数据仍然可用。所以需要保证业务数据保持多个副本，位于不同的节点上。 水平拆分 数据分布 数据分布指的是一种映射关系f，每个业务数据 key 都能通过映射关系确定唯一的实例。主要取决于 Redis 客户端。 hash 映射 将不可控的业务值域 key 映射到可控的有限值域（hash 值）上，且映射做到均匀，再将有限的均匀分布的 hash 值枚举地映射到 Redis 实例上。例如 crc16(key) % 16384 这个 hash 算法将 key 映射到 0~16383 的有限整数集合上，再依据一定的规则将整数集合的不同子集不相交地划分到不同 Redis 实例上，以实现数据分布。 范围映射 选择 key 本身作为数据分布的条件，且每个数据节点存放的 key 的值域是连续的一段范围。如 0 &lt;= key &lt; 100 时，数据存放到实例 1 上；100 &lt;= key &lt; 200 时，数据存放到实例 2 上，以此类推；key 的值域由业务层决定，业务层需要清楚每个区间的范围和 Redis 实例数量才能完整地描述数据分布，使得业务域的信息（key 的值域）和系统域的信息（实例数量）耦合，数据分布无法在纯系统层面实现，从系统层面来看，业务域的 key 值域不确定，不可控。 hash 和范围结合 典型的方式是一致性哈希，首先对 key 进行计算，得到值域有限的 hash 值，再对 hash 值做范围映射，确定该实例的具体存放实例。 该方式的优势在于节点新增或减少时，涉及的数据迁移量小——变更的节点上涉及的数据只需要和相邻节点发生迁移关系；缺点是节点不是成倍变更（数量变成原有的 N 倍或者 1/N）时，会造成数据分布的不均匀。 请求路由 根据请求中涉及到的 Key,用对应的数据分布算法得出数据位于哪个实例，再将请求路由至该实例。需要关注数据跨实例的问题。 只读的跨实例请求 将请求中的多个 Key 分别分发到对应实例上执行，再合并结果。其中涉及语句的拆分和重生成。 跨实例的原子读写请求 事务、集合型数据的转存操作（ZUNIONSTORE），向实例 B 的写入操作依赖于对实例 A 的读取。单实例情况下，Redis 的单线程特性保证此类读写依赖的并发安全，而在跨实例情况下，存在跨节点读写依赖的原子请求是不支持的。 在 Redis Cluster 之前，通常通过 proxy 代理层来处理 sharding 逻辑，代理层可以位于客户端本身（如Predis）,也可以是独立的实例（如 Twemproxy） 主备复制 Redis 采用主备复制的方式保证一致性，即所有的节点中，有一个主节点 master 它对外提供写入服务，所有的数据变更由外界对 master 的写入触发，之后 Redis 内部异步地将数据从主节点复制到其他节点 Slave 上。 主备复制流程 Redis 包含 master 和 slave 两种节点：master 节点对外提供读写服务，slave 节点作为 master 的数据备份，拥有 master 的全量数据，对外不提供写服务。主备复制由 slave 主动触发，主要流程为： slave 向 master 发起 SYNC 命令，这一步在 slave 启动后触发，master 被动地将新 slave 节点加入自己的主备复制集群； master 收到 SYNC 后，开启 BGSAVE 操作。BGSAVE 是 Redis 的一种全量模式的持久化机制； BGSAVE 完成后，master 将快照信息发送给 slave； 发送期间，master 收到来自用户客户端新的写命令，除了正常地响应以外，再存入一份到 backlog 队列； 快照信息发送完成后，master 继续发送 backlog 队列信息； backlog 发送完成后，后续的写操作同步发给 slave，保持实时地异步复制； Slave 端的主要操作为： 发送完 SYNC 命令后，继续对外提供服务； 开始接收 master 的快照信息，此时，将 slave 现有的数据清空，并将 master 快照写入自身内存； 接收 backlog 内容并执行它，即恢复操作，期间对外提供读请求； 继续接收后续来自 master 的命令副本并继续恢复，以保持数据和 master 一致。 有多个 slave 节点并发发送 SYNC 命令时，如果第二个 slave 的 SYNC 命令发生在 master 的 BGSAVE 之前，那么第二个 slave 将收到和第一个 slave 相同的快照和后续 backlog；否则，第二个 slave 的 SYNC 命令将触发 master 的第二次 BGSAVE。 断点续传 每次 Slave 节点向 master 发起同步（SYNC）指令来同步数据时，master 都会 dump 全量数据并发送给 Slave。当一个 Slave 已经和 Master 完成了同步操作并持续保持了长时间，突然网络断开很短的一段时间再重新连接，Master 不得不做一次全量的 dump 的传送，然而由于只是断开了很短的时间，重连之后 master 和 slave 的差异数据较少，全量 dump 数据的绝大部分在 Slave 中已有，故只需要同步连接断开期间的少量数据即可。 Redis 针对断点续传场景提出了 PSYNC 的命令来替代 SYNC，做到 Master-Slave 基于断点续传的主备同步协议。在两端通过维护一个 offset 记录当前已经通不过的命令，slave 断开期间，master 的客户端命令会保持在缓存中，当重连之后，告诉 master 断开时的最新 offset，master 则将缓存中大于 offset 的数据发给 slave，而断开前已经同步过的数据，则不需要再重新同步，减少了数据的传输开销。 故障迁移 Failover Failover：在具有主备关系的实例组成的集群中，当 master 故障时，slave 可以成为新的 master，对外提供读写的服务。 主要问题：谁去发现 master 的故障做 failover 的决策？ 保持一个守护进程，监控所有的 master-slave 节点。（无法保证守护进程本身的可用性） 保持多个守护进程，同时监视所有的 master-slave 节点，解决了可用性问题，但带来了一致性问题：多个守护进程，如何就某个 master 是否可用达成一致 Redis 的 sentinel 提供了一套多守护进程间的交互机制，解决故障发现、故障迁移决策协商机制等问题。多个守护进程组成了一个集群，成为 sentinel 集群，其中的守护进程也被称为 sentinel 节点，节点间互相通信、选举、协商，在 master 节点的故障发现、故障迁移决策上表现出一致性。 sentinel 节点间的相互感知 守护进程集群中需要互相感知的节点，都向他们共同的 master 节点订阅相同的 channel：__sentinel__:hello，新加入的节点向该 channel 发布一条信息，该信息包含了自身的一些信息，该 channel 的订阅者们（其他节点）就可以发现这个新加入的节点，从而新加入的节点和已有的其他节点建立长连接。集群中的所有节点保持两两连接。 master 的故障发现 sentinel 节点通过定期地向 master 发送心跳包判断其存活状态（PING），一旦发现 master 没有正确地响应，sentinel 节点将此 master 置为“主观不可用态”。（该判定还未得到其他节点的确认） 随后将该状态发送给其他所有节点进行确认，当确认的节点数达到一定的阈值（quorum）时，则判定该 master 为客观不可用，随后进入故障迁移流程。（该阈值可配置） 故障迁移决策 可能存在多个 sentinel 节点同时发现 master 的不可用问题并同时通过交互确认了客观不可用的状态，同时打算开始故障迁移流程，此时需要进行选举操作来决定唯一的故障迁移发起者。 Redis 的 sentinel 机制采用了类似 Raft 协议实现了选举算法： sentinelState 的 epoch 变量类似于 Raft 协议的中 term（选举回合）； 每一个确认了 master 的客观不可用态的节点向周围广播自己参选的请求； 每一个接收到参选请求的节点，如果是第一次接收到参选请求，就将该请求对应的节点置为本次选举回合的意向节点并回复它，如果已经给出了回复，将拒绝本回合的其他请求； 参选者如果收到了超过一半的意向同意回复，则确定该节点为leader。如果持续了足够长时间还会竞选出leader，开始下一个回合。 leader sentinel 节点选取出来之后，由 leader 决定根据一定的规则从 master 所有的 slave 中选取一个新的节点作为 master，并告知其他 slave 节点连接这个新的 master。 Redis Cluster Redis 3.0 之后，通过去中心化的方式提供了完整的 sharding、replication(复制机制仍复用原有的机制，只是 cluster 具备感知主备的能力)、failover解决方案，称为 Redis Cluster，即将 Proxy/Sentinel 的工作融合到了普通的 Redis 节点里。 拓扑结构 一个 Redis Cluster 由多个节点组构成，不同节点组的数据无交集，即每一个节点组对应数据 sharding 的一个分片，节点组内部分为主备（1个Master,0-N个Slave）两类节点，两者通过异步化的主备复制保证准实时一致。 Redis Cluster 中的各个节点之间，两两通过 Redis Cluster Bus 交互，交互以下关键信息： 数据分片 slot 和节点的对应关系； 集群中每个节点的可用状态； 集群结构（配置）发生变更时，通过一定的协议对配置信息达成一致；（数据迁移，故障迁移，单点master发现和主备变更等行为） 发布订阅功能内部实现所需要交互的信息。 Redis Cluster Bus 通过单独的端口连接，交互字节序列化信息，在内部进行通信，效率较高，相比于 Client 到 Server 的字符序列化传输。 Redis Cluster 是一个去中心化的分布式实现方案，客户端可以和集群中的任一节点连接，根据某个节点在集群中的交互流程，逐渐获知全集群的数据分片映射关系。 配置的一致性 Redis Cluster 通过引入两个自增的 epoch 变量来保证集群中各个节点配置信息保持最终一致 配置信息的数据结构 clusterState：单个节点的视角看集群的配置状态 currentEpoch; //整个集群的最大版本号 nodes; //包含了本节点所知的集群中的所有节点的信息 stateOne; //分片迁移状态 failoverState;//failover状态 clusterNode：记录了每个节点的信息 nodeId; epoch; //该信息的版本号 slots; //该节点对应的数据分片 slaves; master; ip:port; state; type; 信息交互 由于不存在统一的配置中心，各个节点对集群的认知来自于节点信息间的交互，通过内部通信机制 Redis Cluster Bus 完成。 clusterMsg type; // 消息类型 PING/PONG epoch; // epoch版本号相关 senderNodeId; senderSlots; gossip; // 消息体 clusterMsgDataGossip nodeId; ip:port; state; // 类型状态 Gossip 协议中，每次PING/PONG包只包含全集群的部分节点信息，节点随机选取，以此控制网络流量，由于交互较为频繁，短时间的几次交互之后，集群状态以这样的Gossip协议方式被扩散到了集群中的所有节点。 一致性的达成 当集群结构不发生变化时，可以通过 Gossip 协议在几轮交互之后得知全集群的结构信息，达到一致的状态。 在集群结构发生变化时，只能由优先得知变更信息的节点利用 epoch 变量将自己的最新信息扩散到集群，达到最终一致。 Redis Cluster 在集群结构发生变化时，通过一定的时间窗口控制和更新规则保证每个节点看到 currentEpoch 是最新的。 集群信息更新遵循的规则： 某节点率先知道信息变更，该节点将 currentEpoch 自增使之成为集群中的最大值，再利用自增后的 currentEpoch 作为新的 epoch 版本 某个节点收到比自己大的 currentEpoch 时，更新自己的 currentEpoch 值使之保持最新； 收到 Redis Cluster Bus 消息里某个节点信息的 epoch 值大于接收者自己内部配置信息的存储的值时，将自己的映射信息更新为消息的内容； 收到 Redis Cluster Bus 消息里某个节点信息未包含在接受节点的内部配置信息时，意味着接收者尚未意识到消息所指节点的存在，此时接收者直接将消息的信息添加到自己的内部配置信息中。 sharding 数据分片 slot 数据分布算法：slotId = crc16(key) % 16384 针对关联性较强但可能分散到不同节点的数据，Redis 引入了 HashTag 的概念，使得数据分布算法可以根据 Key 的某一部分计算，让相关的两条记录落到同一个分片，例如(根据{}内的数据作为分布算法的输入)： 商品交易记录：product_trade_{prod123} 商品详情记录：product_detail_{prod123} 客户端路由 Redis Cluster 的客户端具备路由语义的识别能力，且具备一定的路有缓存能力。当一个 client 访问的 key 不在对应 Redis 节点的 slots 中，Redis 返回给 client 一个 moved 命令，告知正确的路由信息。收到 moved 之后，client 端继续向新的 slot 发起请求，但仍然可能不是最新的节点，继续返回 moved，同时客户端更新本地路由缓存，以便下一次直接访问到正确的节点，减少交互次数。 针对数据重分布场景，未命中时服务端通过 ask 指令控制客户端的路由。相比于 moved 的不同之处在于，ask 只重定向到新节点，不更新客户端的路由缓存。避免路有缓存信息发生频繁变更。 分片的迁移 节点和分片的映射关系可能发生变更： 新节点作为 master 加入 某个节点分组需要下线 负载不均衡需要调整 slot 分布 分片迁移的触发和过程控制由外部系统完成，Redis Cluster 只提供迁移过程需要的原语供外部系统调用。 针对迁移过程中的数据处理： 访问 A 节点中的尚未迁出的 key，正常处理； 访问 A 节点中的已经迁移出或者不存在的 Key，回复客户端 ASK 跳转到 B 执； B 节点不会处理非 ASK 操作重定向而来的请求，通过 MOVED 指令让客户端跳转到 A 执行。 MIGRATE 为原子操作，单线程处理，保证了某个 Key 迁移过程中的一致性； CLUSTER SETSLOT 设置 B 的分片信息，使之包含 A 的 slot，设置过程中自增 epoch，将配置信息更新到整个集群，完成分片节点映射的更新 failover 故障发现 Redis Cluster 的节点之间通过 Redis Cluster Bus 两两周期性地进行 PING/PONG 交互，当某个节点宕机时，其他发向它的 PING 消息将无法及时响应，当超过一定时间（NODE_TIMEOUT）未收到 PONG 响应，则发送者认为该节点故障，置为 PFAIL 状态。 针对可能产生误报的情况，在 NODE_TIMEOUT/2 过去了却还未收到 PONG，重建连接发送 PING 消息，若对端正常将会在很短的时间内抵达。 故障确认 集群中的每个节点都是 Gossip 协议的接收者，当 A 与 B 节点无法连接时，A 将 B 置为 PFAIL，A 持续的通过 Gossip 接收关于来自不同节点发送的 B 节点的状态信息，当 A 收到的来自 master 的 PFAIL 累积到一定数量时，PFAIL 会变为 FAIL，后续发起 Slave 选举。 Slave选举 当存在多个 Slave 同时竞选 master 的情况时，需要在选举前进行优先级的协商。根据 Slave 最后一次同步 master 信息的时间来决定优先级，时间越新，优先级越高，更有可能更早地发起选举。 竞选投票的过程与之前的 Redis Failover 中保持一致。 结构变更通知 选举成功（收到超半数的同意时），新 master 节点以最新的 Epoch 通过 PONG 消息广播自己成为 master 的信息，并让集群节点尽快更新集群拓扑信息。 可用性和性能 Redis Cluster 提供了一些手段提升性能和可用性 读写分离 针对有读写分离的需求的场景，读请求交由 Slave 处理，舍弃一定的数据一致性，换取更高的吞吐量。 由于每个 Slot 对应的节点一定是个 master 节点，客户端的请求只能发送到 Master 上，即便是发送到了 Slave，后者也会回复 MOVED 定向到 Master 上。为此提供 READONLY 命令，客户端直接向 Slave 发起该命令时，不再回复 MOVED 而是直接处理。 master 单点保护 当 A1 节点宕机时， A 节点作为 master 将成为单一节点，为了保证高可用性，会将 B 节点的其中一个 Slave 进行副本迁移，使其成为 A 节点的 Slave。 故集群中至少保证 2*master + 1 个数据节点，就可以保证任一节点宕机后仍然能自动地维持高可用的状态，称为 master 的单点保护。若无该机制，则需要维持 3*master 个节点。 ","link":"https://blog.shunzi.tech/post/distributed-cache-notes-one/"},{"title":"编程语言-教程和案例汇总","content":" 源于众多朋友、同学和学弟（对没有学妹）请教过 Java 学习路线的相关问题 在这篇博客中对曾经使用到或者收集的教程和优秀博客进行一个汇总 主要从各大编程语言入门的角度，同时也更新各个方向的相关技术栈 持续更新ing 编程语言导向篇 对于绝大部分未曾深入了解计算机和软件行业的各领域的人而言，往往可能都是以编程语言为导向开始的；以及在面对众多的招聘广告时，往往也是以 C++/Java/Python/PHP 工程师等字眼来进行岗位的总结，在相关技能描述中往往也会涉及到具体的编程语言；所以很多时候可能大家都是以我要学会“某种或者某几种编程语言”为导向来开始学习的，故在此以编程语言分类的方式来进行展开。 BTW，在对相关领域以及各类语言都有了一定的了解之后，很容易地就会发现其实编程语言永远都只是一项工具，比起国内很多程序员每天喋喋不休地争论哪种语言是最xxx的，还不如多花时间去了解每门语言自己所对应的独特的魅力。 Java 个人路线推荐：Java 面向对象基础篇 -&gt; 设计模式和常用库 -&gt; Spring全家桶 -&gt; 中间件（消息队列/缓存）-&gt; 微服务/分布式组件 -&gt; JVM 性能调优 可以多利用 Github，阅读优秀开源项目的源码实现，如有机会可积极参加相关开源项目的实现。 学习路线和教程 可参考 Github 上的优秀 Java 学习手册 [Snailclimb/JavaGuide]，其中涉及到了方方面面（从 Java 基础到计算机领域相关基础知识，再到实际的框架和中间件等，均有狩猎），甚至是面试相关，且长期维护，值得参考。 可参考知乎问题 [Java 学习线路图是怎样的？]中 Hollis 的 具体回答 &lt;-Click，该答案详细介绍了各个阶段对应的知识点，十分详细，但也无须全盘照搬，更多的需要结合实际项目中的需要再进行针对性的学习，但在开始阶段对各个知识点也要有一定的概念； 针对日常开发中最常使用到的 Java 开发框架 Spring，也有优秀的教程。可参考 Github [wuyouzhuguli/SpringAll]。主要涉及了 Spring 系列全家桶的某些具体应用，可供参考。 知乎上有很多 Java 相关的专栏，可以根据需求进行具体的检索。 入门方式 对于无太多相关领域基础的童鞋，建议视频入门或者博客入门的方式； 对于有一定基础以及习惯阅读英文文档的同学，建议从相关工具和框架的官方教程以及手册入手进行深入学习。 视频篇： [1] 慕课网：Java/Spring篇。该类教程更适合于相关方向的入门，便于了解各个知识点的概念以及应用，教程也相对简单，但在深度方面会有所欠缺。 [2] 尚硅谷：Java 学习路线篇。该类教程是培训机构设置的相关课程，其视频资源可通过百度网盘等方式找到。该类视频类似于手把手教学，适合入门，不适合长远的学习，需要自己进行总结和探索。 视频方式，最重要的一点是需要跟随视频，自己动手实现相关示例，才能保证学有所得。 博客篇 [1] 程序猿DD。博主翟永超出版过Java和Spring相关的书籍，同时也常常发布系列教程。 [2] 纯洁的微笑。博主比较结合前沿，主要针对现在行业内使用到的相关技术（如微服务框架等）做系列教程。 [3] Baeldung。国外优秀的Java教程博主，更接近原生的方式进行教学，具有一定的特色，同时可以订阅。 博客的方式，往往更接近实际的操作，所以需要自己动手实现，而不是简单的ctrl+c/v。出现问题时，针对具体问题具体分析，不断归纳总结。 深入学习 Github：互联网 Java 工程师进阶知识完全扫盲：内容涵盖高并发、分布式、高可用、微服务等领域知识，做了一个系统的整理，也可以在 issue 区查看面试相关的讨论。 Github：《后端架构师技术图谱》 ：内容涵盖计算机科学的方方面面，但住要是以 后端工程师 作为职业导向，从基本的数据结构与算法到实际的应用层面的开发，再到相关基础架构设计和其他除后端以外的系统层面的领域知识。 深入学习主要参考相关技术的官方文档或者手册。一般都可通过搜索引擎获得。如 [Spring IO]。工具或者框架的新版本特性往往通过官方手册和文档进行发布，国内外优秀的教程也往往都是参考了官方手册并实践进行撰写的。 针对某些工具的具体模块的实现，往往需要自己去看相关源码实现，可以结合他人博客中对于该模块的理解进行思考。可借助搜索引擎实现。 Python 篇 Python 其实不用太多编程语言的基础，只需掌握的基本的数学逻辑相关思想即可。当然有一定的其他的编程语言基础，对Python的上手应该更加有利。由于本人非Python作为主力语言，所以只能根据自己的一些学习路线和实际需求来大致介绍相关流程。 路线：Python3 基础教程 -&gt; Python 相关库（爬虫、Web等） 入门教程 基础教程可以参考 廖雪峰：Python 3 教程，廖老师的教程其实已经涉及到了一部分相对高级的语法和操作，例如连接数据库和简单的 Web 开发等，可以一步步动手按照教程的顺序进行学习。 Github Python - 100天从新手到大师 该教程除了讲解 Python 的相关知识以外，更多是的以一个 Python 工程师为职业导向的学习路线，如果有志于未来从事 Python 相关的工作可以参考该教程，除了编程语言以外，也涉及到了计算机科学领域其他方方面面的教程。 高级教程 主要是针对现如今 Python 在人工智能领域内不可或缺的地位，很多同学都有志于在算法方向上有长足的发展，故可能需要学习 Python，其实此时更多的是针对 Python 的一些自然语言处理、大数据处理等第三方库的学习和使用，此类教程也不胜枚举。例如 Github：莫烦 Python 和 Github ApacheCN：AILearning ","link":"https://blog.shunzi.tech/post/tutorial/"},{"title":"Tools","content":" 介绍 主要包含一些常用的在线工具网站，常用的课程资源网站，常见的云盘资源网站，以及一些大佬的博客。 分类 常用工具 在线工具集合 在线工具集合 sojson 文本 开源中国-URL转QRCode 在线文本对比工具 JSON 格式化/转义/解析 运维 在线服务器单个端口扫描 在线服务器多个端口扫描 个人总结的相关命令合集 Issue 办公 在线作图：Process On 在线作图：Draw IO 在线作图：Lucid Chart 编程 在线编程：OnlineGDB 在线编程：snippet cs-course 胡神- 名校公开课程评价网 zhenghe - 公开课笔记 资源 云盘精灵：结合Chrome插件食用更佳 博客 DB Professors Harvard University - Stratos Idreos McGill - Oana Balmau Université du Québec - Daniel Lemire Students Xinjing Zhou Chen luo (Graduated) 国内 SJTU - Chen Haibo Tsinghua - Luyouyou SJTU - Chen Rong Zuo Pengfei Hua Yu System CMU - George Amvrosiadis Network Boston University - Alan Liu ","link":"https://blog.shunzi.tech/post/tools/"},{"title":"分布式系统基本概念","content":" 介绍总结一些基本的分布式系统中的概念 简要介绍分布式系统中常见的问题和解决方案 后续会针对部分分布式常用组件进一步介绍 分布式系统理论 CAP定理 在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer's theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点: 一致性，可用性，分区容错性。 一致性（Consistency） 在分布式系统中的所有数据备份，在同一时刻是否有同样的值。等同于所有节点访问同一份最新的数据副本 操作原子性 知乎：范斌 - 漫话分布式系统共识协议: 2PC/3PC篇 2PC（Two-Phase Commit）- 阻塞、数据不一致问题、单点问题 在事务处理、关系型数据库和计算机网络中，2阶段提交协议是一种典型的原子提交协议，它是一种由协调器来处理分布式原子参与者是提交或者回滚事务的分布式算法。 3PC (Thress-Phase Commit) - 解决2PC的阻塞，但还是可能造成数据不一致 为了避免在通知所有参与者提交事务时，其中一个参与者crash不一致时，就出现了三阶段提交的方式。三阶段提交在两阶段提交的基础上增加了一个preCommit的过程，当所有参与者收到preCommit后，并不执行动作，直到收到commit或超过一定时间后才完成操作。 副本一致性 副本一致性常常是指在分布式系统中为了保证服务的高可用，往往会提供副本来进行备份。但引入副本的同时，则意味着需要带来额外的读写维护的开销。对外提供服务的往往又只有主从备份中的主节点，根据对一致性的不同要求选择不同的数据同步方式。 大体可以分类为：强一致性，弱一致性，最终一致性。 Consensus Algorithm 协同算法 Paxos算法（解决单点问题） Paxos 协议是一个解决分布式系统中，多个节点之间就某个值（提案）达到一致（决议）的通信协议，它能够处理在少数节点离弦的情况下，剩余的多数节点仍然能够达成一致。 Raft 一致性算法（解决Paxos的实现难度） 选举（Leader Election） 日志复制（Log Replication） 安全性（Safety） Lease机制 Lease 中文叫租约，是一种广泛应用于分布式系统领域的协议，它是一种维护分布式系统一致性的有效工具。常用于分布式缓存的更新和管理。 Quorum NWR NWR 时分布式系统中一种用于控制一致性级别的策略。反别对应的含义为： N - 同一份数据的拷贝份数， W - 是更新一个数据对象的时候需要确保成功更新的份数， R - 读取一个数据需要读取的拷贝份数。 W + R &gt; N 保证了每次读写的数据都是最新的，从而保证了强一致性 W &gt; N / 2 保证了两个事务不能并发写一个数据 多版本并发控制 MVCC 在数据库中常常使用两种机制来提高事物的并发。一种是基于锁（如行级锁）的并发控制机制，称为悲观并发控制机制；一种是基于乐观锁的乐观并发控制机制，但乐观锁不是一种真正的锁，只是一种并发控制的思想；一种是多版本控制机制 MVCC。 以Mysql InnoDB为例，MVCC的实现主要是借助每张表增加两个字段create version、delete version，在执行对应的CUD操作时对应的更新两个字段，在执行查询操作需要隐式地增加相应的版本条件来查询最终的结果。 Gossip Gossip是一种去中心化思路的分布式协议，解决状态在集群中的传播和状态一致性的保证的问题，是一种实现简单、具备较高容错性和性能的应用广泛的状态同步协议。 状态的传播类似于图计算中以边的形式将各个服务节点联系起来进行通信，对于状态的一致性使用相应的版本号进行保证，状态信息的传播时间收敛在 O(log(N)) 内，其中 N 为服务节点的数量。 可用性（Availability） 在集群中一部分节点故障后，集群整体是否还能响应客户端的读写请求。每次请求都能获取到非错的响应——但是不保证获取的数据为最新数据 可用性 VS 可靠性 可靠性（reliability）：在规格时间间隔内和规定条件下，系统或部件执行所要求功能的能力。例如：在客户端与服务器端通信时，如果网络故障，系统不能出现故障。 可用性（availability）：软件系统在投入使用时可操作和可访问的程度，或能实现其指定系统功能的概率。例如：系统的可用性要达到98%。 通俗的例子解释：可靠性是一个持续性的状态，更多地强调系统自身；而可用性是一个短暂的状态，更多地强调外部的触发。就好比一个人，你找他的时候能不能找到，这是可用性；而他干活靠不靠谱，则是可靠性。一个人如果随叫随到，但是时不时偷懒，就是高可用、低可靠；而如果他经常找不到人，但干活很负责，就是低可用、高可靠。 分区容错性（Partition tolerance） 以实际效果而言，分区相当于对通信的时限要求。系统如果不能在时限内达成数据一致性，就意味着发生了分区的情况，必须就当前操作在C和A之间做出选择。 CA 放弃分区容错性，CA系统更多的是指y允许分区后各个子系统依然保持CA。典型例子 关系型数据库、LDAP(轻量目录访问协议数据库) CP 不要求可用性，相当于每个请求都需要在 Server 之间强一致，而 P 分区会导致同步时间无限延长。典型例子传统的数据库分布式事务以及分布式锁。 AP 放弃一致性，一旦分区发生，节点之间可能失去联系，为了保证高可用，每个节点只能用本地数据提供服务，而这样会导致全局数据的不一致性。典型例子 NoSQL 以及DNS Web缓存等 BASE理论 BASE理论是由eBay架构师提出的。BASE是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网分布式系统实践的总结，是基于CAP定律逐步演化而来。其核心思想是即使无法做到强一致性，但每个应用都可以根据自身业务特点，才用适当的方式来使系统打到最终一致性。 基本可用（Basically Available） 什么是基本可用呢？假设系统，出现了不可预知的故障，但还是能用，相比较正常的系统而言： 响应时间上的损失：正常情况下的搜索引擎0.5秒即返回给用户结果，而基本可用的搜索引擎可以在2秒作用返回结果。 功能上的损失：在一个电商网站上，正常情况下，用户可以顺利完成每一笔订单。但是到了大促期间，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面。 软状态（Soft State） 什么是软状态呢？相对于原子性而言，要求多个节点的数据副本都是一致的，这是一种“硬状态”。 软状态指的是：允许系统中的数据存在中间状态，并认为该状态不影响系统的整体可用性，即允许系统在多个不同节点的数据副本存在数据延时。 最终一致性（Eventually Consistent） 上面说软状态，然后不可能一直是软状态，必须有个时间期限。在期限过后，应当保证所有副本保持数据一致性，从而达到数据的最终一致性。这个时间期限取决于网络延时、系统负载、数据复制方案设计等等因素。 参考链接 [1] Raft 一致性算法动画演示 [2] Raft Github Page [3] 博客园：分布式一致性协议介绍（Paxos、Raft） [4] 2PC到3PC到Paxos到Raft到ISR [5] 分布式系统原理 之3 Lease机制 [6] Quorum NWR [7] 博客园：图解分布式一致性协议Paxos [8] 浅谈数据库并发控制 - 锁和 MVCC [9] P2P 网络核心技术：Gossip 协议 [10] 被误用的“一致性” [11] 掘金：分布式理论-CAP理论 其他概念 状态特性 在大部分应用中都提倡无服务状态，分布式环境中的任何节点（Node）也是无状态的。无状态是指不保存存储状态，则可以随意重启和替代，便于做扩展。 系统重发与幂等性 针对分布式服务中的远程过程调用，往往会使用到诸如HttpClient一样的客户端向服务端发起相应的请求，在请求过程中可能会存在一些链路上的故障，此时将利用客户端对应提供的重试的机制重新发起请求。 幂等性：就是调用一次和调用N次应该返回同样的结果。针对系统重发操作，往往都有幂等性的要求。 分布式系统常见设计策略 心跳检测 心跳检测主要用于“判定某节点是否正常工作”。 注意：心跳检测机制如果运行正常，即能准确地接收到相关心跳信息，则可以确定该节点正常运行，但若无法接收心跳，无法直接判定该节点已经宕机（可能该节点处于繁忙状态，导致检测调用超时）。 周期检测心跳机制和累计失效检测机制 每间隔 t 时间向节点集群发起检测请求，设定超时时间，如果超过超时时间，对应地判定为死亡。（但容易产生误判） 可以进一步统计实际检测节点的返回时间，包括得到一定周期内的最长时间，并根据现有未成功返回的时间在历史统计里的分布来计算得到节点死亡概率。 对于宣告濒临死亡的节点可以发起有限次数的重试。 高可用设计 主备 （Master-Salve） 当主机宕机时，备机接管主机的一切工作，将主机恢复正常后，按使用者设定以自动（热备）或手动（冷备）方式将服务切换到主机上运行。常用于数据库的高可用方案。 互备-双活（Active-Active） 两台主机同时运行各自的服务工作且相互监测情况。 在数据库的高可用MM（Master-Master）模式中，即一个系统中有两个主节点每个主节点都具有相应的读写能力，但需要根据时间戳或者业务版本号对应地合并相应的数据版本。分布式版本控制管理系统Git就是一种常见的MM模式的代表，具备最终一致性。 集群（Cluster） 多个节点在运行，通过主控节点对应地将请求按照一定的规则进行分发，例如Zookeeper。但对于主控节点的高可用性，需要提供相应的保证。可以对主控节点使用相应的主备机制来保证对应的高可用。 容错性 保证分布式环境下相应系统的高可用或者健壮性。 以缓存雪崩的解决方案为例，可以通过设置默认值的方式来针对某些无效的数据查询操作进行优化，减少数据库的查询操作，使用缓存直接返回无效查询的结果，避免缓存击穿。 负载均衡 针对集群中的多个正常运行的节点，可以设置一个调度器，并采用一定的负载均衡策略将相关计算任务的压力对应的分摊到各个服务节点上，从而提高整个系统的负载能力。负载均衡有对应的硬件和软件的解决方案。 硬件解决方案有 F5 ，智能交换机，可以做4-7层负载均衡，具有负载均衡、应用交换、会话交换、状态监控、智能网络地址转换、通用持续性、响应错误处理、IPv6网关、高级路由、智能端口镜像、SSL加速、智能HTTP压缩、TCP优化、第7层速率整形、内容缓冲、内容转换、连接加速、高速缓存、Cookie加密、选择性内容加密、应用攻击过滤、拒绝服务(DoS)攻击和SYN Flood保护、防火墙—包过滤、包消毒等功能。 软件解决方案：LVS(四层， IP+Port)、HAProxy、Nginx 参考链接 [1] （总结）Nginx/LVS/HAProxy负载均衡软件的优缺点详解 [2] LVS三种模式的区别及负载均衡算法 分布式系统设计实践 全局ID生成 UUID 日期或时间 时钟序列 全局唯一的IEEE机器识别号（MAC地址） ID生成表模式 使用MySql自增ID作为全局唯一ID。首先创建单独的数据库，并创建一个表。 CREATE TABLE `Ticket64` ( `id` bigint(20) unsigned NOT NULL auto_increment, `stub` char(1) NOT NULL default '', PRIMARY KEY (`id`), UNIQUE KEY `stub` (`stub`) )ENGINE=MyISAM 然后在需要全局唯一ID的应用端的事务会话里添加以下语句从而获得最新的自增ID REPLACE INTO Ticket64(stub) VALUES ('a'); SELECT LAST_INSERT_ID(); 但此时只是单机基础上的唯一ID，为了解决高可用问题，可以启用多台数据库服务器来生成ID，每一个服务器自增ID设置不同的步长和起始值来进行区分。例如Flicker中使用了两台数据库服务器分别生成奇数和偶数的自增ID。 Snowflake 雪花算法 组成 41位的时间序列（精确到毫秒，长度可以用到69年） 10位的机器标识（10位可以支持部署1024个节点） 12位的计数顺序号（12位的计数序号支持每个节点没毫秒产生4096个ID序号） 优点：高性能、低延迟、独立的应用、按时间有序 缺点：需要独立的开发部署 ID生成表+缓存 通过使用ID生成表成批获取ID，譬如1000，放到缓存中，可以减少数据库交互次数，从而提高性能。 优点：高性能、低延迟 缺点：ID不连贯 哈希取模 哈希方式是最常见的数据分布方式，实现方式是通过可以描述记录的业务的id或key，通过hash函数求余。余数作为处理该数据的服务器索引编号处理。 存在的问题：hash函数的不同可能导致数据产生严重倾斜，在扩容等操作时，数据迁移比较繁琐，之前的映射关系可能存在不匹配的情况。 解决办法：在逻辑上先预设足够大数目的数据库，随着物理负载的增加，把对应逻辑的数据库迁移到新增的物理库上即可，对于应用透明，只需要维护逻辑库和物理库之间的映射关系。 一致性hash 一种分布式哈希（DHT）算法，主要解决单调性和分散性的问题。单调性是指哈希的结果应该能够保证原有已分配的内容可以映射到原有的缓冲中去，避免在节点增减过程中出现不能命中的情况。 实现方式：构建一个Hash环，将对应的key哈希到0~2^32-1的数字空间中，将这些数字头尾相连。 优点：可以任意动态添加删除节点，删除节点只会影响一致性Hash环上相邻的节点。 为了尽可能均匀地分布节点和数据，引入虚节点，增强平衡性。 路由表 针对某些需要全局计算的场景。 路由表信息集中管理，可能存在单点故障，需要解决对应的高可用问题。 数据拆分 以阿里巴巴开源的分布式数据库中间件Cobar为例，可以在分布式的环境下将数据表放入不同的数据库中来实现，并且支持一张数据的水平拆分成多份放入不同的库中。 参考链接 [1] 深入浅出一致性Hash原理 [2] cobar的读写分离及高可用 分布式系统中常见的问题 脑裂问题 问题描述：主备是实现高可用的有效方式，但存在一个脑裂问题。该问题是指在一个高可用系统之中，当联系着的两个节点断开联系时，本来作为一个整体的系统，分裂为两个独立的节点，这时两个节点开始竞争共享资源，结果导致系统紊乱。 根本原因：心跳检测机制存在不确定性。当因为某种原因判死的Master服务器仍然在运行，即出现假死现象，此时系统中将会有两个Master节点进行资源的竞争。 解决办法：设置第三方检测服务器来进行仲裁，当Slave即将接管Master时，第三方服务器Monitor尝试ping一下maste。若无通讯，判定死亡。在Master运行时，间隔一段时间由Master服务器尝试ping slave和Monitor，如果都出现异常，则暂停业务操作，进行重试，重试一定次数之后退出或执行对应的故障处理程序。 存在问题：Monitor节点的高可用无法保障，会出现双主脑裂问题。可以通过引入Lease机制来解决。 RPC (Remote Procedure Call) 远程过程调用（英语：Remote Procedure Call，缩写为 RPC）是一个计算机通信协议。该协议允许运行于一台计算机的程序调用另一台计算机的子程序，而程序员无需额外地为这个交互作用编程。其调用协议通常包含 传输协议 和 序列化协议。RPC过程往往不在一个进程（或线程）内，所以需要其他协议来辅助完成进程（或线程）间的通信。 以RESTFul为代表的HTTP服务本质上是RPC利用了HTTP协议的实现。 基于其他非HTTP协议实现的RPC，主要是为了提高相应的传输性能，相比之下，HTTP协议在高性能的要求下的性能表现不尽人意。 RPC 架构 客户端 (Client) 服务端 (Server) 客户端存根 (Client Stub)，存放服务端的地址消息，再将客户端的请求参数打包成网络消息，然后通过网络远程发送给服务方。 服务端存根 (Server Stub)，接收客户端发送过来的消息，将消息解包，并调用本地的方法。 RPC框架 gRPC: 是Google最近公布的开源软件，基于最新的HTTP2.0协议，并支持常见的众多编程语言。 我们知道HTTP2.0是基于二进制的HTTP协议升级版本，目前各大浏览器都在快马加鞭的加以支持。这个RPC框架是基于HTTP协议实现的，底层使用到了Netty框架的支持。 使用了 Protocol Buffers 的序列化协议。二进制流进行传输，消耗更小。 为了取得更好的支持和兼容性，建议使用 Proto3 Thrift: Facebook的一个开源项目，主要是一个跨语言的服务开发框架。它有一个代码生成器来对它所定义的IDL定义文件自动生成服务代码框架。用户只要在其之前进行二次开发就行，对于底层的RPC通讯等都是透明的。不过这个对于用户来说的话需要学习特定领域语言这个特性，还是有一定成本的。 Dubbo: 阿里集团开源的一个极为出名的RPC框架，在很多互联网公司和企业应用中广泛使用。协议和序列化框架都可以插拔是及其鲜明的特色。同样 的远程接口是基于Java Interface，并且依托于spring框架方便开发。可以方便的打包成单一文件，独立进程运行，和现在的微服务概念一致。 参考链接 [1] 知乎：【RPC】HTTP服务和RPC服务，如何选择？ [2] gRPC 官方手册中文版 ","link":"https://blog.shunzi.tech/post/basic-concept-of-distributed-system/"},{"title":"常见的C语言库函数","content":" 常见的C语言库函数介绍（主要针对IO相关操作） 部分函数仅为linux系统特有 后续会对相关系统调用中涉及到的IO进行讲解 常见的C语言库函数 #include &lt;stdio.h&gt; int open(const char *path,int access[,unsigned mode]) 功能： 文件句柄 = open(&quot;文件名&quot;,打开模式|打开模式); 参数：打开模式： access模式：(最多使用以下模式中的一个，且必须有一个) O_RDONLY 以只读方式打开 O_WRONLY 以只写方式打开 O_RDOWR 以读写方式打开 其他模式：（可选模式，可以同时存在，用或连接） O_APPEND：以后每次写文件时都会先将当前文件偏移量设置到文件末尾，但是读文件时时不影响的 O_CREAT：如果文件不存在则创建 O_EXCL：要打开的文件如果存在则出错，必须要和O_CREAT参数一起使用 O_TRUNC：打开文件的同时将文件中的内容清除 O_NOCTTY：如果打开的文件是终端设备，则不将此设备设置为进程的控制终端 O_NONBLOCK：如果打开的文件是一个管道、一个块设备文件或一个字符设备文件，则后续的I/O操作均设置为非阻塞方式 O_SYNC：使每次write都等到物理I/O操作完成 S_IRUSR： Permits the file's owner to read it. S_IWUSR： Permits the file's owner to write to it. 返回值：打开成功，返回文件句柄；失败返回 -1 int close(int handle) 功能：关闭由文件句柄所指向的文件 关闭结果 = close(文件句柄) 返回值：0(成功)，-1(失败) FILE* fopen(const char *path, const char *mode) 功能：打开一个特定的文件，并把一个流和这个文件相关联 参数： path：文件路径对应的字符串 mode：打开模式，流形态 r：打开只读文件，该文件必须存在 w：打开只写文件，已存在的文件将被清空 r+：以读写的方式打开文件，该文件必须存在 w+：打开可读写文件，已存在文件将被清空 rb：打开一个只读二进制文件，文件必须存在 wb：新建一个只写的二进制文件，已存在的将被清空 rb+：打开一个可读写的二进制文件，该文件必须存在 wb+：新建一个可读可写的二进制文件，已存在的将被清空 a：打开或者新建一个文本文件，只允许文件末尾追加写 a：打开或新建一个文本文件，可以读，但只允许在文件末尾追写 ab：打开或新建一个二进制文件，只允许在文件末尾追写 ab+：打开或新建一个二进制文件，可以读，但只允许在文件末尾追写 返回值： 成功：返回一个指向 FILE 结构的指针，该结构代表这个新创建的流。 失败：返回一个空指针，errno会存储相关错误代码。 int fclose(FILE *stream); 功能：关闭文件流 返回值：关闭成功，返回 0， 否则返回 EOF -1。输出流在关闭前会刷新对应的缓冲区。 int fseek(FILE *stream, long offset, int fromwhere); 功能：从指定位置将文件指针移动指定偏移量长度 参数： stream：文件流指针 offset：要移动的偏移量长度 fromwhere：文件指针移动的起始位置 #define SEEK_CUR 1：当前位置 #define SEEK_END 2：文件尾 #define SEEK_SET 0：文件头 返回值：成功，返回0，失败返回非0值，并设置error的值，可以用perror()函数输出错误。 long ftell(FILE *stream); 功能：用于得到文件位置指针当前位置相对于文件首的偏移字节数。 用法：使用fseek函数后再调用函数ftell()就能非常容易地确定文件的当前位置。利用函数 ftell() 也能方便地知道一个文件的长。如以下语句序列： fseek(fp, 0L,SEEK_END); len =ftell(fp); 首先将文件的当前位置移到文件的末尾，然后调用函数ftell()获得当前位置相对于文件首的位移，该位移值等于文件所含字节数。 size_t fread ( void *buffer, size_t size, size_t count, FILE *stream); 功能：从给定的流 stream 读取数据，最多读取count个项，每个项size个字节。 参数： buffer：用于接受数据的内存地址 size：要读的每个数据项的字节数 count：要读取的数据项数目 stream：操作的输入流 返回值：调用成功返回实际读取到的项个数（小于或等于count），如果不成功或读到文件末尾返回 0。 size_t fwrite(const void *ptr, size_t size, size_t nmemb, FILE *stream); 功能：向指定的文件中写入若干数据块，如成功执行则返回实际写入的数据块数目。该函数以二进制形式对文件进行操作，不局限于文本文件。 参数： ptr：存放需要写入的数据的内存地址 size：要写入内容的单字节数 count：:要进行写入size字节的数据项的个数 stream：目标文件指针 返回值：如果成功，该函数返回一个 size_t 对象，表示元素的总数，该对象是一个整型数据类型。如果该数字与 nmemb 参数不同，则会显示一个错误 #include &lt;sys/uio.h&gt; ssize_t pread(int fd, void *buf, size_t count, off_t offset); 功能：在指定偏移量的位置开始读取指定长度的个字节 参数： fd：文件描述符 *buf：数据缓冲区指针，存放读取出来的数据 count：读取数据的字节数 offset：读的起始地址的偏移量， 读取地址=文件开始+偏移量。执行后文件偏移指针不变。 返回值：读取到缓冲区的字节数 ssize_t pwrite(int fd, const void *buf, size_t count, off_t offset); 功能：在指定偏移量的位置上，写入对应的数据 参数： fd：要写入数据的文件描述符 *buf：数据缓存区指针，存放要写入的数据 count：写入文件中的数据字节数 offset：偏移地址。写入地址=文件开始+offset。注意，执行后，文件偏移指针不变 返回值：写入到文件中的字节数 ssize_t readv(int fd, const struct iovec *iov, int iovcnt); 功能：数据散布读到缓冲区中（散点输入） 参数： fd：文件描述符 *iov：指向 iovec结构数组的一个指针。buffer iov_base：存放数据的缓冲区 iov_len：缓冲区的数据长度 iovcnt：数组元素的个数。缓冲区的个数 返回值：读取的字节数 ssize_t writev(int fd, const struct iovec *iov, int iovcnt); 功能：将散步在缓冲区的数据（可能不连续）一起写入到文件描述符对应的文件中去。 参数： fd：文件描述符 *iov：指向iovec结构数组的一个指针。（要写入的数据） iovcnt：iovec数组的长度，缓冲区的个数 返回值：写入的字节数 ssize_t preadv(int fd, const struct iovec *iov, int iovcnt, off_t offset); 功能：在指定偏移量的位置上，将数据散布读到缓冲区中（散点输入） 参数： fd：文件描述符 *iov：指向 iovec结构数组的一个指针。buffer iov_base：存放数据的缓冲区 iov_len：缓冲区的数据长度 iovcnt：数组元素的个数。缓冲区的个数 offset：偏移量 返回值：读取的字节数 ssize_t pwritev(int fd, const struct iovec *iov, int iovcnt, off_t offset); 功能：在指定偏移量的位置上，将散步在缓冲区的数据（可能不连续）一起写入到文件描述符对应的文件中去。 参数： fd：文件描述符 *iov：指向 iovec结构数组的一个指针。buffer iov_base：存放数据的缓冲区 iov_len：缓冲区的数据长度 iovcnt：数组元素的个数。缓冲区的个数 offset：偏移量 返回值：写入的字节数。 #include&lt;unistd.h&gt; int fsync(int fd) 功能：确保文件 fd 所有已修改的内容已经正确同步到硬盘上，该调用会阻塞等待直到设备报告IO完成。(由于 write() 函数本质更新的是内存中的页缓存，而已经更新但还没刷入磁盘的页面，也叫脏页面，不会立即更新到硬盘中，而是由操作系统统一调度，如由专门的flusher内核线程在满足一定条件时，如一定时间间隔、内存中的脏页达到一定比例等条件，才将脏页面同步到硬盘上，通过将页面放入磁盘设备的IO请求队列来进行同步。所以 write 函数不会等到硬盘IO完成之后才返回，提供了一种‘松散的异步语义’，但是无法保证事务的持久化和一致性，此时则需要操作系统提供的同步IO原语来进行保证) 参数：文件描述符 返回值：成功 返回 0；失败 返回 -1 int fdatasync(int fd); 功能：fdatasync的功能与fsync类似，但是仅仅在必要的情况下才会同步metadata（元数据主要包括文件大小、最后访问时间和修改时间等），因此可以减少一次IO写操作。其中必要的情况主要包括文件的尺寸（st_size）发生变化以及部分应用程序对于文件最后访问时间和修改时间的有严苛要求等。 #incude &lt;sys/mman.h&gt; int msync(void *addr, size_t length, int flags) 功能：针对采用内存映射文件方式进行文件IO的操作，譬如mmap将文件的页缓存直接映射到进程的地址空间，通过写内存的方式来修改对应的文件，也提供了相应的同步系统调用来确保数据的持久化和一致性，往往在调用munmap()后才执行该操作。 参数： addr：文件映射到进程空间的地址； lenth：映射空间的大小; flags：刷新的参数设置 MS_ASYNC：异步，调用后立即返回，不等待更新完成 MS_SYNC：同步，调用后会等到更新完成返回 MS_INVALIDATE：通知使用该共享区域的进程，数据已经改变时，在共享内容更改之后，使得文件的其他映射失效，从而使得共享该文件的其他进程去重新获取最新值； 返回值：成功 返回 0；失败 返回 -1 参考链接： [1] CSDN: 第5章 文件IO [2] CSDN：linux 同步IO: sync、fsync与fdatasync [3] 简书：聊聊BIO，NIO和AIO (2) [4] CSDN：io.h头文件下的一些函数 ","link":"https://blog.shunzi.tech/post/c-header/"},{"title":"一","content":" 主题和《四分之一》一样的系列日志。 距离上次写已经三年了，也很难有以往的文字功底。 经历倒是只增不减，但主要是没了那份初心与热情才拖到了现在。 想写下来的太多，但到了真正写的时候愿意写出来的太少。 开头 纵观四年象牙塔的生活，本来是想用一句言简意赅的话来概括，然后顺着这句鞭辟入里的话来依次展开。但翻遍脑海里仅存的词汇和句子，奈何江郎才尽，很难找到能够将这五味杂陈的生活一言以蔽之的句子。所以还是选择把“佛系”贯彻到底，这篇口水文就先拿“行到水穷处，坐看云起时”的心境奠定一个感情基调吧。 四年来，遇见的人，经历的事，听过的歌，看过的剧，喝过的酒，吹过的B，去过的地方，玩过的游戏，林林总总，其实不难发现，总结下来的事物也大多是娱乐和社交，及时行乐、破坏到底的想法大概被贯彻到了底；再回想起《四分之一》里记录的种种心志，还是觉得有挺多遗憾的。干脆就在这把这些憨批证据给留下来，毕竟好多事情都是咎由自取，总得吸取点教训。 正文 四年社交报告总结 前一半的时间里，因为种种经历以及主动地接触，还是遇见并结识了很多人，有的风趣、有的认真、有的简单，有的复杂，有的纯粹。认识的人无论年长年幼，仿佛总能无话不谈，从高谈阔论人生理想到疯狂吐槽八卦日常，无数次思想的碰撞以及内心的坦诚交流，也仿佛总能成群结队地一起疯、一起闹、一起傻笑、一起往前跑。无论是在学校里各种日常活动中的同学还是实习公司里认识和相处了很久的同事，以及其他娱乐日常里认识的新朋友，每个人都有还算鲜明的个性，作为老好人的我也总能找到一种合适的相处的方式。总体看下来，还是有经历过好几个比较要好的小圈子，只是不同的时间阶段，因为各种主观和客观的因素，关系总会经历周期不同的跌宕起伏。回想自己当初的本意，已经不知道是出于刚上大学时那种对人际关系和社交的迷信，还是心中那份总得和身边的人搞好关系的那套理论，还是单纯地就觉得这个人很有趣出于对对方的欣赏想交个朋友，也可能各种因素都有，很难去界定那份初心是否纯粹，应该还是好的面多一点吧。但无论怎样，总是要感谢缘分的，志同道合的人还是比不相为谋的人多得多的。 有学习路上的启蒙导师wnzh，wnzh一开始严肃认真的让人觉得可怕，很容易让人产生一种距离感。还记得那段难受的日子结束之后，wnzh说：“其实我一开始对你挺失望的，基础不好，又不够主动”。到后来无数次的合作交流，慢慢地也就放下了心里的顾虑，一以良师益友的方式去相处，一起吐槽，一起约饭，一起研究新事物...离开成都的最后一顿饭是和wnzh一起吃的，其实有一句感谢一直憋在心里到最后都没说出口。wnzh其实就是那种认真的实干家呀，只是在不明白的人眼里只会变成执拗，即便是被误解为执拗，也并没影响他为人处世的方式。“你现在觉得重要的需要顾全的小事，往往在未来的日子里都会显得不痛不痒~”——by wnzh. 其实自己很幸运，遇见了好几个跟wnzh一样的良师益友，yx 和 drummond也都在不同的阶段里以及不同的生活方面都帮助了我很多很多，日常闲聊吹水时大家也无话不说，发现很多时候他们才是真正跟你一直站在同一边的人，亦师亦友，气氛融洽。也是真的很感激能在各个阶段里遇见他们~ 到了后半程之后，慢慢也就随了世俗风气，一方面开始孤高自傲，胆小认生，不易相处，另一方面就开始以一份更功利的心态出发去维持以往的关系以及认识新的人。好像对于社交这件事情，渐渐变成了一个规划问题，开始从利益出发，以一个标尺（未来是否有求于人）来衡量一段关系是否有建立或者维持的必要。得承认这种功利的心态导致的最严重的后果就是所以的关系都会变得很应付，以及自己很难再真正坦诚相待并投入感情。到现在来就开始自食其果了，开始害怕社交，不愿认识新的人，持续性自闭，不再主动交流，对于以往的关系也显得漫不经心，面对别人的主动也表现的没有感情。常常感觉到孤独和冷漠就已经不再是空穴来风了，在自己保持冷漠的死样子的时候你很难要求别人些什么，一切的一切好像显得罪有应得。其实很多时候，特别是面对新的环境，新的人群的时候，还是想尽全力去认识新的朋友，自己从来都不擅长单打独斗，但往往都是很难迈出主动的那一步，仿佛经历过利益心态折磨之后很难再有年少时的勇气，还有些时候，面对别人的好，下意识地就会将人的善意同利益进行联想。反观了一下自己一路来的成长环境，其实很少经历险恶的唯利是图的事情，不知道何时心理开始扭曲，显得俗不可耐，也许还是书读的太少，饭吃的太饱，梦做的太好吧~ 自己的现状和感受 渐渐地，所有的关系也就在没有太多现实中的交集之后变得平淡了许多，当然在成年人的世界里，这是一件再也正常不过的事情，大家都自顾不暇，总还有更重要的事情。一段关系毕竟是两个人的事情，相互理解相互包容将心比心这样的套话仿佛从来都是谎言，在都没照顾好自己的情绪之前真的很难说愿意替别人排忧解难，至少目前我不太行。所以对于那些总能在一段关系里持续主动的人始终保持敬意。曾经总觉得处理关系其实是一件很简单的事情，但好像在遇到很多手头上的麻烦事情以及面对一定的压力之后，你很难在像之前一样保持着赤忱。可能到最后，真正的理解和包容其实是一切按照着各自喜欢的样子各行其是，不用刻意地去设身处地，然后有着一直都在的默契。要想把理想照进现实，仿佛还是得需要用心去经营，友情也好爱情也罢，都不如亲情那般理所当然，但奈何我们每一个人都太忙，有太多更要紧的事，身边的人也不停地在变，在成年人的规则里很难坚守着孩童时期“拉钩，上吊。一百年不许变”的誓言。 过去的一年好像一直都是一个死样子，也有过不断勉励自己告诫自己要活成想要的样子的经历，但大多时候又是酒后之言，又或是深夜无病呻吟的安眠药。发现自己好像其实没有真正意义上的坦然面对自己的性格上的缺陷，以及也从未做过实际意义上的改变，始终迷信着“逃避可耻但有用”的狗屁道理，也总指望着会有那么一天焕然一新，身边的所有都重头再来。换个视角来看的话，能够很轻易地发现这一切都不过是个自我欺骗的骗局，但也没有想象中的那么无可救药，只是差了点从小事开始做出改变的一点点决心，太多时候还是过于依赖外界的力量了，修身养性在自己身上仿佛一直都是个伪命题，还是少了点发自内心的力量~ 既简单又复杂，仿佛才是所有关系里的终极奥义。观念上的偏执既不能怪罪于对纯粹和完美的追求，也不能怪罪于理想照不进现实的残酷，只因为意识形态里需要对人进行区分。已经习惯了把“三观是否完全不同”当作了一段关系能否成功建立的最低标准，每一次交流，诉说者都在无时无刻不在传递着情绪、思维、想法、观念等，倾听者也用自己对所有事情的理解去阅读交流内容里所蕴含的信息，无数次角色与分工的互换，慢慢构筑了对彼此的理解。对于有的人来说，这个理解与信任建立的过程太复杂太长，面对快节奏的生活里五花八门的娱乐社交选择，更愿意选择最简单的方式——自我封闭。仿佛就是一个轮回，当初对某些事情越是过分痴迷，在未来的某一段时间里就会越是觉得厌恶。但归根结底，其实很多事情都是咎由自取，客观规律永远都是你想要的东西往往都需要自己去争取，即便是你想要维护的关系也需要自己用心去经营，爱与被爱都不是理所当然。所以既然选择了独处，就得拿出独处的态度来，大胆生活，无须在意周围种种。 总有着大大小小的毛病 回想起年少的往事，记忆中的一件件小事仿佛还是能在最近几年发生的事情里能找到影子，即便随着身体的不断长大，心智日趋成熟，仿佛在潜意识里或者说某个内心深处仍然有着很多不成熟的想法和心理。以至于在面对很多从小到大都会遇到的场景时，许多时候还是和以前一样“睿智”。也总能发现其实好多事情基本都归结于虚荣心。 不难发现其实从小到大自己都很习惯站在一个高点去对很多自己并不擅长的事情评头论足，遇见了稍微专业一点的“抬杠”就开始随口附和或者提几个沙雕的质疑显得自己没那么“睿智”。随之演变而来的就是自从上了高中就开始被众人质疑过的说教风，有的人对此包容觉得是个性格，有的人对此厌恶觉得是个毛病，自己反思了一下区别主要在于是否有喋喋不休地针对一个问题抬杠到底。你可以美名其曰说是那种不服输的性格，但明眼人都知道其实就是在骨子里的执拗。和很多朋友都有过闹得不可开交的争论，落到最后的，有的朋友太过宽容不跟我一般计较，有的朋友就深受其害表示不太理解，为此也跟身边一些人结了怨，从小到大的每一段经历里都会有那么几个被我冷嘲热讽气哭的女孩子，“毒蛇”这个标签仿佛是跟我这辈子结了缘。许多冷言冷语仿佛都是说出去了才会察觉到不对劲，也有很多棱角过于分明的话语总能在不合时宜的场景里中伤他人，自己总能找到说辞说是不经意，客观地来看其实从心底里就是坏，就像好多人总能拿出“我这个人有时候说话比较直”的沙雕借口一样，都只是自己关于坏的说辞。单纯地出于优越感地鄙视他人并不会显得你有多聪明，只会显得你是个“睿智”！当然得承认自己之前也是个“睿智”，有时候戾气太重，也有幸遇见过好几位一样的“睿智”，在“睿智”的对决中占了下风深受其害之后慢慢开始明白了之前那些被自己中伤过的人的感受，也逐渐在开始克制心里那股戾气~ 了解我的人包括我自己从来都可能相信我是一个能够把想做好的事情给做好的人，但现实总是许许多多的事情总能因为我自身的各种各样原因显得不尽人意，所以所谓的那些相信以及看好，在事实面前仿佛都是天方夜谭。自己还是反思总结了一下，发现大致的失败历程其实是：一开始做事之前毫无目标和规划，比起自我突破更热衷于言听计从；做事的时候又总能出于懒惰或者兴趣上的消磨找到借口，也总能发现比现在手头上有意思的事情多了去了；做完事情以后也从未考虑过效果和影响，甚至说还会以一种成就了一番“大事业”的心态纵容自己的怠惰。得承认好逸恶劳是挺容易让人上瘾的，但也得承认现在好逸恶劳的基础并不是自己挣来的，以及并没有可以挥霍的资本。你总会通过各种渠道了解到无论是处在哪个阶层哪个年龄阶段的人都在为生活奔命，喝着小酒的你可能还会轻蔑地觉着是大家都在贩卖焦虑，其实道理可以说的简单一点，问题也可以问的通俗一点，譬如：你是否真的有每天在做你觉得有意义的事？至于说“有意义”的概念就很难界定了，各抒己见下来也无外乎好的/坏的/不好不坏的，但很显然的一个标准其实是在不设前提的情况下有没有从中有所收获。又想起年少时一直喜欢的韩寒那句话“我从小到大听过很多道理，可我依旧过不好我的生活”，现在看来其实更像是句没努力的人借口以及努力的人一句谦辞。至于说自己到底属于哪一类，就还是得看具体的行动表现了~ 四年象牙塔生活总结 以一个整体的思路看下来，相比于普通经历里各种琐碎，发现其实还是在这整个过程里有很多新的尝试，和生活中各类角色的人（恋人/亲人/朋友/同学/老师/认识的人/陌生人）相处的过程中也慢慢形成了自己的性格和风格，在经历各种无论事琐碎的小事还是十字路口抉择的大事的过程中也慢慢找到了自己喜欢的处理方式，最重要的大概还是逐渐以一个成年人的姿态去独自面对一些事情，也慢慢开始肩负起一些自己的责任来。看起来也仿佛曲线救国一般实现了年幼时的梦想——被大人们能以一个大人的身份对待，也能像大人们一样做自己想做的事情。年少时或多或少都会受到些拘束，长大之后开始面对生活中的各种酸甜苦辣，体会各种人世间美好或者阴暗的感情，所有的一切看起来并没小时候想的那么简单纯粹，其实也没有现在心里预设了各种羁绊和前提那样的复杂，无外乎充斥在生活中的就是各式各样的选择。至于说最终的走向，从来都是每一个分叉路口共同的导向决定的，每一次的抉择仿佛很重要，又仿佛没那么重要，即便是走错了方向，下一个路口也都还有挽回的余地，只不过是要看功利角度下的付出回报比例是不是能让人释怀罢了。 从具体的层面和事情去看的话，虽说总得以一个潇洒的姿态宣称没什么好遗憾的以及没什么好值得留恋惋惜的，但既然在四下无人的这里其实可以真诚坦荡一点，做错了的事情就是做错了，做得不好的事情就是做的不好，没什么不好意思承认的。所以干脆就在这里承认一下大大小小的事情里做的不好的地方吧。能想起来或者说还记得住的大概就是：没能好好处理在与朋友相处过程中各种大大小小的矛盾，比较意气用事和孩子气，逞口舌之快；没能正确应对感情生活里的各种低谷期以及没有保持足够的耐心和勇气，无论是爱与被爱，绝大部分时候都选择了逃避，并且透露着那么几分轻浮；有那么一段时间自怨自艾、心术不正、嗜酒成性、熬夜成习，还给挺多身边的人带来了困扰，内心也过于浮躁，极其渴求虚无的认同感；在面对很多要紧的事情的时候，心态不好，持续低迷，总想破罐子破摔，还自命不凡地觉得是万事看破的佛系；间歇性踌躇满志，持续性混吃等死，比起可恶的人贩卖焦虑，好多时候都是自己太瞻前顾后，但又没做成任何实际意义上的改变，深夜抑郁诗人~诸如此类的事情，不胜枚举，但好在承担了相应的后果以及开启人生新阶段之后，很多事情慢慢都有了改观，总体都还是在朝好的方向去发展，自己也在做有意义的事情，也依旧相信&quot;Everything will be OK&quot;。 结尾 这篇写完，通读下来，不出意料地发现写的挺乱以及挺口水的，果真没怎么好好读书之后很难再在文字表达上维持之前的水准，太多时候都总是忙着做对物质生活可能更有意义的事情，或者说带有太多的功利性，已经很少再去精神层面追求点什么，哪怕是静下心来看个篇幅较短的散文，整个过程随时都充满着浮躁。其实想想，有时候是可以把脚步稍微放缓一点，或者说找一个发呆以及摸鱼混吃等死的一小段时间，静下心来，耐心地看完一篇短文乃至日积月累地看完一本书，暂不奢求能从中收获什么，就只是简简单单地磨练一下心性，身上的那股戾气应该会渐渐有所改观吧。无论怎么说，这篇文呢其实主要还是在一个人生新阶段即将开启的时刻以及在一个新的地方总结和回顾了一下过往发生在成都的一段经历（其实本来还想写篇成都的总结篇的），也没想着说能在写的这个过程中会有什么醍醐灌顶的觉悟，只是想着以一个形式简单纪念一段经历的过去。其实过去的这段经历里发生的事情还蛮多的，想写的太多，但最后又觉得好像没有太大必要去纠结过去的陈词滥调，故事就是故事，已故之事，过去的就让它过去，总得抓紧点过好现在的生活。得不到的别留下，留不下的别牵挂。在新的地方，就应该有新的样子，发生新的故事。 毕竟还有好多想做的以及要做的事情，路阻且长~ ","link":"https://blog.shunzi.tech/post/one/"},{"title":"字符编码与文件","content":" 一个在重写tcmu读写接口时遇到的问题：如何转化标准的块设备读写缓冲区？ 由于最初不理解相关数据底层编码方式，导致开发过程效率低下。 在此总结编码相关和文件数据相关的基础知识。 字符编码与文件 字符编码 ASCII 基本概念 American Standard Code for Information Interchange 是基于拉丁字母的一套电脑编码系统。它主要用于显示现代英语，而其扩展版本EASCII則可以部分支持其他西欧语言，并等同于国际标准ISO/IEC 646。 ASCII 由电报码发展而来。至今为止共定义了128个字符；其中33个字符无法显示（一些终端提供了扩展，使得这些字符可显示为诸如笑脸、扑克牌花式等8-bit符号），且这33个字符多数都已是陈废的控制字符。控制字符的用途主要是用来操控已经处理过的文字。在33个字符之外的是95个可显示的字符。用键盘敲下空白键所产生的空白字符也算1个可显示字符（显示为空白）。 组成 对应十进制编码为 0-31 的是 ASCII 控制字符，127 也为控制字符。 控制字符主要表现为无法在进行正确的格式化输出，往往会因为终端机的环境不同，显示方式也会有所区别。 对应十进制编码为 32-126 的是 可显示字符。其中： 数字1 对应编码 49（10进制） 字母A 对应编码 65 字母a 对应编码 97 应用 缺陷 ASCII的局限在于只能显示26个基本拉丁字母、阿拉伯数字和英式标点符号，因此只能用于显示现代美国英语（且处理naïve、café、élite等外来语时，必须去除附加符号）。虽然EASCII解决了部分西欧语言的显示问题，但对更多其他语言依然无能为力。因此，现在的软件系统大多采用Unicode。 Base64 概念 Base64是一种基于64个可打印字符来表示二进制数据的表示方法。由于 { 2^6=64，所以每6个位元为一个单元，对应某个可打印字符。3个字节有24个位元，对应于4个Base64单元，即3个字节可由4个可打印字符来表示。 组成 在Base64中的可打印字符包括字母A-Z、a-z、数字0-9，这样共有62个字符，此外两个可打印符号在不同的系统中而不同。一般情况下另外两个字符为 + 和 /。 经过 Base64 编码之后的字符，长度将会有所增加。对应的关系为 原字符长度除以3 之后向上取整（字节补位变成3的倍数），然后再乘以四得到对应的编码之后的长度。 应用 Base64常用于在通常处理文本数据的场合，表示、传输、存储一些二进制数据，包括MIME的电子邮件及XML的一些复杂数据; Base64编码可用于在HTTP环境下传递较长的标识信息，常用于URL的编码，但由于 = + / 这三种符号在URL中有自己独特的语义，所以需要对标准的 Base64 编码算法进行改进，将这些符号进行替换。 计算方式 如果要编码的字节数不能被3整除，最后会多出1个或2个字节，那么可以使用下面的方法进行处理：先使用0字节值在末尾补足，使其能够被3整除，然后再进行Base64的编码。在编码后的Base64文本后加上一个或两个=号，代表补足的字节数。也就是说，当最后剩余两个八位字节（2个byte）时，最后一个6位的Base64字节块有四位是0值，最后附加上两个等号；如果最后剩余一个八位字节（1个byte）时，最后一个6位的base字节块有两位是0值，最后附加一个等号。 参考下表： Unicode 概念 Unicode（中文：万国码、国际码、统一码、单一码）是计算机科学领域里的一项业界标准。它对世界上大部分的文字系统进行了整理、编码，使得计算机可以用更为简单的方式来呈现和处理文字。 Unicode备受认可，并广泛地应用于计算机软件的国际化与本地化过程。有很多新科技，如可扩展置标语言（Extensible Markup Language，简称：XML）、Java编程语言以及现代的操作系统，都采用Unicode编码。 Unicode 主要做的工作就是将世界上所有的语言都统一到一套编码里，从而解决乱码问题。 组成 Unicode标准也在不断发展，但最常用的是用两个字节表示一个字符（如果要用到非常偏僻的字符，就需要4个字节）。现代操作系统和大多数编程语言都直接支持Unicode。 ASCII编码是1个字节，而Unicode编码通常是2个字节 痛点 如果你写的文本基本上全部是英文的话，用Unicode编码比ASCII编码需要多一倍的存储空间，在存储和传输上就十分不划算。从而推出了可变长编码 UTF-8。 UTF-8 UTF-8编码把一个Unicode字符根据不同的数字大小编码成1-6个字节，常用的英文字母被编码成1个字节，汉字通常是3个字节，只有很生僻的字符才会被编码成4-6个字节。如果你要传输的文本包含大量英文字符，用UTF-8编码就能节省空间 由于UTF-8存在单字节编码，所以UTF-8能兼容ASCII码 在计算机内存中，统一使用Unicode编码，当需要保存到硬盘或者需要传输的时候，就转换为UTF-8编码。 实例 - 记事本 用记事本编辑的时候，从文件读取的UTF-8字符被转换为Unicode字符到内存里，编辑完成后，保存的时候再把Unicode转换为UTF-8保存到文件： 实例 - 浏览器 浏览网页的时候，服务器会把动态生成的Unicode内容转换为UTF-8再传输到浏览器，所以你看到很多网页的源码上会有类似的信息，表示该网页正是用的UTF-8编码。 UTF-32 UTF-32 是固定长度的编码，始终占用 4 个字节，足以容纳所有的 Unicode 字符，所以直接存储 Unicode 编号即可，不需要任何编码转换。浪费了空间，提高了效率。 UTF-16 对于 Unicode 编号范围在 0 ~ FFFF 之间的字符，UTF-16 使用两个字节存储，并且直接存储 Unicode 编号，不用进行编码转换，这跟 UTF-32 非常类似。 对于 Unicode 编号范围在 10000~10FFFF 之间的字符，UTF-16 使用四个字节存储，具体来说就是：将字符编号的所有比特位分成两部分，较高的一些比特位用一个值介于 D800~DBFF 之间的双字节存储，较低的一些比特位（剩下的比特位）用一个值介于 DC00~DFFF 之间的双字节存储。 文件 计算机的存储在物理上是二进制的，所以文本文件与二进制文件的区别并不是物理上的，而是逻辑上的。这两者只是在编码层次上有差异。（严格意义上，文本文件也是二进制文件） 简单来说，文本文件是基于字符编码的文件，常见的编码有ASCII编码，UNICODE编码等等。二进制文件是基于值编码的文件。 文本文件 文本文件是一种计算机文件，它是一种典型的顺序文件，其文件的逻辑结构又属于流式文件。 文本文件是指以ASCII码方式(也称文本方式)存储的文件，更确切地说，英文、数字等字符存储的是ASCII码，而汉字存储的是机内码。文本文件中除了存储文件有效字符信息（包括能用ASCII码字符表示的回车、换行等信息）外，不能存储其他任何信息。 二进制文件 文件在外部设备的存放形式为二进制而得名。狭义的二进制文件即除文本文件以外的文件。 二进制文件可看成是变长编码的，因为是值编码，多少个比特代表一个值，完全由自己决定。 优势 二进制文件比较节约空间，这两者储存字符型数据时并没有差别，但是在储存数字，特别是实型数字时，二进制更节省空间。 内存中参加计算的数据都是用二进制无格式储存起来的，因此，使用二进制储存到文件就更快捷 一些比较精确的数据，使用二进制储存不会造成有效位的丢失。 对比 相同 文件在磁盘上的存储方式都是二进制形式。但二进制读写是将内存里面的数据直接读写入文本中，而文本呢，则是将数据先转换成了字符串，再写入到文本中。 组成：控制信息和内容信息。 （文本文件不包含控制信息） 不同 能存储的数据类型不同：文本文件只能存储char型字符变量。二进制文件可以存储char/int/short/long/float/……各种变量值。 每条数据的长度：文本文件每条数据通常是固定长度的。以ASCII为例，每条数据(每个字符)都是1个字节。进制文件每条数据不固定。如short占两个字节，int占四个字节，float占8个字节…… 读取的软件不同：文本文件编辑器就可以读写。比如记事本、NotePad++、Vim等。二进制文件需要特别的解码器。比如bmp文件需要图像查看器，rmvb需要播放器…… 操作系统对换行符('\\n')的处理不同（不重要）：文本文件，操作系统会对'\\n'进行一些隐式变换，因此文本文件直接跨平台使用会出问题。二进制文件，操作系统不会对'\\n'进行隐式变换，很多二进制文件（如电影、图片等）可以跨平台使用。 参考文献 [1] 简书 - output：ASCII&amp;Base64 [2] Wikipedia - ASCII [3] Wikipedia - Base64 [4] 廖雪峰 - 字符串编码 [5] 简书 - 时待吾：文本文件和二进制文件的差异和区别 [6] Wikipedia - Unicode [7] CSDN - 严长生：字符编码的概念（UTF-8、UTF-16、UTF-32都是什么鬼） [8] jin-yang：字符集与编码杂谈 ","link":"https://blog.shunzi.tech/post/file-and-charset/"},{"title":"网络代理基础","content":" 源于师姐提到的一个问题：什么是反向代理？ 之前本科在自己建站以及做学校项目的时候其实有涉及到，但没进行总结 趁此机会埋个坑，把代理的东西好好理一理，顺便讲讲 VPN 啥的 未完待续~ 基本概念 什么是代理？ 代理：顾名思义，根据汉语拆词为字进行组词就很好理解。代，代表；理，处理。所以代理往往是描述了一个中间人所要做的工作，就是代表某一方进行某些事情的处理。 对于商业场景中关于代理的故事就不再此处进行赘述，此处则主要探讨在计算机领域中主要涉及到的 网络代理。 网络代理 一种特殊的网络服务，英文名做 Proxy，允许一个网络终端（一般为客户端）通过这个服务与另一个网络终端（一般为服务器）进行非直接的连接。一些网关、路由器等网络设备具备网络代理功能。一般认为代理服务有利于保障网络终端的隐私或安全，防止攻击。 一个完整的代理请求过程为：客户端首先与代理服务器创建连接，接着根据代理服务器所使用的代理协议，请求对目标服务器创建连接、或者获得目标服务器的指定资源（如：文件）。在后一种情况中，代理服务器可能对目标服务器的资源下载至本地缓存，如果客户端所要获取的资源在代理服务器的缓存之中，则代理服务器并不会向目标服务器发送请求，而是直接传回已缓存的资源。 一些代理协议允许代理服务器改变客户端的原始请求、目标服务器的原始响应，以满足代理协议的需要。代理服务器的选项和设置在计算机程序中，通常包括一个“防火墙”，允许用户输入代理地址，它会遮盖他们的网络活动，可以允许绕过互联网过滤实现网络访问。 常见类型 正向代理（Forward Proxy） 基本原理 常见的场景中所提到的代理一般都是指 正向代理。 正向代理是指 用户 利用 代理服务器 访问 目标服务器。 主要目的：隐藏客户端，忽略客户端的差异。 使用场景 访问本地无法访问的服务器；最简单的例子就是翻墙，还有针对部分只能内网访问的场景，往往需要借助代理服务器，才能进行相关资源的访问（譬如实验室的服务器）。 缓存（Cahce）作用：cache技术与代理服务技术是紧密联系的，假设我们的电脑是用户A，我们想通过代理服务器X访问服务器A上的数据B，如果在这之前，有人通过代理服务器X访问过该数据，那么代理服务器X会把数据B保存一段时间，在这段时间内，如果我们想访问数据B，代理服务器会直接把缓存的数据B直接发给我们，而不会再去访问服务器A。 权限控制：防火墙授权代理服务器访问权限，客户端通过正向代理可以通过防火墙；故部分内网的服务器会限制登录IP为代理服务器IP，从而保证只有在已知代理服务器的情况下才能访问内网服务器。 隐藏访问者：通过配置，目标服务器无法获取真实客户端信息，只能获取到代理服务器的信息，针对部分安全性要求较高的业务，往往需要隐藏客户端。（BTW，但严格意义上仍是不安全的） 针对某些限制 IP 访问次数的场景，常常使用 IP 代理池来进行代理访问，从而使用代理服务器的多个 IP 来进行相关访问。特别是某些数据爬取的场景。 实现方式 利用 Nginx、Apache 等 Web 服务器来引入相关代理模块并进行配置，从而转发客户端传递来的 Web 请求。 利用一些软件，进行相应协议的通信加密和转换，实现对应的功能。 延申 正向代理根据协议划分主要有HTTP、HTTP over TLS(HTTPS)、Socks、Socks over TLS几种。其中，HTTP和Socks无法用于翻墙，HTTPS和Socks over TLS可以用于翻墙。 四种代理协议都可以通过“用户先将数据发给代理服务器，再由代理服务器转发给目的服务器”的方法达到翻墙目的。但由于HTTP和Socks都是明文协议，GFW可以通过检查数据包内的内容得知用户的真实意图，进而拦截数据包。所以，HTTP和Socks一般只用作本地代理。而HTTPS协议是加密通讯，GFW无法得知数据包内的真实内容，类似于关键词过滤的手段无法施展。HTTPS代理的流量特征和我们平时访问网站时所产生的HTTPS流量几乎一模一样，GFW无法分辨，稳定性爆表。 常见正向代理软件 tinyproxy tinyproxy 是一款轻量级的正向代理软件，主要是比较轻量，常用在一些嵌入式设备以及移动终端中，性能表现一般。 Github Repo Tinyproxy 安装配置教程请移步 Chaos' Blog：centos 7使用tinyproxy配置局域网代理 gost GO 语言实现的安全隧道 Github Repo 官方教程 经过简单对比，gost 代理性能要比 tinyproxy 好很多。 运行教程： wget http://github.com/ginuerzh/gost/releases/download/v2.11.1/gost-linux-amd64-2.11.1.gz gunzip gost-linux-amd64-2.11.1.gz mv gost-linux-amd64-2.11.1 gost chmod +x gost nohup ./gost -L :9002 &gt; ./gost.log &amp; 反向代理 基本原理 正向代理代理的是用户，而反向代理则相反，代理对象是目标服务器。 反向代理：指 服务端 利用 代理服务器 给 用户 提供服务。 主要目的：向客户端隐藏服务端的细节，由代理服务器统一接管，并根据适当的策略（负载均衡等）来分发相应的请求并最终通过代理服务器进行返回。 使用场景 保护和隐藏原始资源服务器；用户A始终认为它访问的是原始服务器B而不是代理服务器Z，但实用际上反向代理服务器接受用户A的应答，从原始资源服务器B中取得用户A的需求资源，然后发送给用户A。由于防火墙的作用，只允许代理服务器Z访问原始资源服务器B。尽管在这个虚拟的环境下，防火墙和反向代理的共同作用保护了原始资源服务器B，但用户A并不知情。 负载均衡：当反向代理服务器不止一个的时候，我们甚至可以把它们做成集群，当更多的用户访问资源服务器B的时候，让不同的代理服务器Z（x）去应答不同的用户，然后发送不同用户需要的资源。当然反向代理服务器像正向代理服务器一样拥有CACHE的作用，它可以缓存原始资源服务器B的资源，而不是每次都要向原始资源服务器B请求数据，特别是一些静态的数据，比如图片和文件，如果这些反向代理服务器能够做到和用户X来自同一个网络，那么用户X访问反向代理服务器X，就会得到很高质量的速度。这正是CDN技术的核心。 缓存作用：数据缓存在代理服务器上，如果客户端请求的内容在缓存中则不去访问目标主机； 延申 利用反向代理实现内网穿透。通过使用SSH将内网服务器的22端口对应地转发到反向代理服务器上的某个端口，从而建立了从内网服务器到代理服务器的反向代理；接着在代理服务器上通过将本机的某个端口转发到反向代理服务端口来建立正向代理，从而使得在外网的任何一台服务器都可】通过代理服务器上提供的正向代理来对远程的内网服务器进行操作。 常见的内网穿透工具可参考文献中的【7】。 ssh ssh 参数解释： -f 后台执行ssh指令 -C 允许压缩数据 -N 不执行远程指令 -R 将远程主机(服务器)的某个端口转发到本地端指定机器的指定端口 -L 将本地机(客户机)的某个端口转发到远端指定机器的指定端口 -p 指定远程主机的端口 ssh 正向代理配置：ssh -fCNL ssh 反向代理配置：ssh -fCNR 场景 内网服务器 I：192.168.3.126 | ssh 端口：22 外网服务器 O：45.45.45.45 | ssh 端口：33 step1 在内网服务器 I 上建立 内网服务器 I 到 外网服务器 O 的反向代理 # 该命令将本地的ssh端口反向代理到了远程服务器的7777端口 # 参数解释：ssh -fCNR # 外网服务器对应的代理通信 IP:Port - 7777 (IP可省略) # 内网服务器对应的代理通信 IP:Port - localhost:22 （为了建立ssh连接使用了22端口） # 登陆远程服务器使用的信息 root@IP -p port - root@45.45.45.45 -p 33 # [root@servermwd ~]# ssh -fCNR 7777:localhost:22 root@45.45.45.45 -p 33 step2 在外网服务器 O 上建立 正向代理 # 该命令建立了本机端口 7776 到 7777 的正向代理 # 参数解释：ssh -fCNR # 服务器A 对应的代理通信 IP:Port - *:7776 (IP可省略,将会使用命令末尾的IP) # 服务器B 对应的代理通信 IP:Port - localhost:7777 (对应内网服务器反向代理的端口) # 登陆A 服务器使用的信息 IP -p port - localhost -p 33 # [root@45 ~]# ssh -fCNL *:7776:localhost:7777 localhost -p 33 step3 使用ssh连接内网服务器 # 该命令建立了到外网服务器指定端口的ssh连接 # 1. 7776 端口对应地被正向代理转发到了 7777端口； # 2. 7777 端口对应地被反向代理代理到了 内网服务器的22端口 # 3. 从而建立起了对内网服务器的 ssh 连接 # &gt; ssh -p 7776 root@45.45.45.45 (可选) step4 内网服务器配置ssh免密登陆并使用 autossh来进行自动重连 # 免密登录 ssh-copy-id root@45.45.45.45 # 安装autossh yum install autossh # 同 ssh 的参数一致 # 多了一个 -M port的参数，主要用于外网服务器接受内网服务器的信息 autossh -M 7775 -fCNR 7777:localhost:22 root@45.45.45.45 -p 33 (可选) step5 将autossh加入内网服务器开机启动项，并使配置生效 vi /etc/rc.d/rc.local # 添加内容 autossh -M 7775 -fCNR 7777:localhost:22 root@45.45.45.45 -p 33 # 赋予权限 chmod +x /etc/rc.d/rc.local Last but not least (猴子都能看懂的步骤) 【Firstly】 先告诉憨憨们 xx.xx.xx.xx =&gt; 你的跳板机IP 8111 =&gt; 跳板机对应的端口 【SSH 公钥私钥生成】 ssh-keygen 【ssh 免密登录：(免密失败检查公私钥和跳板机配置)】 ssh-copy-id root@xx.xx.xx.xx ssh -vvv xx.xx.xx.xx // 查看日志错误信息 【centos 安装 autossh】 yum install autossh 【安装失败使用 RPM 安装】 wget http://download-ib01.fedoraproject.org/pub/epel/7/x86_64/Packages/a/autossh-1.4g-1.el7.x86_64.rpm rpm -i autossh-1.4g-1.el7.x86_64.rpm 【检查是否安装成功】 rpm -qa | grep autossh 【autossh 反代命令】： autossh -M 2222 -NfR 8111:localhost:22 root@xx.xx.xx.xx -p 22 // -M 后的端口时 autossh 接收错误消息自动重连的，不用管 // -NfR 后的端口才是你最后会用的，你懂我意思吧~ // -p 22 列出来是怕有的憨憨默认ssh端口不是22的，专门写出来！！ //很清楚了吧~ 【连接时 connection refused，需要打开跳板机端口，重启ssh service】 vim /etc/ssh/sshd_config ... GatewayPorts yes AuthorizedKeysFile .ssh/authorized_keys ... sudo /etc/init.d/ssh restart 【反代连接】 ssh root@xx.xx.xx.xx -p 8111 参考文献 [1] 维基百科：代理服务器 [2] 知乎 - AntSworD：简单聊聊网络代理原理 [3] 知乎 - 不梦君：proxy代理基础 [4] Google Blog : 浅谈vpn、vps、Proxy以及shadowsocks之间的联系和区别 [5] 知乎 - 波哥：彻底理解正向代理、反向代理、透明代理 [6] 博客园 - 不拨牙：ssh反向代理 [7] 简书 - OSC开源社区：可以实现内网穿透的几款工具 ","link":"https://blog.shunzi.tech/post/proxy/"},{"title":"Hello Diary","content":" Diary 开始篇，大概也不能叫 Diary. 主要记录一些咸鱼日常观点以及胡言乱语灵感 好吧，其实是为了打发晚上无聊的时光 （写在前面：不知道如果不用 Markdown 会是什么样的排版效果，所以第一篇应该很乱） 今天人眼 Debug 了一天，真的没环境没 IDE 编程实在是难受，最重要的是还没什么大的发现，真的是感觉自己大学上的有点本末倒置了。明天可能还是继续，但大概可以考虑换一种方式，通过 GCC 来定义并实现，由自己调用相关函数，看看能不能有什么新的突破。我还是太菜了~ BTW,每天都在 C 语言，确实很久没用 Java/Python 之类的高级语言了，仿佛编程能力不久将要退化，可能还是得把 leetcode 和一些工具包提上日程。还有好多云计算老本行的东西也好久没关注了，不知道最近有什么新风向，至于更前沿的，就更是没涉及到了，有些惭愧~ 最近这一个月好像连书都没看了，也少了一些自己喜欢的电影和节目啥的，真的是 focus 在了这个项目上，但好像又是瞎忙活，可能之后得需要权衡一下了，感觉一开始用力过猛还是很容易造成后劲不足的。还是得找到一个和自己和平相处的方式~ 突然想起来在这儿写一些日常类的东西，原因就跟摘要里说的一样，也不想在其他社交平台上花费时间去刻意记录些什么东西了，往往可能还会有很多顾忌，感觉还是失去了灵魂。比起朋友圈里的各类精致生活，微博上的夜间诗人日常，还有网易云的无病呻吟，好像总是在迎合些什么，总是在向外界刻意地建立有趣、敢爱敢恨、细致、暖心等blabla的狗屁形象，其实回过头去想，社交平台上的日常，大多都是某一瞬间的感性或者情绪的诱导下，把整个人的虚伪的一面全部释放出来，因为自己内心深刻地知道自己的目的：想让人知道自己的内心活动，想让人知道自己其实有血有肉...诸如此类。我承认人总是需要一种认同感，但是仿佛在社交圈里贩卖自己的各种人设带来的也只会是虚无主义层面肯定，掐掉网线，面对真实的时候还是一脸尴尬以及举足无措。 之后在这儿，大概也不会发展成情绪宣泄的地方吧，除了技术以外可能写些见闻、读书笔记和观后感啥的，感觉还是可以把很多有深度的点以及触动的点记录下来，顺便锻炼培养一下自己的表达能力，就跟发Paper之前一定得先学会讲故事是一个道理，越来越佩服那些讲故事讲的好的人了；故事的好坏是一方面，很多时候由现实和题材决定，但讲故事的方式就更多的是人为主观所左右的了。其实之前也想过在这儿每天都坚持写写弄成日记啥的，但好像这样的话就会变成一个见光死的地方，以及好像没太多精力去坚持做这件意义目前我可能觉得不太大的事情。 好了第一篇感觉就显得啰里啰唆，就写到这儿吧，最后祭出那句老话~ The easy way or the right way! ","link":"https://blog.shunzi.tech/post/heelo-diary/"},{"title":"CMake入门","content":" 构建工具 CMake 介绍 官方入门教程翻译 后续增加部分高级功能实现 CMake 背景 你或许听过好几种 Make 工具，例如 GNU Make ，QT 的 qmake ，微软的 MS nmake，BSD Make（pmake），Makepp，等等。这些 Make 工具遵循着不同的规范和标准，所执行的 Makefile 格式也千差万别。这样就带来了一个严峻的问题：如果软件想跨平台，必须要保证能够在不同平台编译。而如果使用上面的 Make 工具，就得为每一种标准写一次 Makefile ，这将是一件让人抓狂的工作。 CMake 就是针对上面问题所设计的工具：它首先允许开发者编写一种平台无关的 CMakeLists.txt 文件来定制整个编译流程，然后再根据目标用户的平台进一步生成所需的本地化 Makefile 和工程文件，如 Unix 的 Makefile 或 Windows 的 Visual Studio 工程。从而做到“Write once, run everywhere”。显然，CMake 是一个比上述几种 make 更高级的编译配置工具。一些使用 CMake 作为项目架构系统的知名开源项目有 VTK、ITK、KDE、OpenCV、OSG 等 [1]。 流程 编写 CMake 配置文件 CMakeLists.txt 。 执行命令 cmake PATH 或者 ccmake PATH 生成 Makefile 。其中， PATH 是 CMakeLists.txt 所在的目录。 使用 make 命令进行编译。 实例 Step 1. 基本的CMakeLists.txt 最基本的 CMakeLists.txt cmake_minimum_required (VERSION 2.6) project (Tutorial) add_executable(Tutorial tutorial.cxx) tutorial.cxx // A simple program that computes the square root of a number #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;math.h&gt; int main (int argc, char *argv[]) { if (argc &lt; 2) { fprintf(stdout,&quot;Usage: %s number\\n&quot;,argv[0]); return 1; } double inputValue = atof(argv[1]); double outputValue = sqrt(inputValue); fprintf(stdout,&quot;The square root of %g is %g\\n&quot;, inputValue, outputValue); return 0; } 添加版本号，配置头文件 CMakeLists.txt # Basic two lines.(cmake version &amp; project name) cmake_minimum_required (VERSION 2.6) project (Tutorial) # Add the version number. set (Tutorial_VERSION_MAJOR 1) set (Tutorial_VERSION_MINOR 0) # Configure a header file to pass some of the CMake settings to the source code configure_file ( &quot;${PROJECT_SOURCE_DIR}/TutorialConfig.h.in&quot; &quot;${PROJECT_BINARY_DIR}/TutorialConfig.h&quot; ) # Add the binary tree to the search path for include files # so that we will find TutorialConfig.h include_directories(&quot;${PROJECT_BINARY_DIR}&quot;) # add the executable add_executable(Tutorial tutorial.cxx) 因为配置文件会被写入到生成路径（binary tree） 中，所以我们必须将该文件夹添加到头文件搜索路径中。接下来我们在源码中创建一个包含以下内容的 TutorialConfig.h.in 文件 // Define the configured options and settings for Tutorial #define Tutorial_VERSION_MAJOR @Tutorial_VERSION_MAJOR@ #define Tutorial_VERSION_MINOR @Tutorial_VERSION_MINOR@ 当 CMake 配置这个头文件的时候，@Tutorial_VERSION_MAJOR@ 和 @Tutorial_VERSION_MINOR@ 就会用 CMakeLists.txt 文件中对应的值替换。接下来我们修改 tutorial.cxx 源码包含配置头文件并使用版本号，修改后的源码如下： // A simple program that computes the square root of a number #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;math.h&gt; #include &quot;TutorialConfig.h&quot; int main (int argc, char *argv[]) { // Print the version info stored in configuration file. if (argc &lt; 2) { fprintf(stdout,&quot;%s Version %d.%d\\n&quot;, argv[0], Tutorial_VERSION_MAJOR, Tutorial_VERSION_MINOR); fprintf(stdout,&quot;Usage: %s number\\n&quot;,argv[0]); return 1; } double inputValue = atof(argv[1]); double outputValue = sqrt(inputValue); fprintf(stdout,&quot;The square root of %g is %g\\n&quot;, inputValue, outputValue); return 0; } Step 2. 导入库文件 现在我们将会给我们的项目添加一个库文件。这个库文件包含了我们自己实现的开方运算。可执行文件使用这个库替代编译器提供的标准开方运算。本教程中我们将其放到 MathFunctions 文件夹下，该文件夹下还有一个包含下面一行代码的 CMakeLists.txt 文件。 add_library(MathFunctions mysqrt.cxx) mysqrt.cxx 文件只有一个名为 mysqrt 的函数，其提供了和标准库 sqrt 相同的功能。内容如下 #include &quot;MathFunctions.h&quot; #include &lt;stdio.h&gt; // a hack square root calculation using simple operations double mysqrt(double x) { if (x &lt;= 0) { return 0; } double result; double delta; result = x; // do ten iterations int i; for (i = 0; i &lt; 10; ++i) { if (result &lt;= 0) { result = 0.1; } delta = x - (result * result); result = result + 0.5 * delta / result; fprintf(stdout, &quot;Computing sqrt of %g to be %g\\n&quot;, x, result); } return result; } 对应的头文件为 MathFunction.h，其内容如下： double mysqrt(double x); 为了构建并使用新的库文件，我们需要在顶层 CMakeList.txt 文件添加 add_subdirectory 语句。我们需要添加额外的头文件包含路径，以便将包含函数原型的 MathFunctions/MathFunctions.h 头文件包含进来。最后我们还需要给可执行文件添加库。最终顶层 CMakeList.txt 文件的最后几行如下所示： include_directories (&quot;${PROJECT_SOURCE_DIR}/MathFunctions&quot;) add_subdirectory (MathFunctions) # add the executable add_executable (Tutorial tutorial.cxx) target_link_libraries (Tutorial MathFunctions) 现在我们考虑将 MathFunctions 库作为一个可选项。虽然在这里并没有什么必要，但是如果库文件很大或者库文件依赖第三方库你可能就希望这么做了。首先先在顶层 CMakeLists.txt 文件添加一个选项： # should we use our own math functions? option (USE_MYMATH &quot;Use tutorial provided math implementation&quot; ON) 这个选项会在 CMake GUI 中显示并会将默认值设置为 ON，用户可以根据需求修改该值。这个设置会本保存下来，所以用户无需在每次运行 CMake 时都去设置。接下来就是将构建和连接 MathFunctions 设置为可选项。修改顶层的 CMakeLists.txt 文件如下所示： # add the MathFunctions library? if (USE_MYMATH) include_directories (&quot;${PROJECT_SOURCE_DIR}/MathFunctions&quot;) add_subdirectory (MathFunctions) set (EXTRA_LIBS ${EXTRA_LIBS} MathFunctions) endif (USE_MYMATH) # add the executable add_executable (Tutorial tutorial.cxx) target_link_libraries (Tutorial ${EXTRA_LIBS}) 这里使用 USE_MYMATH 来决定是否编译并使用 MathFunctions 。注意收集可执行文件的可选连接库所使用的变量（这里为 EXTRA_LIBS）的使用方法。这种方法在保持有许多可选组件的大型项目整洁时经常使用。对应的我们修改源码如下： // A simple program that computes the square root of a number #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;math.h&gt; #include &quot;TutorialConfig.h&quot; #ifdef USE_MYMATH #include &quot;MathFunctions.h&quot; #endif int main (int argc, char *argv[]) { if (argc &lt; 2) { fprintf(stdout,&quot;%s Version %d.%d\\n&quot;, argv[0], Tutorial_VERSION_MAJOR, Tutorial_VERSION_MINOR); fprintf(stdout,&quot;Usage: %s number\\n&quot;,argv[0]); return 1; } double inputValue = atof(argv[1]); #ifdef USE_MYMATH double outputValue = mysqrt(inputValue); #else double outputValue = sqrt(inputValue); #endif fprintf(stdout,&quot;The square root of %g is %g\\n&quot;, inputValue, outputValue); return 0; } 在源码中我们同样使用了 USE_MYMATH 宏。这个宏由 CMake 通过在配置文件 TutorialConfig.h 添加以下代码传递给源码： #cmakedefine USE_MYMATH Step 3. 安装和测试 下一步我们将给我们的项目添加安装规则和测试。安装规则简单明了。对于 MathFunctions 库的安装，我们通过在 MathFunction 的 CMakeLists.txt 文件中添加以下两行来设置其库和头文件的安装。 install (TARGETS MathFunctions DESTINATION bin) install (FILES MathFunctions.h DESTINATION include) 对于本文这个应用通过在顶层 CMakeLists.txt 添加以下内容来安装可执行文件和配置的头文件： # add the install targets install (TARGETS Tutorial DESTINATION bin) install (FILES &quot;${PROJECT_BINARY_DIR}/TutorialConfig.h&quot; DESTINATION include) 以上就是安装的全部步骤。现在你应该可以编译本教程了。输入 make install（或在 IDE 中编译 install 项目），对应的头文件、库文件和可执行文件就会被安装。CMake 的 CMAKE_INSTALL_PREFIX 参数可以指定安装文件的根目录（之前还可以加上 -D 参数，具体意义可以参考 what does the parameter &quot;-D&quot; mean）。添加测试过程同样简单明了。在顶层 CMakeLists.txt 文件的最后我们可以添加一个基础测试数据来验证该应用程序是否正常运行。 include(CTest) # does the application run add_test (TutorialRuns Tutorial 25) # does it sqrt of 25 add_test (TutorialComp25 Tutorial 25) set_tests_properties (TutorialComp25 PROPERTIES PASS_REGULAR_EXPRESSION &quot;25 is 5&quot;) # does it handle negative numbers add_test (TutorialNegative Tutorial -25) set_tests_properties (TutorialNegative PROPERTIES PASS_REGULAR_EXPRESSION &quot;-25 is 0&quot;) # does it handle small numbers add_test (TutorialSmall Tutorial 0.0001) set_tests_properties (TutorialSmall PROPERTIES PASS_REGULAR_EXPRESSION &quot;0.0001 is 0.01&quot;) # does the usage message work? add_test (TutorialUsage Tutorial) set_tests_properties (TutorialUsage PROPERTIES PASS_REGULAR_EXPRESSION &quot;Usage:.*number&quot;) 在编译完成之后，我们可以运行 ctest 命令行工具来执行测试。第一个测试简单的验证了程序是否工作，是否有严重错误并且返回0.这是 Ctest 测试的基础。接下来的一些测试都使用了 PASS_REGULAR_EXPRESSION 测试属性（正则表达式）来验证输出中是否包含了特定的字符串。这里验证开方是否正确并且在计算错误时输出输出对应信息。如果你希望添加很多的测试来测试不同的输入值，你可以考虑定义一个像下面这样的宏： #define a macro to simplify adding tests, then use it macro (do_test arg result) add_test (TutorialComp${arg} Tutorial ${arg}) set_tests_properties (TutorialComp${arg} PROPERTIES PASS_REGULAR_EXPRESSION ${result}) endmacro (do_test) # do a bunch of result based tests do_test (25 &quot;25 is 5&quot;) do_test (-25 &quot;-25 is 0&quot;) Step 4. 添加系统自检 接下来我们考虑给我们的项目添加一些取决于目标平台是否有一些特性的代码。这里我们将添加一些取决于目标平台是否有 log 和 exp 函数的代码。当然对于大多数平台都会有这些函数，但这里我们认为这并不常见。如果平台有 log 函数我们将在 mysqrt 函数中使用它计算平方根。我们首先在顶层 CMakeLists.txt 文件中使用 CheckFunctionExists 宏测试这些函数是否可用，代码如下： # does this system provide the log and exp functions? include (${CMAKE_ROOT}/Modules/CheckFunctionExists.cmake) check_function_exists (log HAVE_LOG) check_function_exists (exp HAVE_EXP) 接下来我们修改 TutorialConfig.h.in 文件定义一些宏以表示 CMake 是否在平台上找到这些函数： // does the platform provide exp and log functions? #cmakedefine HAVE_LOG #cmakedefine HAVE_EXP 一定要在使用 configure_file 生成 TutorialConfig.h 之前测试 log 和 exp。因为 configure_file 命令会立刻使用当前 CMake 的设置配置文件。最后根据 log 和 exp 是否在我们的平台上可用我们给 mysqrt 函数提供一个可选的实现，代码如下: // if we have both log and exp then use them #if defined (HAVE_LOG) &amp;&amp; defined (HAVE_EXP) result = exp(log(x)*0.5); #else // otherwise use an iterative approach . . . Step 5. 添加一个生成的文件和生成器 在这一章节我们将会展示如何在构建一个应用的过程中添加一个生成的源文件。在本例中我们将创建一个预先计算的平方根表作为构建过程的一部分，然后将其编译到我们的应用中。为了做到这一点我们首先需要一个能产生这张表的程序。在 MathFunctions 文件夹下创建一个新的名为 MakeTable.cxx 的文件，内容如下： // A simple program that builds a sqrt table #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;math.h&gt; int main (int argc, char *argv[]) { int i; double result; // make sure we have enough arguments if (argc &lt; 2) { return 1; } // open the output file FILE *fout = fopen(argv[1],&quot;w&quot;); if (!fout) { return 1; } // create a source file with a table of square roots fprintf(fout,&quot;double sqrtTable[] = {\\n&quot;); for (i = 0; i &lt; 10; ++i) { result = sqrt(static_cast&lt;double&gt;(i)); fprintf(fout,&quot;%g,\\n&quot;,result); } // close the table with a zero fprintf(fout,&quot;0};\\n&quot;); fclose(fout); return 0; } 注意到这张表使用 C++ 代码生成且文件的名字通过输入参数指定。下一步通过在 MathFunctions 的 CMakeLists.txt 中添加合适的代码来构建 MakeTable 可执行文件，并将它作为构建的一部分运行。只需要一点代码就能实现这个功能： # first we add the executable that generates the table add_executable(MakeTable MakeTable.cxx) # add the command to generate the source code add_custom_command ( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/Table.h COMMAND MakeTable ${CMAKE_CURRENT_BINARY_DIR}/Table.h DEPENDS MakeTable ) # add the binary tree directory to the search path for # include files include_directories( ${CMAKE_CURRENT_BINARY_DIR} ) # add the main library add_library(MathFunctions mysqrt.cxx ${CMAKE_CURRENT_BINARY_DIR}/Table.h ) 首先添加的 MakeTable 可执行文件和其它可执行文件相同。接下来我们添加一个自定义的命令来指定如何通过运行 MakeTable 生成 Table.h 文件。接下来我们必须让 CMake 知道 mysqrt.cxx 依赖于生成的 Table.h 文件。这一点通过将生成的 Table.h 文件添加到 MathFunctions 库的源文件列表实现。我们同样必须将当前二进制文件路径添加到包含路径中，以保证 Table.h 文件被找到并被 mysqrt.cxx 包含。该项目在构建时会首先构建 MakeTable 可执行文件。接下来会运行该可执行文件并生成 Table.h 文件。最后它将会编译包含 Table.h 的 mysqrt.cxx 文件并生成 MathFunctions 库。此时包含了所有我们添加的特性的顶层 CMakeLists.txt 文件应该像下面这样： cmake_minimum_required (VERSION 2.6) project (Tutorial) include(CTest) # The version number. set (Tutorial_VERSION_MAJOR 1) set (Tutorial_VERSION_MINOR 0) # does this system provide the log and exp functions? include (${CMAKE_ROOT}/Modules/CheckFunctionExists.cmake) # check the function exists? return bool value named HAVE_LOG/HAVE_EXP check_function_exists (log HAVE_LOG) check_function_exists (exp HAVE_EXP) # should we use our own math functions option(USE_MYMATH &quot;Use tutorial provided math implementation&quot; ON) # configure a header file to pass some of the CMake settings # to the source code configure_file ( &quot;${PROJECT_SOURCE_DIR}/TutorialConfig.h.in&quot; &quot;${PROJECT_BINARY_DIR}/TutorialConfig.h&quot; ) # add the binary tree to the search path for include files # so that we will find TutorialConfig.h include_directories (&quot;${PROJECT_BINARY_DIR}&quot;) # add the MathFunctions library? if (USE_MYMATH) include_directories (&quot;${PROJECT_SOURCE_DIR}/MathFunctions&quot;) add_subdirectory (MathFunctions) set (EXTRA_LIBS ${EXTRA_LIBS} MathFunctions) endif (USE_MYMATH) # add the executable add_executable (Tutorial tutorial.cxx) target_link_libraries (Tutorial ${EXTRA_LIBS}) # add the install targets install (TARGETS Tutorial DESTINATION bin) install (FILES &quot;${PROJECT_BINARY_DIR}/TutorialConfig.h&quot; DESTINATION include) # does the application run add_test (TutorialRuns Tutorial 25) # does the usage message work? add_test (TutorialUsage Tutorial) set_tests_properties (TutorialUsage PROPERTIES PASS_REGULAR_EXPRESSION &quot;Usage:.*number&quot; ) #define a macro to simplify adding tests macro (do_test arg result) add_test (TutorialComp${arg} Tutorial ${arg}) set_tests_properties (TutorialComp${arg} PROPERTIES PASS_REGULAR_EXPRESSION ${result} ) endmacro (do_test) # do a bunch of result based tests do_test (4 &quot;4 is 2&quot;) do_test (9 &quot;9 is 3&quot;) do_test (5 &quot;5 is 2.236&quot;) do_test (7 &quot;7 is 2.645&quot;) do_test (25 &quot;25 is 5&quot;) do_test (-25 &quot;-25 is 0&quot;) do_test (0.0001 &quot;0.0001 is 0.01&quot;) TutorialConfig.h 文件如下： // the configured options and settings for Tutorial #define Tutorial_VERSION_MAJOR @Tutorial_VERSION_MAJOR@ #define Tutorial_VERSION_MINOR @Tutorial_VERSION_MINOR@ #cmakedefine USE_MYMATH // does the platform provide exp and log functions? #cmakedefine HAVE_LOG #cmakedefine HAVE_EXP 最后 MathFunctions 的 CMakeLists.txt 文件如下： # first we add the executable that generates the table add_executable(MakeTable MakeTable.cxx) # add the command to generate the source code add_custom_command ( OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/Table.h DEPENDS MakeTable COMMAND MakeTable ${CMAKE_CURRENT_BINARY_DIR}/Table.h ) # add the binary tree directory to the search path # for include files include_directories( ${CMAKE_CURRENT_BINARY_DIR} ) # add the main library add_library(MathFunctions mysqrt.cxx ${CMAKE_CURRENT_BINARY_DIR}/Table.h) install (TARGETS MathFunctions DESTINATION bin) install (FILES MathFunctions.h DESTINATION include) Step 6. 构造一个 Installer 接下来假设我们想将我们的项目发布给其他人以便供他们使用。我们想提供在不同平台上的二进制文件和源码的发布版本。这一点和我们在之前安装和测试章节（步骤3）略有不同，步骤三安装的二进制文件是我们从源码构建的。这里我们将构建一个支持二进制文件安装的安装包和可以在 cygwin,debian,RPMs 等中被找到的安装管理特性。为了实现这一点我们将使用 CPack 来创建在 Packaging with CPack 章节中介绍过的平台特定安装器（platform specific installers）。我们需要在顶层 CMakeLists.txt 文件添加以下几行内容： # build a CPack driven installer package include (InstallRequiredSystemLibraries) set (CPACK_RESOURCE_FILE_LICENSE &quot;${CMAKE_CURRENT_SOURCE_DIR}/License.txt&quot;) set (CPACK_PACKAGE_VERSION_MAJOR &quot;${Tutorial_VERSION_MAJOR}&quot;) set (CPACK_PACKAGE_VERSION_MINOR &quot;${Tutorial_VERSION_MINOR}&quot;) include (CPack) 首先我们添加了 InstallRequiredSystemLibraries。该模块会包含我们项目在当前平台所需的所有运行时库（runtime libraries）。接下来我们设置了一些 CPack 变量来指定我们项目的许可文件和版本信息。版本信息使用我们在之前设置的内容。最后我们包含 CPack 模块，它会使用这些变量和其它你安装一个应用程序所需的系统属性。 接下来就是正常编译你的项目然后使用 CPack 运行它，为了编译二进制发布版本你需要运行： cpack --config CPackConfig.cmake 创建一个源文件发布版本你应该使用下面命令： cpack --config CPackSourceConfig.cmake Step 7. 添加 Dashboard 支持 添加将我们测试结果提交到仪表盘的功能非常简单。在本教程的之前步骤中我们已经给我们的项目定义了一些测试。我们只需要运行这些测试然后提交到仪表盘即可。为了支持仪表盘功能我们需要在顶层 CMakeLists.txt 文件中增加 CTest 模块。 # enable dashboard scripting include (CTest) 我们同样可以创建一个 CTestConfig.cmake 文件来在表盘工具中指定本项目的名字。 set (CTEST_PROJECT_NAME &quot;Tutorial&quot;) CTest 会在运行时读取该文件。你可以在你的项目上运行 CMake 来创建一个简单的仪表盘，切换目录到二进制文件夹下，然后运行 ctest -DExperimental。你仪表盘的运行结果会上传到 Kitware 的公共仪表盘上。 实例 1.编译生成可执行文件 源码：object_storage.cpp cmake_minimum_required (VERSION 2.8) # 项目信息 project (Demo) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) # 指定生成目标 add_executable(Demo object_storage.cpp) target_link_libraries(Demo -laws-cpp-sdk-core -laws-cpp-sdk-s3) 2.动态库/静态库 编译 编译静态库/动态库 源码结构： [root@ecs-sn3-medium-2-linux-20200222090658 s3util]# tree . ├── CMakeLists.txt ├── include │ ├── S3Util.h ├── Makefile └── src ├── CMakeLists.txt └── S3Util.cpp 项目根目录下的 CMakeLists.txt #CMake最低版本号要求 cmake_minimum_required(VERSION 2.8) #指定项目名称 project(s3util) #指定版本信息 set(CMAKE_SYSTEM_VERSION 1) #若是需要指定编译器路径 #set(CROSS_TOOLCHAIN_PREFIX &quot;/path/arm-linux-&quot;) #指定编译器 #set(CMAKE_C_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}gcc&quot;) #set(CMAKE_CXX_COMPILER &quot;${CROSS_TOOLCHAIN_PREFIX}g++&quot;) #使用默认路径的g++指定编译器 #set(CMAKE_CXX_COMPILER &quot;g++&quot;) #指定编译选项 set(CMAKE_BUILD_TYPE Debug ) #指定编译目录 set(PROJECT_BINARY_DIR ${PROJECT_SOURCE_DIR}/build) #添加子目录,这样进入源码文件src目录可以继续构建 add_subdirectory(${PROJECT_SOURCE_DIR}/src) src 目录下的 CMakeLists.txt CMakeLists.txt #查找当前目录下的所有源文件， #并将名称保存到DIR_LIB_SRCS目录 #aux_source_directory(. DIR_LIB_SRCS) #指定头文件目录,PROJECT_SOURCE_DIR为工程的根目录 include_directories(${PROJECT_SOURCE_DIR}/include) add_compile_options( -std=c++11 ) include_directories(/c++/C-Example/linux/install/usr/local/include) link_directories(/c++/C-Example/linux/install/usr/local/lib64) #指定可执行文件的输出目录，输出到bin下面 set(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/bin) #生成动态库 add_library(s3_shared_demo SHARED S3Util.cpp) #设置库输出名为 shared =&gt; libshared.so set_target_properties(s3_shared_demo PROPERTIES OUTPUT_NAME &quot;s3shared&quot;) #生成静态库 add_library(s3_static_demo STATIC S3Util.cpp) #设置输库出名为 static =&gt; libstatic.a set_target_properties(s3_static_demo PROPERTIES OUTPUT_NAME &quot;s3static&quot;) #指定库文件输出路径 set(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/lib) #在指定目录下查找库，并保存在LIBPATH变量中 find_library(LIBPATHS shared ${PROJECT_SOURCE_DIR}/lib) #指定生成目标 #add_executable(main main.cpp) #链接共享库 #target_link_libraries(main ${LIBPATHS}) #target_link_libraries(s3util -laws-cpp-sdk-core -laws-cpp-sdk-s3) 参考文献 [1] CMake 入门实战 [2] CMake 官方教程 [3] 简书-刘亚彬_244c：CMake 使用教程 ","link":"https://blog.shunzi.tech/post/cmake/"},{"title":"存储基本概念","content":" 存储的一些基本概念 一些专业的术语解释 持续更新ing 从逻辑上存储通常分为块存储，文件存储，对象存储。 存储分类及应用 块存储：块存储（DAS/SAN）通常应用在某些专有的系统中，这类应用要求很高的随机读写性能和高可靠性，上面搭载的通常是Oracle/DB2这种传统数据库，连接通常是以FC光纤（8Gb/16Gb）为主，走光纤协议。如果要求稍低一些，也会出现基于千兆/万兆以太网的连接方式，MySQL这种数据库就可能会使用IP SAN，走iSCSI协议。通常使用块存储的都是系统而非用户，并发访问不会很多，经常出现一套存储只服务一个应用系统，例如如交易系统，计费系统。典型行业如金融，制造，能源，电信等。块存储通常都是通过光纤网络连接，服务器/小机上配置FC光纤HBA卡，通过光纤交换机连接存储（IP SAN可以通过千兆以太网，以iSCSI客户端连接存储），主机端以逻辑卷（Volume）的方式访问。连接成功后，应用访问存储是按起始地址，偏移量Offset的方法来访问的。 文件存储：文件存储（NAS）相对来说就更能兼顾多个应用和更多用户访问，同时提供方便的数据共享手段。毕竟大部分的用户数据都是以文件的形式存放，在PC时代，数据共享也大多是用文件的形式，比如常见的的FTP服务，NFS服务，Samba共享这些都是属于典型的文件存储。几十个用户甚至上百用户的文件存储共享访问都可以用NAS存储加以解决。在中小企业市场，一两台NAS存储设备就能支撑整个IT部门了。CRM系统，SCM系统，OA系统，邮件系统都可以使用NAS存储统统搞定。甚至在公有云发展的早几年，用户规模没有上来时，云存储的底层硬件也有用几套NAS存储设备就解决的，甚至云主机的镜像也有放在NAS存储上的例子。文件存储的广泛兼容性和易用性，是这类存储的突出特点。但是从性能上来看，相对SAN就要低一些。NAS存储基本上是以太网访问模式，普通千兆网，走NFS/CIFS协议。文件存储通常只要是局域网内，千兆/百兆的以太网环境皆可。网线连上，服务器端通过操作系统内置的NAS客户端，如NFS/CIFS/FTP客户端挂载存储成为一个本地的文件夹后访问，只要符合POXIS标准，应用就可以用标准的open，seek, write/read,close这些方法对其访问操作。 对象存储：对象存储概念出现得晚一些，存储标准化组织SINA早在2004年就给出了定义，但早期多出现在超大规模系统，所以并不为大众所熟知，相关产品一直也不温不火。一直到云计算和大数据的概念全民强推，才慢慢进入公众视野。前面说到的块存储和文件存储，基本上都还是在专有的局域网络内部使用，而对象存储的优势场景却是互联网或者公网，主要解决海量数据，海量并发访问的需求。基于互联网的应用才是对象存储的主要适配（当然这个条件同样适用于云计算，基于互联网的应用最容易迁移到云上，因为没出现云这个名词之前，他们已经在上面了），基本所有成熟的公有云都提供了对象存储产品，不管是国内还是国外。对象存储常见的适配应用如网盘、媒体娱乐，医疗PACS，气象，归档等数据量超大而又相对“冷数据”和非在线处理的应用类型。这类应用单个数据大，总量也大，适合对象存储海量和易扩展的特点。网盘类应用也差不多，数据总量很大，另外还有并发访问量也大，支持10万级用户访问这种需求就值得单列一个项目了（这方面的扫盲可以想想12306）。归档类应用只是数据量大的冷数据，并发访问的需求倒是不太突出。另外基于移动端的一些新兴应用也是适合的，智能手机和移动互联网普及的情况下，所谓UGD（用户产生的数据，手机的照片视频）总量和用户数都是很大挑战。毕竟直接使用HTTP get/put就能直接实现数据存取，对移动应用来说还是有一定吸引力的。对象存储的访问通常是在互联网，走HTTP协议，性能方面，单独看一个连接的是不高的（还要解决掉线断点续传之类的可靠性问题），主要强大的地方是支持的并发数量，聚合起来的性能带宽就非常可观了。对象存储不在乎网络，而且它的访问比较有特色，只能存取删（put/get/delete），不能打开修改存盘。只能取下来改好后上传，去覆盖原对象。//因为中间是不可靠的互联网啊，不能保证你在修改时候不掉线啊。所谓你在这头，对象在那头，所爱对象隔山海，山海不可平。 应用 块存储：通过某种协议（e.g.SAS、SCSI、SAN、iSCSI）从后端存储 Assigned、Attached 块设备（Volume），然后分区格式化、创建文件系统并 mount 到操作系统，然后就可以在此文件系统之上存储数据，或者也可以直接使用裸硬盘来存储数据（e.g. 数据库系统） 文件存储：通过 NFS、CIFS 等协议，mount 远程文件系统到本地操作系统。NAS、NFS 服务器，以及各种分布式文件系统提供的都是这种存储。 对象存储：对象存储是以对象形式存储数据的存储系统，最大的优势就是可以让用户更加灵活的处理海量数据。操作系统客户端可以通过对象存储提供的存储网关接口（一般是 HTTP/S）来上传或下载存储数据。 参考文献 [1] EricChan: 从应用角度看块/文件/对象三种存储 2017-05-24 [2] EricChan: 块存储，文件存储，对象存储的层次关系 2017-05-24 [3] RedHat: 文件存储、块存储还是对象存储？ [4] 博客园：Cinder 架构分析、高可用部署与核心功能解析 RAID (Redundant Array of Independent Disks) 独立硬盘冗余阵列（RAID, Redundant Array of Independent Disks），旧称廉价磁盘冗余阵列（Redundant Array of Inexpensive Disks），简称磁盘阵列。利用虚拟化存储技术把多个硬盘组合起来，成为一个或多个硬盘阵列组，目的提升性能或数据冗余或是两者同时提升。 在运作中，取决于 RAID 层级不同，数据会以多种模式分散于各个硬盘，RAID 层级的命名会以 RAID 开头并带数字，例如.RAID 0，RAID 1，RAID 5，RAID 6，RAID 7，RAID 01，RAID 10，RAID 50，RAID 60。每种等级都有其理论上的优缺点，不同的等级在两个目标间获取平衡，分别是增加数据可靠性以及增加存储器（群）读写性能。 简单来说，RAID把多个硬盘组合成为一个逻辑硬盘，因此，操作系统只会把它当作一个硬盘。RAID常被用在服务器计算机上，并且常使用完全相同的硬盘作为组合。由于硬盘价格的不断下降与RAID功能更加有效地与主板集成，它也成为普通用户的一个选择，特别是需要大容量存储空间的工作，如：视频与音频制作。 JBOD（Just a Bunch Of Disks）指将数个物理硬盘，在操作系统中合并成一个逻辑硬盘，以直接增加容量。 RAID0 条带化 RAID0 是一种简单的、无数据校验的数据条带化技术。实际上不是一种真正的 RAID ，因为它并不提供任何形式的冗余策略。 RAID0 将所在磁盘条带化后组成大容量的存储空间，将数据分散存储在所有磁盘中，以独立访问方式实现多块磁盘的并读访问。由于可以并发执行 I/O 操作，总线带宽得到充分利用。再加上不需要进行数据校验，RAID0 的性能在所有 RAID 等级中是最高的。理论上讲，一个由 n 块磁盘组成的 RAID0 ，它的读写性能是单个磁盘性能的 n 倍，但由于总线带宽等多种因素的限制，实际的性能提升低于理论值。 RAID0 具有低成本、高读写性能、 100% 的高存储空间利用率等优点，但是它不提供数据冗余保护，一旦数据损坏，将无法恢复。 因此， RAID0 一般适用于对性能要求严格但对数据安全性和可靠性不高的应用，如视频、音频存储、临时数据缓存空间等。 RAID1 镜像 RAID1 称为镜像，它将数据完全一致地分别写到工作磁盘和镜像 磁盘，它的磁盘空间利用率为 50% 。 RAID1 在数据写入时，响应时间会有所影响，但是读数据的时候没有影响。 RAID1 提供了最佳的数据保护，一旦工作磁盘发生故障，系统自动从镜像磁盘读取数据，不会影响用户工作。 RAID1 与 RAID0 刚好相反，是为了增强数据安全性使两块 磁盘数据呈现完全镜像，从而达到安全性好、技术简单、管理方便。 RAID1 拥有完全容错的能力，但实现成本高。 RAID1 应用于对顺序读写性能要求高以及对数据保护极为重视的应用，如对邮件系统的数据保护。 RAID2 2n海明码校验 RAID2 称为纠错海明码磁盘阵列，其设计思想是利用海明码实现数据校验冗余。海明码是一种在原始数据中加入若干校验码来进行错误检测和纠正的编码技术，其中第 2n 位（ 1, 2, 4, 8, … ）是校验码，其他位置是数据码。因此在 RAID2 中，数据按位存储，每块磁盘存储一位数据编码，磁盘数量取决于所设定的数据存储宽度，可由用户设定。图 4 所示的为数据宽度为 4 的 RAID2 ，它需要 4 块数据磁盘和 3 块校验磁盘。如果是 64 位数据宽度，则需要 64 块 数据磁盘和 7 块校验磁盘。可见， RAID2 的数据宽度越大，存储空间利用率越高，但同时需要的磁盘数量也越多。 海明码自身具备纠错能力，因此 RAID2 可以在数据发生错误的情况下对纠正错误，保证数据的安全性。它的数据传输性能相当高，设计复杂性要低于后面介绍的 RAID3 、 RAID4 和 RAID5 。 但是，海明码的数据冗余开销太大，而且 RAID2 的数据输出性能受阵列中最慢磁盘驱动器的限制。再者，海明码是按位运算， RAID2 数据重建非常耗时。由于这些显著的缺陷，再加上大部分磁盘驱动器本身都具备了纠错功能，因此 RAID2 在实际中很少应用，没有形成商业产品，目前主流存储磁盘阵列均不提供 RAID2 支持。 RAID3 校验盘 RAID3 是使用专用校验盘的并行访问阵列，它采用一个专用的磁盘作为校验盘，其余磁盘作为数据盘，数据按位可字节的方式交叉存储到各个数据盘中。RAID3 至少需要三块磁盘，不同磁盘上同一带区的数据作 XOR 校验，校验值写入校验盘中。 RAID3 完好时读性能与 RAID0 完全一致，并行从多个磁盘条带读取数据，性能非常高，同时还提供了数据容错能力。向 RAID3 写入数据时，必须计算与所有同条带的校验值，并将新校验值写入校验盘中。一次写操作包含了写数据块、读取同条带的数据块、计算校验值、写入校验值等多个操作，系统开销非常大，性能较低。 如果 RAID3 中某一磁盘出现故障，不会影响数据读取，可以借助校验数据和其他完好数据来重建数据。假如所要读取的数据块正好位于失效磁盘，则系统需要读取所有同一条带的数据块，并根据校验值重建丢失的数据，系统性能将受到影响。当故障磁盘被更换后，系统按相同的方式重建故障盘中的数据至新磁盘。 RAID3 只需要一个校验盘，阵列的存储空间利用率高，再加上并行访问的特征，能够为高带宽的大量读写提供高性能，适用大容量数据的顺序访问应用，如影像处理、流媒体服务等。目前， RAID5 算法不断改进，在大数据量读取时能够模拟 RAID3 ，而且 RAID3 在出现坏盘时性能会大幅下降，因此常使用 RAID5 替代 RAID3 来运行具有持续性、高带宽、大量读写特征的应用。 RAID4 块组织校验盘 RAID4 与 RAID3 的原理大致相同，区别在于条带化的方式不同。 RAID4 （图 6 ）按照 块的方式来组织数据，写操作只涉及当前数据盘和校验盘两个盘，多个 I/O 请求可以同时得到处理，提高了系统性能。 RAID4 按块存储可以保证单块的完整性，可以避免受到其他磁盘上同条带产生的不利影响。 RAID4 在不同磁盘上的同级数据块同样使用 XOR 校验，结果存储在校验盘中。写入数据时， RAID4 按这种方式把各磁盘上的同级数据的校验值写入校验 盘，读取时进行即时校验。因此，当某块磁盘的数据块损坏， RAID4 可以通过校验值以及其他磁盘上的同级数据块进行数据重建。 RAID4 提供了 非常好的读性能，但单一的校验盘往往成为系统性能的瓶颈。对于写操作， RAID4 只能一个磁盘一个磁盘地写，并且还要写入校验数据，因此写性能比较差。而且随着成员磁盘数量的增加，校验盘的系统瓶颈将更加突出。正是如上这些限制和不足， RAID4 在实际应用中很少见，主流存储产品也很少使用 RAID4 保护。 RAID5 校验数据散列分布 RAID5 应该是目前最常见的 RAID 等级，它的原理与 RAID4 相似，区别在于校验数据分布在阵列中的所有磁盘上，而没有采用专门的校验磁盘。对于数据和校验数据，它们的写操作可以同时发生在完全不同的磁盘上。因此， RAID5 不存在 RAID4 中的并发写操作时的校验盘性能瓶颈问题。另外， RAID5 还具备很好的扩展性。当阵列磁盘 数量增加时，并行操作量的能力也随之增长，可比 RAID4 支持更多的磁盘，从而拥有更高的容量以及更高的性能。 RAID5 （图 7）的磁盘上同时存储数据和校验数据，数据块和对应的校验信息存保存在不同的磁盘上，当一个数据盘损坏时，系统可以根据同一条带的其他数据块和对应的校验数据来重建损坏的数据。与其他 RAID 等级一样，重建数据时， RAID5 的性能会受到较大的影响。 RAID5 兼顾存储性能、数据安全和存储成本等各方面因素，它可以理解为 RAID0 和 RAID1 的折中方案，是目前综合性能最佳的数据保护解决方案。 RAID5 基本上可以满足大部分的存储应用需求，数据中心大多采用它作为应用数据的保护方案。 RAID6 校验数据冗余分布 双重校验 前面所述的各个 RAID 等级都只能保护因单个磁盘失效而造成的数据丢失。如果两个磁盘同时发生故障，数据将无法恢复。 RAID6 （如图 8 ）引入双重校验的概念，它可以保护阵列中同时出现两个磁盘失效时，阵列仍能够继续工作，不会发生数据丢失。 RAID6 等级是在 RAID5 的基础上为了进一步增强数据保护而设计的一种 RAID 方式，它可以看作是一种扩展的 RAID5 等级。 RAID6 不仅要支持数据的恢复，还要支持校验数据的恢复，因此实现代价很高，控制器的设计也比其他等级更复杂、更昂贵。 RAID6 思想最常见的实现方式是采用两个独立的校验算法，假设称为 P 和 Q ，校验数据可以分别存储在两个不同的校验盘上，或者分散存储在所有成员磁盘中。当两个磁盘同时失效时，即可通过求解两元方程来重建两个磁盘上的数据。 RAID6 具有快速的读取性能、更高的容错能力。但是，它的成本要高于 RAID5 许多，写性能也较差，并有设计和实施非常复杂。因此， RAID6 很少得到实际应用，主要用于对数据安全等级要求非常高的场合。它一般是替代 RAID10 方案的经济性选择。 RAID00 RAID01 RAID10 RAID100 RAID30 RAID50 RAID53 RAID60 标准 RAID 等级各有优势和不足。自然地，我们想到把多个 RAID 等级组合起来，实现优势互补，弥补相互的不足，从而达到在性能、数据安全性等指标上更高的 RAID 系统。目前在业界和学术研究中提到的 RAID 组合等级主要有 RAID00 、 RAID01 、 RAID10 、 RAID100 、 RAID30 、 RAID50 、 RAID53 、 RAID60 ，但实际得到较为广泛应用的只有 RAID01 和 RAID10 两个等级。当然，组合等级的实现成本一般都非常昂贵，只是在 少数特定场合应用。 RAID01 和 RAID10 一些文献把这两种 RAID 等级看作是等同的，本文认为是不同的。 RAID01 是先做条带化再作镜像，本质是对物理磁盘实现镜像；而 RAID10 是先做镜像再作条带化，是对虚拟磁盘实现镜像。相同的配置下，通常 RAID01 比 RAID10 具有更好的容错能力，原理如图 9 所示。 RAID01 兼备了 RAID0 和 RAID1 的优点，它先用两块磁盘建立镜像，然后再在镜像内部做条带化。 RAID01 的数据将同时写入到两个磁盘阵列中，如果其中一个阵列损坏，仍可继续工作，保证数据安全性的同时又提高了性能。 RAID01 和 RAID10 内部都含有 RAID1 模式，因此整体磁盘利用率均仅为 50% . 参考链接 [1] 博客园：RAID技术超详细讲解 ROM (Read Only Memory) ROM 一种非易失性只读存储器。在制造过程中，将资料以一特制光罩（mask）烧录于线路中，其资料内容在写入后就不能更改，所以有时又称为“光罩式只读内存”（mask ROM）。成本相对较低，常常用于电脑中的开启启动如启动光盘，BIOS芯片等，以及作为固件存放一些硬件的驱动程序。 PROM 可编程只读存储器（Programmable ROM，PROM）其内部有行列式的镕丝，可依用户（厂商）的需要，利用电流将其烧断，以写入所需的数据及程序，镕丝一经烧断便无法再恢复，亦即数据无法再更改。 EPROM 可抹除可编程只读存储器（Erasable Programmable Read Only Memory，EPROM）可利用高电压将数据编程写入，但抹除时需将线路曝光于紫外线下一段时间，数据始可被清空，再供重复使用。因此，在封装外壳上会预留一个石英玻璃所制的透明窗以便进行紫外线曝光。写入程序后通常会用贴纸遮盖透明窗，以防日久不慎曝光过量影响数据。 OTPROM 一次编程只读存储器（One Time Programmable Read Only Memory，OTPROM）内部所用的芯片与写入原理同EPROM，但是为了节省成本，封装上不设置透明窗，因此编程写入之后就不能再抹除改写。 EEPROM 电子抹除式可复写只读存储器（Electrically Erasable Programmable Read Only Memory，EEPROM）之运作原理类似EPROM，但是抹除的方式是使用高电场来完成，因此不需要透明窗。 FLASH 闪存（Flash memory）的每一个记忆胞都具有一个“控制闸”与“浮动闸”，利用高电场改变浮动闸的临限电压即可进行编程动作。闪存主要分为NAND型与NOR型。现在NAND常用于固态硬盘、U盘、存储卡等用途，NOR则用于BIOS ROM芯片等用途。 NOR Flash NOR Flash需要很长的时间进行抹写，但是它提供完整的寻址与数据总线，并允许随机存取存储器上的任何区域，这使的它非常适合取代老式的ROM芯片。当时ROM芯片主要用来存储几乎不需更新的代码，例如电脑的BIOS或机上盒(Set-top Box)的固件。NOR Flash可以忍受一万到一百万次抹写循环，它同时也是早期的可移除式快闪存储媒体的基础。 NAND Flash NAND Flash式东芝在1989年的国际固态电路研讨会(ISSCC)上发表的， 要在NandFlash上面读写数据，要外部加主控和电路设计。。NAND Flash具有较快的抹写时间, 而且每个存储单元的面积也较小，这让NAND Flash相较于NOR Flash具有较高的存储密度与较低的每比特成本。同时它的可抹除次数也高出NOR Flash十倍。然而NAND Flash 的I/O接口并没有随机存取外部地址总线，它必须以区块性的方式进行读取，NAND Flash典型的区块大小是数百至数千比特。 EMMC emmc存储器eMMC (Embedded Multi Media Card) 为MMC协会所订立的，eMMC 相当于 NandFlash+主控IC ，对外的接口协议与SD、TF卡一样，主要是针对手机或平板电脑等产品的内嵌式存储器标准规格。eMMC的一个明显优势是在封装中集成了一个控制器，它提供标准接口并管理闪存，使得手机厂商就能专注于产品开发的其它部分，并缩短向市场推出产品的时间。这些特点对于希望通过缩小光刻尺寸和降低成本的NAND供应商来说，同样的重要。 RAM (Random Access Memory) 与CPU直接交换数据的内部存储器。它可以随时读写（刷新时除外），而且速度很快，通常作为操作系统或其他正在运行中的程序的临时数据存储介质。 SRAM (Static Random Access Memory) 是随机存取存储器的一种。所谓的“静态”，是指这种存储器只要保持通电，里面储存的数据就可以恒常保持。相对之下，动态随机存取存储器（DRAM）里面所储存的数据就需要周期性地更新。然而，当电力供应停止时，SRAM储存的数据还是会消失（被称为volatile memory），这与在断电后还能储存资料的ROM或闪存是不同的。 DRAM (Dynamic Random Access Memory) 主要的作用原理是利用电容内存储电荷的多寡来代表一个二进制比特（bit）是1还是0。由于在现实中晶体管会有漏电电流的现象，导致电容上所存储的电荷数量并不足以正确的判别数据，而导致数据毁损。因此对于DRAM来说，周期性地充电是一个无可避免的要件。由于这种需要定时刷新的特性，因此被称为“动态”存储器。 Function DRAM SRAM 刷新 需要刷新。如果存储单元没有被刷新，存储的信息就会丢失。 不需要刷新。这种存储器只要保持通电，里面储存的数据就可以恒常保持 速度 慢（相对于SRAM）, 由于需要不断地刷新数据 快 集成度 高 低。管子数量多，结构复杂，因此集成度低 静态功耗 高,电容结构，需要对电容刷新充电 低 动态功耗 低 高。管子数量多。 价格 低 高 用途 内存条 CPU中cache，一级缓存，二级缓存 SDRAM 同步动态随机存取內存（synchronous dynamic random-access memory） 是有一个同步接口的动态随机存取內存（DRAM）。通常DRAM是有一个异步接口的，这样它可以随时响应控制输入的变化。而SDRAM有一个同步接口，在响应控制输入前会等待一个时钟信号，这样就能和计算机的系统总线同步。时钟被用来驱动一个有限状态机，对进入的指令进行管线(Pipeline)操作。这使得SDRAM与没有同步接口的异步DRAM(asynchronous DRAM)相比，可以有一个更复杂的操作模式。 SDRAM从发展到现在已经经历了五代，分别是：第一代SDR SDRAM，第二代DDR SDRAM，第三代DDR2 SDRAM，第四代DDR3 SDRAM，第五代，DDR4 SDRAM。 SSD (Solid-State Drive) 固态硬盘或固态驱动器（英语：Solid-state drive或Solid-state disk，简称SSD）,是一种主要以闪存(NAND Flash)作为永久性存储器的计算机存储设备，此处固态主要相对于以机械臂带动磁头转动实现读写操作的磁盘而言，NAND或者其他固态存储以电位高低或者相位状态的不同记录0和1，用集成电路代替了物理旋转磁盘。 SSD 组成：Flash Controller (闪存控制器) and NAND Flash Memory chips (NAND 闪存芯片) 固态硬盘采用SATA 3、M.2或者PCI Express、mSATA、U.2、ZIF、IDE、CF、CFast等接口。但由于价格及存储空间与机械硬盘有巨大差距，固态硬盘当前仍无法完全取代机械式硬盘。 易失性存储器 由易失性存储器制成的固态硬盘主要用于临时性存储。因为这类存储器需要靠外界电力维持其记忆，所以由此制成的固态硬盘还需要配合电池才能使用。易失性存储器，例如SDRAM，具有访问速度快的特点。利用这一特点，可以将需要运行的程序从传统硬盘复制到易失性存储器中，然后再交由计算机运行，这样可以避免由于传统硬盘的引导延迟、搜索延迟等对程序以及系统造成的影响。 由易失性存储器制成的固态硬盘通常会依靠电池来保证完成应急备份：当电源意外中断时，靠电池驱动的这类固态硬盘可以有足够的时间将数据转移到传统硬盘中。当电力恢复后，再从传统硬盘中恢复数据。 非易失性存储器 非易失性存储器的数据访问速度介于易失性存储器和传统硬盘之间。和易失性存储器相比，非易失性存储器一经写入数据，就不需要外界电力来维持其记忆。因此更适于作为传统硬盘的替代品。 闪存当中的NAND Flash是最常见的非易失性存储器。小容量的NAND闪存可被制作成带有USB接口的移动存储设备，亦即人们常说的“U盘”。随着生产成本的下降，将多个大容量闪存模块集成在一起，制成以闪存为存储介质的固态硬盘已经是当前的趋势。 参考链接 [1] 维基百科： RAID [2] 维基百科：ROM [3] 维基百科：闪存 [4] NorFlash、NandFlash、eMMC闪存的比较与区别 [5] 维基百科：RAM [6] CSDN ：DDR4, DDR3, DDR2, DDR1 及SDRAM各有何不同? [7] 维基百科：SSD [8] searchstorage：SSD (solid-state drive) iSCSI (Internet Small Computer System Interface) Internet小型计算机系统接口，又称为IP-SAN，是一种基于因特网及SCSI-3协议下的存储技术，由IETF提出，并于2003年2月11日成为正式的标准。 iSCSI利用了TCP/IP的port 860 和 3260 作为沟通的渠道。透过两部计算机之间利用iSCSI的协议来交换SCSI命令，让计算机可以透过高速的局域网集线来把SAN模拟成为本地的储存设备。 iSCSI使用 TCP/IP 协议（一般使用TCP端口860和3260）。 本质上，iSCSI 让两个主机通过 IP 网络相互协商然后交换 SCSI 命令。这样一来，iSCSI 就是用广域网仿真了一个常用的高性能本地存储总线，从而创建了一个存储局域网（SAN）。不像某些 SAN 协议，iSCSI 不需要专用的电缆；它可以在已有的交换和 IP 基础架构上运行。然而，如果不使用专用的网络或者子网（ LAN 或者 VLAN ），iSCSI SAN 的部署性能可能会严重下降。于是，iSCSI 常常被认为是光纤通道（Fiber Channel）的一个低成本替代方法，而光纤通道是需要专用的基础架构的。但是，基于以太网的光纤通道（FCoE）则不需要专用的基础架构。 SCSI (Small Computer System Interface) 概念：一种用于计算机和外部设备之间（硬盘、光驱、软驱、打印机等）系统级接口的独立处理器标准。SCSI是一种智能的通用接口标准，它是各种计算机和外部设备之间的接口标准。SCSI标准定义命令、通信协议以及实体的电气特性（换成OSI的说法，就是占据物理层、链接层、套接层、应用层），最大部分的应用是在存储设备上（例如硬盘、磁带机），除外，SCSI可以连接的设备包括有扫描仪、光学设备（像CD、DVD）、打印机等等，SCSI命令中有条列出支持的设备SCSI周边设备 SAN (Storage Area Network) 开放系统的存储：内置存储 和 外挂存储 外挂存储根据连接的方式分为直连式存储（Direct-Attached Storage，简称DAS）和网络化存储（Fabric-Attached Storage，简称FAS）；网络化存储根据传输协议又分为：网络接入存储（Network-Attached Storage，简称NAS）和存储区域网络（Storage Area Network，简称SAN）。 DAS: （Direct Attached Storage）直接附加存储，直接附加存储是指将存储设备通过总线（SCSI、PCI、IDE等）接口直接连接到一台服务器上使用。 NAS: (Network Attached Storage）网络附加存储。在NAS存储结构中，存储系统不再通过I/O总线附属于某个服务器或客户机，而直接通过网络接口与网络直接相连，由用户通过网络访问。 SAN: （Storage Area Network）存储区域网络，是一种高速的、专门用于存储操作的网络，通常独立于计算机局域网（LAN）。SAN将主机（管理server，业务server等）和存储设备连接在一起，能够为其上的任意一台主机和任意一台存储设备提供专用的通信通道。SAN将存储设备从服务器中独立出来，实现了服务器层次上的存储资源共享。SAN将通道技术和网络技术引入存储环境中，提供了一种新型的网络存储解决方案，能够同时满足吞吐率、可用性、可靠性、可扩展性和可管理性等方面的要求。 FC-SAN: FC-SAN顾名思义就是直接通过FC(光纤)通道来连接磁盘阵列，数据通过发送SCSI命令来直接与硬件进行通信，从而提高了整体的速率。 IP-SAN: IP-SAN（IP存储）的通信通道是使用IP通道，而不是光纤通道，把服务器与存储设备连接起来的技术，除了标准已获通过的iSCSI，还有FCIP、iFCP等正在制定的标准。而iSCSI发展最快，已经成了IP存储一个有力的代表。 iSCSI的使用 iSCSI技术在工作形式上分为服务端（target）与客户端（initiator），iSCSI服务端即用于存放硬盘存储资源的服务器，作为前面创建RAID磁盘阵列组的存储端，能够为用户提供可用的存储资源。而iSCSI客户端则是用户使用的软件，用于获取远程服务端的存储资源 IQN (iSCSI Qualified Name)：“iSCSI限定名称”。每个发起端和目标需要唯一名称进行表示最好的作法是使用一个在INTERNET上可能独一无二的名称 iSCSI Client/Host：系统中的iSCSI客户端或主机（也称为iSCSI initiator），诸如服务器，连接在IP网络并对iSCSI target发起请求以及接收响应。每一个iSCSI主机通过唯一的IQN来识别，类似于光纤通道的WWN。要在IP网络上传递SCSI块命令，必须在iSCSI主机上安装iSCSI驱动。推荐通过GE适配器（每秒1000 megabits）连接至iSCSI target。如同标准10/100适配器，大多数Gigabit适配器使用Category 5 或Category 6E线缆。适配器上的各端口通过唯一的IP地址来识别。 iSCSI Target：iSCSI target是接收iSCSI命令的设备。此设备可以是终端节点，如存储设备，或是中间设备，如IP和光纤设备之间的连接桥。每一个iSCSI target通过唯一的IQN来标识，存储阵列控制器上（或桥接器上）的各端口通过一个或多个IP地址来标识。 参考链接 [1] NAS、DAS和SAN三种存储方式的区别 [2] 维基百科：iSCSI [3] iSCSI存储系统知识 [4] CSDN: SAN技术及应用 LVM (Logical Volumn Manager) 逻辑卷管理，主要解决的问题是，弹性调整文件系统的容量。 与传统的磁盘与分区相比，LVM为计算机提供了更高层次的存储，通过在磁盘分区和文件系统之间增加一个逻辑层，提供一个抽象的逻辑盘卷。 参考链接 [1] Yikun: 存储知识学习 磁盘基础知识 盘面：硬盘一般含有一个或多个盘片，一个盘片包含两个盘面。一个盘面对应一个磁头。 磁道：每个盘面被划成多个狭窄的同心圆环，这样的圆环叫做磁道。 扇区：每个磁道的每段圆弧叫做一个扇区，是读写的最小单位。 柱面：所有盘面上的同一磁道，在竖直方向构成一个圆柱，称为柱面。 磁盘容量 = 磁头数（盘面数） × 磁道(柱面)数 × 每道扇区数 × 每扇区字节数 磁盘读取响应时间： 寻道时间：磁头从开始移动到数据所在磁道所需要的时间，寻道时间越短，I/O操作越快。目前磁盘的平均寻道时间一般在3－15ms，一般都在10ms左右。 旋转延迟：盘片旋转将请求数据所在扇区移至读写磁头下方所需要的时间，旋转延迟取决于磁盘转速。普通硬盘一般都是7200rpm，慢的5400rpm。 数据传输时间：完成传输所请求的数据所需要的时间。 块/簇 -&gt; 外存 磁盘块/簇（虚拟出来的）。 块是操作系统中最小的逻辑存储单位。操作系统与磁盘打交道的最小单位是磁盘块。 通俗的来讲，在Windows下如NTFS等文件系统中叫做簇；在Linux下如Ext4等文件系统中叫做块（block）。每个簇或者块可以包括2、4、8、16、32、64…2的n次方个扇区。 为什么存在磁盘块？ 读取方便：由于扇区的数量比较小，数目众多在寻址时比较困难，所以操作系统就将相邻的扇区组合在一起，形成一个块，再对块进行整体的操作。 分离对底层的依赖：操作系统忽略对底层物理存储结构的设计。通过虚拟出来磁盘块的概念，在系统中认为块是最小的单位。 页 -&gt; 内存 操作系统经常与内存和硬盘这两种存储设备进行通信，类似于“块”的概念，都需要一种虚拟的基本单位。所以，与内存操作，是虚拟一个页的概念来作为最小单位。与硬盘打交道，就是以块为最小单位。 总结 扇区： 硬盘的最小读写单元 块/簇： 是操作系统针对硬盘读写的最小单元 page： 是内存与操作系统之间操作的最小单元 扇区 &lt;= 块/簇 &lt;= page 磁盘分区 MBR 分区表：(1) 主要开机区（MBR）446 bytes; (2) 分区表（Partition Table）64 bytes GPT 分区表：分区容量扩充较多，支持更大容量的磁盘。 参考链接 [1] 博客园: 硬盘基本知识（磁头、磁道、扇区、柱面） ","link":"https://blog.shunzi.tech/post/storage-basic-concept/"},{"title":"TCMU学习笔记","content":" iSCSI Target 端的用户态实现 TCMU 介绍 TCMU 原理剖析 TCMU 对接自定义后端存储代码示例讲解 TCMU 简介 TCMU 优势 设计约束 实现概览 MailBox Command Ring TCMU_OP_CMD TCMU_OP_PAD Data Area TCMU 动态配置加载 TCMU Plugin Handler 实现方式一 全权接管命令处理 实现方式二 仅实现具体的IO操作 TCMU 深入 targetcli-fb rtslib-fb TCMU 简介 TCM 是 LIO 的别名，是内核态的 scsi target 实现。TCM targets 运行在内核态，TCMU（TCM in Userspace）是 LIO 的用户态实现。 现有的内核为不同的SCSI传输协议提供了模块。TCM 也模块化数据存储，目前已经支持文件(fileio)，块设备(block)，RAMDISK 或其他iSCSI设备（pscsi），这些称为 backstore 或 storage engine。这些内建的模块完全在内核态实现。 LIO 仅能使用内核态的 backstore。而如 tgt 这样的其他的用户态 target 解决方案，能够支持 Gluster 的 GLFS 和 Ceph 的 RBD 作为 backstore。target 相当于 translator，允许 initiators 通过标准协议存储数据到这些非传统的网络存储系统。 为 LIO 增加这些支持存在更多的困难，因为 LIO 是纯内核态代码。要解决这个问题有两种方案，一种是将 GLFS 和 RBD API 和 protocols 库移植到内核，另一种方案是为 LIO 创建用户空间 pass-through backstore，称为 TCMU。前者的工作远比后者复杂。 TCMU 优势 除了可以容易支持 RBD 和 GLFS，TCMU 将允许更加简单的方式来开发新的 backstore。TCMU 与 LIO loopback 组合，实现类似于 FUSE(File system in Userspace) 的机制，只是 SCSI layer 替换了 file system layer。 这种机制的劣势是需要配置更多不同的组件，存在故障的风险。如果我们希望工作量尽可能少的话，这些不可避免，只能希望故障不是致命的影响。 设计约束 高性能： 高吞吐量，低延迟。 简洁处理，如果用户空间发生如下故障： 1） nerver attaches 2） hangs 3） dies 4） misbehaves 允许未来在用户空间和内核空间的灵活实现。 合理的内存使用。 简单地配置和运行。 简单地编写用户空间 backend。 实现概览 TCMU 接口的核心是一段由用户态和内核态共享的内存区域。这块区域包括： 控制区域（mailbox）； 无锁的生产者消费者环形buffer，用于命令的传递和状态的返回； 数据in/out的缓冲区。 TCMU 使用了已经存在的 UIO 子系统。 UIO 子系统允许设备驱动在用户态开发，这个概念十分贴近 TCMU 的使用案例，除了物理设备， TCMU 为 SCSI 命令实现了内存映射层。使用 UIO 也有利于 TCMU 处理设备的自省，如通过用户态去决定使用多大的共享区域，和两端的信号机制。 内存区域是没有指针的，只有相对于内存区域起始位置的 offset。通过这种机制可以使在用户进程挂掉或者重启使得内存区域在不同的虚拟地址空间的情况下，仍然保持工作。可以查看target_core_user.h查看结构的定义。 MailBox mailbox总是在共享内存的开始位置，并且包含了version，开始位置的offset，command ring的大小，用户态和内核态存放ring command和command完成状态的head和tail指针。 struct tcmu_mailbox { __u16 version; // 2 如果是别的值，用户空间应该废弃） __u16 flags; //TCMU_MAILBOX_FLAG_CAP_OOOC: 标志支持out-of-order completion __u32 cmdr_off; //command ring在内存区域的起始位置的偏移量。 __u32 cmdr_size; //command ring区域的大小。这不需要2的幂来表示。 __u32 cmd_head; //由内核修改，表示一个command已经放置到ring中。 /* Updated by user. On its own cacheline */ __u32 cmd_tail __attribute__((__aligned__(ALIGN_SIZE))); //由用户空间修改，表示一个command已经处理完成。 } __attribute__((packed)); Command Ring Command放置到 ring 上，kernel 根据 command 的 size 移动 mailbox.cmd_head 指针，取模 cmdr_size，并且通过 uio_event_notify() 通知用户空间。当命令执行完成，用户空间更新 mailbox.cmd_tail，并且通过 write() 4个字节通知内核。当 cmd_head 等于 cmd_tail，ring 为空 – 说明没有 command 等待用户空间处理。 TCMU commands 是 8 字节对齐的。command 以通用 header 起头，header 包含了 len_op，32 位的值，用于存储 command 的长度，同时使用了最低的 3 个 bit 存储操作码 opcode。也包含了 cmd_id 和 flags，由内核设置的kflags 和用户空间设置的 uflags。 现在只定义了两种操作码opcode： TCMU_OP_CMD 和 TCMU_OP_PAD。 TCMU_OP_CMD 在command ring的结构是struct tcmu_cmd_entry。 struct tcmu_cmd_entry { struct tcmu_cmd_entry_hdr hdr; union { struct { uint32_t iov_cnt; // The size of iov array. uint32_t iov_bidi_cnt; // size of iov array for Data-In uint32_t iov_dif_cnt; // uint64_t cdb_off; // Command data block offset uint64_t __pad1; uint64_t __pad2; struct iovec iov[0]; // iov array - buffer } req; struct { uint8_t scsi_status; // Command executed, return status. uint8_t __pad1; uint16_t __pad2; uint32_t __pad3; char sense_buffer[TCMU_SENSE_BUFFERSIZE]; // response data } rsp; }; } __attribute__((packed)); /* * Only a few opcodes, and length is 8-byte aligned, so use low bits for opcode. */ struct tcmu_cmd_entry_hdr { __u32 len_op; __u16 cmd_id; __u8 kflags; #define TCMU_UFLAG_UNKNOWN_OP 0x1 __u8 uflags; } __attribute__((packed)); 用户空间通过tcmu_cmd_entry.req.cdb_off找到SCSI CDB（Command Data Block）。这是command在整个共享内存区域起始位置的偏移量，而不是结构体或ring中的里面的偏移量。 通过req.iov[]数据访问数据区域。iov_cnt包含了iov[]结构的数量，需要区分Data-In还是Data-Out的数据。对于双向的command，iov_cnt指定多少iovec覆盖了Data-Out区域，iov_bidi_cnt指定了多少iovec覆盖了Data-In区域（紧接在Data-Out区域）。就像别的field一样，iov.iov_base是相对于内存区域起始位置的偏移量。 当command执行完成，用户空间设置rsp.scsi_status，如果有需要也设置rsp.sense_buffer。 用户空间根据entry.hdr.length(mod cmdr_size)增加mailbox.cmd_tail，并且通过UIO方法通知内核，4字节写到文件描述符。 如果设置TCMU_MAILBOX_FLAG_CAP_OOOC到mailbox-&gt;flags，kernel 可以处理 out-of-order completions。在这种情况下，用户空间能够处理与源头不同的顺序。由于kernel处理command的顺序与command ring中的一致，所以用户空间在command执行完成时，需要更新cmd-&gt;id。 TCMU_OP_PAD 当操作码opcode是PAD，用户空间只会更新cmd_tail -- 因为是一个no-op。kernel 插入PAD entries 确保每个CMD entry在command ring中是连续的。 未来会加入更多的opcode。如果用户空间遇到一个不能处理的opcode，必须要设置 hdr.uflags 的第0个bit为UNKNOWN_OP，更新cmd_tail，处理附加的commands。 Data Area 数据区域是在command ring的后面，TCMU接口没有定义这片区域的结构，用户空间应该只访问pengding iovs指定的区域。 TCMU 动态配置加载 TCMU 使用了一个时刻监听 TCMU 配置文件的守护进程来检查配置文件的变化，对应地更新 TCMU 的相关配置，诸如日志级别, 日志输出路径等 TCMU Plugin Handler 为了在 TCMU 相关接口的基础之上，处理自定义后端存储的处理方法，需要自定义实现 TCMU Handler. struct tcmur_handler { const char *name; /* Human-friendly name */ const char *subtype; /* Name for cfgstring matching */ const char *cfg_desc; /* Description of this backstore's config string */ void *opaque; /* Handler private data. */ /* * As much as possible, check that the cfgstring will result * in a working device when given to us as dev-&gt;cfgstring in * the -&gt;open() call. * * This function is optional but gives configuration tools a * chance to warn users in advance if the device they're * trying to create is invalid. * * Returns true if string is valid. Only if false, set *reason * to a string that says why. The string will be free()ed. * Suggest using asprintf(). */ bool (*check_config)(const char *cfgstring, char **reason); int (*reconfig)(struct tcmu_device *dev, struct tcmulib_cfg_info *cfg); /* Per-device added/removed callbacks */ int (*open)(struct tcmu_device *dev, bool reopen); void (*close)(struct tcmu_device *dev); /* * If &gt; 0, runner will execute up to nr_threads IO callouts from * threads. * if 0, runner will call IO callouts from the cmd proc thread or * completion context for compound commands. */ int nr_threads; /* * Async handle_cmd only handlers return: * * - TCMU_STS_OK if the command has been executed successfully * - TCMU_STS_NOT_HANDLED if opcode is not handled * - TCMU_STS_ASYNC_HANDLED if opcode is handled asynchronously * - Non TCMU_STS_OK code indicating failure * - TCMU_STS_PASSTHROUGH_ERR For handlers that require low level * SCSI processing and want to setup their own sense buffers. * * Handlers that set nr_threads &gt; 0 and async handlers * that implement handle_cmd and the IO callouts below return: * * - TCMU_STS_OK if the handler has queued the command. * - TCMU_STS_NOT_HANDLED if the command is not supported. * - TCMU_STS_NO_RESOURCE if the handler was not able to allocate * resources for the command. * * If TCMU_STS_OK is returned from the callout the handler must call * the tcmulib_cmd-&gt;done function with TCMU_STS return code. */ handle_cmd_fn_t handle_cmd; /* * Below callbacks are only executed by generic_handle_cmd. * Returns: * - TCMU_STS_OK if the handler has queued the command. * - TCMU_STS_NO_RESOURCE if the handler was not able to allocate * resources for the command. * * If TCMU_STS_OK is returned from the callout the handler must call * the tcmulib_cmd-&gt;done function with TCMU_STS return code. */ rw_fn_t write; rw_fn_t read; flush_fn_t flush; unmap_fn_t unmap; /* * If the lock is acquired and the tag is not TCMU_INVALID_LOCK_TAG, * it must be associated with the lock and returned by get_lock_tag on * local and remote nodes. When unlock is successful, the tag * associated with the lock must be deleted. * * Returns a TCMU_STS indicating success/failure. */ int (*lock)(struct tcmu_device *dev, uint16_t tag); int (*unlock)(struct tcmu_device *dev); /* * Return tag set in lock call in tag buffer and a TCMU_STS * indicating success/failure. */ int (*get_lock_tag)(struct tcmu_device *dev, uint16_t *tag); /* * Must return TCMUR_DEV_LOCK state value. */ int (*get_lock_state)(struct tcmu_device *dev); /* * internal field, don't touch this * * indicates to tcmu-runner whether this is an internal handler loaded * via dlopen or an external handler registered via dbus. In the * latter case opaque will point to a struct dbus_info. */ bool _is_dbus_handler; /* * Update the logdir called by dynamic config thread. */ bool (*update_logdir)(void); }; 实现方式一 全权接管命令处理 以 file_optical.c 为例： /* * Return scsi status or TCMU_STS_NOT_HANDLED */ static int fbo_handle_cmd(struct tcmu_device *dev, struct tcmulib_cmd *cmd) { uint8_t *cdb = cmd-&gt;cdb; struct iovec *iovec = cmd-&gt;iovec; size_t iov_cnt = cmd-&gt;iov_cnt; uint8_t *sense = cmd-&gt;sense_buf; struct fbo_state *state = tcmur_dev_get_private(dev); bool do_verify = false; int ret; /* Check for format in progress */ /* Certain commands can be executed even if a format is in progress */ if (state-&gt;flags &amp; FBO_FORMATTING &amp;&amp; cdb[0] != INQUIRY &amp;&amp; cdb[0] != REQUEST_SENSE &amp;&amp; cdb[0] != GET_CONFIGURATION &amp;&amp; cdb[0] != GPCMD_GET_EVENT_STATUS_NOTIFICATION) { tcmu_sense_set_key_specific_info(sense, state-&gt;format_progress); ret = TCMU_STS_FRMT_IN_PROGRESS; return ret; } switch(cdb[0]) { case TEST_UNIT_READY: ret = tcmu_emulate_test_unit_ready(cdb, iovec, iov_cnt); break; case REQUEST_SENSE: ret = fbo_emulate_request_sense(dev, cdb, iovec, iov_cnt, sense); break; case FORMAT_UNIT: ret = fbo_emulate_format_unit(dev, cdb, iovec, iov_cnt, sense); break; case READ_6: case READ_10: case READ_12: ret = fbo_read(dev, cdb, iovec, iov_cnt, sense); break; case WRITE_VERIFY: do_verify = true; case WRITE_6: case WRITE_10: case WRITE_12: ret = fbo_write(dev, cdb, iovec, iov_cnt, sense, do_verify); break; case INQUIRY: ret = fbo_emulate_inquiry(cdb, iovec, iov_cnt, sense); break; case MODE_SELECT: case MODE_SELECT_10: ret = fbo_emulate_mode_select(cdb, iovec, iov_cnt, sense); break; case MODE_SENSE: case MODE_SENSE_10: ret = fbo_emulate_mode_sense(cdb, iovec, iov_cnt, sense); break; case START_STOP: ret = tcmu_emulate_start_stop(dev, cdb); break; case ALLOW_MEDIUM_REMOVAL: ret = fbo_emulate_allow_medium_removal(dev, cdb, sense); break; case READ_FORMAT_CAPACITIES: ret = fbo_emulate_read_format_capacities(dev, cdb, iovec, iov_cnt, sense); break; case READ_CAPACITY: if ((cdb[1] &amp; 0x01) || (cdb[8] &amp; 0x01)) /* Reserved bits for MM logical units */ return TCMU_STS_INVALID_CDB; else ret = tcmu_emulate_read_capacity_10(state-&gt;num_lbas, state-&gt;block_size, cdb, iovec, iov_cnt); break; case VERIFY: ret = fbo_verify(dev, cdb, iovec, iov_cnt, sense); break; case SYNCHRONIZE_CACHE: ret = fbo_synchronize_cache(dev, cdb, sense); break; case READ_TOC: ret = fbo_emulate_read_toc(dev, cdb, iovec, iov_cnt, sense); break; case GET_CONFIGURATION: ret = fbo_emulate_get_configuration(dev, cdb, iovec, iov_cnt, sense); break; case GPCMD_GET_EVENT_STATUS_NOTIFICATION: ret = fbo_emulate_get_event_status_notification(dev, cdb, iovec, iov_cnt, sense); break; case READ_DISC_INFORMATION: ret = fbo_emulate_read_disc_information(dev, cdb, iovec, iov_cnt, sense); break; case READ_DVD_STRUCTURE: ret = fbo_emulate_read_dvd_structure(dev, cdb, iovec, iov_cnt, sense); break; case MECHANISM_STATUS: ret = fbo_emulate_mechanism_status(dev, cdb, iovec, iov_cnt, sense); break; default: ret = TCMU_STS_NOT_HANDLED; } return ret; } 实现方式二 仅实现具体的IO操作 以 file.example.c为例 /* * Copyright (c) 2014 Red Hat, Inc. * * This file is licensed to you under your choice of the GNU Lesser * General Public License, version 2.1 or any later version (LGPLv2.1 or * later), or the Apache License 2.0. */ /* * Example code to demonstrate how a TCMU handler might work. * * Using the example of backing a device by a file to demonstrate: * * 1) Registering with tcmu-runner * 2) Parsing the handler-specific config string as needed for setup * 3) Opening resources as needed * 4) Handling SCSI commands and using the handler API */ #define _GNU_SOURCE #include &lt;stddef.h&gt; #include &lt;stdint.h&gt; #include &lt;stdbool.h&gt; #include &lt;stdio.h&gt; #include &lt;stdlib.h&gt; #include &lt;string.h&gt; #include &lt;unistd.h&gt; #include &lt;sys/types.h&gt; #include &lt;sys/stat.h&gt; #include &lt;fcntl.h&gt; #include &lt;endian.h&gt; #include &lt;errno.h&gt; #include &lt;scsi/scsi.h&gt; #include &quot;scsi_defs.h&quot; #include &quot;libtcmu.h&quot; #include &quot;tcmu-runner.h&quot; #include &quot;tcmur_device.h&quot; struct file_state { int fd; }; static int file_open(struct tcmu_device *dev, bool reopen) { struct file_state *state; char *config; state = calloc(1, sizeof(*state)); if (!state) return -ENOMEM; // Init the file state of tcmu tcmur_dev_set_private(dev, state); // Move the pointer to the first '/' location in path string config = strchr(tcmu_dev_get_cfgstring(dev), '/'); if (!config) { tcmu_err(&quot;no configuration found in cfgstring\\n&quot;); goto err; } config += 1; /* get past '/' */ // Enable the tcmu write cache.(Set the value of tcmu_device as true) tcmu_dev_set_write_cache_enabled(dev, 1); // Open the file with path.(With mode) state-&gt;fd = open(config, O_CREAT | O_RDWR, S_IRUSR | S_IWUSR); if (state-&gt;fd == -1) { tcmu_err(&quot;could not open %s: %m\\n&quot;, config); goto err; } tcmu_dbg(&quot;config %s\\n&quot;, tcmu_dev_get_cfgstring(dev)); return 0; err: free(state); return -EINVAL; } static void file_close(struct tcmu_device *dev) { // Get the file state of tcmu_device. struct file_state *state = tcmur_dev_get_private(dev); // Close the file close(state-&gt;fd); // free the state free(state); } /** * * @param *dev tcmu device * @param *cmd Command line interface.(not used in this method) * @param *iov buffer array to syore the data * @param iov_cnt buffer array size * @param length read length * @param offset start address * * return the size of read data. */ static int file_read(struct tcmu_device *dev, struct tcmulib_cmd *cmd, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { // Get the file state of tecmu_device struct file_state *state = tcmur_dev_get_private(dev); size_t remaining = length; ssize_t ret; // Read the file in loop while (remaining) { // Read the data and store in the iov array. Return the data size. ret = preadv(state-&gt;fd, iov, iov_cnt, offset); if (ret &lt; 0) { tcmu_err(&quot;read failed: %m\\n&quot;); ret = TCMU_STS_RD_ERR; goto done; } if (ret == 0) { /* EOF, then zeros the iovecs left */ tcmu_iovec_zero(iov, iov_cnt); break; } // Consume the iov array. tcmu_iovec_seek(iov, ret); // Move the offset offset += ret; // Change the length and continute to read file. remaining -= ret; } // Read finished, and status is OK. ret = TCMU_STS_OK; done: return ret; } /** * * @param *dev tcmu device * @param *cmd Command line interface.(not used in this method) * @param *iov buffer array to syore the data * @param iov_cnt buffer array size * @param length write length * @param offset start address * * return the size of read data. */ static int file_write(struct tcmu_device *dev, struct tcmulib_cmd *cmd, struct iovec *iov, size_t iov_cnt, size_t length, off_t offset) { // Get the file state of tecmu_device struct file_state *state = tcmur_dev_get_private(dev); size_t remaining = length; ssize_t ret; // Write the file in loop while (remaining) { // Wirte the data in the iov array to file. Return the data size. ret = pwritev(state-&gt;fd, iov, iov_cnt, offset); if (ret &lt; 0) { tcmu_err(&quot;write failed: %m\\n&quot;); ret = TCMU_STS_WR_ERR; goto done; } // Consume an inv array. tcmu_iovec_seek(iov, ret); // Move the offset. offset += ret; // Change the length and continue to write. remaining -= ret; } ret = TCMU_STS_OK; done: return ret; } static int file_flush(struct tcmu_device *dev, struct tcmulib_cmd *cmd) { // Get the file state of tcmu_device. struct file_state *state = tcmur_dev_get_private(dev); int ret; // Sync(Flush) the data in page cache to disk. if (fsync(state-&gt;fd)) { tcmu_err(&quot;sync failed\\n&quot;); ret = TCMU_STS_WR_ERR; goto done; } ret = TCMU_STS_OK; done: return ret; } static int file_reconfig(struct tcmu_device *dev, struct tcmulib_cfg_info *cfg) { switch (cfg-&gt;type) { // Extend or Reduce the size of file. case TCMULIB_CFG_DEV_SIZE: /* * TODO - For open/reconfig we should make sure the FS the * file is on is large enough for the requested size. For * now assume we can grow the file and return 0. */ return 0; case TCMULIB_CFG_DEV_CFGSTR: // Handle the write cache. case TCMULIB_CFG_WRITE_CACHE: default: return -EOPNOTSUPP; } } static const char file_cfg_desc[] = &quot;The path to the file to use as a backstore.&quot;; // Init the tcmu_handler with given static method defined in this class. static struct tcmur_handler file_handler = { .cfg_desc = file_cfg_desc, .reconfig = file_reconfig, .open = file_open, .close = file_close, .read = file_read, .write = file_write, .flush = file_flush, .name = &quot;File-backed Handler (example code)&quot;, .subtype = &quot;file&quot;, .nr_threads = 2, }; /* Entry point must be named &quot;handler_init&quot;. */ int handler_init(void) { // Regist the file_handler to running_handler list return tcmur_register_handler(&amp;file_handler); } TCMU 深入 targetcli-fb A command shell for managing the Linux LIO kernel target 首先需要了解 targetcli，然后再了解 targetcli-fb tcmu-runner 由 open_iscsi 维护，默认使用 targetcli-fb targetcli-fb 又依赖了很多其他魔改后的组件: rtslib-fb: A Python object API for managing the Linux LIO kernel target configshell-fb: A Python library for building configuration shells rtslib-fb 关于 tcmu-runner 中对于用户自定义存储后端的创建代码主要位于 tcm.py class UserBackedStorageObject(StorageObject): ''' An interface to configFS storage objects for userspace-backed backstore. ''' def __init__(self, name, config=None, size=None, wwn=None, hw_max_sectors=None, control=None, index=None): ''' @param name: The name of the UserBackedStorageObject. @type name: string @param config: user-handler-specific config string. - e.g. &quot;rbd/machine1@snap4&quot; @type config: string @param size: The size of the device to create, in bytes. @type size: int @param wwn: T10 WWN Unit Serial, will generate if None @type wwn: string @hw_max_sectors: Max sectors per command limit to export to initiators. @type hw_max_sectors: int @control: String of control=value tuples separate by a ',' that will passed to the kernel control file. @type: string @return: A UserBackedStorageObject object. ''' if size is not None: if config is None: raise RTSLibError(&quot;'size' and 'config' must be set when &quot; &quot;creating a new UserBackedStorageObject&quot;) if '/' not in config: raise RTSLibError(&quot;'config' must contain a '/' separating subtype &quot; &quot;from its configuration string&quot;) super(UserBackedStorageObject, self).__init__(name, 'create', index) try: self._configure(config, size, wwn, hw_max_sectors, control) except: self.delete() raise else: super(UserBackedStorageObject, self).__init__(name, 'lookup', index) def _configure(self, config, size, wwn, hw_max_sectors, control): self._check_self() if ':' in config: raise RTSLibError(&quot;':' not allowed in config string&quot;) self._control(&quot;dev_config=%s&quot; % config) self._control(&quot;dev_size=%d&quot; % size) if hw_max_sectors is not None: self._control(&quot;hw_max_sectors=%s&quot; % hw_max_sectors) if control is not None: self._control(control) self._enable() super(UserBackedStorageObject, self)._configure(wwn) def _get_size(self): self._check_self() return int(self._parse_info('Size')) def _get_hw_max_sectors(self): self._check_self() return int(self._parse_info('HwMaxSectors')) def _get_control_tuples(self): self._check_self() tuples = [] # 1. max_data_area_mb val = self._parse_info('MaxDataAreaMB') if val != &quot;NULL&quot;: tuples.append(&quot;max_data_area_mb=%s&quot; % val) val = self.get_attribute('hw_block_size') if val != &quot;NULL&quot;: tuples.append(&quot;hw_block_size=%s&quot; % val) # 3. add next ... return &quot;,&quot;.join(tuples) def _get_config(self): self._check_self() val = self._parse_info('Config') if val == &quot;NULL&quot;: return None return val def _get_alua_supported(self): self._check_self() return storage_object_get_alua_support_attr(self) hw_max_sectors = property(_get_hw_max_sectors, doc=&quot;Get the max sectors per command.&quot;) control_tuples = property(_get_control_tuples, doc=&quot;Get the comma separated string containing control=value tuples.&quot;) size = property(_get_size, doc=&quot;Get the size in bytes.&quot;) config = property(_get_config, doc=&quot;Get the TCMU config.&quot;) alua_supported = property(_get_alua_supported, doc=&quot;Returns true if ALUA can be setup. False if not supported.&quot;) def dump(self): d = super(UserBackedStorageObject, self).dump() d['wwn'] = self.wwn d['size'] = self.size d['config'] = self.config d['hw_max_sectors'] = self.hw_max_sectors d['control'] = self.control_tuples return d 此处我们以分析&quot;对自定义存储后端创建过程中使用的 size 参数为空时的处理”为例： 如下查找参数处理的流程，无果。只是找到了 create/lookup 的处理流程 &gt; super(UserBackedStorageObject, self).__init__(name, 'lookup', index) #class UserBackedStorageObject &gt; __init__(self, name, mode, index=None): #class StorageObject &gt; _Backstore(name, type(self), mode, index) &gt; __init__(self, name, storage_object_cls, mode, index=None): #class _Backstore &gt; self._create_in_cfs_ine(mode) &gt; def _create_in_cfs_ine(self, mode): # class node 通过在 targetcli 中打开日志 loglevel_console（还可以将日志输出到 file，对应 lof_file），设置 loglevel 为 debug，再进行错误命令的执行，查看错误日志来源 /&gt; set global loglevel_console=debug /&gt; set global loglevel_console=info 结果如下： /backstores/user:hcs_obj&gt; create shunzi cfgstring=/iqn-2021-com-tcmu-target/shunzi/1MB Running command line 'create shunzi cfgstring=/iqn-2021-com-tcmu-target/shunzi/1MB'. /usr/lib/python2.7/site-packages/configshell_fb/shell.py:757 _parse_cmdline() Parsing commandline. /usr/lib/python2.7/site-packages/configshell_fb/shell.py:775 _parse_cmdline() Parse gave path='' command='create' pparams=['shunzi'] kparams={'cfgstring': '/iqn-2021-com-tcmu-target/shunzi/1MB'} /usr/lib/python2.7/site-packages/configshell_fb/node.py:1846 get_node() Looking for path '.' /usr/lib/python2.7/site-packages/configshell_fb/node.py:1398 execute_command() Executing command create with pparams ['shunzi'] and kparams {'cfgstring': '/iqn-2021-com-tcmu-target/shunzi/1MB'}. /usr/lib/python2.7/site-packages/configshell_fb/node.py:1445 assert_params() Min params: 3 /usr/lib/python2.7/site-packages/configshell_fb/node.py:1446 assert_params() Max params: 6 /usr/lib/python2.7/site-packages/configshell_fb/node.py:1447 assert_params() Required params: name, size, cfgstring /usr/lib/python2.7/site-packages/configshell_fb/node.py:1448 assert_params() Optional params: wwn, hw_max_sectors, control /usr/lib/python2.7/site-packages/configshell_fb/node.py:1449 assert_params() Got 2 standard params. /usr/lib/python2.7/site-packages/configshell_fb/node.py:1450 assert_params() Got 0 extended params. /usr/lib/python2.7/site-packages/configshell_fb/node.py:1451 assert_params() Variable positional params: None /usr/lib/python2.7/site-packages/configshell_fb/node.py:1452 assert_params() Variable keyword params: None Missing required parameter size /usr/lib/python2.7/site-packages/configshell_fb/node.py:1846 get_node() Looking for path '/backstores/user:hcs_obj' /usr/lib/python2.7/site-packages/configshell_fb/node.py:1846 get_node() Looking for path 'backstores/user:hcs_obj' /usr/lib/python2.7/site-packages/configshell_fb/node.py:1846 get_node() Looking for path 'user:hcs_obj' 找到对应的参数检查逻辑： 位于 configshell-fb 中，不难发现该参数校验逻辑获取了对象对应的方法以及参数列表 inspect.getargspec(method) 通过参数列表来确定哪些参数是必须的，从而对参数的必填or可选进行了校验。 class ConfigNode(object): ... def assert_params(self, method, pparams, kparams): ''' Checks that positional and keyword parameters match a method definition, or raise an ExecutionError. @param method: The method to check call signature against. @type method: method @param pparams: The positional parameters. @type pparams: list @param kparams: The keyword parameters. @type kparams: dict @raise ExecutionError: When the check fails. ''' spec = inspect.getargspec(method) args = spec.args[1:] pp = spec.varargs kw = spec.keywords if spec.defaults is None: nb_opt_params = 0 else: nb_opt_params = len(spec.defaults) nb_max_params = len(args) nb_min_params = nb_max_params - nb_opt_params req_params = args[:nb_min_params] opt_params = args[nb_min_params:] unexpected_keywords = sorted(set(kparams) - set(args)) missing_params = sorted(set(args[len(pparams):]) - set(opt_params) - set(kparams.keys())) nb_params = len(pparams) + len(kparams) nb_standard_params = len(pparams) \\ + len([param for param in kparams if param in args]) nb_extended_params = nb_params - nb_standard_params self.shell.log.debug(&quot;Min params: %d&quot; % nb_min_params) self.shell.log.debug(&quot;Max params: %d&quot; % nb_max_params) self.shell.log.debug(&quot;Required params: %s&quot; % &quot;, &quot;.join(req_params)) self.shell.log.debug(&quot;Optional params: %s&quot; % &quot;, &quot;.join(opt_params)) self.shell.log.debug(&quot;Got %s standard params.&quot; % nb_standard_params) self.shell.log.debug(&quot;Got %s extended params.&quot; % nb_extended_params) self.shell.log.debug(&quot;Variable positional params: %s&quot; % pp) self.shell.log.debug(&quot;Variable keyword params: %s&quot; % kw) if len(missing_params) == 1: raise ExecutionError( &quot;Missing required parameter %s&quot; % missing_params[0]) elif missing_params: raise ExecutionError( &quot;Missing required parameters %s&quot; % &quot;, &quot;.join(&quot;'%s'&quot; % missing for missing in missing_params)) if spec.keywords is None: if len(unexpected_keywords) == 1: raise ExecutionError( &quot;Unexpected keyword parameter '%s'.&quot; % unexpected_keywords[0]) elif unexpected_keywords: raise ExecutionError( &quot;Unexpected keyword parameters %s.&quot; % &quot;, &quot;.join(&quot;'%s'&quot; % kw for kw in unexpected_keywords)) all_params = args[:len(pparams)] all_params.extend(kparams.keys()) for param in all_params: if all_params.count(param) &gt; 1: raise ExecutionError( &quot;Duplicate parameter %s.&quot; % param) if nb_opt_params == 0 \\ and nb_standard_params != nb_min_params \\ and pp is None: raise ExecutionError( &quot;Got %d positionnal parameters, expected exactly %d.&quot; % (nb_standard_params, nb_min_params)) if nb_standard_params &gt; nb_max_params and pp is None: raise ExecutionError( &quot;Got %d positionnal parameters, expected at most %d.&quot; % (nb_standard_params, nb_max_params)) 对于 method 的获取发现是按照一定的前缀规则进行匹配的，一开始以为会是一个写在配置文件中的 hardcode 的 method，但后来想了想觉得应该是别的实现类里面重新定义实现了。毕竟要支持多种存储后端，每个存储后端可能有不同的规则。 经过搜索发现，在 targetcli-fb 有一个新的继承关系 ConfigNode -&gt; UINode -&gt; UIBackstores -&gt; UIUserBackedBackstore，从而定位到具体的后端存储类型 https://github.com/open-iscsi/targetcli-fb/blob/2c3eccac082a2980aaed371a1fdf0efc6a49dd59/targetcli/ui_backstore.py#L622 通过和文件存储后端做对比，最终发现 ui_command_create 方法列表中的 size 字段是 mandatory 的 class UIFileIOBackstore(UIBackstore): def ui_command_create(self, name, file_or_dev, size=None, write_back=None, sparse=None, wwn=None): class UIUserBackedBackstore(UIBackstore): def ui_command_create(self, name, size, cfgstring, wwn=None, hw_max_sectors=None, control=None): class UIUserBackedBackstore(UIBackstore): ... def ui_command_create(self, name, size, cfgstring, wwn=None, hw_max_sectors=None, control=None): ''' Creates a User-backed storage object. SIZE SYNTAX =========== - If size is an int, it represents a number of bytes. - If size is a string, the following units can be used: - B or no unit present for bytes - k, K, kB, KB for kB (kilobytes) - m, M, mB, MB for MB (megabytes) - g, G, gB, GB for GB (gigabytes) - t, T, tB, TB for TB (terabytes) ''' size = human_to_bytes(size) wwn = self.ui_eval_param(wwn, 'string', None) config = self.handler + &quot;/&quot; + cfgstring ok, errmsg = self.iface.CheckConfig('(s)', config) if not ok: raise ExecutionError(&quot;cfgstring invalid: %s&quot; % errmsg) try: so = UserBackedStorageObject(name, size=size, config=config, wwn=wwn, hw_max_sectors=hw_max_sectors, control=control) except: raise ExecutionError(&quot;UserBackedStorageObject creation failed.&quot;) ui_so = UIUserBackedStorageObject(so, self) self.shell.log.info(&quot;Created user-backed storage object %s size %d.&quot; % (name, size)) return self.new_node(ui_so) ","link":"https://blog.shunzi.tech/post/tcmu/"},{"title":"Cinder核心","content":" OpenStack 存储调度管理基础设施 Cinder 组件介绍 Cinder 核心流程和实现方式 Cinder 高可用架构实现方式 Cinder 主要模块 cinder-api 核心思想 核心流程 细节 CRUD of Cinder Volumn CRUD of Cinder Volumn metadata Volume manage extension (manageable_volumes)¶ Volume transfer Services (os-services)¶ Volume actions (volumes, action)¶ Generic volume groups cinder-scheduler cinder-volume cinder-volumn Driver cinder-volumn Plugin cinder-backup Cinder HA (Cinder 高可用) Cinder的分布式锁 本地锁 Tooz 参考链接 Cinder Cinder在OpenStack中为虚拟机实例提供volume的块存储服务，可将卷挂载到实例上，作为虚拟机实例的本地磁盘使用。 组件化：便于添加新功能 高可用：能承受高负载 容错：隔离机制避免级联故障 可恢复：简便及时的故障检测与恢复 开放标准：提供标准实现，为社区提供参考 主要模块 /- ( LDAP ) [ Auth Manager ] --- | \\- ( DB ) | | cinderclient | / \\ | /- [ scheduler ] -- [ volume ] -- ( iSCSI ) [ Web Dashboard ]- -[ api ] -- &lt; AMQP &gt; -- \\ / | \\- [ backup ] novaclient | | | | &lt; REST &gt; cinder-api 对外提供稳定而统一的北向 RESTful API，cinder-api service 服务进程通常运行在控制节点，支持多 Workers 进程（通过配置项 osapivolumeworkers 设定）。接收到的合法请求会经由 MQ 传递到 cinder-volume 执行。Cinder API 现存 v2(DEPRECATED)、v3(CURRENT) 两个版本，可以通过配置文件来启用。 官方API介绍：https://developer.openstack.org/api-ref/block-storage/ 核心思想 TaskFlow : TaskFlow 能够控制应用程序中的长流程业务逻辑任务的暂停、重启、恢复以及回滚, 主要用于保证长流程任务执行的可靠性和一致性。应用场景主要为 面向任务 的工作场景。 Atom: Atom 是 TaskFlow 的最小单位, 其他的所有类, 包括 Task 类都是 Atom 类的子类. Task: 拥有执行和回滚功能额最小工作单元. 在 Task 类中开发者能够自定义 execute(执行) 和 revert(回滚) method。 Flow：在 TaskFlow 中使用 flow(流) 来关联各个 Task, 并且规定这些 Task 之间的执行和回滚顺序. flow 中不仅能够包含 task 还能够嵌套 flow. 常见类型有以下几种： Linear(linear_flow.Flow): 线性流, 该类型 flow 中的 task/flow 按照加入的顺序来依次执行, 按照加入的倒序依次回滚. Unordered(unordered_flow.Flow): 无序流, 该类型 flow 中的 task/flow 可能按照任意的顺序来执行和回滚. Graph(graph_flow.Flow): 图流, 该类型 flow 中的 task/flow 按照显式指定的依赖关系或通过其间的 provides/requires 属性的隐含依赖关系来执行和回滚. Retry：一个控制当错误发生时, 如何进行重试的特殊工作单元, 而且当你需要的时候还能够以其他参数来重试执行别的 Atom 子类. 常见类型: AlwaysRevert: 错误发生时, 回滚子流 AlwaysRevertAll: 错误发生时, 回滚所有流 Times: 错误发生时, 重试子流 ForEach: 错误发生时, 为子流中的 Atom 提供一个新的值, 然后重试, 直到成功或 retry 中定义的值用完为止. ParameterizedForEach: 错误发生时, 从后台存储(由 store 参数提供)中获取重试的值, 然后重试, 直到成功或后台存储中的值用完为止. Engine: Engines 才是真正运行 Atoms 的对象, 用于 load(载入) 一个 flow, 然后驱动这个 flow 中的 task/flow 开始运行. 我们可以通过 engine_conf 参数来指定不同的 engine 类型. 常见的 engine 类型如下: serial: 所有的 task 都在调用了 engine.run 的线程中运行. parallel: task 可以被调度到不同的线程中运行. worker-based: task 可以被调度到不同的 woker 中运行. 核心流程 创建 Volume 的流程： cinder-api 创建 volume_create_api 工作流（flow） ExtractVolumeRequestTask：获取（Extract）、验证（Validates）create volume 在 cinder-api 阶段相关的信息 QuotaReserverTask：预留配额 EntryCreateTask：在数据库中创建 Volumn 条目 QuotaCommitTask：确认配额 VolumeCastTask：发出一个 Cast 异步请求，将创建请求丢到 MQ，最终被 cinder-scheduler service 接收 cinder-scheduler 启动一个 volume_create_scheduler flow ExtractSchedulerSpecTask：将 request body 转换成为 Scheduler Filter 中通用的 RequestSpec 数据结构（实例对象）。 SchedulerCreateVolumeTask：完成 Filter 和 Weight 的调度算法。 RPC 请求发送到 cinder-volume service 接收 cinder-volume 启动工作流 volume_create_manager： ExtractVolumeRefTask ： 解析请求体 OnFailureRescheduleTask： ExtractVolumeSpecTask：将 request body 转换成为 Volume 模块中通用的 RequestSpec 数据结构（实例对象） NotifyVolumeActionTask：当对已有的卷采取措施的时候发送相应的通知 CreateVolumeFromSpecTask：创建对应的卷 CreateVolumeFromSpecTask：执行创建成功之后的后续操作 创建 Volume 的方式 建立raw格式的新卷 从快照建立新卷 从已有卷建立新卷 从副本建立新卷 从镜像建立新卷 细节 根据现在最新的 V3 版本的 API总结如下: Github: Volume 资源模型 Volumn Type 主要是针对多后端存储进行卷类型的管理 Volumes 针对 卷 来进行管理。其中一个卷（Volume）类似于一个如USB硬盘的可插拔的块存储设备，每一次可以将卷对应的挂载到实例上。 对 Volumes 设备进行 CRUD 时，对应的会有很多种状态，可以参考对应官方文档中的状态表。 CRUD of Cinder Volumn CRUD of Cinder Volumn POST /v3/{project_id}/volumes Request Body { &quot;volume&quot;: { &quot;size&quot;: 10, //int The size of the volume, in gibibytes (GiB). &quot;availability_zone&quot;: null, //The name of the availability zone. &quot;source_volid&quot;: null, //The UUID of the source volume. &quot;description&quot;: null, //The volume description. &quot;multiattach&quot;: false, //To enable this volume to attach to more than one server, set this value to true. Default is false. &quot;snapshot_id&quot;: null, //The UUID of the volume snapshot &quot;backup_id&quot;: null, //The UUID of the backup. &quot;name&quot;: null, //The volume name. &quot;imageRef&quot;: null, //The UUID of the image from which you want to create the volume. &lt;!--&quot;volume_type&quot;: null, //The volume type (either name or ID). &quot;metadata&quot;: {}, //One or more metadata key and value pairs to be associated with the new volume. &quot;consistencygroup_id&quot;: null //The UUID of the consistency group. }, &quot;OS-SCH-HNT:scheduler_hints&quot;: { //The dictionary of data to send to the scheduler. &quot;same_host&quot;: [ &quot;a0cf03a5-d921-4877-bb5c-86d26cf818e1&quot;, &quot;8c19174f-4220-44f0-824a-cd1eeef10287&quot; ] } } POST /v3/{project_id}/volumes Response Body { &quot;volume&quot;: { &quot;attachments&quot;: [], //Instance attachment information.(Server/Host/Device/Volume) &quot;availability_zone&quot;: &quot;nova&quot;,//The name of the availability zone. &quot;bootable&quot;: &quot;false&quot;,//Enables or disables the bootable attribute. You can boot an instance from a bootable volume. &quot;consistencygroup_id&quot;: null,//The UUID of the consistency group. &quot;created_at&quot;: &quot;2018-11-28T06:21:12.715987&quot;,//The date and time when the resource was created.CCYY-MM-DDThh:mm:ss±hh:mm &quot;description&quot;: null, //The volume description. &quot;encrypted&quot;: false, //If true, this volume is encrypted. &quot;id&quot;: &quot;2b955850-f177-45f7-9f49-ecb2c256d161&quot;,//The UUID of the volume. &quot;links&quot;: [ //The volume links. { &quot;href&quot;: &quot;http://127.0.0.1:33951/v3/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/2b955850-f177-45f7-9f49-ecb2c256d161&quot;, &quot;rel&quot;: &quot;self&quot; }, { &quot;href&quot;: &quot;http://127.0.0.1:33951/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/2b955850-f177-45f7-9f49-ecb2c256d161&quot;, &quot;rel&quot;: &quot;bookmark&quot; } ], &quot;metadata&quot;: {},//A metadata object. Contains one or more metadata key and value pairs that are associated with the volume. &quot;migration_status&quot;: null, &quot;multiattach&quot;: false,//If true, this volume can attach to more than one instance. &quot;name&quot;: null,//The volume name. &quot;replication_status&quot;: null,//The volume replication status. &quot;size&quot;: 10,//The size of the volume, in gibibytes (GiB). &quot;snapshot_id&quot;: null,// The UUID of the volume snapshot &quot;source_volid&quot;: null,//The UUID of the source volume. &quot;status&quot;: &quot;creating&quot;,//The volume status. &quot;updated_at&quot;: null, //The date and time when the resource was updated. CCYY-MM-DDThh:mm:ss±hh:mm &quot;user_id&quot;: &quot;c853ca26-e8ea-4797-8a52-ee124a013d0e&quot;,//The UUID of the user. &quot;volume_type&quot;: null //The associated volume type for the volume. } GET /v3/{project_id}/volumes (List accessible volumes) Response Body { &quot;volumes&quot;: [ { &quot;id&quot;: &quot;efa54464-8fab-47cd-a05a-be3e6b396188&quot;, &quot;links&quot;: [ { &quot;href&quot;: &quot;http://127.0.0.1:37097/v3/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/efa54464-8fab-47cd-a05a-be3e6b396188&quot;, &quot;rel&quot;: &quot;self&quot; }, { &quot;href&quot;: &quot;http://127.0.0.1:37097/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/efa54464-8fab-47cd-a05a-be3e6b396188&quot;, &quot;rel&quot;: &quot;bookmark&quot; } ], &quot;name&quot;: null } ] } GET /v3/{project_id}/volumes/{volume_id} Response Body { &quot;volume&quot;: { &quot;attachments&quot;: [], &quot;availability_zone&quot;: &quot;nova&quot;, &quot;bootable&quot;: &quot;false&quot;, &quot;consistencygroup_id&quot;: null, &quot;created_at&quot;: &quot;2018-11-29T06:50:07.770785&quot;, &quot;description&quot;: null, &quot;encrypted&quot;: false, &quot;id&quot;: &quot;f7223234-1afc-4d19-bfa3-d19deb6235ef&quot;, &quot;links&quot;: [ { &quot;href&quot;: &quot;http://127.0.0.1:45839/v3/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/f7223234-1afc-4d19-bfa3-d19deb6235ef&quot;, &quot;rel&quot;: &quot;self&quot; }, { &quot;href&quot;: &quot;http://127.0.0.1:45839/89afd400-b646-4bbc-b12b-c0a4d63e5bd3/volumes/f7223234-1afc-4d19-bfa3-d19deb6235ef&quot;, &quot;rel&quot;: &quot;bookmark&quot; } ], &quot;metadata&quot;: {}, &quot;migration_status&quot;: null, &quot;multiattach&quot;: false, &quot;name&quot;: null, &quot;os-vol-host-attr:host&quot;: null,//Current back-end of the volume. Host format is host@backend#pool. &quot;os-vol-mig-status-attr:migstat&quot;: null,//The status of this volume migration (None means that a migration is not currently in progress). &quot;os-vol-mig-status-attr:name_id&quot;: null,//The volume ID that this volume name on the back- end is based on. &quot;os-vol-tenant-attr:tenant_id&quot;: &quot;89afd400-b646-4bbc-b12b-c0a4d63e5bd3&quot;,//The project ID which the volume belongs to. &quot;replication_status&quot;: null, &quot;size&quot;: 10, &quot;snapshot_id&quot;: null, &quot;source_volid&quot;: null, &quot;status&quot;: &quot;creating&quot;, &quot;updated_at&quot;: null, &quot;user_id&quot;: &quot;c853ca26-e8ea-4797-8a52-ee124a013d0e&quot;, &quot;volume_type&quot;: null } } PUT /v3/{project_id}/volumes/{volume_id} Request Body { &quot;volume&quot;: { &quot;name&quot;: &quot;vol-003&quot;, &quot;description&quot;: &quot;This is yet, another volume.&quot;, &quot;metadata&quot;: { &quot;name&quot;: &quot;metadata0&quot; } } } DELETE /v3/{project_id}/volumes/{volume_id} Preconditions: status available, in-use, error, error_restoring, error_extending, error_managing. Async Postconditions: Delete index and volumn on storage node. CRUD of Cinder Volumn metadata POST /v3/{project_id}/volumes/{volume_id}/metadata Request Body (Same with response) { &quot;metadata&quot;: { &quot;name&quot;: &quot;metadata0&quot; } } GET /v3/{project_id}/volumes/{volume_id}/metadata PUT /v3/{project_id}/volumes/{volume_id}/metadata GET /v3/{project_id}/volumes/{volume_id}/metadata/{key} //Show a volume’s metadata for a specific key DELETE /v3/{project_id}/volumes/{volume_id}/metadata/{key} PUT /v3/{project_id}/volumes/{volume_id}/metadata/{key} GET /v3/{project_id}/volumes/summary Response Body //Get volumes summary { &quot;volume-summary&quot;: { &quot;total_size&quot;: 4, &quot;total_count&quot;: 4, &quot;metadata&quot;: { &quot;key1&quot;: [&quot;value1&quot;, &quot;value2&quot;], &quot;key2&quot;: [&quot;value2&quot;] } } } Volume manage extension (manageable_volumes)¶ POST /v3/{project_id}/manageable_volumes Creates a Block Storage volume by using existing storage rather than allocating new storage. Request { &quot;volume&quot;: { &quot;host&quot;: &quot;geraint-VirtualBox&quot;, &quot;ref&quot;: { &quot;source-name&quot;: &quot;existingLV&quot;, &quot;source-id&quot;: &quot;1234&quot; }, &quot;name&quot;: &quot;New Volume&quot;, &quot;availability_zone&quot;: &quot;az2&quot;, &quot;description&quot;: &quot;Volume imported from existingLV&quot;, &quot;volume_type&quot;: null, &quot;bootable&quot;: true, &quot;metadata&quot;: { &quot;key1&quot;: &quot;value1&quot;, &quot;key2&quot;: &quot;value2&quot; } } } GET /v3/{project_id}/manageable_volumes List summary of volumes available to manage Response { &quot;manageable-volumes&quot;: [ { &quot;safe_to_manage&quot;: false, &quot;reference&quot;: { &quot;source-name&quot;: &quot;volume-3a81fdac-e8ae-4e61-b6a2-2e14ff316f19&quot; }, &quot;size&quot;: 1 }, { &quot;safe_to_manage&quot;: true, &quot;reference&quot;: { &quot;source-name&quot;: &quot;lvol0&quot; }, &quot;size&quot;: 1 } ] } GET /v3/{project_id}/manageable_volumes/detail List detail of volumes available to manage Response { &quot;manageable-volumes&quot;: [ { &quot;cinder_id&quot;: &quot;9ba5bb53-4a18-4b38-be06-992999da338d&quot;, &quot;reason_not_safe&quot;: &quot;already managed&quot;, &quot;reference&quot;: { &quot;source-name&quot;: &quot;volume-9ba5bb53-4a18-4b38-be06-992999da338d&quot; }, &quot;safe_to_manage&quot;: false, &quot;size&quot;: 1, &quot;extra_info&quot;: null }, { &quot;cinder_id&quot;: null, &quot;reason_not_safe&quot;: null, &quot;reference&quot;: { &quot;source-name&quot;: &quot;lvol0&quot; }, &quot;safe_to_manage&quot;: true, &quot;size&quot;: 1, &quot;extra_info&quot;: null } ] } Volume transfer Transfers a volume from one user to another user. Services (os-services)¶ Administrator only. Lists all Cinder services, enables or disables a Cinder service, freeze or thaw the specified cinder-volume host, failover a replicating cinder-volume host. Volume actions (volumes, action)¶ POST /v3/{project_id}/volumes/{volume_id}/action { // Extend the size &quot;os-extend&quot;: { &quot;new_size&quot;: 3 } // Reset a volume’s statuses &quot;os-reset_status&quot;: { &quot;status&quot;: &quot;available&quot;, &quot;attach_status&quot;: &quot;detached&quot;, &quot;migration_status&quot;: &quot;migrating&quot; } // Revert volume to snapshot &quot;revert&quot;: { &quot;snapshot_id&quot;: &quot;5aa119a8-d25b-45a7-8d1b-88e127885635&quot; } // Set image metadata for a volume &quot;os-set_image_metadata&quot;: { &quot;metadata&quot;: { &quot;image_id&quot;: &quot;521752a6-acf6-4b2d-bc7a-119f9148cd8c&quot;, &quot;image_name&quot;: &quot;image&quot;, &quot;kernel_id&quot;: &quot;155d900f-4e14-4e4c-a73d-069cbf4541e6&quot;, &quot;ramdisk_id&quot;: &quot;somedisk&quot; } } // Remove image metadata from a volume &quot;os-unset_image_metadata&quot;: { &quot;key&quot;: &quot;ramdisk_id&quot; } // Show image metadata for a volume &quot;os-show_image_metadata&quot;: {} // Attach volume to a server &quot;os-attach&quot;: { &quot;instance_uuid&quot;: &quot;95D9EF50-507D-11E5-B970-0800200C9A66&quot;, //The UUID of the attaching instance. &quot;mountpoint&quot;: &quot;/dev/vdc&quot; //The attaching mount point. } // Detach volume from server &quot;os-detach&quot;: { &quot;attachment_id&quot;: &quot;d8777f54-84cf-4809-a679-468ffed56cf1&quot; } // Unmanage a volume &quot;os-unmanage&quot;: {} // Force detach a volume &quot;os-force_detach&quot;: { &quot;attachment_id&quot;: &quot;d8777f54-84cf-4809-a679-468ffed56cf1&quot;, &quot;connector&quot;: { &quot;initiator&quot;: &quot;iqn.2012-07.org.fake:01&quot; } } //Retype a volume //Migrate a volume //Complete migration of a volume //Force delete a volume //Update a volume’s bootable status //Upload volume to image //Reserve volume //Unmark volume as reserved. //Update volume status to detaching //Roll back volume status to in-use //Terminate volume attachment //Initialize volume attachment //Updates volume read-only access-mode flag ... } Generic volume groups Generic volume groups enable you to create a group of volumes and manage them together. cinder-scheduler 如果说 cinder-api 接收的是关于 “创建” 的请求（e.g. Create Volume），那么该请求就会通过 MQ 转发到 cinder-scheduler service 服务进程，cinder-scheduler 与 nova-scheduler 一般，顾名思义是调度的层面。通过 Filters 选择最 “合适” 的 Storage Provider Node 来对请求资源（e.g. Volume）进行创建。不同的 Filters 具有不同的过滤（调度）算法，所谓的 “合适” 就是达到客户预期的结果，用户还可以自定义 Filter Class 来实现符合自身需求的过滤器，让调度更加灵活。与 nova-scheduler 一般，cinder-scheduler 同样需要维护调度对象（存储节点）“实时” 状态，cinder-volume service 会定期的向 cinder-scheduler service 上报存储节点状态（注：这实际上是通过后端存储设备的驱动程序上报了该设备的状态）。 首先判断存储节点状态，只有状态为 up 的存储节点才会被考虑。 创建 Volume 时，根据 Filter 和 Weight 算法选出最优存储节点。 迁移 Volume 时，根据 Filter 和 Weight 算法来判断目的存储节点是否符合要求。 cinder-volume cinder-volume service 是 Cinder 的核心服务进程，运行该服务进程的节点都可以被称之为存储节点。cinder-volume 通过抽象出统一的 Back-end Storage Driver 层，让不同存储厂商得以通过提供各自的驱动程序来对接自己的后端存储设备，实现即插即用（通过配置文件指定），多个这样的存储节点共同构成了一个庞大而复杂多样的存储资源池系统。 cinder-volumn Driver OpenStack 作为开放的 Infrastracture as a Service 云操作系统，支持业界各种优秀的技术，这些技术可能是开源免费的，也可能是商业收费的。 这种开放的架构使得 OpenStack 保持技术上的先进性，具有很强的竞争力，同时又不会造成厂商锁定（Lock-in）。 以 Cinder 为例，存储节点支持多种 Volume Provider，包括 LVM、NFS、Ceph、GlusterFS 以及 EMC、IBM 等商业存储系统。 cinder-volume 为这些 Volume Provider 抽象了统一的 Driver 接口，Volume Provider 只需要实现这些接口，就可以以 Driver 的形式即插即（volume_driver 配置项）用到 OpenStack 中。下面是 Cinder Driver 的架构示意图： cinder-volumn Plugin Driver 和 Plugin 通常不会分家，Driver 是由各存储厂商提供的，那么 Plugins（插槽）就应该有 Cinder 的提供。 根据 FileSystem Storage 和 Block Storage 两个不同类型的外部存储系统，Cinder Plugins 也提供了 FileSystem based 和 Block based 两种不同类型 Plugin。除此之外，Cinder Plugins 还提供了 iSCSC、FC、NFS 等常用的数据传输协议 Plugin 框架，上传逻辑得以根据实际情况来使用（Attached/Dettached）存储资源。 cinder-backup 提供 Volume 的备份功能，支持将 Volume 备份到对象存储中（e.g. Swift、Ceph、IBM TSM、NFS），也支持从备份 Restore 成为 Volume。 Cinder HA (Cinder 高可用) 无状态服务（cinder-api、cinder-scheduler、cinder-volume）使用多活（无状态服务利于横向扩展，高并发）：Active/Active（A/A） 有状态服务使用主备：Active/Passive（A/P） cinder-api + cinder-scheduler 都部署在 Controller，多个 Controller 同时共享同一个 VIP 实现多活 一个存储设备可以对应多个 cinder-volume，结合 cinder-volume 分布式锁实现多活，分布式锁可以避免不同的 cinder-scheduler 同时调用到同一个 cinder-volume，从而避免多活部署的 cinder-volume 同时操作后端存储设备，简而言之就是避免并发操作带来（cinder-volume 与后端存储设备之间的）数据不一致性。锁是为了通过互斥访问来保证共享资源在并发环境中的数据一致性。分布式锁主要解决的是分布式资源访问冲突的问题，保证数据的一致性（Etcd、Zookeeper）。 Cinder的分布式锁 本地锁 Volume 资源本身也是一种共享资源，也需要处理并发访问冲突的问题，比如：删除一个 Volume 时，另一个线程正在基于该 Volume 创建快照；又或者同时有两个线程都在执行 Volume 挂载操作。cinder-volume 也是使用锁机制来实现 Volume 资源的并发访问的，Volume 的删除、挂载、卸载等操作都会对 Volume 上锁。在 Newton 版本以前，cinder-volume 的锁实现是基于本地文件实现的，使用了 Linux 的 flock 工具进行锁的管理。Cinder 执行加锁操作默认会从配置指定的 lockpath 目录下创建一个命名为 cinder-volume_uuid-{action} 的空文件，并对该文件使用 flock 加锁。flock 只能作用于同一个操作系统的文件锁。即使使用共享存储，另一个操作系统也不能通过 flock 工具判断该空文件是否有锁。 本地锁的局限，只能够保证同一个 cinder-volume 下的共享资源（e.g. Volume）的数据一致性，也就只能使用 A/P 主备的模式进行高可用，不能管理分布式 cinder-volume 下共享资源，导致了 cinder-volume 不支持多实例高可用的问题。所以为了避免 cinder-volume 服务宕机，就需要引入自动恢复机制来进行管理。比如： Pacemaker，Pacemaker 轮询判断 cinder-volume 的存活状态，一旦发现挂了，Pacemaker 会尝试重启服务。如果 Pacemaker 依然重启失败，则尝试在另一台主机启动该服务，实现故障的自动恢复。 Tooz 用户不需要分布式锁时，只需要指定后端为本地文件即可，此时不需要部署任何 DLM，和引入分布式锁之前的方式保持一致，基本不需要执行大的变更。当用户需要 cinder-volume 支持 AA 时，可以选择部署一种 DLM，比如 Zookeeper 服务。Cinder 对 Tooz 又封装了一个单独的 coordination 模块，其源码位于 cinder/coordination.py，当代码需要使用同步锁时，只需要在函数名前面加上 @coordination.synchronized 装饰器即可，方便易用，并且非常统一。 社区为了解决项目中的分布式问题，开发了一个非常灵活的通用框架，项目名为 Tooz，它是一个 Python 库，提供了标准的 coordination API，其主要目标是解决分布式系统的通用问题，比如节点管理、主节点选举以及分布式锁等。简而言之，Tooz 实现了非常易用的分布式锁接口。 Tooz 封装了一套锁管理的库或者框架，只需要简单调用 lock、trylock、unlock 即可完成实现，不用关心底层细节，也不用了解后端到底使用的是 Zookeeper、Redis 还是 Etcd。使用 Tooz 非常方便，只需要三步： 与后端 DLM 建立连接，获取 coordination 实例。 声明锁名称，创建锁实例。 使用锁。 参考链接 [1] OpenStack: Cinder官方文档 [2] 博客园：Cinder 组件解析 [3] ZhiHu: OpenStack-Cinder [4] Github Repo: OpenStack/Cinder [5] 九州云：CINDER 架构分析、高可用部署与核心功能解析 [6] 博客园：Cinder 架构分析、高可用部署与核心功能解析 [7] CSDN：Openstack 实现技术分解 (4) 通用技术 — TaskFlow ","link":"https://blog.shunzi.tech/post/cinder/"},{"title":"RabbitMQ","content":" Introduce the RabbitMQ. Write some example code. Refer the official document. Introduction * RabbitMQ is a message broker（消息中间件）: Brief model (Product-Consume-Model) Produce Consume Start with &quot;Hello,RabbitMQ&quot; 1. Simple Send And Receive with Named Queue 2. Distribute time-consuming tasks among multiple workers Publish/Subscribe Introduction Full Message Model in Rabbit Previous model Problems existed: New Model Example of Fanout Exchange Example of Direct Exchange Example of Topic Exchange Relationship between Different Exchange Header Exchange RPC system with Rabbit MQ Introduction RabbitMQ is a message broker（消息中间件）: it accepts and forwards messages. You can think about it as a post office: when you put the mail that you want posting in a post box, you can be sure that Mr. Postman will eventually deliver the mail to your recipient. In this analogy, RabbitMQ is a post box, a post office and a postman. The major difference between RabbitMQ and the post office is that it doesn't deal with paper, instead it accepts, stores and forwards binary blobs of data ‒ messages. RabbitMQ, and messaging in general, uses some jargon. According to &quot;Hello world in RabbitMQ with Java&quot; Brief model (Product-Consume-Model) Producing means nothing more than sending. A program that sends messages is a producer : Produce A queue is the name for a post box which lives inside RabbitMQ. Although messages flow through RabbitMQ and your applications, they can only be stored inside a queue. A queue is only bound by the host's memory &amp; disk limits, it's essentially a large message buffer. Many producers can send messages that go to one queue, and many consumers can try to receive data from one queue. This is how we represent a queue: // queue_name 口口口口口口口口口口...... // I am sorry about that I am not good at painting :( Consuming has a similar meaning to receiving. A consumer is a program that mostly waits to receive messages: Consume Note that the producer, consumer, and broker do not have to reside on the same host; indeed in most applications they don't. Start with &quot;Hello,RabbitMQ&quot; In this part of the tutorial we'll write two programs in Java; a producer that sends a single message, and a consumer that receives messages and prints them out. We'll gloss over some of the detail in the Java API, concentrating on this very simple thing just to get started. It's a &quot;Hello World&quot; of messaging. 1. Simple Send And Receive with Named Queue SimpleSender.java package tech.shunzi.mq.demo; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import static tech.shunzi.mq.demo.MQConstants.QUEUE_NAME; public class SimpleSender { public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); // connect to a broker with its name or ip address factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); // create a channel, which is where most of the API for getting things done resides. Channel channel = connection.createChannel(); // Declaring a queue is idempotent - it will only be created if it doesn't exist already. // String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments channel.queueDeclare(QUEUE_NAME, false, false, false, null); String message = &quot;Hello World!&quot;; // The message content is a byte array, so you can encode whatever you like there. // String exchange, String routingKey, BasicProperties props, byte[] body channel.basicPublish(&quot;&quot;, QUEUE_NAME, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } } SimpleReceiver.java package tech.shunzi.mq.demo; import com.rabbitmq.client.*; import java.io.IOException; import java.io.UnsupportedEncodingException; import static tech.shunzi.mq.demo.MQConstants.QUEUE_NAME; public class SimpleReceiver { public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(QUEUE_NAME, false, false, false, null); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException, UnsupportedEncodingException { String message = new String(body, &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); } }; channel.basicConsume(QUEUE_NAME, true, consumer); } } Key Config: ConnectionFactory, Host, Connection, Channel, Queue 2. Distribute time-consuming tasks among multiple workers NewTask.java package tech.shunzi.mq.demo.multi.consumer; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; import com.rabbitmq.client.MessageProperties; public class NewTask { private static final String TASK_QUEUE_NAME = &quot;task_queue&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.queueDeclare(TASK_QUEUE_NAME, true, false, false, null); // get the argv from the command line and format e.g.: xxx xxx xxx String message = getMessage(argv); // MessageProperties.PERSISTENT_TEXT_PLAIN can make sure message persistent // Marking messages as persistent doesn't fully guarantee that a message won't be lost. channel.basicPublish(&quot;&quot;, TASK_QUEUE_NAME, MessageProperties.PERSISTENT_TEXT_PLAIN, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } private static String getMessage(String[] strings) { if (strings.length &lt; 1) { return &quot;Hello World!&quot;; } return joinStrings(strings, &quot; &quot;); } private static String joinStrings(String[] strings, String delimiter) { int length = strings.length; if (length == 0) return &quot;&quot;; StringBuilder words = new StringBuilder(strings[0]); for (int i = 1; i &lt; length; i++) { words.append(delimiter).append(strings[i]); } return words.toString(); } } Worker.java package tech.shunzi.mq.demo.multi.consumer; import com.rabbitmq.client.*; import java.io.IOException; public class Worker { private static final String TASK_QUEUE_NAME = &quot;task_queue&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); final Connection connection = factory.newConnection(); final Channel channel = connection.createChannel(); // make the queue durable. true channel.queueDeclare(TASK_QUEUE_NAME, true, false, false, null); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); // producer can only send a message to a consumer. balanced load channel.basicQos(1); final Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); try { // simulate a task which cost some time. doWork(message); } finally { System.out.println(&quot; [x] Done&quot;); // false means need ack to verify. channel.basicAck(envelope.getDeliveryTag(), false); } } }; channel.basicConsume(TASK_QUEUE_NAME, false, consumer); } private static void doWork(String task) { for (char ch : task.toCharArray()) { if (ch == '.') { try { Thread.sleep(1000); } catch (InterruptedException _ignored) { Thread.currentThread().interrupt(); } } } } } Some key functions details: interface Channel.java /** * Declare a queue * @see com.rabbitmq.client.AMQP.Queue.Declare * @see com.rabbitmq.client.AMQP.Queue.DeclareOk * @param queue the name of the queue * @param durable true if we are declaring a durable queue (the queue will survive a server restart) * @param exclusive true if we are declaring an exclusive queue (restricted to this connection) * @param autoDelete true if we are declaring an autodelete queue (server will delete it when no longer in use) * @param arguments other properties (construction arguments) for the queue * @return a declaration-confirm method to indicate the queue was successfully declared * @throws java.io.IOException if an error is encountered */ Queue.DeclareOk queueDeclare(String queue, boolean durable, boolean exclusive, boolean autoDelete, Map&lt;String, Object&gt; arguments) throws IOException; /** * Publish a message. * * Publishing to a non-existent exchange will result in a channel-level * protocol exception, which closes the channel. * * Invocations of &lt;code&gt;Channel#basicPublish&lt;/code&gt; will eventually block if a * &lt;a href=&quot;http://www.rabbitmq.com/alarms.html&quot;&gt;resource-driven alarm&lt;/a&gt; is in effect. * * @see com.rabbitmq.client.AMQP.Basic.Publish * @see &lt;a href=&quot;http://www.rabbitmq.com/alarms.html&quot;&gt;Resource-driven alarms&lt;/a&gt; * @param exchange the exchange to publish the message to * @param routingKey the routing key * @param props other properties for the message - routing headers etc * @param body the message body * @throws java.io.IOException if an error is encountered */ void basicPublish(String exchange, String routingKey, BasicProperties props, byte[] body) throws IOException; /** * Request a specific prefetchCount &quot;quality of service&quot; settings * for this channel. * * @see #basicQos(int, int, boolean) * @param prefetchCount maximum number of messages that the server * will deliver, 0 if unlimited * @throws java.io.IOException if an error is encountered */ void basicQos(int prefetchCount) throws IOException; /** * Acknowledge one or several received * messages. Supply the deliveryTag from the {@link com.rabbitmq.client.AMQP.Basic.GetOk} * or {@link com.rabbitmq.client.AMQP.Basic.Deliver} method * containing the received message being acknowledged. * @see com.rabbitmq.client.AMQP.Basic.Ack * @param deliveryTag the tag from the received {@link com.rabbitmq.client.AMQP.Basic.GetOk} or {@link com.rabbitmq.client.AMQP.Basic.Deliver} * @param multiple true to acknowledge all messages up to and * including the supplied delivery tag; false to acknowledge just * the supplied delivery tag. * @throws java.io.IOException if an error is encountered */ void basicAck(long deliveryTag, boolean multiple) throws IOException; /** * Start a non-nolocal, non-exclusive consumer, with * a server-generated consumerTag. * @param queue the name of the queue * @param autoAck true if the server should consider messages * acknowledged once delivered; false if the server should expect * explicit acknowledgements * @param callback an interface to the consumer object * @return the consumerTag generated by the server * @throws java.io.IOException if an error is encountered * @see com.rabbitmq.client.AMQP.Basic.Consume * @see com.rabbitmq.client.AMQP.Basic.ConsumeOk * @see #basicConsume(String, boolean, String, boolean, boolean, Map, Consumer) */ String basicConsume(String queue, boolean autoAck, Consumer callback) throws IOException; autoAck: true - The msg will be acknowledged once delivered false - The server expects explicit acknowledgements. (It means that manual ack publish is necessary. basicAck()) durable: (Producer &amp; Consumer Both Config this) true - To make sure that RabbitMQ will never lose our queue. And it only makes sense when first create/setup. BasicProperties props : MessageProperties.PERSISTENT_TEXT_PLAIN - Make sure the message persistent. Attention: The persistence guarantees aren't strong, but it's more than enough for our simple task queue. One situation is that message is stored in cache and hasn't be stored on disk. prefetchCount: maximum number of messages that the server will deliver 1 - This tells RabbitMQ not to give more than one message to a worker at a time. Or, in other words, don't dispatch a new message to a worker until it has processed and acknowledged the previous one. Instead, it will dispatch it to the next worker that is not still busy. ... Publish/Subscribe Introduction In this part we'll do something completely different -- we'll deliver a message to multiple consumers. This pattern is known as &quot;publish/subscribe&quot;. To illustrate the pattern, we're going to build a simple logging system. It will consist of two programs -- the first will emit log messages and the second will receive and print them. And there will be many receivers to receive the messages. Full Message Model in Rabbit Previous model A producer is a user application that sends messages. A queue is a buffer that stores messages. A consumer is a user application that receives messages. Problems existed: The core idea in the messaging model in RabbitMQ is that the producer never sends any messages directly to a queue. Actually, quite often the producer doesn't even know if a message will be delivered to any queue at all. There may be a lot of queues. And one producer may send msg to many queues. Duplicate code is bad practice. New Model Exchange : An exchange is a very simple thing. On one side it receives messages from producers and the other side it pushes them to queues. The exchange must know exactly what to do with a message it receives. Direct : Same routing key.(Same string) Topic : Same string pattern of routing key.(eg. *.*.#, lazy.orange.rabbit) Headers : Ignore the routing key and match header attributes. Fanout : Ignore routing key.(Broadcast to all queues) Example of Fanout Exchange Publisher package tech.shunzi.mq.demo.publish; import com.rabbitmq.client.BuiltinExchangeType; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; public class EmitLog { private static final String EXCHANGE_NAME = &quot;logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // Actively declare a non-autodelete, non-durable exchange with no extra arguments // Declare an FANOUT exchange with name &quot;logs&quot; channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT); String message = getMessage(argv); // Publishing to a non-existent exchange named &quot;log&quot; channel.basicPublish(EXCHANGE_NAME, &quot;&quot;, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } private static String getMessage(String[] strings) { if (strings.length &lt; 1) { return &quot;info: Hello World!&quot;; } return joinStrings(strings, &quot; &quot;); } private static String joinStrings(String[] strings, String delimiter) { int length = strings.length; if (length == 0) { return &quot;&quot;; } StringBuilder words = new StringBuilder(strings[0]); for (int i = 1; i &lt; length; i++) { words.append(delimiter).append(strings[i]); } return words.toString(); } } Consumer package tech.shunzi.mq.demo.publish; import com.rabbitmq.client.*; import java.io.IOException; public class ReceiveLogs { private static final String EXCHANGE_NAME = &quot;logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // Actively declare a non-auto-delete, non-durable exchange with no extra arguments // Declare an FANOUT exchange with name &quot;logs&quot; channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.FANOUT); // Create a non-durable, exclusive, auto-delete queue with a generated name String queueName = channel.queueDeclare().getQueue(); // Bind the queues to the exchange, maintain the binding relationship between queues and exchange. // The FANOUT exchange will ignore the routing-key value. channel.queueBind(queueName, EXCHANGE_NAME, &quot;&quot;); System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + message + &quot;'&quot;); } }; channel.basicConsume(queueName, true, consumer); } } Example of Direct Exchange We will use a direct exchange instead. The routing algorithm behind a direct exchange is simple - a message goes to the queues whose binding key exactly matches the routing key of the message. If the routing keys are the same as other queues, it will behave like FANOUT exchange. Producer package tech.shunzi.mq.demo.exchange.direct; import com.rabbitmq.client.BuiltinExchangeType; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; public class EmitLogDirect { private static final String EXCHANGE_NAME = &quot;direct_logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // Actively declare a non-auto-delete, non-durable exchange with no extra arguments // Declare an Direct exchange with name &quot;direct_logs&quot; channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT); // Declare routing key name String severity = getSeverity(argv); String message = getMessage(argv); channel.basicPublish(EXCHANGE_NAME, severity, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + severity + &quot;':'&quot; + message + &quot;'&quot;); channel.close(); connection.close(); } private static String getSeverity(String[] strings) { if (strings.length &lt; 1) { return &quot;info&quot;; } return strings[0]; } private static String getMessage(String[] strings) { if (strings.length &lt; 2) { return &quot;Hello World!&quot;; } return joinStrings(strings, &quot; &quot;, 1); } private static String joinStrings(String[] strings, String delimiter, int startIndex) { int length = strings.length; if (length == 0) { return &quot;&quot;; } if (length &lt; startIndex) { return &quot;&quot;; } StringBuilder words = new StringBuilder(strings[startIndex]); for (int i = startIndex + 1; i &lt; length; i++) { words.append(delimiter).append(strings[i]); } return words.toString(); } } Consumer package tech.shunzi.mq.demo.exchange.direct; import com.rabbitmq.client.*; import java.io.IOException; public class ReceiveLogsDirect { private static final String EXCHANGE_NAME = &quot;direct_logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); // Actively declare a non-auto-delete, non-durable exchange with no extra arguments // Declare an Direct exchange with name &quot;direct_logs&quot; channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.DIRECT); // Create a non-durable, exclusive, auto-delete queue with a generated name String queueName = channel.queueDeclare().getQueue(); if (argv.length &lt; 1){ System.err.println(&quot;Usage: ReceiveLogsDirect [info] [warning] [error]&quot;); System.exit(1); } for(String severity : argv){ // Bind the queues to the exchange // Maintain the binding relationship between queues and exchange with given routing key name. channel.queueBind(queueName, EXCHANGE_NAME, severity); } System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + envelope.getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); } }; channel.basicConsume(queueName, true, consumer); } } Example of Topic Exchange * (star) can substitute for exactly one word. # (hash) can substitute for zero or more words. Producer package tech.shunzi.mq.demo.exchange.topic; import com.rabbitmq.client.BuiltinExchangeType; import com.rabbitmq.client.Channel; import com.rabbitmq.client.Connection; import com.rabbitmq.client.ConnectionFactory; public class EmitLogTopic { private static final String EXCHANGE_NAME = &quot;topic_logs&quot;; public static void main(String[] argv) { Connection connection = null; Channel channel = null; try { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); connection = factory.newConnection(); channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); String routingKey = getRouting(argv); String message = getMessage(argv); channel.basicPublish(EXCHANGE_NAME, routingKey, null, message.getBytes(&quot;UTF-8&quot;)); System.out.println(&quot; [x] Sent '&quot; + routingKey + &quot;':'&quot; + message + &quot;'&quot;); } catch (Exception e) { e.printStackTrace(); } finally { if (connection != null) { try { connection.close(); } catch (Exception ignore) { } } } } private static String getRouting(String[] strings) { if (strings.length &lt; 1) { return &quot;anonymous.info&quot;; } return strings[0]; } private static String getMessage(String[] strings) { if (strings.length &lt; 2) { return &quot;Hello World!&quot;; } return joinStrings(strings, &quot; &quot;, 1); } private static String joinStrings(String[] strings, String delimiter, int startIndex) { int length = strings.length; if (length == 0) { return &quot;&quot;; } if (length &lt; startIndex) { return &quot;&quot;; } StringBuilder words = new StringBuilder(strings[startIndex]); for (int i = startIndex + 1; i &lt; length; i++) { words.append(delimiter).append(strings[i]); } return words.toString(); } } Consumer package tech.shunzi.mq.demo.exchange.topic; import com.rabbitmq.client.*; import java.io.IOException; public class ReceiveLogsTopic { private static final String EXCHANGE_NAME = &quot;topic_logs&quot;; public static void main(String[] argv) throws Exception { ConnectionFactory factory = new ConnectionFactory(); factory.setHost(&quot;localhost&quot;); Connection connection = factory.newConnection(); Channel channel = connection.createChannel(); channel.exchangeDeclare(EXCHANGE_NAME, BuiltinExchangeType.TOPIC); String queueName = channel.queueDeclare().getQueue(); if (argv.length &lt; 1) { System.err.println(&quot;Usage: ReceiveLogsTopic [binding_key]...&quot;); System.exit(1); } for (String bindingKey : argv) { channel.queueBind(queueName, EXCHANGE_NAME, bindingKey); } System.out.println(&quot; [*] Waiting for messages. To exit press CTRL+C&quot;); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, &quot;UTF-8&quot;); System.out.println(&quot; [x] Received '&quot; + envelope.getRoutingKey() + &quot;':'&quot; + message + &quot;'&quot;); } }; channel.basicConsume(queueName, true, consumer); } } Relationship between Different Exchange When a queue is bound with &quot;#&quot; (hash) binding key - it will receive all the messages, regardless of the routing key - like in fanout exchange. When special characters &quot;*&quot; (star) and &quot;#&quot; (hash) aren't used in bindings, the topic exchange will behave just like a direct one. Header Exchange Refer link : [CSDN Blog: Topic Exchange of Rabbit MQ] RPC system with Rabbit MQ Refer link: [Jianshu: RabbitMQ Tutorial Six] ","link":"https://blog.shunzi.tech/post/rabbitmq/"},{"title":"教你写测试","content":" 测试开发 结合实际例子介绍相关测试框架的使用 Findbugs, Fortify，SonarQube，PMD也会做简单介绍 单元测试 Unit Test (UT) * JUnit4/5 Mockito How service/dao unit test SpringTest mockMvc unit test PowerMock How to use: Caution: 集成测试 Integration Test (IT) * Spring Integration Test Prepare: 代码实例： 系统测试 System Test (ST) JMeter Download and Launch How to use JMeter Example Code Head First Step 1. Create Thread Group Step 2. Define some common variables Step 3. New Http Request Step 4. New Asserters.(JSON Assertion, Response Assertion etc.) Step 5. Processor (Pre/Post) Step 6. Result Tree 自动化测试（Auto UI Test） Katalon Studio Groovy (Based on Java) 测试的重要性不做过多介绍。简单介绍一种 TDD(Test Driven Development)的模式。 TDD 旨在通过先编写相关的测试类和测试代码来驱动实际业务代码的开发。对于测试代码，业务代码更多的像是在承担一个黑盒的角色，而测试代码只需要关注输入输出，从逻辑意义上保证输入输出和我们预期的结果相对应则是业务代码需要去保证实现的。 TDD具体实现此处仅提供一种思路。测试类编写完成之后，使其一直运行在本地搭建的服务器上，编写相关业务代码完成后，通过在运行测试类的服务器上指定输入参数，通过测试代码的断言实时反应业务代码的正确性。 单元测试 Unit Test (UT) 应用场景：从代码层面去测试相关代码的正确性，是否符合预期的结果。此处不涉及和其他软件或者应用交互的过程，仅仅针对代码的逻辑处理。常常用于寻找程序中可能出现的异常以及代码逻辑的正确性。 工具：JUnit4/5, Mockito, SpringTest, PowerMock JUnit4/5 JUnit5 User Guide JUnit 主要是利用 Java 注解构建了一套单元测试的相关体系，同时提供了很多工具方便单元测试的编写。具体注解的使用，参照官方文档。 Mockito 官方主页 How service/dao unit test Before: You must init mockito in @Before of JUnit. @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); } Annotation: @Mock @InjectMocks @Mock : This annotation is used to mock an object. It seems like new Object() to give memory for this pointer. @InjectMocks : This annotation is used to inject other mocks objects to the annotation decorated obejct. Method: when() verify() any() when() : This method is used to listen the emthod if be invoked.And when this method is invoked, you can give return value for this method. So you do not need to care the specific implemention of this method. verify(): It is used to verify if this method be invoked and you also can verify the invoke times as params of this method. any(): It is used for mock objects. You can only give the type of the param without specific value. It is common used in when method to instead of specific params. Example: package tech.shunzi.testdev.service; import org.junit.Before; import org.junit.Test; import org.mockito.InjectMocks; import org.mockito.Mock; import org.mockito.MockitoAnnotations; import tech.shunzi.testdev.model.User; import tech.shunzi.testdev.model.dto.UserDto; import tech.shunzi.testdev.repo.UserRepository; import tech.shunzi.testdev.service.impl.UserServiceImpl; import java.util.ArrayList; import java.util.List; import static org.junit.Assert.assertEquals; import static org.mockito.Mockito.verify; import static org.mockito.Mockito.when; public class UserServiceImplTest { @Mock private UserRepository userRepository; @InjectMocks private UserServiceImpl userService; @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); } @Test public void testGetAllUsers() { // Prepare data List&lt;User&gt; userList = new ArrayList&lt;&gt;(); User user = new User(); user.setDesc(&quot;DESC&quot;); user.setId(1111); user.setName(&quot;Shunzi&quot;); userList.add(user); List&lt;UserDto&gt; dtos = new ArrayList&lt;&gt;(); UserDto dto = new UserDto(); dto.setGroupNo(1); dto.setName(user.getName()); dto.setId(user.getId()); dto.setIntroduction(&quot;Hello, I am Shunzi. And my id is 1111. DESC&quot;); dtos.add(dto); // When when(userRepository.findAll()).thenReturn(userList); // Act List&lt;UserDto&gt; users = userService.findAllUsers(); // Verify verify(userRepository).findAll(); // Assert // override method equals() in UserDto class assertEquals(dtos, users); } @Test public void testGetUserById() { // prepare data int id = 1; User user = new User(); user.setId(id); user.setName(&quot;shunzi&quot;); user.setDesc(&quot;desc&quot;); // when // anyInt() will match all params whose type is int when(userRepository.findById(anyInt())).thenReturn(user); // Act UserDto userDto = userService.findSingleUser(id); // verify verify(userRepository).findById(anyInt()); // assert assertEquals(id, userDto.getId()); } } SpringTest mockMvc unit test Before: You must init the MockMvc and bind it to controller class. private HelloController helloController; private MockMvc mockMvc; @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); //use controller to build mock mvc. mockMvc = MockMvcBuilders.standaloneSetup(helloController).build(); } Class : MockMvc Method : perform(),get() post() put() delete(), contentType(), param(), andExpect(),status(),isOk(),andReturn(),getResponse(), getContentAsString() There are some use cases of methods. // GET String responseStr = mockMvc.perform(get(requestUrl).contentType(MediaType.APPLICATION_JSON)) .andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); //POST form String responseData = mockMvc.perform(post(requestURI).param(&quot;tenantId&quot;, &quot;dev&quot;)) .andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); //PUT String responseData = mockMvc.perform(put(requestURI).param(&quot;tenantId&quot;, &quot;dev&quot;)). andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); //DELETE String data = mockMvc .perform(delete(requestURI).contentType(MediaType.APPLICATION_JSON).content(&quot;the params with json format&quot;) .andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); Example: package tech.shunzi.testdev.controller; import com.fasterxml.jackson.databind.ObjectMapper; import org.junit.Before; import org.junit.Test; import org.mockito.InjectMocks; import org.mockito.Mock; import org.mockito.MockitoAnnotations; import org.springframework.http.MediaType; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.setup.MockMvcBuilders; import tech.shunzi.testdev.model.dto.UserDto; import tech.shunzi.testdev.service.impl.UserServiceImpl; import java.util.ArrayList; import java.util.List; import static org.junit.Assert.assertEquals; import static org.mockito.Mockito.verify; import static org.mockito.Mockito.when; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; public class UserControllerTest { private MockMvc mockMvc; @Mock private UserServiceImpl userService; @InjectMocks private UserController userController; @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); mockMvc = MockMvcBuilders.standaloneSetup(userController).build(); } @Test public void testFindAll() throws Exception { // Prepare data String requestUrl = &quot;/users&quot;; List&lt;UserDto&gt; userDtoList = new ArrayList&lt;&gt;(); UserDto userDto = new UserDto(); userDto.setName(&quot;shunzi&quot;); userDtoList.add(userDto); ObjectMapper mapper = new ObjectMapper(); String expectedValue = mapper.writeValueAsString(userDtoList); // When when(userService.findAllUsers()).thenReturn(userDtoList); // Act String responseStr = mockMvc.perform(get(requestUrl).accept(MediaType.APPLICATION_JSON_UTF8)).andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); // Verify verify(userService).findAllUsers(); // Assert assertEquals(expectedValue, responseStr); } } PowerMock We usually use PowerrMock to mock static method.public static returnValue methodName(T param) How to use: Prepare: Use annotation @PrepareForTest(IncludeStaticMethodClass.class) to annotate unit test class. Use annotation @RunWith(PowerMockRunner.class) to annotate unit test class. Use: method: PowerMockito.mockStatic() // mock static class PowerMocckito.when() // mock static method invoke Example: @PrepareForTest(ObjectFieldEmptyUtil.class) @RunWith(PowerMockRunner.class) public class UserServiceImplTest { @Mock private UserRepository userRepository; @InjectMocks private UserServiceImpl userService; @Before public void setUp() throws Exception { MockitoAnnotations.initMocks(this); } @Test(expected = RuntimeException.class) public void testSaveUser() { // Prepare data UserDto user = new UserDto(); user.setName(&quot;shunzi&quot;); user.setIntroduction(&quot;intro&quot;); User model = new User(); model.setDesc(&quot;intro&quot;); model.setName(&quot;shunzi&quot;); model.setId(1); List&lt;String&gt; stringList = new ArrayList&lt;&gt;(); stringList.add(&quot;name&quot;); PowerMockito.mockStatic(ObjectFieldEmptyUtil.class); PowerMockito.when(ObjectFieldEmptyUtil.findEmptyFields(anyObject(), anyList())).thenReturn(stringList); // Act UserDto userDto = userService.saveUser(user); // Assert assertEquals(&quot;shunzi&quot;, userDto.getName()); } } Caution: Please refer PowerMock Github Repo version issues. It must match Mockito version 集成测试 Integration Test (IT) 应用场景：比起单元测试，集成测试加载了相关的环境变量和上下文，能够对整个Spring上下文里的代码进行测试。而在单元测试中是通过@Mock的方式进行注入的，在集成测试的场景下可以直接 @Autowire 相关对象，同时能够和DB进行交互，也能对相关接口进行实际场景的测试。 工具: JUnit4/5, Spring Test, SpringBoot Test. Spring Integration Test 官方文档 Prepare: 添加 application-test.xml 配置文件，为专门的测试系统进行配置。 便于指定 ActiveProfiles(&quot;test&quot;) 代码实例： BaseIntegrationTest package tech.shunzi.testdev.integration.test; import org.junit.Before; import org.junit.Rule; import org.junit.contrib.java.lang.system.EnvironmentVariables; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.boot.test.autoconfigure.web.servlet.AutoConfigureMockMvc; import org.springframework.test.context.ActiveProfiles; import org.springframework.test.context.junit4.SpringRunner; import org.springframework.test.web.servlet.MockMvc; @RunWith(SpringRunner.class) @SpringBootTest(webEnvironment = SpringBootTest.WebEnvironment.MOCK) @AutoConfigureMockMvc @ActiveProfiles(&quot;test&quot;) public abstract class BaseIntegrationTest { @Autowired protected MockMvc mockMvc; @Rule public final EnvironmentVariables environmentVariables = new EnvironmentVariables(); @Before public void setUpEnv() { environmentVariables.set(&quot;key&quot;,&quot;value&quot;); } } 其余具体的 Integration Test Class均可继承 BaseIntegrationTest package tech.shunzi.testdev.integration.test; import org.junit.Test; import org.springframework.http.MediaType; import static org.springframework.test.web.servlet.request.MockMvcRequestBuilders.get; import static org.springframework.test.web.servlet.result.MockMvcResultMatchers.status; public class UserControllerIntTest extends BaseIntegrationTest { @Test public void testFindAll() throws Exception { // Prepare data String requestUrl = &quot;/users&quot;; // Act String responseStr = mockMvc.perform(get(requestUrl).accept(MediaType.APPLICATION_JSON_UTF8)).andExpect(status().isOk()).andReturn().getResponse().getContentAsString(); // Advise to use assert, sout in here is just to show query result from DB System.out.println(responseStr); } } 针对 Service 和 Dao 层可以不加载相关 Web 环境 package tech.shunzi.testdev.integration.test; import org.junit.Test; import org.junit.runner.RunWith; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.test.context.ActiveProfiles; import org.springframework.test.context.junit4.SpringRunner; import tech.shunzi.testdev.service.UserService; @RunWith(SpringRunner.class) @SpringBootTest @ActiveProfiles(&quot;test&quot;) public class UserServiceTest { @Autowired private UserService userService; @Test public void testFindAll() { System.out.println(userService.findAllUsers()); } } 系统测试 System Test (ST) 应用场景：系统测试相比于其他之前的测试，是从系统层面出发，对整个功能或者整套流程进行测试。是一种黑盒测试。也常常用来测试相关功能模块的性能。 工具： JMeter JMeter 官方主页 Download and Launch download JMeter: https://jmeter.apache.org/download_jmeter.cgi and select &quot;apache-jmeter-4.0.zip&quot; unzip the jmeter to your own folder go to your jmeter folder: [your folder]\\apache-jmeter-4.0\\bin and run jmeter.bat or run ApacheJMeter.jar How to use JMeter Short video Example Code Github JMX Head First Aim: Test Create User function Procedure: Query All User. Verify init data Create User And Verify Response Dara Query All User. Verify new added data Step 1. Create Thread Group Step 2. Define some common variables You can define some common data. Such as host, port and even params etc. Step 3. New Http Request Create Http Request Example Http Request Main Param You can control the header attribute (key-value) JSON Extractor: Extract json response data to variables Step 4. New Asserters.(JSON Assertion, Response Assertion etc.) Assertion should be combined with Assertion Results. JSON Assertion Response Assertion Step 5. Processor (Pre/Post) BeanShell Post Processor Step 6. Result Tree Result tree shows all result of test http request. 自动化测试（Auto UI Test） 应用场景：为了减小人工测试过程中重复的工作量，通过编写相关脚本语言，实现模拟人为操作，进而对UI上的元素进行点击等操作，测试相关功能。除此以外，也可将部分固定流程的操作进行抽象，并绑定相关定时任务，实现对应的需求。 工具： Katalon Studio(Groovy) Katalon Studio 官方主页 Groovy (Based on Java) 官方主页 ","link":"https://blog.shunzi.tech/post/how-to-develop-test/"},{"title":"Spring Event","content":" Introduce the Event in Spring. Explore the core concept in Spring Event. At the same time, introduce the design pattern Observer Spring Event Observer Design Pattern Scenario Design Model Abstract Source code Company.java#notifyObserver() Employee.java#takeAction(String msg) Differences with Publish-Subscribe Spring Events Event Publisher 1. @DomainEvents - Spring Data JPA 2. Mannual Publish Event Listener 1. @EventListener 2. @TransactionEventListener Explore How to event process method in listener class Where to call method multicastEvent() Reference : Spring Event Observer Design Pattern Scenario Elvis, James and Ervin are looking for a job. And they all send their resume to SAP. The SAP will make an announcement, arrange interviews and send notification to them. So three guys will take the interviews. Design Model Abstract Source code Company.java#notifyObserver() @Override public void notifyObserver() { employees.forEach(employee -&gt; employee.takeAction(announcement)); } public void announcenMsg(String msg) { this.setAnnouncement(msg); notifyObserver(); } Employee.java#takeAction(String msg) @Override public void takeAction(String msg) { this.msg = msg; interview(); } public void interview() { System.out.println(name + &quot; received msg : &quot; + msg + &quot; and will go to interview&quot;); } Differences with Publish-Subscribe Spring Events sequenceDiagram Listener-&gt;&gt;Spring Context: Listen Event Observer-&gt;&gt;Publisher: Action Trigger Publisher-&gt;&gt;Spring Context: Publish Event Spring Context-&gt;&gt;Listener: Event Listener-&gt;&gt;Event Process: Event Event Publisher 1. @DomainEvents - Spring Data JPA Annotate method in Domain Model class. (DDD) Publish single or multi events with return value of the method Method invoking time: Spring Data repository's save(…) methods is called. @AfterDomainEventPublication is used to potentially clean the list of events to be published (among other uses) when @DomainEvents exists. @DomainEvents AccountSaveEvent accountSaveEvent() { AccountSaveEvent accountSaveEvent = new AccountSaveEvent(); accountSaveEvent.setAccount(this); accountSaveEvent.setEventType(&quot;AccountSaveEventByJPA&quot;); return accountSaveEvent; } @AfterDomainEventPublication void callbackMethod() { System.out.println(&quot;DATA SAVED!\\n&quot;+&quot;WELL DONE&quot;); } 2. Mannual Publish ApplicationEventPublisher ApplicationContext Extends interface ApplicationEventPublisher @Component public class AccountEventPublisher { @Autowired private ApplicationContext applicationContext; @Autowired private ApplicationEventPublisher publisher; public void publishEventByContext(Account account) { AccountSaveEvent event = new AccountSaveEvent(); event.setAccount(account); event.setEventType(&quot;AccountSaveEventByContext&quot;); System.out.println(&quot;====================================&quot;); System.out.println(&quot;Start to publish AccountSaveEvent by context&quot;); applicationContext.publishEvent(event); System.out.println(&quot;End&quot;); System.out.println(&quot;====================================&quot;); } public void publishEventByPublisher(Account account) { AccountSaveEvent event = new AccountSaveEvent(); event.setAccount(account); event.setEventType(&quot;AccountSaveEventByPublisher&quot;); System.out.println(&quot;====================================&quot;); System.out.println(&quot;Start to publish AccountSaveEvent by publisher&quot;); publisher.publishEvent(event); System.out.println(&quot;End&quot;); System.out.println(&quot;====================================&quot;); } } Publisher can publish any Object as Event, because Spring will help to use PayloadApplicationEvent to package the object. Event Listener 1. @EventListener Annotate method. Support SpEL for event condition match. Can listen multi event class. @EventListener(condition = &quot;#accountSaveApplicationEvent.valid&quot;) public void handleEvent(AccountSaveApplicationEvent accountSaveApplicationEvent) { System.out.println(&quot;======================================&quot;); System.out.println(&quot;Listener listened AccountSaveApplicationEvent&quot;); System.out.println(accountSaveApplicationEvent.getEventType()); System.out.println(&quot;======================================&quot;); } 2. @TransactionEventListener Base on @EventListener. phase : Bind the handling of an event to Transaction lifecycle. fallbackExecution: Config Transaction if mandatory for this event trigger. @Async @TransactionalEventListener(phase = TransactionPhase.AFTER_COMMIT, fallbackExecution = true) public void handleEvent(AccountSaveEvent accountSaveEvent) { System.out.println(&quot;======================================&quot;); System.out.println(&quot;Listener listened AccountSaveEvent&quot;); System.out.println(accountSaveEvent.getEventType()); System.out.println(&quot;======================================&quot;); } Explore How to event process method in listener class class SimpleApplicationEventMulticaster @Override public void multicastEvent(ApplicationEvent event) { multicastEvent(event, resolveDefaultEventType(event)); } @Override public void multicastEvent(final ApplicationEvent event, ResolvableType eventType) { ResolvableType type = (eventType != null ? eventType : resolveDefaultEventType(event)); for (final ApplicationListener&lt;?&gt; listener : getApplicationListeners(event, type)) { Executor executor = getTaskExecutor(); if (executor != null) { // new Runnable(){...} just define Runnable variable executor.execute(new Runnable() { @Override public void run() { invokeListener(listener, event); } }); } else { invokeListener(listener, event); } } } As for executor in SimpleApplicationEventMulticaster, it is global executor for all Spring Event Listeners. So if we config an async executor and inject it to multicaster, all listeners will process event async. @Configuration public class AsynchronousSpringEventsConfig implements AsyncConfigurer { @Bean(name = &quot;applicationEventMulticaster&quot;) public ApplicationEventMulticaster simpleApplicationEventMulticaster() { SimpleApplicationEventMulticaster eventMulticaster = new SimpleApplicationEventMulticaster(); // Inject the ThreadPoolTaskExecutor eventMulticaster.setTaskExecutor(getAsyncExecutor()); return eventMulticaster; } @Override public Executor getAsyncExecutor() { ThreadPoolTaskExecutor taskExecutor = new ThreadPoolTaskExecutor(); taskExecutor.setCorePoolSize(5); taskExecutor.setMaxPoolSize(50); taskExecutor.setQueueCapacity(25); taskExecutor.initialize(); return taskExecutor; } } But if we want only several listeners to process event async, we can use @Async annotation and add some thread pool config. And @Async uses CglibAopDynamicProxy and AsyncExecutionInterceptor to find suitable executors to execute the annotated method. public class AsyncExecutionInterceptor extends ... { @Nullable public Object invoke(MethodInvocation invocation) throws Throwable { Class&lt;?&gt; targetClass = invocation.getThis() != null ? AopUtils.getTargetClass(invocation.getThis()) : null; Method specificMethod = ClassUtils.getMostSpecificMethod(invocation.getMethod(), targetClass); Method userDeclaredMethod = BridgeMethodResolver.findBridgedMethod(specificMethod); // Find executors configured in configuration class. AsyncTaskExecutor executor = this.determineAsyncExecutor(userDeclaredMethod); if (executor == null) { throw new IllegalStateException(&quot;No executor specified and no default executor set on AsyncExecutionInterceptor either&quot;); } else { Callable&lt;Object&gt; task = () -&gt; { try { Object result = invocation.proceed(); if (result instanceof Future) { return ((Future)result).get(); } } catch (ExecutionException var4) { this.handleError(var4.getCause(), userDeclaredMethod, invocation.getArguments()); } catch (Throwable var5) { this.handleError(var5, userDeclaredMethod, invocation.getArguments()); } return null; }; return this.doSubmit(task, executor, invocation.getMethod().getReturnType()); } } } How does ThreadPoolTaskExecutor execute method async, we can refer the source code in ThreadPoolExecutor.class /** * Executes the given task sometime in the future. The task * may execute in a new thread or in an existing pooled thread. * * If the task cannot be submitted for execution, either because this * executor has been shutdown or because its capacity has been reached, * the task is handled by the current {@code RejectedExecutionHandler}. * * @param command the task to execute * @throws RejectedExecutionException at discretion of * {@code RejectedExecutionHandler}, if the task * cannot be accepted for execution * @throws NullPointerException if {@code command} is null */ public void execute(Runnable command) { if (command == null) throw new NullPointerException(); /* * Proceed in 3 steps: * * 1. If fewer than corePoolSize threads are running, try to * start a new thread with the given command as its first * task. The call to addWorker atomically checks runState and * workerCount, and so prevents false alarms that would add * threads when it shouldn't, by returning false. * * 2. If a task can be successfully queued, then we still need * to double-check whether we should have added a thread * (because existing ones died since last checking) or that * the pool shut down since entry into this method. So we * recheck state and if necessary roll back the enqueuing if * stopped, or start a new thread if there are none. * * 3. If we cannot queue task, then we try to add a new * thread. If it fails, we know we are shut down or saturated * and so reject the task. */ int c = ctl.get(); if (workerCountOf(c) &lt; corePoolSize) { if (addWorker(command, true)) return; c = ctl.get(); } if (isRunning(c) &amp;&amp; workQueue.offer(command)) { int recheck = ctl.get(); if (! isRunning(recheck) &amp;&amp; remove(command)) reject(command); else if (workerCountOf(recheck) == 0) addWorker(null, false); } else if (!addWorker(command, false)) reject(command); } Where to call method multicastEvent() abstract class AbstractApplicationContext protected void publishEvent(Object event, ResolvableType eventType) { Assert.notNull(event, &quot;Event must not be null&quot;); if (logger.isTraceEnabled()) { logger.trace(&quot;Publishing event in &quot; + getDisplayName() + &quot;: &quot; + event); } // Decorate event as an ApplicationEvent if necessary ApplicationEvent applicationEvent; if (event instanceof ApplicationEvent) { applicationEvent = (ApplicationEvent) event; } else { applicationEvent = new PayloadApplicationEvent&lt;Object&gt;(this, event); if (eventType == null) { eventType = ((PayloadApplicationEvent) applicationEvent).getResolvableType(); } } // Multicast right now if possible - or lazily once the multicaster is initialized if (this.earlyApplicationEvents != null) { this.earlyApplicationEvents.add(applicationEvent); } else { getApplicationEventMulticaster().multicastEvent(applicationEvent, eventType); } // Publish event via parent context as well... if (this.parent != null) { if (this.parent instanceof AbstractApplicationContext) { ((AbstractApplicationContext) this.parent).publishEvent(event, eventType); } else { this.parent.publishEvent(event); } } } Reference : [1] Baeldung: How to use events in Spring [2] Spring Blog: Better application events in Spring Framework 4.2 [3] Zoltan Altfatter: Publishing domain events from aggregate roots [4] Spring IO: Spring Data JPA - Reference Documentation [5] Pursue: Simple Analysis Domain Driven Design [6] Spring IO: Spring 5.1.1 RELEASE Reference Events [7] luohanguo: The Obeserver Design Pattern [8] miaoyu: Differences between Observer and Publish-Subscribe Pattern [9] Github Source Code Example of Spring Events by Elvis [10] Github Source Code Example of Observer Design Pattern by Elvis [11] CSDN Blog: Spring Event System ","link":"https://blog.shunzi.tech/post/spring-event/"},{"title":"设计模式之工厂模式","content":" 代码不完全参照原书 , 借鉴书中相关例子和部分概念 顺序部分参考原书，部分引用原书 UML 类图，会结合实际应用描述优缺点 工厂模式（简单工厂模式，工厂方法模式，抽象工厂模式） 工厂模式 概念理解 工厂：主要负责对象的创建。其中涉及到的对象一般都继承自同一接口或同一父类，具有部分相似的性质。 实例讲解 场景： 目标：选择通讯方式实现发送消息的操作 分析：发送消息的方式在我们日常生活中有很多种，比较常见的方式有 短信（Message,SMS） 邮件（EMail） 微信等网络即时通讯工具 （Wechat） 常规实现： 思路：通过用户选择的操作进行相关类型的传递，利用 If-Else 或者 Swicth-Case 语句来进行判断，针对每一种方式进行具体实现。 代码实现： import java.util.Scanner; public class SingleClassImpl { public static void main(String[] args) { Scanner in = new Scanner(System.in); System.out.println(&quot;Please enter receiver name:&quot;); String receiverName = in.next(); System.out.println(&quot;Please choose one type to send msg:[Enter numbers]&quot;); System.out.println(&quot;1. Send by message app on your phone.&quot;); System.out.println(&quot;2. Send by email&quot;); System.out.println(&quot;3. Send by WeChat&quot;); System.out.println(); int typeNo = 0; try { typeNo = in.nextInt(); } catch (Exception e) { // handle exception System.out.println(&quot;Invalid input!&quot;); } switch (typeNo) { case 1: System.out.println(String.format(&quot;Send to %s by message&quot;, receiverName)); break; case 2: System.out.println(String.format(&quot;Send to %s by email&quot;, receiverName)); break; case 3: System.out.println(String.format(&quot;Send to %s by WeChat&quot;, receiverName)); break; default: System.out.println(&quot;Invalid Input!&quot;); break; } } } 存在的问题： 没有使用面向对象的特性。 如果我们需要添加新的发送消息的方式，则需要修改该类大量的业务逻辑，最主要的问题则是 和用户交互的逻辑同业务代码耦合度较高。修改代码增加了风险。 简单工厂模式： 思路： 充分利用面向对象的特性，将所有发送消息的方式进行抽象，建立发送消息的接口类，定义共同的 sendMsg() 方法。 新建一个Factory类来管理各种信息发送方式。 代码实现（一）：Common Interface 和具体的 Sender 类 // common interface public interface CommonInterfaceSender { void sendMsg(String receiver); } // SmsSender class(Message app in phone) public class SmsSender implements CommonInterfaceSender { @Override public void sendMsg(String receiver) { System.out.println(String.format(&quot;Send to %s by message&quot;, receiver)); } } // MailSender class public class MailSender implements CommonInterfaceSender { @Override public void sendMsg(String receiver) { System.out.println(String.format(&quot;Send to %s by email&quot;, receiver)); } } // WeChatSender class public class WechatSender implements CommonInterfaceSender { @Override public void sendMsg(String receiver) { System.out.println(String.format(&quot;Send to %s by WeChat&quot;, receiver)); } } 代码实现（二）：Factory Class 和 Main Class // Create different sender class instance according to param public class OrdinaryFactory { public CommonInterfaceSender sendMsg(int type) { if (1 == type) { return new SmsSender(); } else if (2 == type) { return new MailSender(); } else if (3 == type) { return new WechatSender(); } else { System.out.println(&quot;Please check the input!&quot;); return null; } } } public class Main { public static void main(String[] args) { OrdinaryFactory ordinaryFactory = new OrdinaryFactory(); Scanner in = new Scanner(System.in); System.out.println(&quot;Please enter receiver name:&quot;); String receiverName = in.next(); System.out.println(&quot;Please choose one type to send msg:[Enter numbers]&quot;); System.out.println(&quot;1. Send by message app on your phone.&quot;); System.out.println(&quot;2. Send by email&quot;); System.out.println(&quot;3. Send by WeChat&quot;); System.out.println(); int typeNo = 0; try { typeNo = in.nextInt(); } catch (Exception e) { // handle exception System.out.println(&quot;Invalid input!&quot;); } // Use parent interface to point the object. CommonInterfaceSender sender = ordinaryFactory.sendMsg(typeNo); // invoke parent interface method. sender.sendMsg(receiverName); } } 优点：充分发挥了面向对象的特点，使得具体的业务代码一定程度上和 Main 解耦，添加新的发送短信的方式时，只需要新建对应的 Sender，并简单调整对应的 Factory。 缺点： 严格意义上并没有实现完全解耦。客户端（Main）和业务逻辑代码中仍然存在依赖，Factory起了决定作用。同时对于传递的参数具有依赖性，参数的不合法导致程序运行的不合法。 违反了 开放-封闭原则。需要调整 Factory 的判断逻辑，对修改开放。 开放-封闭原则：软件实体应该是可扩展，而不可修改的。也就是说，对扩展是开放的，而对修改是封闭的。 工厂方法模式 思路：通过为每一个Sender Class创建Factory类，来代替简单工厂模式中Factory Class的作用，将判断的逻辑（实例化子类的逻辑）由Factory转移到客户端。 代码实现（一）：工厂类 // common factory public interface IMsgFactory { CommonInterfaceSender sendMsg(); } // Mail factory public class SendMailFactory implements IMsgFactory { @Override public CommonInterfaceSender sendMsg() { return new MailSender(); } } // Sms factory public class SendSmsFactory implements IMsgFactory { @Override public CommonInterfaceSender sendMsg() { return new SmsSender(); } } // WeChat factory public class WechatFactory implements IMsgFactory { @Override public CommonInterfaceSender sendMsg() { return new WechatSender(); } } 代码实现（二）：客户端 Main public class FactoryMethodMain { public static void main(String[] args) { // Choose one specific class to create factory referring demand IMsgFactory factory = new SendMailFactory(); //IMsgFactory factory = new SendSmsFactory(); //IMsgFactory factory = new WechatFactory(); CommonInterfaceSender sender = factory.createSender(); sender.sendMsg(&quot;Elvis&quot;); } } 优点：解决了简单工厂模式中的开放-封闭原则问题。通过客户端选择实例化哪一个具体的工厂类来达到最少修改代码的目的。同时保持了简单工厂模式面向对象的优点。 缺点：本质上仍然没有解决选择的问题，当有新的需求的时候，客户端的代码仍然需要修改。除此之外，增加了代码量，引进一个新的发送消息的方式时，还需要建立对应的工厂类。 Tips：反射解决分支判断的问题。类比于Spring IOC容器中根据 beanName 注入相应的实例 抽象工厂模式 使用场景：多个属性形成一组属性（一套属性），需要对一组属性中的某一个属性进行消费时 结合本例：各种通信方式可能还会有接受消息的功能。 代码实现（一）：Receivers public interface CommonInterfaceReceiver { void receiveMsg(String from); } public class MailReceiver implements CommonInterfaceReceiver { @Override public void receiveMsg(String from) { System.out.println(String.format(&quot;Receive message from %s by email&quot;, from)); } } public class SmsReceiver implements CommonInterfaceReceiver { @Override public void receiveMsg(String from) { System.out.println(String.format(&quot;Receive message form %s by sms&quot;, from)); } } public class WechatReceiver implements CommonInterfaceReceiver { @Override public void receiveMsg(String from) { System.out.println(String.format(&quot;Receive message from %s by wechat&quot;, from)); } } 代码实现（二）：建立抽象工厂类并根据功能提供各自的实现 public abstract class AbstractCommunicationFactory { public abstract CommonInterfaceSender getSender(int typeNum); public abstract CommonInterfaceReceiver getReceiver(int typeNum); } public class ReceiverFactory extends AbstractCommunicationFactory { @Override public CommonInterfaceSender getSender(int typeNum) { return null; } @Override public CommonInterfaceReceiver getReceiver(int typeNum) { switch (typeNum) { case 0: return new SmsReceiver(); case 1: return new MailReceiver(); case 2: return new WechatReceiver(); default: return null; } } } public class SenderFactory extends AbstractCommunicationFactory { @Override public CommonInterfaceSender getSender(int typeNum) { switch (typeNum) { case 0: return new SmsSender(); case 1: return new MailSender(); case 2: return new WechatSender(); default: return null; } } @Override public CommonInterfaceReceiver getReceiver(int typeNum) { return null; } } 代码实现（三）：利用简单工厂模式对具体的功能对应的工厂进行管理 ublic class FactoryProducer { public static AbstractCommunicationFactory getFactory(String direction) { if (&quot;Send&quot;.equals(direction)) { return new SenderFactory(); } else if (&quot;Receive&quot;.equals(direction)) { return new ReceiverFactory(); } return null; } } 代码实现（四）：Main public class AbstractFactoryMain { public static void main(String[] args) { String to = &quot;Elvis&quot;; String from = &quot;Shunzi&quot;; AbstractCommunicationFactory senderFactory = FactoryProducer.getFactory(&quot;Send&quot;); // sms sender CommonInterfaceSender smsSender = senderFactory.getSender(0); smsSender.sendMsg(to); // MailSender CommonInterfaceSender mailSender = senderFactory.getSender(1); smsSender.sendMsg(to); // WeChat Sender CommonInterfaceSender wechatSender = senderFactory.getSender(2); wechatSender.sendMsg(to); AbstractCommunicationFactory receiverFactory = FactoryProducer.getFactory(&quot;Receive&quot;); // sms receiver CommonInterfaceReceiver smsReceiver = receiverFactory.getReceiver(0); smsReceiver.receiveMsg(from); CommonInterfaceReceiver mailReceiver = receiverFactory.getReceiver(1); mailReceiver.receiveMsg(from); CommonInterfaceReceiver wechatReceiver = receiverFactory.getReceiver(2); wechatReceiver.receiveMsg(from); } } 输出结果： &gt; Send to Elvis by message &gt; Send to Elvis by message &gt; Send to Elvis by WeChat &gt; Receive message form Shunzi by sms &gt; Receive message from Shunzi by email &gt; Receive message from Shunzi by wechat 优点：便于修改对应的业务逻辑，具体的工厂类实现只需要一行代码，其余操作都是抽象工厂的继承，减少代码修改的成本。同时让创建具体对象的逻辑和客户端分离，通过实例化对应的抽象工厂类来实现。 缺点： 仍然不可避免地违反了开放-封闭原则，需要在客户端中根据不同的业务需求调整实例化工厂的代码。可以采用反射解决。 抽象工厂在扩展相关功能的基础上成本较高，譬如新建拒收消息的功能，则需要按照接受和发送消息的类结构再次新建对应的类，并修改工厂中的部分逻辑。 源码地址：Elvis Github 友情链接一（略有参考其他前辈的博客） 友情链接二（菜鸟教程设计模式大汇总） UML 图（感觉不怎么需要，例子比较简单。觉得之后有必要的时候再画吧..） 后续会结合实际开发以及相关框架中的应用进一步解释工厂模式带来的实际作用。留坑。。（逃 ","link":"https://blog.shunzi.tech/post/design-pattern-factory/"},{"title":"SpringData-JPA","content":" 此篇博文主要是结合官方文档以及对JPA的一些使用进行记录。 对于相关介绍，可参考相应博客。 后续会对相关实现机制和核心思想进行深入探讨。 参考书目： 汪云飞：JavaEE开发的颠覆者 Spring Boot实战 参考博客： 乐百川：Spring Data JPA 介绍和使用 Javahih：Spring data jpa入门教程 参考文档： Spring Data JPA - Reference Documentation 相关使用 1、引入依赖 &lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-jpa&lt;/artifactId&gt; &lt;!-- 版本号省略，maven相关不过多介绍 --&gt; &lt;/dependency&gt; 2、建立数据访问层 The goal of Spring Data repository abstraction is to significantly reduce the amount of boilerplate code required to implement data access layers for various persistence stores. 建立抽象的 Repository或Dao 从而减少持久层相关代码的编写（模板化的代码，重复的代码） 空接口、标记接口：没有包含方法声明的接口 public interface Repository&lt;T, ID extends Serializable&gt; { } 以自定义 PersonRepository 为例： 使得 Spring 容器能对自定义的 PersonRepository 进行管理（两种实现） /** * 继承 Repository&lt;T, ID extends Serializable&gt; 接口使得 Spring 容器能对自定义的 PersonRepository 进行管理 */ public interface PersonRepository extends Repository&lt;Person, Long&gt; { //定义数据访问操作的方法 } /** * 除了继承 Repository 接口以外，还可以通过注解实现 Spring 容器对自定义 repository 进行管理 */ @RepositoryDefinition(domainClass = Person.class,idClass = Long.class) public interface PersonRepository { //定义数据访问操作的方法 } Repository 的相关子类实现 自带Repository 1)、CrudRepository public interface CrudRepository&lt;T, ID extends Serializable&gt; extends Repository&lt;T, ID&gt; { // Saves the given entity. &lt;S extends T&gt; S save(S entity); // Returns the entity identified by the given id. Optional&lt;T&gt; findById(ID primaryKey); // Returns all entities. Iterable&lt;T&gt; findAll(); // Returns the number of entities long count(); // Deletes the given entity. void delete(T entity); // Indicates whether an entity with the given id exists. boolean existsById(ID primaryKey); // … more functionality omitted. } 2)、PagingAndSortingRepository extends CrudRepository public interface PagingAndSortingRepository&lt;T, ID extends Serializable&gt; extends CrudRepository&lt;T, ID&gt; { Iterable&lt;T&gt; findAll(Sort sort); Page&lt;T&gt; findAll(Pageable pageable); } 3)、JpaRepository extends PagingAndSortingRepository 自定义抽象 Repository 选择性地暴露相关持久层数据操作的相关方法 Repository 中关于 null 的处理，使用 spring 相关注解: @NonNullApi – to be used on the package level to declare that the default behavior for parameters and return values is to not accept or produce null values. @NonNull – to be used on a parameter or return value that must not be null (not needed on parameter and return value where @NonNullApi applies). @Nullable – to be used on a parameter or return value that can be null. **************************************interface MyBaseRepository**************************** // Make sure you add that annotation to all repository interfaces // that Spring Data should not create instances for at runtime. @NoRepositoryBean interface MyBaseRepository&lt;T, ID extends Serializable&gt; extends Repository&lt;T, ID&gt; { // Java 8 Optinal refering http://www.importnew.com/6675.html // 也可以返回其他包装类型 Optional&lt;T&gt; findById(ID id); &lt;S extends T&gt; S save(S entity); } **************************************package-info.java********************************** @NonNullApi package tech.shunzi.repository; import org.springframework.lang.NonNullApi; **************************************interface UserRepository**************************** package tech.shunzi.repository; import org.springframework.lang.Nullable; public interface UserRepository extends MyBaseRepository&lt;User, Long&gt; { // test success @Nullable User findByEmailAddress(@Nullable EmailAddress emailAddress); } 关于 Repository 或 项目 中可能出现的 SpringData 多模块问题，所谓的 多模块 指的是在 classpath 中同时存在多个 Spring Data 模块，譬如 MongoDB 和 JPA 两个模块。 对于 仓库 中如果显式继承了模块的特定接口：JpaRepository，则不存在多模块的问题。 对于 域对象domain 中如果显式使用了模块特定的注解，如JPA的@entity，如Spring Data MongoDB/Spring Data Elasticsearch的@document注解，也不存在多模块的问题。 存在多模块的问题的情况有： 使用通用接口的仓库定义 Repository 使用有多个注解的域类型的仓库定义。 @Entity、@Document 同时修饰一个对象 用于区分仓库的方法是限制仓库的 base packages @EnableJpaRepositories(basePackages = &quot;com.acme.repositories.jpa&quot;) @EnableMongoRepositories(basePackages = &quot;com.acme.repositories.mongo&quot;) interface Configuration { } 2、JPA的基本使用: 4 steps 1)、建立相关接口 two steps // step 1：Declare an interface extending Repository or one of its subinterfaces and type it to the domain class and ID type that it will handle. interface PersonRepository extends Repository&lt;Person, Long&gt; { // step 2: Declare query methods on the interface. List&lt;Person&gt; findByLastname(String lastname); } 其中定义相应的查询方法主要有两种方式：It can derive the query from the method name directly, or by using a manually defined query. direct name: manual define: 2)、在 Spring 中为定义好的接口创建代理实例 step 3：Set up Spring to create proxy instances for those interfaces. use xml &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns:jpa=&quot;http://www.springframework.org/schema/data/jpa&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd&quot;&gt; &lt;jpa:repositories base-package=&quot;com.acme.repositories&quot;/&gt; &lt;/beans&gt; use java annotation to config import org.springframework.data.jpa.repository.config.EnableJpaRepositories; @EnableJpaRepositories class Config {} 3)、获取 Repository 实例并调用相关方法 step 4: Get the repository instance injected and use it. class SomeClient { private final PersonRepository repository; SomeClient(PersonRepository repository) { this.repository = repository; } void doSomething() { List&lt;Person&gt; persons = repository.findByLastname(&quot;Matthews&quot;); } } 3、定义查询方法 1）、查询策略 CREATE：根据查询方法名称构建，从方法名称中移除一组已知的前缀并解析该方法的其余部分。find…By, read…By, query…By, count…By, get…By, And and Or. USE_DECLARED_QUERY：试图找到一个声明的查询（注释或其他方式声明），如果未找到会抛出一个异常。 CREATE_IF_NOT_FOUND：default.它首先查找已声明的查询，并且如果未找到已声明的查询，则会创建一个基于自定义方法名称的查询。这是默认的查找策略，因此如果不明确配置任何内容，将会使用它。它允许通过方法名称快速查询定义，还可以根据需要引入已声明的查询来自定义这些查询。 2）、属性表达式 分辨算法会对findBy后的字段进行匹配。从右往左开始匹配。先匹配AddressZipCode，没有该属性，匹配失败，从右往左，匹配Code和AddressZip，匹配失败，继续匹配Address和ZipCode，匹配成功。对应的则为Address.ZipCode。 List&lt;Person&gt; findByAddressZipCode(ZipCode zipCode); 注意：如果Person有addressZip属性，则会因为addressZip没有code属性匹配失败，此时需要使用下划线来消除歧义。--不推荐使用 List&lt;Person&gt; findByAddress_ZipCode(ZipCode zipCode); 3）、特殊参数处理（Pageable,Sort） // A Page knows about the total number of elements and pages available. Page&lt;User&gt; findByLastname(String lastname, Pageable pageable); // A Slice only knows about whether there’s a next Slice available // Slice is suitable for large result set. Slice&lt;User&gt; findByLastname(String lastname, Pageable pageable); // Can use sort parameter to sort data. List&lt;User&gt; findByLastname(String lastname, Sort sort); // List can avoid extra query for page instance,such as total amount. List&lt;User&gt; findByLastname(String lastname, Pageable pageable); To find out how many pages you get for a query entirely you have to trigger an additional count query. By default this query will be derived from the query you actually trigger. 4）、限制查询结果 First/Top 关键字，可以追加数字进行结果集中结果数量的限制，默认为 1。 Distinct 关键字 结合 Sort 和 First/Top 可实现寻找 第K大/小。 User findFirstByOrderByLastnameAsc(); User findTopByOrderByAgeDesc(); Page&lt;User&gt; queryFirst10ByLastname(String lastname, Pageable pageable); Slice&lt;User&gt; findTop3ByLastname(String lastname, Pageable pageable); List&lt;User&gt; findFirst10ByLastname(String lastname, Sort sort); List&lt;User&gt; findTop10ByLastname(String lastname, Pageable pageable); 5）、流式查询结果 利用 Java8 的 Stream 作为返回结果，使用完对应的 stream 后需要 close @Query(&quot;select u from User u&quot;) Stream&lt;User&gt; findAllByCustomQueryAndStream(); Stream&lt;User&gt; readAllByFirstnameNotNull(); @Query(&quot;select u from User u&quot;) Stream&lt;User&gt; streamAllPaged(Pageable pageable); try (Stream&lt;User&gt; stream = repository.findAllByCustomQueryAndStream()) { stream.forEach(…); } 6）、异步查询结果 方法调用后立即返回结果， // java.util.concurrent.Future @Async Future&lt;User&gt; findByFirstname(String firstname); // Java 8 java.util.concurrent.CompletableFuture @Async CompletableFuture&lt;User&gt; findOneByFirstname(String firstname); // org.springframework.util.concurrent.ListenableFuture @Async ListenableFuture&lt;User&gt; findOneByLastname(String lastname); 4、创建 Repo 实例 1、XML 配置 &lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt; &lt;beans:beans xmlns:beans=&quot;http://www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xmlns=&quot;http://www.springframework.org/schema/data/jpa&quot; xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd http://www.springframework.org/schema/data/jpa http://www.springframework.org/schema/data/jpa/spring-jpa.xsd&quot;&gt; &lt;!-- Use filter to exclude some repo to from being instantiated. --&gt; &lt;repositories base-package=&quot;com.acme.repositories&quot;&gt; &lt;context:exclude-filter type=&quot;regex&quot; expression=&quot;.*SomeRepository&quot; /&gt; &lt;/repositories&gt; &lt;/beans:beans&gt; 2、Java Config (Recommend) Annotation @Enable${store}Repositories @Configuration @EnableJpaRepositories(&quot;com.acme.repositories&quot;) class ApplicationConfiguration { @Bean EntityManagerFactory entityManagerFactory() { // … } } 3、独立使用 RepositoryFactorySupport factory = … // Instantiate factory here UserRepository repository = factory.getRepository(UserRepository.class); Repo 的自定义实现 定制个人Repo 1、定义接口和实现 interface CustomizedUserRepository { void someCustomMethod(User user); } class CustomizedUserRepositoryImpl implements CustomizedUserRepository { public void someCustomMethod(User user) { // Your custom implementation } // you can add some beans according to your demand @Autowired private JdbcTemplate jdbcTemplate; } // Doing so combines the CRUD and custom functionality class UserRepository extends CrudRepository&lt;User, Long&gt;, CustomizedUserRepository { // Declare query methods here } 注意： 当 CustomRepo 中包含和 BaseRepo（JPA自带Repo）相同的方法时，譬如如下例子的 save，CustomRepo 有更高的优先级，所以可以实现 自定义Repo 对原生的 Repo 的相关方法的重写。与此同时，可以结合 泛型 使得自定义 Repo 被更多的 RepoImpl 类继承实现。 interface CustomizedSave&lt;T&gt; { &lt;S extends T&gt; S save(S entity); } class CustomizedSaveImpl&lt;T&gt; implements CustomizedSave&lt;T&gt; { public &lt;S extends T&gt; S save(S entity) { // Your custom implementation } } interface UserRepository extends CrudRepository&lt;User, Long&gt;, CustomizedSave&lt;User&gt; { } interface PersonRepository extends CrudRepository&lt;Person, Long&gt;, CustomizedSave&lt;Person&gt; { } 对于自定义Repo的相关配置，如果使用 namespace 相关配置，会自动检测扫描指定包下 指定格式的自定义 Repo，默认后缀为 Impl，可以通过修改相关配置进行定制。 &lt;repositories base-package=&quot;com.acme.repository&quot; /&gt; &lt;repositories base-package=&quot;com.acme.repository&quot; repository-impl-postfix=&quot;FooBar&quot; /&gt; 当一个 Repo 接口有多个实现，且位于不同的包下时， Spring 将基于 beanName 对 RepoImpl进行扫描.接口未指定对应的 beanName 时，默认为 接口名 + Impl，指定了Component 时，则对应扫描 Component 对应的 Repo. 注意：也可以在配置中自定义实例化相关 Bean package com.acme.impl.one; class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } package com.acme.impl.two; @Component(&quot;specialCustomImpl&quot;) class CustomizedUserRepositoryImpl implements CustomizedUserRepository { // Your custom implementation } interface UserRepository implements CustomizedUserRepository { // default matches CustomizedUserRepositoryImpl // if @Component(&quot;specialCustomImpl&quot;),matches CustomizedUserRepositoryImpl } 2、自定义基础Repo 未完待续 实体类 Entity 结合文档官方文档 Version 2.0.4.RELEASE, 2018-02-19 1、@Entity 注解修饰实体类 ","link":"https://blog.shunzi.tech/post/learn-spring-data-jpa/"},{"title":"tinySpring学习笔记（二）-实现AOP","content":" tinySpring学习笔记系列之二 主要介绍AOP实现过程 并介绍不同的动态代理方式 AOP及其实现 AOP分为配置(Pointcut，Advice)，织入(Weave)两部分工作，当然还有一部分是将AOP整合到整个容器的生命周期中。 step1-使用JDK动态代理实现AOP织入 git checkout step-7-method-interceptor-by-jdk-dynamic-proxy 织入（weave）相对简单，我们先从它开始。Spring AOP的织入点是AopProxy，它包含一个方法Object getProxy()来获取代理后的对象。 public interface AopProxy { Object getProxy(); } AOP 中两个重要角色：MethodInterceptor和MethodInvocation 这两个角色都是AOP联盟的标准，它们分别对应AOP中两个基本角色：Advice和Joinpoint。Advice定义了在切点指定的逻辑，而Joinpoint则代表切点。 public interface MethodInterceptor extends Interceptor { Object invoke(MethodInvocation invocation) throws Throwable; } Spring的AOP只支持方法级别的调用，所以其实在AopProxy里，我们只需要将MethodInterceptor放入对象的方法调用即可。 我们称被代理对象为TargetSource，而AdvisedSupport就是保存TargetSource和MethodInterceptor的元数据对象。这一步我们先实现一个基于JDK动态代理的JdkDynamicAopProxy，它可以对接口进行代理。于是我们就有了基本的织入功能。 public class JdkDynamicAopProxy implements AopProxy, InvocationHandler { /** * AdvisedSupport * Fields: TargetSource targetSource * [Object target,Class targetClass];//target 代理对象,targetClass 继承的接口 * * MethodInterceptor methodInterceptor;//Advice切面逻辑 * Methods：对应属性的getter、setter */ private AdvisedSupport advised; public JdkDynamicAopProxy(AdvisedSupport advised) { this.advised = advised; } //为要代理的对象的的类对应的创建动态代理 @Override public Object getProxy() { /** * public static Object newProxyInstance(ClassLoader loader, * Class&lt;?&gt;[] interfaces, * InvocationHandler h) * @param loader the class loader to define the proxy class * @param interfaces the list of interfaces for the proxy class to implement * @param h the invocation handler to dispatch method invocations to */ return Proxy.newProxyInstance(getClass().getClassLoader(), new Class[] { advised.getTargetSource() .getTargetClass() }, this); } /** * 重写 InvocationHandler 对应的 invoke() 方法 * 调用拦截器对应的方法 * （通过反射获取对应的切点，再根据切点指定的逻辑进行执行） * @Param Object proxy 代理 * @Param Method method 对应的要调用方法 * @Param Object[] args 方法需要的参数 */ @Override public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable { MethodInterceptor methodInterceptor = advised.getMethodInterceptor(); /** * 其中 class MethodInterceptor extends Interceptor { * Object invoke(MethodInvocation invocation) throws Throwable; * } * class ReflectiveMethodInvocation implements MethodInvocation * 故可通过继承 MethodInterceptor 重写相关 invoke() 方法实现 Advice 逻辑 */ return methodInterceptor.invoke(new ReflectiveMethodInvocation(advised.getTargetSource().getTarget(), method, args)); } } ReflectiveMethodInvocation.java public class ReflectiveMethodInvocation implements MethodInvocation { private Object target; private Method method; private Object[] args; public ReflectiveMethodInvocation(Object target, Method method, Object[] args) { this.target = target; this.method = method; this.args = args; } @Override public Method getMethod() { return method; } @Override public Object[] getArguments() { return args; } // 反射，相当于 target.method(args);即调用代理对象对应的方法 @Override public Object proceed() throws Throwable { return method.invoke(target, args); } @Override public Object getThis() { return target; } @Override public AccessibleObject getStaticPart() { return method; } } 实现步骤： 1、设置被代理对象，指定切点（JoinPoint） // 创建AdvisedSupport对象,设置 Joinpoint AdvisedSupport advisedSupport = new AdvisedSupport(); TargetSource targetSource = new TargetSource(helloWorldService, HelloWorldService.class); advisedSupport.setTargetSource(targetSource); 2、设置拦截器（Advice） /** * // Advice 逻辑实现 * public class TimerInterceptor implements MethodInterceptor { * @Override * public Object invoke(MethodInvocation invocation) throws Throwable { * long time = System.nanoTime(); * System.out.println(&quot;Invocation of Method &quot; + invocation.getMethod().getName() + &quot; start!&quot;); * Object proceed = invocation.proceed(); * System.out.println(&quot;Invocation of Method &quot; + invocation.getMethod().getName() + &quot; end! takes &quot; + (System.nanoTime() - time) + &quot; nanoseconds.&quot;); * return proceed; * } * } */ TimerInterceptor timerInterceptor = new TimerInterceptor(); // 设置Advice advisedSupport.setMethodInterceptor(timerInterceptor); 3、创建代理（Proxy） JdkDynamicAopProxy jdkDynamicAopProxy = new JdkDynamicAopProxy(advisedSupport); HelloWorldService helloWorldServiceProxy = (HelloWorldService) jdkDynamicAopProxy.getProxy(); 4、基于AOP的调用 helloWorldServiceProxy.helloWorld(); step2-使用AspectJ管理切面 git checkout step-8-invite-pointcut-and-aspectj 完成了织入之后，我们要考虑另外一个问题：对什么类以及什么方法进行AOP？对于“在哪切”这一问题的定义，我们又叫做“Pointcut”。Spring中关于Pointcut包含两个角色：ClassFilter和MethodMatcher，分别是对类和方法做匹配。Pointcut有很多种定义方法，例如类名匹配、正则匹配等，但是应用比较广泛的应该是和AspectJ表达式的方式。 public interface Pointcut { ClassFilter getClassFilter(); MethodMatcher getMethodMatcher(); } public interface ClassFilter { boolean matches(Class targetClass); } public interface MethodMatcher { boolean matches(Method method, Class targetClass); } AspectJ是一个“对Java的AOP增强”。它最早是其实是一门语言，我们跟写Java代码一样写它，然后静态编译之后，就有了AOP的功能。下面是一段AspectJ代码： aspect PointObserving { private Vector Point.observers = new Vector(); public static void addObserver(Point p, Screen s) { p.observers.add(s); } public static void removeObserver(Point p, Screen s) { p.observers.remove(s); } ... } 这种方式无疑太重了，为了AOP，还要适应一种语言？所以现在使用也不多，但是它的Pointcut表达式被Spring借鉴了过来。于是我们实现了一个AspectJExpressionPointcut： public class AspectJExpressionPointcut implements Pointcut, ClassFilter, MethodMatcher { // 对用户自定义的相关关键字子集合，可进行切点表达式PointcutExpression的构造 private PointcutParser pointcutParser; // 表达式字符串 private String expression; // 要构造的切点表达式 private PointcutExpression pointcutExpression; // 存储表达式相关关键字 private static final Set&lt;PointcutPrimitive&gt; DEFAULT_SUPPORTED_PRIMITIVES = new HashSet&lt;PointcutPrimitive&gt;(); static { DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.EXECUTION); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.ARGS); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.REFERENCE); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.THIS); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.TARGET); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.WITHIN); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.AT_ANNOTATION); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.AT_WITHIN); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.AT_ARGS); DEFAULT_SUPPORTED_PRIMITIVES.add(PointcutPrimitive.AT_TARGET); } // 进行关键字集合初始化 public AspectJExpressionPointcut() { this(DEFAULT_SUPPORTED_PRIMITIVES); } // 初始化表达式构造器 public AspectJExpressionPointcut(Set&lt;PointcutPrimitive&gt; supportedPrimitives) { pointcutParser = PointcutParser .getPointcutParserSupportingSpecifiedPrimitivesAndUsingContextClassloaderForResolution(supportedPrimitives); } protected void checkReadyToMatch() { if (pointcutExpression == null) { pointcutExpression = buildPointcutExpression(); } } // 字符串转换为切点表达式 private PointcutExpression buildPointcutExpression() { return pointcutParser.parsePointcutExpression(expression); } public void setExpression(String expression) { this.expression = expression; } // 对类做匹配 @Override public ClassFilter getClassFilter() { return this; } // 对方法做匹配 @Override public MethodMatcher getMethodMatcher() { return this; } // 将表达式和类做匹配，返回匹配结果 @Override public boolean matches(Class targetClass) { checkReadyToMatch(); return pointcutExpression.couldMatchJoinPointsInType(targetClass); } // 将表达式和方法做匹配，返回匹配结果 @Override public boolean matches(Method method, Class targetClass) { checkReadyToMatch(); ShadowMatch shadowMatch = pointcutExpression.matchesMethodExecution(method); if (shadowMatch.alwaysMatches()) { return true; } else if (shadowMatch.neverMatches()) { return false; } // TODO:其他情况不判断了！见org.springframework.aop.aspectj.RuntimeTestWalker return false; } } 实现测试： // 对类做匹配 返回匹配结果 @Test public void testClassFilter() throws Exception { String expression = &quot;execution(* us.codecraft.tinyioc.*.*(..))&quot;; AspectJExpressionPointcut aspectJExpressionPointcut = new AspectJExpressionPointcut(); aspectJExpressionPointcut.setExpression(expression); boolean matches = aspectJExpressionPointcut.getClassFilter().matches(HelloWorldService.class); Assert.assertTrue(matches); } // 对方法做匹配 返回匹配结果 @Test public void testMethodInterceptor() throws Exception { String expression = &quot;execution(* us.codecraft.tinyioc.*.*(..))&quot;; AspectJExpressionPointcut aspectJExpressionPointcut = new AspectJExpressionPointcut(); aspectJExpressionPointcut.setExpression(expression); boolean matches = aspectJExpressionPointcut.getMethodMatcher().matches(HelloWorldServiceImpl.class.getDeclaredMethod(&quot;helloWorld&quot;),HelloWorldServiceImpl.class); Assert.assertTrue(matches); } step3-将AOP融入Bean的创建过程中 git checkout step-9-auto-create-aop-proxy 在step1 中已经能够进行 weave 织入，step2 中实现了 Pointcut 的匹配。现在需要在 Spring 中整合这两者。Spring给了一个巧妙的答案：使用 BeanPostProcessor。 BeanPostProcessor是BeanFactory提供的，在Bean初始化过程中进行扩展的接口。只要你的Bean实现了BeanPostProcessor接口，那么Spring在初始化时，会优先找到它们，并且在Bean的初始化过程中，调用这个接口，从而实现对BeanFactory核心无侵入的扩展。 AOP的xml配置 &lt;aop:aspectj-autoproxy/&gt; &lt;!-- 等价 --&gt; &lt;bean id=&quot;autoProxyCreator&quot; class=&quot;org.springframework.aop.aspectj.autoproxy.AspectJAwareAdvisorAutoProxyCreator&quot;&gt;&lt;/bean&gt; AspectJAwareAdvisorAutoProxyCreator就是AspectJ方式实现织入的核心。它其实是一个BeanPostProcessor。在这里它会扫描所有Pointcut，并对bean做织入。 BeanPostProcessor: public interface BeanPostProcessor { Object postProcessBeforeInitialization(Object bean, String beanName) throws Exception; Object postProcessAfterInitialization(Object bean, String beanName) throws Exception; } AspectJAwareAdvisorAutoProxyCreator: public class AspectJAwareAdvisorAutoProxyCreator implements BeanPostProcessor, BeanFactoryAware { private AbstractBeanFactory beanFactory; @Override public Object postProcessBeforeInitialization(Object bean, String beanName) throws Exception { return bean; } @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws Exception { if (bean instanceof AspectJExpressionPointcutAdvisor) { return bean; } if (bean instanceof MethodInterceptor) { return bean; } /** * class AspectJExpressionPointcutAdvisor implements PointcutAdvisor * Fields: AspectJExpressionPointcut pointcut * Advice advice * Methods: Advice getter/setter * Pointcut getter * void setExpression(String expression) {this.pointcut.setExpression(expression);} * */ // 根据 Type 获取所有的 PointCut 和 Advice 组成的 bean List&lt;AspectJExpressionPointcutAdvisor&gt; advisors = beanFactory .getBeansForType(AspectJExpressionPointcutAdvisor.class); for (AspectJExpressionPointcutAdvisor advisor : advisors) { // 判断是否是要拦截的类 if (advisor.getPointcut().getClassFilter().matches(bean.getClass())) { /** * Class AdvisedSupport * Fields: TargetSource targetSource; * MethodInterceptor methodInterceptor; * MethodMatcher methodMatcher; * Methods: getter/setter */ AdvisedSupport advisedSupport = new AdvisedSupport(); // 从扫描出的 bean 中获取 Advice 逻辑并注入 // 设置 Advice advisedSupport.setMethodInterceptor((MethodInterceptor) advisor.getAdvice()); /** * MethodMatcher getMethodMatcher() {return this;} * AspectJExpressionPointcut 实现了 MethodMatcher 接口。 */ // 设置切点 Pointcut,即哪些方法需要做拦截 advisedSupport.setMethodMatcher(advisor.getPointcut().getMethodMatcher()); // 设置要代理的对象 TargetSource targetSource = new TargetSource(bean, bean.getClass().getInterfaces()); advisedSupport.setTargetSource(targetSource); // 创建动态代理 return new JdkDynamicAopProxy(advisedSupport).getProxy(); } } return bean; } // 获取容器的引用，进而获取容器中所有的切点对象 @Override public void setBeanFactory(BeanFactory beanFactory) throws Exception { this.beanFactory = (AbstractBeanFactory) beanFactory; } } 此时的JdkDynamicAopProxy public class JdkDynamicAopProxy implements AopProxy, InvocationHandler { private AdvisedSupport advised; public JdkDynamicAopProxy(AdvisedSupport advised) { this.advised = advised; } @Override public Object getProxy() { return Proxy.newProxyInstance(getClass().getClassLoader(), advised.getTargetSource().getTargetClass(), this); } @Override public Object invoke(final Object proxy, final Method method, final Object[] args) throws Throwable { MethodInterceptor methodInterceptor = advised.getMethodInterceptor(); // invoke 时判断是否为要拦截的方法，是则执行 Advice 逻辑 if (advised.getMethodMatcher() != null &amp;&amp; advised.getMethodMatcher().matches(method, advised.getTargetSource().getTarget().getClass())) { return methodInterceptor.invoke(new ReflectiveMethodInvocation(advised.getTargetSource().getTarget(), method, args)); } else { return method.invoke(advised.getTargetSource().getTarget(), args); } } } 为了简化xml配置，在tiny-spring中直接使用Bean的方式，而不是用aop前缀进行配置： &lt;bean id=&quot;autoProxyCreator&quot; class=&quot;us.codecraft.tinyioc.aop.AspectJAwareAdvisorAutoProxyCreator&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;timeInterceptor&quot; class=&quot;us.codecraft.tinyioc.aop.TimerInterceptor&quot;&gt;&lt;/bean&gt; &lt;!-- Creator 将对 Advisor 类型的 bean 进行扫描和处理 --&gt; &lt;bean id=&quot;aspectjAspect&quot; class=&quot;us.codecraft.tinyioc.aop.AspectJExpressionPointcutAdvisor&quot;&gt; &lt;!-- 调用对应的 setter 方法进行 Property 的注入 --&gt; &lt;property name=&quot;advice&quot; ref=&quot;timeInterceptor&quot;&gt;&lt;/property&gt; &lt;property name=&quot;expression&quot; value=&quot;execution(* us.codecraft.tinyioc.*.*(..))&quot;&gt;&lt;/property&gt; &lt;/bean&gt; 动态代理的步骤 AutoProxyCreator（实现了 BeanPostProcessor 接口）在实例化所有的 Bean 前，最先被实例化。postProcessBeforeInitialization 其他普通 Bean 被实例化、初始化，在初始化的过程中，AutoProxyCreator 加载 BeanFactory 中所有的 PointcutAdvisor（这也保证了 PointcutAdvisor 的实例化顺序优于普通 Bean。），然后依次使用 PointcutAdvisor 内置的 ClassFilter，判断当前对象是不是要拦截的类。 如果是，则生成一个 TargetSource（要拦截的对象和其类型），并取出 AutoProxyCreator 的 MethodMatcher（对哪些方法进行拦截）、Advice（拦截的具体操作），再交给 AopProxy 去生成代理对象。 AopProxy 生成一个 InvocationHandler，在它的 invoke 函数中，首先使用 MethodMatcher 判断是不是要拦截的方法，如果是则交给 Advice 来执行（Advice 由用户来编写，其中也要手动/自动调用原始对象的方法），如果不是，则直接交给 TargetSource 的原始对象来执行。 step4-使用CGLib进行类的织入 git checkout step-10-invite-cglib-and-aopproxy-factory 前面的JDK动态代理只能对接口进行代理，对于类则无能为力。这里我们需要一些字节码操作技术。这方面大概有几种选择：ASM，CGLib和javassist，后两者是对ASM的封装。Spring中使用了CGLib。 在这一步，我们还要定义一个工厂类ProxyFactory，用于根据TargetSource类型自动创建代理，这样就需要在调用者代码中去进行判断。 TargetSource: public class TargetSource { private Class&lt;?&gt; targetClass; private Class&lt;?&gt;[] interfaces; private Object target; public TargetSource(Object target, Class&lt;?&gt; targetClass,Class&lt;?&gt;... interfaces) { this.target = target; this.targetClass = targetClass; this.interfaces = interfaces; } public Class&lt;?&gt; getTargetClass() { return targetClass; } public Object getTarget() { return target; } public Class&lt;?&gt;[] getInterfaces() { return interfaces; } } ProxyFactory : 策略模式和工厂模式的结合使用 public class ProxyFactory extends AdvisedSupport implements AopProxy { // 在 creator 中，生成相应的动态代理的时候就可以使用工厂类的 getProxy() @Override public Object getProxy() { return createAopProxy().getProxy(); } protected final AopProxy createAopProxy() { return new Cglib2AopProxy(this); } //...可以根据 TargetSource 决定使用不同的动态代理 protected final AopProxy createAopProxy(TargetSource targetSource) { return new JdkDynamicAopProxy(this); } } Cglib2AopProxy public class Cglib2AopProxy extends AbstractAopProxy { public Cglib2AopProxy(AdvisedSupport advised) { super(advised); } @Override public Object getProxy() { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(advised.getTargetSource().getTargetClass()); enhancer.setInterfaces(advised.getTargetSource().getInterfaces()); enhancer.setCallback(new DynamicAdvisedInterceptor(advised)); Object enhanced = enhancer.create(); return enhanced; } private static class DynamicAdvisedInterceptor implements MethodInterceptor { private AdvisedSupport advised; private org.aopalliance.intercept.MethodInterceptor delegateMethodInterceptor; private DynamicAdvisedInterceptor(AdvisedSupport advised) { this.advised = advised; this.delegateMethodInterceptor = advised.getMethodInterceptor(); } @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { if (advised.getMethodMatcher() == null || advised.getMethodMatcher().matches(method, advised.getTargetSource().getTargetClass())) { return delegateMethodInterceptor.invoke(new CglibMethodInvocation(advised.getTargetSource().getTarget(), method, args, proxy)); } return new CglibMethodInvocation(advised.getTargetSource().getTarget(), method, args, proxy).proceed(); } } private static class CglibMethodInvocation extends ReflectiveMethodInvocation { private final MethodProxy methodProxy; public CglibMethodInvocation(Object target, Method method, Object[] args, MethodProxy methodProxy) { super(target, method, args); this.methodProxy = methodProxy; } @Override public Object proceed() throws Throwable { return this.methodProxy.invoke(this.target, this.arguments); } } } 代理模式相关参见 代理模式（Proxy Pattern）- 最易懂的设计模式解析 ","link":"https://blog.shunzi.tech/post/tinyspring-learnTwo-AOP/"},{"title":"Head Frist Spring Cloud","content":" Spring Cloud一些基础概念 介绍Spring Cloud的一些基本组件 介绍一些组件的替代方案 服务提供者和服务消费者 服务提供者：对应的进行相关数据库操作并向外界提供API接口 服务消费者：对应的消费服务提供者提供的API，以实现相应的需求。 关键实现：使用RestTemplate作为客户端向相应API发出请求 RestTemplate详解-参考博文 存在的问题： 1、API硬编码问题 2、服务发现与注册 服务消费者和服务提供者均需要在服务发现组件中进行注册。 服务发现组件的功能：服务注册表（数据库）的相关维护，服务注册，健康检查（心跳机制） 3、服务发现的方式 客户端发现：Eureka 和 Zk 服务端发现： Consul + nginx 服务发现和注册 Eureka 服务发现 @EnableEurekaServer 参考博客：springcloud(二)：注册中心Eureka Spring cloud Eureka 参数配置 Eureka 既可以作为 server,又可以作为 client 简单配置 单机系统 eureka-server 高可用配置 集群 多节点 peer1 2 3 两种服务调用方式 Ribbon + RestTemplate Ribbon 是一个负载均衡客户端，很好地控制http和tcp的行为 对RestTemplate使用 @LoadBalanced注解，实现负载均衡。 可以使用 @RibbonClient注解来指定相关配置，并编写配置类自定义负载均衡策略。（通过代码的方式自定义配置） 也可以使用配置文件进行负载均衡的相关配置。 LoadBalancerClient可以获取相关客户端信息。 Feign Feign 是一个声明式的伪Http客户端，它使得写Http客户端变得更简单。使用Feign，只需要创建一个接口并注解。它具有可插拔的注解特性，可使用Feign 注解和JAX-RS注解。Feign支持可插拔的编码器和解码器。Feign默认集成了Ribbon，并和Eureka结合，默认实现了负载均衡的效果。 @FeignClients表明客户端继承了 Feign 使用 @FeignClients(&quot;spring-application-name&quot;)修饰自定义的xxxFeignClient类,又因为SpringCloud集成了SpringMVC的相关特性，默认使用了mvc的contract。故可以在接口中定义的方法上使用 mvc 相关注解修饰并进行 url 以及 method的匹配从而模拟出相应的http请求。 再在消费者的Controller中注入对应的xxxFeignCLient，在对应的方法中调用client的相关模拟请求实现需求。 对于FeignClient也可以使用自定义配置，通过在注解中指定configuration，并编写相关 Config类来实现。config类使用 @Configuration注解。 Feign 和 Ribbon 的常见问题： Hystrix相关： Feign和Ribbon在整合了Hystrix后，可能会出现首次调用失败的问题 原因： Hystrix默认的超时时间是1秒，如果超过这个时间尚未响应，将会进入fallback代码。而首次请求往往会比较慢（因为Spring的懒加载机制，要实例化一些类），这个响应时间可能就大于1秒了。 解决方案：在配置文件中进行相关配置： hystrix.command.default.execution.isolation.thread.timeoutInMilliseconds: 5000 hystrix.command.default.execution.timeout.enabled: false feign.hystrix.enabled: false ","link":"https://blog.shunzi.tech/post/Head-Frist-Spring-Cloud/"},{"title":"2017.11找实习-面经-Java","content":" 2017年11月，为了2018年上半年的企业实习投递了一系列简历并参与了相关面试。 主要分为Java篇，数据库篇，网络篇，框架篇，算法篇和相关工具篇 当时无太多项目经验，一切都还是入门阶段 面试的岗位主要是Java后端开发实习生的岗位. 主要面的有今日头条、百度、网易、搜狐、滴滴、SAP 较为简单的有所省略直接给相关问题或者直接上链接 较为常见的或者说比较重要的后续会单独写博文或者贴一些友链进行探讨 因为确实面试相关的博客已经很多很多，我想做的其实更多的是一个汇总 另外，不同岗位的直接看CS公共基础知识部分的即可（OS、计网、计组） 此篇为Java篇，持续更新ing 1、Java基本知识 实现多线程的方式 之后会统一的进行并发编程的相关知识点的整理和总结 常见方式一：继承 Thread 类；对应的重写run()方法，start() 方法执行 常见方式二：实现 Runable 接口；对应的重写run()方法，start() 方法执行 方式三：实现Callable接口通过FutureTask包装器来创建Thread线程 方式四：使用ExecutorService、Callable、Future实现有返回结果的线程 参考博文：Java中的多线程你只要看这一篇就够了 参考博文：JAVA多线程实现的四种方式 list、map、set的区别和应用场景（集合框架） 1.存放 (1)List存放元素是有序，可重复，继承Collection (2)Set存放元素无序，不可重复，继承Collection public interface Collection&lt;E&gt; extends Iterable&lt;E&gt; { int size(); boolean isEmpty(); boolean contains() boolean add(E e); boolean remove(Object o); boolean containsAll(Collection&lt;?&gt; c); boolean addAll(Collection&lt;? extends E&gt; c); boolean removeAll(Collection&lt;?&gt; c); // 取交集 boolean retainAll(Collection&lt;?&gt; c); Object[] toArray(); &lt;T&gt; T[] toArray(T[] a); Iterator&lt;E&gt; iterator(); // ... } (3)Map元素键值对形式存放，键无序不可重复，值可重复 public interface Map&lt;K,V&gt; { int size(); boolean isEmpty(); boolean containsKey(Object key); boolean containsValue(Object value); V get(Object key); V put(K key, V value); V remove(Object key); void putAll(Map&lt;? extends K, ? extends V&gt; m); void clear(); Set&lt;K&gt; keySet(); Collection&lt;V&gt; values(); Set&lt;Map.Entry&lt;K, V&gt;&gt; entrySet(); //... } 2.取出 Java8 可使用 lamda表达式 以及 函数式编程 实现访问集合中元素，此处暂不涉及，后续会统一对 Java8 的相关特性进行讲解。 (1)List取出元素for循环，foreach循环，Iterator迭代器迭代 // 使用 for 循环 public void printAllUsingFor(List&lt;E&gt; list) { // 注意将size() 的调用尽可能减少调用次数并使用临时空间变量节省空间 for (int i = 0, size = list.size(); i &lt; size; i++) { System.out.print(list.get(i)); } System.out.println(); } // 使用 foreach （其本质仍为迭代器，Java编译器在生成字节码时对应的修改为迭代器） public void printAllUsingForEach(List&lt;E&gt; list) { for (E e : list) { System.out.print(e); } System.out.println(); } // 使用 迭代器 public void printAllUsingIterator(List&lt;E&gt; list) { for (Iterator iterator = list.iterator(); iterator.hasNext(); ) { E e = (E) iterator.next(); System.out.print(e); } System.out.println(); } // 最佳实践（list中存放的元素数目较少时无论采用哪种遍历方式，结果都相差不大 // 但当数据量相对较大时，则需要根据list的类型对应的进行遍历方式的调整。 public void printAll(List&lt;E&gt; list) { if (list instanceof RandomAccess) { printAllUsingFor(list); } else { printAllUsingIterator(list); } } (2)Set取出元素foreach循环，Iterator迭代器迭代 // 迭代器 与 List的迭代器访问同理，此处不再演示 // 同时 迭代器 与 foreach 效率相差无几，不再具体进行相关对比 public void printAllUsingForEach(Set&lt;E&gt; set) { for (E e : set) { System.out.print(e); } System.out.println(); } (3)Map取出元素需转换为Set，然后进行Iterator迭代器迭代，或转换为Entry对象进行Iterator迭代器迭代 // 遍历 key 和 value，对应的也可以根据key去获取value public void printKeySetAndValueSet(Map&lt;E, Object&gt; map) { for (E e : map.keySet()) { System.out.println(&quot;keys = &quot; + e); } for (Object value : map.values()) { System.out.println(&quot;values = &quot; + value); } } // foreach public void printAllUsingForEach(Map&lt;E, Object&gt; map) { for (Map.Entry&lt;E, Object&gt; entry : map.entrySet()) { System.out.println(&quot;key = &quot; + entry.getKey() + &quot;;value = &quot; + entry.getValue()); } } // iterator 两种方式（是否使用泛型） public void printAllUsingIterator(Map&lt;E, Object&gt; map) { for (Iterator iterator = map.entrySet().iterator(); iterator.hasNext(); ) { // 需要强转，遍历器返回的类型为 Object Map.Entry&lt;E, Object&gt; entry = (Map.Entry) iterator.next(); System.out.println(&quot;key = &quot; + entry.getKey() + &quot;; value = &quot; + entry.getValue()); } for (Iterator&lt;Map.Entry&lt;E, Object&gt;&gt; iterator = map.entrySet().iterator(); iterator .hasNext(); ) { // 不需要强转，由于返回类型同 获得的遍历器 泛型一致 Map.Entry&lt;E, Object&gt; entry = iterator.next(); System.out.println(&quot;key = &quot; + entry.getKey() + &quot;; value = &quot; + entry.getValue()); } } 相关博文：Java迭代器的使用以及和for的区别 List中的相关集合框架（ArrayList、LinkedList、Stack、Vector） linkedList和arrayList的区别 区别最主要是 顺序存储 和 链式存储 的区别 1．对ArrayList和LinkedList而言，在列表末尾增加一个元素所花的开销都是固定的。对ArrayList而言，主要是在内部数组中增加一项，指向所添加的元素，偶尔可能会导致对数组重新进行分配；而对LinkedList而言，这个开销是统一的，分配一个内部Entry对象。 2．在ArrayList的中间插入或删除一个元素意味着这个列表中剩余的元素都会被移动；而在LinkedList的中间插入或删除一个元素的开销是固定的。 3．LinkedList不支持高效的随机元素访问。 4．ArrayList的空间浪费主要体现在在list列表的结尾预留一定的容量空间，而LinkedList的空间花费则体现在它的每一个元素都需要消耗相当的空间 可以这样说：当操作是在一列数据的后面添加数据而不是在前面或中间,并且需要随机地访问其中的元素时,使用ArrayList会提供比较好的性能；当你的操作是在一列数据的前面或中间添加或删除数据,并且按照顺序访问其中的元素时,就应该使用LinkedList了。 ArrayList、Stack和Vector的主要区别 1、Vector、Stack：线程安全；ArrayList、LinkedList：非线程安全。对于相关线程安全实现的机制将在并发编程中进行统一的讲解！ 2、实现方式： LinkedList：双向链表，ArrayList，Vector，Stack：数组； 3、Stack继承自Vector，只在Vector的基础上添加了几个Stack相关的方法 public class Stack&lt;E&gt; extends Vector&lt;E&gt; { public E push(E item) { // public synchronized void addElement(E obj) addElement(item); return item; } public synchronized E pop() {...} public synchronized E peek() {...} public boolean empty() { return size() == 0; } public synchronized int search(Object o) { int i = lastIndexOf(o); if (i &gt;= 0) { return size() - i; } return -1; } ... } 4、ArrayList和Vector在动态扩容时有所不同。 // ArrayList int newCapacity = (oldCapacity * 3)/2 + 1; // Vector int newCapacity = (capacityIncrement &gt; 0) ? (oldCapacity + capacityIncrement) : (oldCapacity * 2); Set相关的集合框架（HashSet、TreeSet） HashSet 和 TreeSet 1.HashSet 是无序的，不能保证元素排列顺序，底层是由 HashMap 进行实现的。（讲HashMap时会进行具体阐述），会根据对应的hashCode进行相关排序；TreeSet 是有序的，底层是通过 TreeMap 实现 （对应的会单独讲实现原理---红黑树 Entry） 的。TreeSet 通过使用 compareTo() 方法来比较然后升序排列，也可以定制排序逻辑。 TreeSet treeSet = new TreeSet(); HashSet hashSet = new HashSet(); hashSet.add(1); hashSet.add(0); hashSet.add(3); hashSet.add(2); hashSet.add(-1); System.out.println(hashSet);// &gt;&gt; [0, -1, 1, 2, 3] treeSet.add(1); treeSet.add(0); treeSet.add(3); treeSet.add(2); treeSet.add(-1); System.out.println(treeSet);// &gt;&gt; [-1, 0, 1, 2, 3] 2、HashSet 允许存放 null，但只能存放一个 null （由于Hash表的原因），而 TreeSet 不能，会抛出对应的空指针异常。 hashSet.add(1); hashSet.add(null); hashSet.add(null); System.out.println(hashSet); // &gt;&gt; [null,1] hashMap的原理 会新开博文具体阐释三者原理和区别 参考友链：HashMap实现原理分析 hashTable的原理 会新开博文结合源码具体阐释三者原理和区别 参考博文：深入Java集合学习系列：Hashtable的实现原理 concurrentHashMap的原理 会新开博文具体阐释三者原理和区别 参考友链：ConcurrentHashMap原理分析 Object中常用的方法 hashCode Object 类会提供 hashCode() 方法的声明并使用 native 关键字标志其由非java语言实现，具体的方法实现在外部。 public native int hashCode(); String 中的 hashCode()。hash 对应的算法. 相关博文：关于hashcode 里面 使用31 系数的问题 String类是用它的value值作为参数来计算hashCode的，也就是说，相同的value就一定会有相同的hashCode值。但反之不成立，即hashCode相同，value值不一定相同。这种情况又被称为hash冲撞（冲突）,hashMap中对应的解决方式就是hashCode相同的对象再构造一个线性表。 public int hashCode() { int h = hash; if (h == 0 &amp;&amp; value.length &gt; 0) { char val[] = value; for (int i = 0; i &lt; value.length; i++) { h = 31 * h + val[i]; } hash = h; } return h; } 当然最好的 hashCode() 方法则是所有不同的对象都能有各自的 Hash码，同时注意在设计对应的 hashCode() 方法时， 无论何时，对同一个对象调用hashCode()都应该产生同样的值。如果在讲一个对象用put()添加进HashMap时产生一个hashCdoe值，而用get()取出时却产生了另一个hashCode值，那么就无法获取该对象了。所以如果你的hashCode方法依赖于对象中易变的数据，用户就要当心了，因为此数据发生变化时，hashCode()方法就会生成一个不同的散列码.---By Effective Java 参考博文：重写HashCode的内存变化过程以及两种重写hashCode方式的比较 参考博文：通用的Java hashCode重写方案 equals 和 == 参考博文：equals和==的区别小结 == == 比较的是变量(栈)内存中存放的对象的(堆)内存地址，用来判断两个对象的地址是否相同，即是否是指相同一个对象。比较的是真正意义上的指针操作。 equals() equals用来比较的是两个对象的内容是否相等，由于所有的类都是继承自java.lang.Object类的，所以适用于所有对象，如果没有对该方法进行覆盖的话，调用的仍然是Object类中的方法，而Object中的equals方法返回的却是==的判断。 默认的 equals() 和 String 中的 equals() public boolean equals(Object obj) { return (this == obj); } public boolean equals(Object anObject) { if (this == anObject) { return true; } if (anObject instanceof String) { String anotherString = (String)anObject; int n = count; if (n == anotherString.count) { char v1[] = value; char v2[] = anotherString.value; int i = offset; int j = anotherString.offset; while (n-- != 0) { if (v1[i++] != v2[j++]) return false; } return true; } } return false; } 对于自定义的对象，当重写 equals() 方法时需要对应的重写 hashCode() 方法 1、如果两个对象相同（即用equals比较返回true），那么它们的hashCode值一定要相同； 2、如果两个对象的hashCode相同，它们并不一定相同(即用equals比较返回false) 考虑到 Java 的集合框架中有很多关于 Hash 表的相关实现。hashCode() 的效率相对于复杂的业务逻辑对应的 equals() 方法而言是要高得多的。所以在相关数据结构中采用先 hash 的方式再利用 equals() 进行相关比较能够更好的提高性能，同时保证 hash 表中存放的元素不相同。 应用场景 1、原生类型如:int/char/boolean等使用 == 进行比较，自定义的对象使用 equals() 2、== 返回true如果两个引用指向相同的对象，equals() 的返回结果依赖于具体业务实现 3、字符串的对比使用 equals() 代替 == 操作符 clone 参考博文：详解Java中的clone方法 -- 原型模式 参考博文：java之clone方法的使用 浅拷贝 和 深拷贝 的区别此处不再赘述。其实是 引用复制 和 内存复制的区别 用法：实现对应的Cloneable接口，重写对应 clone() 方法，并调用父类 super.clone() 注意：完全意义上的深拷贝几乎是不存在的如果在拷贝一个对象时，要想让这个拷贝的对象和源对象完全彼此独立，那么在引用链上的每一级对象都要被显式的拷贝。所以创建彻底的深拷贝是非常麻烦的，尤其是在引用关系非常复杂的情况下， 或者在引用链的某一级上引用了一个第三方的对象，而这个对象没有实现clone方法， 那么在它之后的所有引用的对象都是被共享的。 toString 若自定义的对象未重写 toString() 方法，则会调用父类 Object 对应的 toString() 输出对应的 对象的地址. public String toString() { return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); } 调用 print相关函数时，会默认调用 toString() public void println(Object x) { String s = String.valueOf(x); synchronized (this) { print(s); newLine(); } } public static String valueOf(Object obj) { return (obj == null) ? &quot;null&quot; : obj.toString(); } Java程序从编写到运行的全过程： 简单描述 1、编写代码 2、编译，Java 文件 编译为 class 文件 3、类加载 ClassLoader为执行程序寻找和装载所需要的类。 4、字节码校验。对class文件的代码进行校验，保证代码的安全性。 5、解释（Interpreter）。解释器解释class文件转为机器码。 6、运行：由运行环境中的Runtime对代码进行运行 参考博文：简述Java 从代码到运行的全过程 参考博文：Java代码编译过程简述 之后会结合Java内存结构进行具体阐释。敬请期待 ","link":"https://blog.shunzi.tech/post/apply-intern-java/"},{"title":"事务和锁","content":" 此篇博文主要是结合例子介绍数据库事务。 同时介绍数据库相关锁协议。 后续会对相关实现机制和核心思想进行深入探讨。 事务的定义 事务（Transaction），一般是指要做的或所做的事情。在计算机术语中是指访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。事务(Transaction)是访问并可能更新数据库中各种数据项的一个程序执行单元(unit)。 事务通常由高级数据库操纵语言或编程语言（如SQL，C++或Java）书写的用户程序的执行所引起，并用形如begin transaction和end transaction语句（或函数调用）来界定。事务由事务开始(begin transaction)和事务结束(end transaction)之间执行的全体操作组成。--来自百度百科 简而言之： 事务（Transaction）是并发控制的基本单位。 所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 举个例子：银行转账问题中，账户A向账户B转账，对应的会有两个操作，账户A减去对应的转账金额，账户B增加对应的转账金额。这两个操作要么都执行要么都不执行，此时应该把这两个操作看做一个事务，从而保证数据一致性。 事务的特点 ACID 原子性（Atomicity）：一个事务是一个不可分割的工作单位，事务中包括的诸操作要么都做，要么都不做。 一致性（Consistency）：事务必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 隔离性（Isolation）：一个事务的执行不能被其他事务干扰。即一个事务内部的操作及使用的数据对并发的其他事务是隔离的，并发执行的各个事务之间不能互相干扰。 持久性（Durability）：持久性也称永久性（permanence），指一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。接下来的其他操作或故障不应该对其有任何影响。 事务对应的语句 BEGIN TRANSACTION 开始事务 COMMIT TRANSACTION 提交事务 ROLLBACK TRANSACTION 回滚事务 事务并发控制 事务不考虑隔离性引发的问题 脏读：此种异常时因为一个事务读取了另一个事务修改了但是未提交的数据，当修改的事务进行回滚操作时将造成读取事务异常。 不可重复读：在一个事务内读取表中的某一行数据，多次读取结果不同。（一个事务读取到了另外一个事务提交的数据） 幻读（虚读）：指在一个事务内读取到了别的事务插入的数据，导致前后读取不一致。例如读整个表，即表的行数，例如第一次读某个表有3条记录，第二次读该表又有4条记录 （和不可重复读的不同：不可重复读针对的是数据的值，幻读针对的是数据的数量） 数据库事务隔离级别（SQL标准定义） READ UNCOMMITTED（未提交读）：事务中的修改，即使没有提交，其他事务也可以看得到。很容易导致脏读等众多问题，如无必要，很少使用 READ COMMITTED（提交读）：大多数数据库系统默认的隔离级别（除Mysql等）。这种隔离级别就是一个事务的开始，只能看到已经完成的事务的结果，正在执行的，是无法被其他事务看到的。这种级别会出现读取旧数据的现象，即不可重复读的问题。 REPEATABLE READ（可重复读）：解决了脏读的问题，该级别保证了每行的记录的结果是一致的，也就是上面说的读了旧数据的问题，但是却无法解决另一个问题，幻行，顾名思义就是突然蹦出来的行数据。指的就是某个事务在读取某个范围的数据，但是另一个事务又向这个范围的数据去插入数据，导致多次读取的时候，数据的行数不一致。即幻读。--MYSQL默认隔离级别 SERIALIZABLE（可串行化）：最高的隔离级别，它通过强制事务串行执行（注意是串行），避免了前面的幻读情况，由于他大量加上锁，导致大量的请求超时，因此性能会比较底下，在特别需要数据一致性且并发量不需要那么大的时候才可能考虑这个隔离级别 隔离级别 脏读可能性 不可重复读可能性 幻读可能性 加锁读 READ UNCOMMITTED Yes Yes Yes No READ COMMITTED No Yes Yes No REPEATABLE READ No No Yes No SERIALIZABLE No No No Yes 数据库锁 数据库锁的基本类型： X锁：exclusive 用于写操作 - 某数据对象在没有加任何锁的情况下，一个事务可以对其加X锁，而其他事务就不能对其再加任何锁 S锁：share 用于读操作 - 一个事务对某数据对象加了S 锁后，其他事务就不能对其加X锁，但可以加S锁 U锁：update - 事务要更新数据对象时，先申请该对象的U 锁。对象加了U锁，允许其他事务对它加S锁。在最后写入时，再申请将U锁升级为X锁。不必在全过程中加X 不同级别的加锁协议 一级封锁协议（脏数据、不可重复读） 任一事务在写某数据前，必须对其加上X锁，该事务结束后才释放。不采用S锁，读数据不用加锁。 事务结束包括正常结束（COMMIT）和非正常结束（ROLLBACK）。 二级封锁协议（不可重复读） 满足一级封锁协议，且任一事务在读取某数据前，必须对其加上S锁，读完后 就释放 三级封锁协议（） 满足一级封锁协议，且任一事务在读取某数据前，必须对其加上S锁，事务结束后 释放锁 对应的我们便可以看到 隔离级别和加锁协议之间的关系： 一级封锁协议 -&gt; READ UNCOMMITTED 二级封锁协议 -&gt; READ COMMITTED 三级封锁协议 -&gt; REPEATABLE READ 其他加锁协议 两阶段加锁协议： 整个事务分为两个阶段，前一个阶段为加锁，后一个阶段为解锁。在加锁阶段，事务只能加锁，也可以操作数据，但不能解锁，直到事务释放第一个锁，就进入解锁阶段，此过程中事务只能解锁，也可以操作数据，不能再加锁。两阶段锁协议使得事务具有较高的并发度，因为解锁不必发生在事务结尾。它的不足是没有解决死锁的问题，因为它在加锁阶段没有顺序要求。如两个事务分别申请了A, B锁，接着又申请对方的锁，此时进入死锁状态。 定理：若所有事务均遵守两段锁协议，则这些事务的所有交叉调度都是可串行化的。 多粒度加锁协议 行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。只在存储引擎层实现 页级锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般 表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。 ","link":"https://blog.shunzi.tech/post/transaction-and-lock/"},{"title":"常见算法之寻找K大数","content":" 此篇博文主要是对面试过程中的算法题进行记录。 该题作为面试常见题型，同时拥有多种实现方式。 后续会对相关算法进行具体代码实现。 题目描述： 有很多个无序的数，怎么选出其中最大的若干个数？即，从n个数中选出最大的K个数。 解法一: 元素数量不多的情况下，使用对应的排序算法来对数据进行排序，再输出最大的前K个数。 两种排序（部分排序，全部排序） 全部排序：( 时间复杂度均为O(Nlog2N) ) 快速排序： 选择一个基准元素,通常选择第一个元素或者最后一个元素,通过一趟扫描，将待排序列分成两部分,一部分比基准元素小,一部分大于等于基准元素,此时基准元素在其排好序后的正确位置,然后再用同样的方法递归地排序划分的两部分。 堆排序： 创建大顶堆或小顶堆，然后将第一个和最后一个节点进行交换，输出最后一个节点，再调整堆 存在的问题： 即便是K=1的情况，上面的算法复杂度仍然是O(Nlog2N)，而显然，我们可以通过n-1此的比较和交换得到结果，不需要对整个数组进行排序。 要避免做后面N-K个数的排序，可以使用部分排序算法， 部分排序：把n个数中的前K个数排序出来，复杂度是O(N*K)。 选择排序： 假设第一个是最小的数，遍历该数之后的数比较大小，若更小，则min指向该数，循环遍历下来之后每一次都能找到未排序数组中的最小值。 冒泡排序： 每次比较相邻的两个数，根据大小关系进行交换。一趟下来，最大值位于最后 结论: 哪一个更好呢？O(nlog2n) 和O(n*K)？这取决于K的大小，在K&lt;log2n的情况下，可以选择部分排序。 解法二 利用快速排序的思想 快排回顾： 快排中的每一步，都是将数据分为两组，其中一组的任何数都小于另一组中的任何数，不断地对数据进行分割直到不能再分即完成排序。 解决方案： 假设n个数存储在数组S中，从S中找出一个元素X，它把数组分为比它大的和比它小的两部分，假设比它大的一部分数组是Smax，比它小的部分是Smin。 此时两种情况： Smax中元素不足K个，说明Smax中的所有数和Smin中最大的K-|Smax|个元素就是数组S中最大的K个数；对于Smin则递归进行对应的操作。 Smax中元素的个数大于或等于K，则需要返回Smax中最大的K个元素。 结论： 递归下去，问题的规模不断地变小，平均时间复杂度O(n * log2K)。 解法三（部分堆排序实现） 前两个解法都存在统一的问题，需要对数据访问多次，只能适用于数据量较小的情况。若数据量较大，则不能全部装入内存，所以要求尽可能少地遍历数据。 最好的情况就是K个数，则该K个数则为最大的K个数。如果有K+1个数，则需要比较原K个数中最小的数min和后新加入的一个数temp。若temp&gt;min，则temp替换min。 对于第K+2和K+3个数则与之同理。 步骤概括：所耗费的时间复杂度为O(n * K)。 判断前一步是否有交换？ 若有寻找出K个值中最小的值，若无保持上一次的最小值 比较要新加入的值和选出的最小值 若temp&gt;min,则替换对应的min，否则保持 对于寻找最小值以及替换的问题 可以利用堆排序来实现 堆排序实现： 可以使用容量为K的最小堆来存储最大的K个数、最小堆的堆顶就是最小值，考虑加入新的元素时，则只需要比较堆顶和新加入的元素的大小，来对应的进行堆顶的替换。与此同时进行堆结构的调整。保证最小堆的性质。更新过程花费的时间复杂度为O(log2K)。 此时的时间复杂度则为O(n * log2K) 后续会用相关代码实现。敬请期待！ 😃 ","link":"https://blog.shunzi.tech/post/arithmetic-sort-exercise-one/"},{"title":"Redis数据类型","content":" 此篇博文主要是结合命令行工具介绍Redis数据类型。 Redis基础知识篇。 后续会对相关实现机制和核心思想进行深入探讨。 Redis支持五种数据类型：string（字符串），hash（哈希），list（列表），set（集合）及zset(sorted set：有序集合) string string是redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 string类型是二进制安全的。意思是redis的string可以包含任何数据。比如jpg图片或者序列化的对象 。 string类型是Redis最基本的数据类型，一个键最大能存储512MB。 127.0.0.1:6739 &gt; set name elvis OK 127.0.0.1:6739 &gt; get name &quot;elvis&quot; 127.0.0.1:6739 &gt; getset name shunzi &quot;elvis&quot; 127.0.0.1:6739 &gt; get name &quot;shunzi&quot; 127.0.0.1:6739 &gt; del name (interger) 1 127.0.0.1:6739 &gt; get name (nil) // 使用自增自减符，若变量不存在则初始化为0 127.0.0.1:6739 &gt; incr num (interger) 1 127.0.0.1:6739 &gt; get num &quot;1&quot; 127.0.0.1:6739 &gt; incr num (interger) 2 127.0.0.1:6739 &gt; get num &quot;2&quot; 127.0.0.1:6739 &gt; incr name (error) ERR value is not an integer or out of range 127.0.0.1:6739 &gt; decr num (interger) 1 127.0.0.1:6739 &gt; get num &quot;1&quot; 127.0.0.1:6739 &gt; decr num2 (interger) -1 // incryby decrby 127.0.0.1:6739 &gt; incrby num 5 (interger) 6 127.0.0.1:6739 &gt; incrby num3 5 (interger) 5 127.0.0.1:6739 &gt; decrby num 3 (interger) 3 127.0.0.1:6739 &gt; decrby num4 3 (interger) -3 // append返回拼接的字符串长度 127.0.0.1:6739 &gt; append num 5 (interger) 2 127.0.0.1:6739 &gt; get num &quot;35&quot; 127.0.0.1:6739 &gt; append num5 123 (interger) 3 127.0.0.1:6739 &gt; get num5 &quot;123&quot; Hash Redis hash 是一个键值(key=&gt;value)对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 每个 hash 可以存储 232 -1 键值对（40多亿）。 HSET key field value//设置键以及其中的属性（单个属性） HMSET key field value key value...//设置键以及其中的属性(多个属性) HGETALL key// 获取该键对应的所有属性名和属性值 HGET key field //获取该健对应的属性对应的值 HMGET key field1 field2//获取该键对应的多个属性的值 HDEL key field1 field2//删除对应的键对应的属性 del key//删除键对应的value HEXISTS key field//判断键对应的对象中是否存在field属性,存在返回1 否返回0 HLEN key//获取键对应的属性个数 hkeys key//获取键对应的属性的属性名 hvals key//获取键对应的属性 的属性值 127.0.0.1:6379&gt; HMSET user:1 username zhangjianshun password 123456 age 18 college UESTC OK 127.0.0.1:6379&gt; HGETALL user:1 1) &quot;username&quot; 2) &quot;zhnagjianshun&quot; 3) &quot;password&quot; 4) &quot;123456&quot; 5) &quot;age&quot; 6) &quot;18&quot; 7) &quot;college&quot; 8) &quot;UESTC&quot; 127.0.0.1:6379&gt; HGET user:1 college &quot;UESTC&quot; 127.0.0.1:6379&gt; hincrby user:1 age 2 (interger) 20 127.0.0.1:6379&gt; hexists user:1 username (interger) 1 List Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 列表最多可存储 232 - 1 元素 (4294967295, 每个列表可存储40多亿)。 lpush key value1 value2 ...//从左开始添加 rpush key value1 value2 ...//从右开始添加 lrange key index1 index2//从左开始查看index1 ~ index2,若index为负数，则倒数对应的个数 lpop key//左端弹出，之后list中该元素 rpop key//右端弹出 llen key//获取元素个数 lpushx key value//仅仅当key存在时才进行插入，同理rpushx lremo key count value//删除指定count个值为value的元素，若count&lt;0，删除顺序反向，若count=0，则删除所有的对应的value lset key index value//设定对应的角标的值，index若为-1，则为末尾 linsert key before/after value newValue//在指定元素前后插入新元素 常用于消息队列的使用场景 rpoplpush key1 key2//弹出key1的一个元素压入key2 redis 127.0.0.1:6379&gt; lpush database redis (integer) 1 redis 127.0.0.1:6379&gt; lpush database mongodb (integer) 2 redis 127.0.0.1:6379&gt; lpush database rabitmq (integer) 3 redis 127.0.0.1:6379&gt; lrange database 0 10 1) &quot;rabitmq&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot; redis 127.0.0.1:6379&gt; Set Redis的Set是string类型的无序集合。不允许出现重复的元素， 集合是通过哈希表实现的，所以添加，删除，查找的复杂度都是O(1)。 集合中最大的成员数为 232 - 1(4294967295, 每个集合可存储40多亿个成员)。 sadd key member// 添加一个string元素到,key对应的set集合中，成功返回1,如果元素已经在集合中返回0,key对应的set不存在返回错误。 srem key value1 value2//删除key对应的set中指定的value smembers key//列举key对应的set中的所有value sismember key value//判断在该集合中是否存在该元素，返回布尔值 sdiff key1 key2//两个集合的差集运算 sinter key1 key2//两个集合的交集运算 sunion key1 key2//两个集合的并集运算，自动去重 scard key//集合对应的元素个数 srandmember key//随机返回集合中的成员 sdiffstore key1 key2 key3//求交集之后存储到kwy3，同理差集并集 应用场景 获取唯一性数据 用于维护数据对象之间的关联关系（集合运算） redis 127.0.0.1:6379&gt; sadd runoob redis (integer) 1 redis 127.0.0.1:6379&gt; sadd runoob mongodb (integer) 1 redis 127.0.0.1:6379&gt; sadd runoob rabitmq (integer) 1 redis 127.0.0.1:6379&gt; sadd runoob rabitmq (integer) 0 redis 127.0.0.1:6379&gt; smembers runoob 1) &quot;rabitmq&quot; 2) &quot;mongodb&quot; 3) &quot;redis&quot; zset (sorted set：有序集合) Redis zset 和 set 一样也是string类型元素的集合,且不允许重复的成员。 不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 zset的成员是唯一的,但分数(score)却可以重复。 zadd key score member//添加元素到集合，元素在集合中存在则更新对应score zscore key member//获取对应的分数 zcard key//获取对应的元素个数 zrem key mem1 mem2//删除集合中对应的元素 zrange key index1 index2//显示对应的下表范围内的元素，可使用参数withscores来决定是否显示对应的参数。默认由小到大排序 zrevrange key index1 index2//从大到小 zrenrangebyrank key index1 index2//根据顺序删除指定范围的元素 zrenrangebyscore key index1 index2//根据分数范围删除指定的元素 zrangebyscore key1 index1 index2 withscores//显示分数在指定范围内的元素，还可以使用limit index1 index2限定返回的个数 zincrby key score member//指定成员的分数加分score zcount key score1 score2//显示指定的分数范围内的元素个数 应用场景： 积分排行榜 构建数据索引 redis 127.0.0.1:6379&gt; zadd runoob 0 redis (integer) 1 redis 127.0.0.1:6379&gt; zadd runoob 0 mongodb (integer) 1 redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq (integer) 1 redis 127.0.0.1:6379&gt; zadd runoob 0 rabitmq (integer) 0 redis 127.0.0.1:6379&gt; ZRANGEBYSCORE runoob 0 1000 1) &quot;redis&quot; 2) &quot;mongodb&quot; 3) &quot;rabitmq&quot; ","link":"https://blog.shunzi.tech/post/redis-data-structure-two/"},{"title":"Redis安装与配置","content":" 此篇博文主要是记录Redis安装过程。 对于其他相关介绍，可参考相应博客。 可能会因为版本差异，步骤稍有不同，但都大同小异。 1、为什么使用Redis？ MySQL为代表的关系型数据库在web2.0时代出现一系列效率低下，可扩展性差等问题，从而产生了NoSQL NoSQL介绍--菜鸟教程 而Redis则是NoSQL中的键值对数据库中最广泛应用的一种。除此以为还有MongoDB(文档存储数据库) Redis相关命令 菜鸟教程Redis命令 Redis通用命令 keys *//查看所有的key keys my?//通配符查看 del key1 key2 key3//删除key exists key//判断是否存在 rename key1 key2//更改键的名字 expire key num//设置过期时间，单位为s ttl key//key剩余时间 type key//查看类型 Redis的安装启动 windows下 在redis的安装目录下使用 redis-server.exe redis.windows.conf命令启动Redis 如果设置了redis的安装路径为相应的系统环境变量，则可以省略路径,直接使用redis-server.exe,如果省略配置文件信息，对应的使用默认配置。 **注意：**如果启动时遇到了Creating Server TCP listening socket *:6379: bind:,则需要 &gt; redis-cli.exe &gt; shutdown &gt; exit &gt; redis-server.exe redis.windows.conf 指定redis对应的主机地址和端口号redis-cli.exe -h 127.0.0.1 -p 6379 可以进行如下操作则启动成功： 127.0.0.1:6739 &gt; set name elvis OK 127.0.0.1:6739 &gt; get name &quot;elvis&quot; linux下 安装： $ wget http://download.redis.io/releases/redis-4.0.2.tar.gz $ tar xzf redis-4.0.2.tar.gz $ cd redis-4.0.2 $ make//编译之后在安装目录下的src目录下会出现编译后的redis服务程序redis-server,还有用于测试的客户端程序redis-cl 启动：(注意要使用的redis.conf的位置) $ cd src $ ./redis-server ../redis.conf 服务器端运行redis-server时建议将配置文件中的daemonize为yes，即默认以后台程序方式运行。 服务器端运行client进行测试 $ ./redis-cli.exe $ set key xxx OK $ get key xxx Redis的配置 获取相关配置：CONFIG GET 配置名 注：可以使用通配符 * 获取全部配置 编辑配置： 方式一：修改对应conf配置文件 方式二：CONFIG SET 配置名 新的值 参数说明： 1. Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 daemonize no 2. 当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 3. 指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字 port 6379 4. 绑定的主机地址 bind 127.0.0.1 5.当 客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 300 6. 指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose loglevel verbose 7. 日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8. 设置数据库的数量，默认数据库为0，可以使用SELECT &lt;dbid&gt;命令在连接上指定数据库id databases 16 9. 指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save &lt;seconds&gt; &lt;changes&gt; Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 10. 指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11. 指定本地数据库文件名，默认值为dump.rdb dbfilename dump.rdb 12. 指定本地数据库存放目录 dir ./ 13. 设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步 slaveof &lt;masterip&gt; &lt;masterport&gt; 14. 当master服务设置了密码保护时，slav服务连接master的密码 masterauth &lt;master-password&gt; 15. 设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH &lt;password&gt;命令提供密码，默认关闭 requirepass foobared 16. 设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 128 17. 指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 maxmemory &lt;bytes&gt; 18. 指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no appendonly no 19. 指定更新日志文件名，默认为appendonly.aof appendfilename appendonly.aof 20. 指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快） always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全） everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21. 指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制） vm-enabled no 22. 虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 23. 将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0 vm-max-memory 0 24. Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值 vm-page-size 32 25. 设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，，在磁盘上每8个pages将消耗1byte的内存。 vm-pages 134217728 26. 设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4 vm-max-threads 4 27. 设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28. 指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29. 指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍） activerehashing yes 30. 指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf ","link":"https://blog.shunzi.tech/post/redis-start-one/"},{"title":"tinySpring学习笔记（一）-实现IOC容器","content":" tinySpring 是一个模仿Spring框架的迷你项目。 通过理解和实现相关代码，掌握Spring的核心思想。 后续会对Spring本身的一些实现再做深入探讨。 为了更好的理解Spring的核心思想（IOC和AOP），开始阅读并总结体会 code4craft 的 tinySpring模仿Spring的微型项目，并做一些笔记记录。 step1-最基本的容器 git checkout step-1-container-register-and-get IOC最基本的组成：BeanFactory 和 Bean（容器和实体） 实现步骤： 1、初始化beanFactory（容器） /** * BeanFactory 主要包含一个存储 Bean 的 ConcurrentHashMap 对象，以及对应的注入 Bean 和获取 Bean 的方法 * Fields: private Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String, BeanDefinition&gt;(); * Methods: public Object getBean(String name) * public void registerBeanDefinition(String name, BeanDefinition beanDefinition) */ BeanFactory beanFactory = new BeanFactory(); 2、向容器中注入Bean（实体） /** * 为了保存一些额外的元信息，使用 BeanDefinition 来包装对应的 Bean * Fields： private Object bean; * Extra info（Class，ClassName） * Method: public BeanDefinition(Object bean); * 属性对应的 set 和 get方法 */ BeanDefinition beanDefinition = new BeanDefinition(new HelloWorldService()); //注入时，则是利用 HashMap 存放相应的键值对 beanFactory.registerBeanDefinition(&quot;helloWorldService&quot;, beanDefinition); 3、获取对应的Bean实例 // 获取 Bean 时对应的利用 HashMap 从中取出一开始注入的 Bean HelloWorldService helloWorldService = (HelloWorldService) beanFactory.getBean(&quot;helloWorldService&quot;); helloWorldService.helloWorld(); step2-将Bean创建放入工厂 git checkout step-2-abstract-beanfactory-and-do-bean-initilizing-in-it 实际中需要使用容器来对所有的Bean进行统一的管理，自然对于Bean的整个生命周期都需要容器进行相应的干预。所以对于Bean的创建和初始化需要放入容器中进行处理，同时考虑到程序的扩展性，需要使用Extra Interface将BeanFactory更改为接口并提供相应的实现。 对于 BeanFactory 的继承关系 /** * Interface BeanFactory -&gt; * Abstract class AbstractBeanFactory -&gt; * class AutowireCapableBeanFactory */ public interface BeanFactory { Object getBean(String name); void registerBeanDefinition(String name, BeanDefinition beanDefinition); } public abstract class AbstractBeanFactory implements BeanFactory { private Map&lt;String, BeanDefinition&gt; beanDefinitionMap = new ConcurrentHashMap&lt;String, BeanDefinition&gt;(); @Override public Object getBean(String name) { // 先获取对应的包装对象，再获取对应的实体 return beanDefinitionMap.get(name).getBean(); } @Override public void registerBeanDefinition(String name, BeanDefinition beanDefinition) { // 注入Bean之前先创建对应Bean Object bean = doCreateBean(beanDefinition); // 创建Bean之后并进行相应的包装 beanDefinition.setBean(bean); // 注入包装后的Bean对象 beanDefinitionMap.put(name, beanDefinition); } /** * 初始化bean （仅提供抽象方法声明，不提供具体实现） * @param beanDefinition * @return */ protected abstract Object doCreateBean(BeanDefinition beanDefinition); } public class AutowireCapableBeanFactory extends AbstractBeanFactory { @Override protected Object doCreateBean(BeanDefinition beanDefinition) { try { // 利用包装类对应的 class 对象，再利用 反射 创建对象 Object bean = beanDefinition.getBeanClass().newInstance(); return bean; } catch (InstantiationException e) { e.printStackTrace(); } catch (IllegalAccessException e) { e.printStackTrace(); } return null; } } 实现步骤：（相比step1区别仅在于 初始化BeanFactory和注入Bean） 1、初始化BeanFactory // 创建能够 使用容器创建Bean实例 的容器对象 BeanFactory beanFactory = new AutowireCapableBeanFactory(); 2、注入Bean /** * BeanDefinition: * Fields: Object bean; * String beanClassName; * Class beanclass; * 其中在 beanClassName 的 set 方法进行注入时， * 利用反射和全类名创建出对应的 Class 对象并赋予给 beanClass属性 * public void setBeanClassName(String beanClassName) { * this.beanClassName = beanClassName; * try { * this.beanClass = Class.forName(beanClassName); * } catch (ClassNotFoundException e) { * e.printStackTrace(); * } * } */ BeanDefinition beanDefinition = new BeanDefinition(); // 通过使用 包装类BeanDefinition的全类名className 为容器使用反射创建对象做准备 beanDefinition.setBeanClassName(&quot;us.codecraft.tinyioc.HelloWorldService&quot;); // 利用反射进行对象的创建并注入 beanFactory.registerBeanDefinition(&quot;helloWorldService&quot;, beanDefinition); step3-为Bean注入属性 git checkout step-3-inject-bean-with-property 将属性注入信息保存成PropertyValue对象，并且保存到BeanDefination，初始化Bean的时候，可以根据PropertyValue来进行Bean属性的注入，Spring本身是用了setter来注入，该demo使用Field注入。 实现步骤： 1、初始化Factory BeanFactory beanFactory = new AutowireCapableBeanFactory(); 2、Bean定义 // BeanDefinition 定义中添加对应的 PropertyValues 属性 BeanDefinition beanDefinition = new BeanDefinition(); // 注入全类名便于之后利用反射创建相应的对象 beanDefinition.setBeanClassName(&quot;us.codecraft.tinyioc.HelloWorldService&quot;); 3、设置属性 /** * PropertyValue: 属性(键值对) * Fields： final String name; * final Object value; * * PropertyValues：保存对象的全部属性 * Fields： final List&lt;PropertyValue&gt; propertyValueList; * Methods: void addPropertyValue(PropertyValue p); */ PropertyValues propertyValues = new PropertyValues(); propertyValues.addPropertyValue(new PropertyValue(&quot;text&quot;, &quot;Hello World!&quot;)); beanDefinition.setPropertyValues(propertyValues); 4、生成Bean /** * 1、利用反射创建相应的Bean对象 * 2、向创建的对象中注入对应的属性（PropertyValues） * protected void applyPropertyValues(Object bean, BeanDefinition mbd) throws Exception { * // 遍历 PropertyValues中对应的属性 * for (PropertyValue propertyValue : mbd.getPropertyValues().getPropertyValues()) { * // 获取对象中指定属性名（ PropertyValue 的 key）对应的属性 * Field declaredField = bean.getClass().getDeclaredField(propertyValue.getName()); * // 设置Access使得私有属性能够通过反射访问 * declaredField.setAccessible(true); * // 向实体对象对应的属性字段注入属性值（value） 类似于setter * declaredField.set(bean, propertyValue.getValue()); * } * } */ beanFactory.registerBeanDefinition(&quot;helloWorldService&quot;, beanDefinition); 5、获取 Bean HelloWorldService helloWorldService = (HelloWorldService) beanFactory.getBean(&quot;helloWorldService&quot;); helloWorldService.helloWorld(); step4-读取xml配置来初始化bean git checkout step-4-config-beanfactory-with-xml 定义BeanDefinitionReader接口，提供void loadBeanDefinitions(String location)方法从指定位置获取相应的配置来进行初始化，定义抽象类AbstractBeanDefinitionReader,其中一个实现是XmlBeanDefinitionReader /** * interface BeanDefinitionReader -&gt; * abstract class AbstractBeanDefinitionReader -&gt; * class XmlBeanDefinitionReader */ public interface BeanDefinitionReader { void loadBeanDefinitions(String location) throws Exception; } public abstract class AbstractBeanDefinitionReader implements BeanDefinitionReader { // 保存从配置文件中加载的所有的 beanDefinition 对象 private Map&lt;String,BeanDefinition&gt; registry; /** * 依赖 ResourceLoader，该类又依赖 UrlResource * UrlResource 继承自 Spring 自带的 Resource 内部资源定位接口 * Resource 接口，标识一个外部资源。通过 getInputStream() 方法 获取资源的输入流 。 * UrlResource 实现 Resource 接口的资源类，通过 URL 获取资源。 * ResourceLoader 资源加载类。通过 getResource(String) 方法获取一个 Resource 对象，是获取 Resource 的主要途径. */ private ResourceLoader resourceLoader; protected AbstractBeanDefinitionReader(ResourceLoader resourceLoader){ this.registry = new HashMap&lt;String, BeanDefinition&gt;(); this.resourceLoader = resourceLoader; } public Map&lt;String, BeanDefinition&gt; getRegistry() { return registry; } public ResourceLoader getResourceLoader() { return resourceLoader; } } public class XmlBeanDefinitionReader extends AbstractBeanDefinitionReader{ public XmlBeanDefinitionReader(ResourceLoader resourceLoader) { super(resourceLoader); } // 从配置文件中加载 Bean 的相关信息 @Override public void loadBeanDefinitions(String location) throws Exception { // 利用文件路径创建相应的输入流 InputStream inputStream = getResourceLoader().getResource(location).getInputStream(); doLoadBeanDefinitions(inputStream); } protected void doLoadBeanDefinitions(InputStream inputStream) throws Exception { DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance(); DocumentBuilder docBuilder = factory.newDocumentBuilder(); // 输入流转换成对应的 Document 对象便于获取对应的元素 Document doc = docBuilder.parse(inputStream); // 解析bean registerBeanDefinitions(doc); inputStream.close(); } public void registerBeanDefinitions(Document doc) { // 获取文件中包含的元素 &lt;beans&gt;&lt;/beans&gt; Element root = doc.getDocumentElement(); parseBeanDefinitions(root); } protected void parseBeanDefinitions(Element root) { // 获取元素包含的子节点链 &lt;bean&gt;&lt;/bean&gt; 链 NodeList nl = root.getChildNodes(); for (int i = 0; i &lt; nl.getLength(); i++) { // 获取子节点链上对应的子节点 单个&lt;bean&gt;&lt;/bean&gt; Node node = nl.item(i); if (node instanceof Element) { Element ele = (Element) node; // 将节点强转为 Element 对象并进行解析 processBeanDefinition(ele); } } } protected void processBeanDefinition(Element ele) { // 获取子节点对应的属性（name，class）来对应 Bean // &lt;bean name=&quot;&quot; class=&quot;&quot;&gt;&lt;/bean&gt; String name = ele.getAttribute(&quot;name&quot;); String className = ele.getAttribute(&quot;class&quot;); // 创建与之对应的 BeanDefinition ，并设置相应的属性 BeanDefinition beanDefinition = new BeanDefinition(); // 将节点包含的 Bean相关的属性信息注入创建的 BeanDefinition 中。 processProperty(ele,beanDefinition); beanDefinition.setBeanClassName(className); // 统一管理（HashMap）通过配置文件加载的 beanDefinition 对象 getRegistry().put(name, beanDefinition); } private void processProperty(Element ele,BeanDefinition beanDefinition) { // 获取元素对应的 Property 节点 // &lt;bean&gt;&lt;property name=&quot;&quot; value=&quot;&quot;&gt;&lt;/property&gt;&lt;/bean&gt; NodeList propertyNode = ele.getElementsByTagName(&quot;property&quot;); for (int i = 0; i &lt; propertyNode.getLength(); i++) { // 遍历节点并取出节点对应的 key-value，添加到 BeanDefinition 对应的属性中 Node node = propertyNode.item(i); if (node instanceof Element) { Element propertyEle = (Element) node; String name = propertyEle.getAttribute(&quot;name&quot;); String value = propertyEle.getAttribute(&quot;value&quot;); beanDefinition.getPropertyValues().addPropertyValue(new PropertyValue(name,value)); } } } } 实现步骤： 1、读取配置 XmlBeanDefinitionReader xmlBeanDefinitionReader = new XmlBeanDefinitionReader(new ResourceLoader()); xmlBeanDefinitionReader.loadBeanDefinitions(&quot;tinyioc.xml&quot;); 2、初始化BeanFactory并注册bean BeanFactory beanFactory = new AutowireCapableBeanFactory(); // 通过Map自带的Entry实体属性遍历Map中存放的实体集合 for (Map.Entry&lt;String, BeanDefinition&gt; beanDefinitionEntry : xmlBeanDefinitionReader.getRegistry().entrySet()) { beanFactory.registerBeanDefinition(beanDefinitionEntry.getKey(), beanDefinitionEntry.getValue()); } 3、获取bean HelloWorldService helloWorldService = (HelloWorldService) beanFactory.getBean(&quot;helloWorldService&quot;); helloWorldService.helloWorld(); step5-为bean注入bean（处理Bean之间的依赖） git checkout step-5-inject-bean-to-bean 通过定义一个BeanReference，表示这个属性是对另一个Bean的引用，在读取xml配置文件的时候进行初始化，并在初始化bean的时候，进行解析和真实bean的注入。 class AutowireCapableBeanFactory 中： protected void applyPropertyValues(Object bean, BeanDefinition mbd) throws Exception { for (PropertyValue propertyValue : mbd.getPropertyValues().getPropertyValues()) { // 获取对象中指定属性名（ PropertyValue 的 key）对应的属性 Field declaredField = bean.getClass().getDeclaredField(propertyValue.getName()); declaredField.setAccessible(true); Object value = propertyValue.getValue(); // 若该属性的值对应的是一个Bean，是Bean对Bean的引用 if (value instanceof BeanReference) { // 原类型为 Object，需要强转为 BeanReference BeanReference beanReference = (BeanReference) value; // 再利用BeanReference存储的name属性获取到对应的bean（包装类 -&gt; 实体类） value = getBean(beanReference.getName()); } declaredField.set(bean, value); } } /** * 对于 getBean(),采用&quot;lazy-init&quot;的方式（延迟加载）。 * 避免两个循环依赖的Bean在创建时陷入死锁。 * 在注入Bean的时候，先尝试获取，获取不到再创建，故总是先创建后注入。 */ public Object getBean(String name) throws Exception { BeanDefinition beanDefinition = beanDefinitionMap.get(name); if (beanDefinition == null) { throw new IllegalArgumentException(&quot;No bean named &quot; + name + &quot; is defined&quot;); } Object bean = beanDefinition.getBean(); if (bean == null) { bean = doCreateBean(beanDefinition); } return bean; } 实现步骤： 1、读取配置 XmlBeanDefinitionReader xmlBeanDefinitionReader = new XmlBeanDefinitionReader(new ResourceLoader()); xmlBeanDefinitionReader.loadBeanDefinitions(&quot;tinyioc.xml&quot;); 2、初始化BeanFactory并注册Bean AbstractBeanFactory beanFactory = new AutowireCapableBeanFactory(); for (Map.Entry&lt;String, BeanDefinition&gt; beanDefinitionEntry : xmlBeanDefinitionReader.getRegistry().entrySet()) { beanFactory.registerBeanDefinition(beanDefinitionEntry.getKey(), beanDefinitionEntry.getValue()); } 3、初始化Bean /** * class AbstractBeanFactory: * Fileds: + final List&lt;String&gt; beanDefinitionNames 属性 * Methods：在registerBeanDefinition中对应的添加 beanDefinitionNames.add(name); * * public void preInstantiateSingletons() throws Exception { * // 使用迭代器依次创建初始化对应 Bean * for (Iterator it = this.beanDefinitionNames.iterator(); it.hasNext();) { * String beanName = (String) it.next(); * getBean(beanName); * } * } * * 确保所有没有设置lazy-init的单例被实例化 */ beanFactory.preInstantiateSingletons(); 4、获取Bean HelloWorldService helloWorldService = (HelloWorldService) beanFactory.getBean(&quot;helloWorldService&quot;); helloWorldService.helloWorld(); step6-ApplicationContext git checkout step-6-invite-application-context 为了便于使用BeanFactory，引入ApplicationContext接口，并在AbstractApplicationContext的refresh()方法中进行bean的初始化工作。 /** * class BeanFactory -&gt; extends * interface ApplicationContext -&gt; implements * abstract class AbstractApplicationContext -&gt; extends * class ClassPathXmlApplicationContext */ public interface ApplicationContext extends BeanFactory { } public abstract class AbstractApplicationContext implements ApplicationContext { // 依赖抽象容器 protected AbstractBeanFactory beanFactory; public AbstractApplicationContext(AbstractBeanFactory beanFactory) { this.beanFactory = beanFactory; } //声明方法 初始化所有的 Bean public void refresh() throws Exception{ } // 调用容器的获取Bean方法 @Override public Object getBean(String name) throws Exception { return beanFactory.getBean(name); } } public class ClassPathXmlApplicationContext extends AbstractApplicationContext { private String configLocation; // 默认使用自动装配容器 public ClassPathXmlApplicationContext(String configLocation) throws Exception { this(configLocation, new AutowireCapableBeanFactory()); } // 根据 配置文件路径 容器类型 构造对应的 ApplicationContext public ClassPathXmlApplicationContext(String configLocation, AbstractBeanFactory beanFactory) throws Exception { super(beanFactory); this.configLocation = configLocation; refresh(); } // 将读取配置文件以及将读取到的属性值注入Bean的方法进行封装 @Override public void refresh() throws Exception { // 读取配置文件 封装原来的实现步骤一 XmlBeanDefinitionReader xmlBeanDefinitionReader = new XmlBeanDefinitionReader(new ResourceLoader()); xmlBeanDefinitionReader.loadBeanDefinitions(configLocation); // 将获得的信息注入Bean 封装原来的实现步骤二 for (Map.Entry&lt;String, BeanDefinition&gt; beanDefinitionEntry : xmlBeanDefinitionReader.getRegistry().entrySet()) { beanFactory.registerBeanDefinition(beanDefinitionEntry.getKey(), beanDefinitionEntry.getValue()); } } } 实现步骤： 1、初始化ApplicationContext（加载配置文件并初始化 Bean） ApplicationContext applicationContext = new ClassPathXmlApplicationContext(&quot;tinyioc.xml&quot;); 2、获取对应的 Bean 并执行相关方法 HelloWorldService helloWorldService = (HelloWorldService) applicationContext.getBean(&quot;helloWorldService&quot;); helloWorldService.helloWorld(); ","link":"https://blog.shunzi.tech/post/tinySpring-learnOne-IOC/"},{"title":"Servlet常用用法（二）-文件上传下载","content":" 此篇博文主要是结合JavaEE基础知识了解原理。 提供常见的例子实现来介绍现代JavaWeb框架的本质。 后续会结合相关框架实现进行深入探讨。 文件上传： 原理（Servlet） 通过使用文件上传下载组件，通过读取请求中的文件的输入流，在预先协商好的路径下根据对应的文件名创建相应的文件对象，将提交的文件对象FileItem写入对应的文件中。 在Servlet3.0中可以使用注解@MultiPartConfig将一个servlet标识为支持文件上传，将Multipart/form-data的POST请求封装为成Part，通过Part对上传的文件进行操作。对应的在Servlet处理请求时使用getPart()和getParts()方法来获取对应的文件对象（input对应的file标签） 框架对应的文件上传方法 SpringMVC中用的是MultipartFile来进行文件上传 所以我们首先要配置MultipartResolver:用于处理表单中的file &lt;!-- 文件上传组件 --&gt; &lt;bean class=&quot;org.springframework.web.multipart.commons.CommonsMultipartResolver&quot;&gt; &lt;property name=&quot;defaultEncoding&quot; value=&quot;UTF-8&quot;/&gt; &lt;property name=&quot;maxUploadSize&quot; value=&quot;500000&quot;/&gt; &lt;/bean&gt; 传输过程中一直使用MultipartFile，而在磁盘对应的文件读写中则使用java.io.file；对应MultipartFile有几个关键方法： String getContentType()//获取文件MIME类型 InputStream getInputStream()//后去文件流 String getName() //获取表单中文件组件的名字 String getOriginalFilename()//获取上传文件的原名 boolean isEmpty() //是否为空 void transferTo(File dest) //保存到一个目标文件中。 文件上传过程中可能存在文件名中文乱码的问题，常常是因为request的相关编码问题，为了能够统一解决，最好的方式时利用字符过滤器对所有请求进行整体的编码。其中Spring自带的字符过滤器就是比较好的选择。也可以使用自己定义的字符过滤器。 &lt;!--start Spring自带字符过滤器 --&gt; &lt;filter&gt; &lt;filter-name&gt;EncodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt; &lt;/filter&gt; &lt;filter-mapping&gt; &lt;filter-name&gt;EncodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt; &lt;/filter-mapping&gt; &lt;!--end Spring自带字符过滤器 --&gt; 文件下载： 原理（Servlet） 通过请求提交的指定的文件名，在服务器端根据路径和请求参数创建相应的文件对象和文件流，并利用字节数组将创建得到的文件流写入到response对应的输出流ServletOutputStream中，并指定response对应的一些头的属性，譬如ContentType中对应的MIME类型和Content-Disposition中对应的文件打开方式，然后随response发送到客户端。 在使用框架下载文件的过程中，通常和原生Servlet的处理方法保持一致，但往往需要构造RestFul风格的响应，故常常需要将下载文件作为一种特殊的响应类型（SteamRestResponse流类型），还是需要设置response对应的头的属性。 /** * 响应流返回结果 * * @param file 文件信息 * @param fileName 下载时显示的文件名称 * @return 空 * @throws IOException 将文件写入流中可能存在IO异常 */ public RestResponse streamResponse(FileInfo file, String fileName) throws IOException { HttpServletResponse response = getResponseContext(); //设置响应类型 response.setContentType(MediaType.APPLICATION_OCTET_STREAM_VALUE); response.setHeader(&quot;Accept-Ranges&quot;, &quot;bytes&quot;); response.setIntHeader(&quot;Accept-Length&quot;, Math.toIntExact(file.getSize()));// 解决乱码问题 // FireFox用ISO-8859-1 String encodedFileName; if (Browser.IE.getName() == getClientBrowser()){ encodedFileName = URLEncoder.encode(fileName, &quot;UTF-8&quot;); } else { encodedFileName = new String(fileName.getBytes(), &quot;ISO-8859-1&quot;); } response.setHeader(&quot;Content-Disposition&quot;,&quot;attachment; filename=\\&quot;&quot; + encodedFileName + &quot;\\&quot;; filename*=utf-8''&quot; + encodedFileName); Path path = Paths.get(file.getPath()); Files.copy(path, response.getOutputStream()); return successResponse(); } 文件上传下载过程中对应的文件路径的问题 由于开发和生产环境的不同，可能对应的需要保存文件上传下载的目录也有所不同，为了保证程序的的可移植性，常常需要采用相对路径来进行文件的相关存储，但又为了保证各个项目之间的数据能够独立，所以我们常把对应的文件存储在对应的项目目录下，并利用数据库持久化文件的相关数据，譬如MIME类型，文件大小，MD5摘要,存放的路径等等一系列信息。 为了实现上述功能，我们常常使用Context或Config来获取在web.xml中的定义的初始化参数（文件相对路径）。代码示例如下： web.xml &lt;!-- 通过使用ServletContext去获取 --&gt; &lt;context-param&gt; &lt;param-name&gt;uploadPath&lt;/param-name&gt; &lt;param-value&gt;data&lt;/param-value&gt; &lt;/context-param&gt; &lt;!-- 通过使用ServletConfig去获取 --&gt; &lt;servlet&gt; &lt;init-param&gt; &lt;param-name&gt;uploadPath&lt;/param-name&gt; &lt;param-value&gt;data&lt;/param-value&gt; &lt;/init-param&gt; &lt;/servlet&gt; servlet或controller中 ServletContext servletContext = req.getServletContext(); ServletConfig servletConfig = getServletConfig(); //使用servlet String uploadPath = servletContext.getRealPath(&quot;/&quot;) + File.separator + servletConfig.getInitParameter(&quot;uploadPath&quot;); //使用context String directory = servletContext.getRealPath(&quot;/&quot;) + File.separator + servletContext.getInitParameter(&quot;uploadPath&quot;); ","link":"https://blog.shunzi.tech/post/Servlet-apply-two/"},{"title":"Servlet常用用法（一）","content":" 该系列博文主要是介绍JavaEE基础知识。 结合当初项目实例，介绍几种Servlet的具体应用 后续会解析相关JavaWeb框架的具体实现原理。 1、Servlet编写过滤器(Filter) Servlet 过滤器 可以动态地拦截请求【请求预处理】和响应【响应后处理】，以变换或使用包含在请求或响应中的信息。可以将一个或多个 Servlet 过滤器附加到一个 Servlet 或一组 Servlet。Servlet 过滤器也可以附加到 JavaServer Pages (JSP) 文件和 HTML 页面。调用 Servlet 前调用所有附加的 Servlet 过滤器。从而实现一些特殊的功能。 请求预处理： 在HttpServletRequest到达 Servlet 之前，拦截客户的HttpServletRequest 。根据需要检查HttpServletRequest，也可以修改HttpServletRequest 头和数据。 响应后处理： 在HttpServletResponse到达客户端之前，拦截HttpServletResponse 。根据需要检查HttpServletResponse，也可以修改HttpServletResponse头和数据。 分类 身份验证过滤器（Authentication Filters）。【权限访问控制】 数据压缩过滤器（Data compression Filters）。【压缩响应信息】 加密过滤器（Encryption Filters）。 触发资源访问事件过滤器。 图像转换过滤器（Image Conversion Filters）。 日志记录和审核过滤器（Logging and Auditing Filters）。 MIME-TYPE 链过滤器（MIME-TYPE Chain Filters）。 标记化过滤器（Tokenizing Filters）。 XSL/T 过滤器（XSL/T Filters），转换 XML 内容。 Filter的生命周期 /*初始化，Filter的创建和销毁由WEB服务器负责。 web 应用程序启动时，web 服务器将创建Filter 的实例对象, 并调用其init方法，读取web.xml配置，完成对象的初始化功能， 从而为后续的用户请求作好拦截的准备工作（filter对象只会创建一次，init方法也只会执行一次）。 开发人员通过init方法的参数，可获得代表当前filter配置信息的FilterConfig对象。 */ public void init(FilterConfig filterConfig) throws ServletException; /* 拦截请求,完成实际的过滤操作。当客户请求访问与过滤器关联的URL的时候， Servlet过滤器将先执行doFilter方法。FilterChain参数用于访问后续过滤器。 */ public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; /* 销毁,Filter对象创建后会驻留在内存，当web应用移除或服务器停止时才销毁。 在Web容器卸载 Filter 对象之前被调用。该方法在Filter的生命周期中仅执行一次。 在这个方法中，可以释放过滤器使用的资源。 */ public void destroy(); 字符编码过滤器、静态资源过滤器、禁止缓存过滤器、非法字符过滤器 编码实现： JavaWeb中常用的Filter对应我的Github仓库地址 2、Servlet 跟踪Session 维持客户端和服务器之间的session会话的方式： Cookies 隐藏的表单字段 &lt;input type=&quot;hidden&quot; name=&quot;sessionid&quot; value=&quot;12345&quot;&gt; URL重写：在每个URL末尾追加一些额外的数据来标识session 会话，服务器会把该session会话标识符与已存储的有关session 会话的数据相关联。http://w3cschool.cc/file.htm;sessionid=12345 HttpSession对象：Servlet还提供了HttpSession接口，该接口提供了一种跨多个页面请求或访问网站时识别用户以及存储有关用户信息的方式。Servlet容器使用这个接口来创建一个HTTP客户端和 HTTP服务器之间的session会话。会话持续一个指定的时间段，跨多个连接或页面请求。 HttpSession session = request.getSession(); 方法 描述 public Object getAttribute(String name) 该方法返回在该 session 会话中具有指定名称的对象，如果没有指定名称的对象，则返回 null。 public Enumeration getAttributeNames() 该方法返回 String 对象的枚举，String 对象包含所有绑定到该 session 会话的对象的名称。 public long getCreationTime() 该方法返回该 session 会话被创建的时间，自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。 public String getId() 该方法返回一个包含分配给该 session 会话的唯一标识符的字符串。 public long getLastAccessedTime() 该方法返回客户端最后一次发送与该 session 会话相关的请求的时间自格林尼治标准时间 1970 年 1 月 1 日午夜算起，以毫秒为单位。 public int getMaxInactiveInterval() 该方法返回 Servlet 容器在客户端访问时保持 session 会话打开的最大时间间隔，以秒为单位。 public void invalidate() 该方法指示该 session 会话无效，并解除绑定到它上面的任何对象。 public boolean isNew() 如果客户端还不知道该 session 会话，或者如果客户选择不参入该 session 会话，则该方法返回 true。 public void removeAttribute(String name) 该方法将从该 session 会话移除指定名称的对象。 public void setAttribute(String name, Object value) 该方法使用指定的名称绑定一个对象到该 session 会话。 public void setMaxInactiveInterval(int interval) 该方法在 Servlet 容器指示该 session 会话无效之前，指定客户端请求之间的时间，以秒为单位。 删除Session会话数据 当您完成了一个用户的 session 会话数据，您有以下几种选择： 移除一个特定的属性：您可以调用 public void removeAttribute(String name) 方法来删除与特定的键相关联的值。 删除整个 session 会话：您可以调用 public void invalidate() 方法来丢弃整个 session 会话。 设置 session 会话过期时间：您可以调用 public void setMaxInactiveInterval(int interval) 方法来单独设置 session 会话超时。 注销用户：如果使用的是支持 servlet 2.4 的服务器，您可以调用 logout 来注销 Web 服务器的客户端，并把属于所有用户的所有 session 会话设置为无效。 web.xml 配置：如果您使用的是 Tomcat，除了上述方法，您还可以在 web.xml 文件中配置 session 会话超时，如下所示： &lt;session-config&gt; &lt;session-timeout&gt;15&lt;/session-timeout&gt; &lt;/session-config&gt; 上面实例中的超时时间是以分钟为单位，将覆盖 Tomcat 中默认的 30 分钟超时时间。 在一个 Servlet 中的 getMaxInactiveInterval() 方法会返回 session 会话的超时时间，以秒为单位。 session相关代码示例Github地址 sessionId:sessionid是一个会话的key，浏览器第一次访问服务器会在服务器端生成一个session，有一个sessionid和它对应。tomcat生成的sessionid叫做jsessionid。 session在访问tomcat服务器HttpServletRequest的getSession(true)的时候创建，tomcat的ManagerBase类提供创建sessionid的方法：随机数+时间+jvmid； 参考博文：sessionid如何产生？由谁产生？保存在哪里？ ","link":"https://blog.shunzi.tech/post/Servlet-apply-one/"},{"title":"验证码Java实现","content":" 此篇博文源于对验证码的机制好奇从而想一探究竟。 此处只介绍了最简单的字符型验证码实现，其他高级验证码机制更为复杂。 后续结合相关图像识别技术和自动化技术介绍验证码的攻与防。 验证码Java实现 1、Servlet实现验证码 实现步骤: 1、服务端随机生成验证码：利用java.awt中的相关类，譬如Graphics （核心）,BufferedImage 2、利用Servlet进行Client和Server的交互，进行验证码的获取，以及表单的客户端验证码输入和服务端存储的记录进行验证。 核心代码：（验证码的生成） public class ImageServlet extends HttpServlet { @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { //初始化图片类型 BufferedImage bufferedImage = new BufferedImage(68, 22, BufferedImage.TYPE_INT_RGB); //进行制图 Graphics graphics = bufferedImage.getGraphics(); //RGB设置颜色并填充对应图像坐标像素点 Color color = new Color(200, 150, 255); graphics.setColor(color); graphics.fillRect(0, 0, 68, 22); char[] chars = &quot;ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789&quot;.toCharArray(); Random random = new Random(); int length = chars.length, index; //循环随机生成验证码 StringBuffer stringBuffer = new StringBuffer(); for (int i = 0; i &lt; 4; i++) { index = random.nextInt(length); //设置验证码的不同颜色并记录验证码 graphics.setColor( new Color(random.nextInt(88), random.nextInt(188), random.nextInt(255))); graphics.drawString(chars[index] + &quot;&quot;, (i * 15) + 3, 18); stringBuffer.append(chars[index]); } //将验证码的记录值存储在session中 req.getSession().setAttribute(&quot;picCode&quot;, stringBuffer.toString()); //将生成的图片写入Response的输出流中 ImageIO.write(bufferedImage, &quot;JPG&quot;, resp.getOutputStream()); } } 2、插件实现验证码： 常见验证码组件 Jcaptcha和Kaptcha 敬请期待 😃 3、Java图片验证码 敬请期待 😃 ","link":"https://blog.shunzi.tech/post/VerifyCode-Java/"},{"title":"Servlet基础知识","content":" 该系列博文主要是介绍JavaEE基础知识。 可以结合相关Web应用容器技术进行食用。（Tomcat） 后续会解析相关JavaWeb框架的具体实现原理。 1、什么是Servlet？ Java Servlet 是运行在 Web 服务器或应用服务器上的程序，它是作为来自 Web 浏览器或其他 HTTP 客户端的请求和 HTTP 服务器上的数据库或应用程序之间的中间层。 个人见解:Servlet本质属于MVC架构模式中的C（controller）层，充当控制器的角色 2、Servlet架构 3、Servlet声明周期 Servlet 通过调用 init () 方法进行初始化。 init 方法被设计成只调用一次。它在第一次创建 Servlet 时被调用，在后续每次用户请求时不再调用。当用户调用一个 Servlet 时，就会创建一个 Servlet 实例，每一个用户请求都会产生一个新的线程，适当的时候移交给 doGet 或 doPost 方法。init() 方法简单地创建或加载一些数据，这些数据将被用于 Servlet 的整个生命周期。 博文推荐：关于的servlet的单例模式(单实例多线程)的解释 Servlet 调用 service() 方法来处理客户端的请求。 service() 方法是执行实际任务的主要方法。Servlet 容器（即 Web 服务器）调用 service() 方法来处理来自客户端（浏览器）的请求，并把格式化的响应写回给客户端。 扩展阅读：Http协议中的方法 Servlet 通过调用 destroy() 方法终止（结束）。 destroy() 方法只会被调用一次，在 Servlet 生命周期结束时被调用。destroy() 方法可以让您的 Servlet 关闭数据库连接、停止后台线程、把 Cookie 列表或点击计数器写入到磁盘，并执行其他类似的清理活动。在调用 destroy() 方法之后，servlet 对象被标记为垃圾回收。 最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。 4、Servlet获取请求头信息 方法 描述 Cookie[] getCookies() 返回一个数组，包含客户端发送该请求的所有的 Cookie 对象。 Enumeration getAttributeNames() 返回一个枚举，包含提供给该请求可用的属性名称。 Enumeration getHeaderNames() 返回一个枚举，包含在该请求中包含的所有的头名。 Enumeration getParameterNames() 返回一个 String 对象的枚举，包含在该请求中包含的参数的名称。 HttpSession getSession() 返回与该请求关联的当前 session 会话，或者如果请求没有 session 会话，则创建一个。 HttpSession getSession(boolean create) 返回与该请求关联的当前 HttpSession，或者如果没有当前会话，且创建是真的，则返回一个新的 session 会话。 Locale getLocale() 基于 Accept-Language 头，返回客户端接受内容的首选的区域设置。 Object getAttribute(String name) 以对象形式返回已命名属性的值，如果没有给定名称的属性存在，则返回 null。 ServletInputStream getInputStream() 使用 ServletInputStream，以二进制数据形式检索请求的主体。 String getAuthType() 返回用于保护 Servlet 的身份验证方案的名称，例如，&quot;BASIC&quot; 或 &quot;SSL&quot;，如果JSP没有受到保护则返回 null。 String getCharacterEncoding() 返回请求主体中使用的字符编码的名称。 String getContentType() 返回请求主体的 MIME 类型，如果不知道类型则返回 null。 String getContextPath() 返回指示请求上下文的请求 URI 部分。 String getHeader(String name) 以字符串形式返回指定的请求头的值。 String getMethod() 返回请求的 HTTP 方法的名称，例如，GET、POST 或 PUT。 String getParameter(String name) 以字符串形式返回请求参数的值，或者如果参数不存在则返回 null。 String getPathInfo() 当请求发出时，返回与客户端发送的 URL 相关的任何额外的路径信息。 String getProtocol() 返回请求协议的名称和版本。 String getQueryString() 返回包含在路径后的请求 URL 中的查询字符串。 String getRemoteAddr() 返回发送请求的客户端的互联网协议（IP）地址。 String getRemoteHost() 返回发送请求的客户端的完全限定名称。 String getRemoteUser() 如果用户已通过身份验证，则返回发出请求的登录用户，或者如果用户未通过身份验证，则返回 null。 String getRequestURI() 从协议名称直到 HTTP 请求的第一行的查询字符串中，返回该请求的 URL 的一部分。 String getRequestedSessionId() 返回由客户端指定的 session 会话 ID。 String getServletPath() 返回调用 JSP 的请求的 URL 的一部分。 String[] getParameterValues(String name) 返回一个字符串对象的数组，包含所有给定的请求参数的值，如果参数不存在则返回 null。 boolean isSecure() 返回一个布尔值，指示请求是否使用安全通道，如 HTTPS。 int getContentLength() 以字节为单位返回请求主体的长度，并提供输入流，或者如果长度未知则返回 -1。 int getIntHeader(String name) 返回指定的请求头的值为一个 int 值。 int getServerPort() 返回接收到这个请求的端口号。 int getParameterMap() 将参数封装成 Map 类型。 示例代码 Enumeration headerNames = request.getHeaderNames(); while(headerNames.hasMoreElements()) { String paramName = (String)headerNames.nextElement(); out.print(&quot;&lt;tr&gt;&lt;td&gt;&quot; + paramName + &quot;&lt;/td&gt;\\n&quot;); String paramValue = request.getHeader(paramName); out.println(&quot;&lt;td&gt; &quot; + paramValue + &quot;&lt;/td&gt;&lt;/tr&gt;\\n&quot;); } 5、设置HTTP响应报头的方法 方法 描述 String encodeRedirectURL(String url) 为 sendRedirect 方法中使用的指定的 URL 进行编码，或者如果编码不是必需的，则返回 URL 未改变。 String encodeURL(String url) 对包含 session 会话 ID 的指定 URL 进行编码，或者如果编码不是必需的，则返回 URL 未改变。 boolean containsHeader(String name) 返回一个布尔值，指示是否已经设置已命名的响应报头。 boolean isCommitted() 返回一个布尔值，指示响应是否已经提交。 void addCookie(Cookie cookie) 把指定的 cookie 添加到响应。 void addDateHeader(String name, long date) 添加一个带有给定的名称和日期值的响应报头。 void addHeader(String name, String value) 添加一个带有给定的名称和值的响应报头。 void addIntHeader(String name, int value) 添加一个带有给定的名称和整数值的响应报头。 void flushBuffer() 强制任何在缓冲区中的内容被写入到客户端。 void reset() 清除缓冲区中存在的任何数据，包括状态码和头。 void resetBuffer() 清除响应中基础缓冲区的内容，不清除状态码和头。 void sendError(int sc) 使用指定的状态码发送错误响应到客户端，并清除缓冲区。 void sendError(int sc, String msg) 使用指定的状态发送错误响应到客户端。 void sendRedirect(String location) 使用指定的重定向位置 URL 发送临时重定向响应到客户端。 void setBufferSize(int size) 为响应主体设置首选的缓冲区大小。 void setCharacterEncoding(String charset) 设置被发送到客户端的响应的字符编码（MIME 字符集）例如，UTF-8。 void setContentLength(int len) 设置在 HTTP Servlet 响应中的内容主体的长度，该方法设置 HTTP Content-Length 头。 void setContentType(String type) 如果响应还未被提交，设置被发送到客户端的响应的内容类型。 void setDateHeader(String name, long date) 设置一个带有给定的名称和日期值的响应报头。 void setHeader(String name, String value) 设置一个带有给定的名称和值的响应报头。 void setIntHeader(String name, int value) 设置一个带有给定的名称和整数值的响应报头。 void setLocale(Locale loc) 如果响应还未被提交，设置响应的区域。 void setStatus(int sc) 为该响应设置状态码。 6、Servlet中的HTTP状态码 请求报文和响应报文的格式 初始状态行 + 回车换行符（回车+换行） 零个或多个标题行+回车换行符 一个空白行，即回车换行符 一个可选的消息主体，比如文件、查询数据或查询输出 例子如下： HTTP/1.1 200 OK // HTTP版本 状态码 对应状态码的短消息 Content-Type: text/html // 报文头中包含的各个属性信息:键值对的形式 Header2: ... ... HeaderN: ... (Blank Line) &lt;!doctype ...&gt; &lt;html&gt; &lt;head&gt;...&lt;/head&gt; &lt;body&gt; ... &lt;/body&gt; &lt;/html&gt; 设置 HTTP 状态代码的方法 由于状态代码的产生都是服务端对于客户端发起的请求进行的相应的回应的过程中产生的，所以我们通过调用HttpServletResponse中的相关方法来进行对状态码的设置。 方法 描述 public void setStatus ( int statusCode ) 该方法设置一个任意的状态码。setStatus 方法接受一个 int（状态码）作为参数。如果您的反应包含了一个特殊的状态码和文档，请确保在使用 PrintWriter 实际返回任何内容之前调用 setStatus。 public void sendRedirect(String url) 该方法生成一个 302 响应，连同一个带有新文档 URL 的 Location 头。 public void sendError(int code, String message) 该方法发送一个状态码（通常为 404），连同一个在 HTML 文档内部自动格式化并发送到客户端的短消息。 ","link":"https://blog.shunzi.tech/post/Servlet-basic/"},{"title":"Docker常用命令","content":" 该系列博文主要是记录Docker学习过程中使用到的一些命令。 部分命令可能会因Docker版本不同有所差异 后续会不断更新。 1、安装与启动、更新 安装Docker : apt-get install -y docker.io 启动Docker : systemctl start docker 运行系统引导时启用 docker: systemctl enable docker 核对Docker版本：docker version 更新Docker：docker-machine upgrade default 2、镜像、容器相关 镜像 images 查看安装的镜像image：docker images 搜索可用的Docker镜像：docker search 镜像名字 运行镜像：docker run 镜像名称 要执行的命令 指定参数 -d 让容器在后台运行 -P 将容器内部使用的网络端口映射到我们的主机上 -p 绑定指定端口 -p 5000:5000，-p 127.0.0.1:5001:5002 -t:在新容器内指定一个伪终端或终端。 -i:允许你对容器内的标准输入 (STDIN) 进行交互。 --name：为容器命名 &gt; docker run -p 8080:80 --name nginx_web -it hub.c.163.com/library/nginx 从dockerhub上pull镜像：docker pull 镜像名称 构建镜像：docker build 镜像名 docker build -t runoob/centos:6.7 . - -t ：指定要创建的目标镜像名 - . ：Dockerfile 文件所在目录，可以指定Dockerfile 的绝对路径 设置镜像标签：docker tag 镜像ID 镜像名：标签 删除镜像：docker rmi 镜像名称 容器 containers 查看运行的容器container：docker ps -l:查询最后一次创建的容器 -a:查看所有容器 查看容器的详细信息（JSON格式）：docker inspect 容器编号 提交容器修改：docker commit id 容器 docker commit -m=&quot;has update&quot; -a=&quot;runoob&quot; e218edb10161 runoob/ubuntu:v2 -m 提交的描述信息 -a 指定镜像作者 停止容器：docker stop 容器名 或 容器ID 查看指定容器的某个确定端口映射到宿主机的端口号：docker port 容器ID或容器名 查看指定容器的标准输出：docker logs 容器ID或容器名 查看容器内部运行的进程：docker top 容器ID或容器名 删除指定的容器：docker rm 容器名或容器编号 进入后台容器并提供bash: docker exec -it 容器名 或 容器编号 bash -it 同run命令中的-it一致 退出容器: Ctrl + D 或 exit ","link":"https://blog.shunzi.tech/post/Docker-cmd/"},{"title":"Docker入门笔记","content":" 该系列博文主要是介绍Docker的一些基本概念。 学习笔记，仅此而已 后续...。 1、什么是Docker？ Docker是一个开源的引擎，可以轻松的为任何应用创建一个轻量级的、可移植的、自给自足的容器。开发者在笔记本上编译测试通过的容器可以批量地在生产环境中部署，包括VMs（虚拟机）、bare metal、OpenStack 集群和其他的基础应用平台。 2、Docker的组成 Docker系统有两个程序：docker服务端和docker客户端。其中docker服务端是一个服务进程，管理着所有的容器。docker客户端则扮演着docker服务端的远程控制器，可以用来控制docker的服务端进程。大部分情况下，docker服务端和客户端运行在一台机器上。 3、Docker在容器中安装新的程序 在使用docker run 镜像名称 要执行的命令时，对于要执行的命令，有的可能存在需要交互的情况，即需要用户输入命令来进行确认，例如apt-get，但在docker环境中是无法响应这种交互的，故常常需要加上些参数，例如apt-get -y，来省略交互过程。 4、保存对容器的修改 docker commit id 容器 当你对某一个容器做了修改之后（通过在容器中运行某一个命令），可以把对容器的修改保存下来，这样下次可以从保存后的最新状态运行该容器。docker中保存状态的过程称之为committing，它保存的新旧状态之间的区别，从而产生一个新的版本。 对于commit过程中id，需要先使用docker ps -l命令获得安装完ping命令之后的容器id。` 无需拷贝完整的id，通常来讲最开始的三至四个字母即可区分。（非常类似git里面的版本号) 执行完后，会返回新版本镜像的id号。通过使用dockr images命令可查看最近的镜像版本 提交前需要停止容器 5、创建镜像 当我们从docker镜像仓库中下载的镜像不能满足我们的需求时，我们可以通过以下两种方式对镜像进行更改。 1.从已经创建的容器中更新镜像，并且提交这个镜像 2.使用 Dockerfile 指令来创建一个新的镜像 6、构建镜像 从零开始来创建一个新的镜像。为此，我们需要创建一个 Dockerfile 文件，其中包含一组指令来告诉 Docker 如何构建我们的镜像。 runoob@runoob:~$ cat Dockerfile FROM centos:6.7 MAINTAINER Fisher &quot;fisher@sudops.com&quot; RUN /bin/echo 'root:123456' |chpasswd RUN useradd runoob RUN /bin/echo 'runoob:123456' |chpasswd RUN /bin/echo -e &quot;LANG=\\&quot;en_US.UTF-8\\&quot;&quot; &gt;/etc/default/local EXPOSE 22 EXPOSE 80 CMD /usr/sbin/sshd -D 每一个指令都会在镜像上创建一个新的层，每一个指令的前缀都必须是大写的。 第一条FROM，指定使用哪个镜像源 RUN 指令告诉docker在镜像内执行命令，安装了什么。然后，我们使用 Dockerfile 文件，通过 docker build 命令来构建一个镜像。 ","link":"https://blog.shunzi.tech/post/Docker-start/"},{"title":"深入剖析Tomcat读书笔记（一）","content":" 该系列博文主要是介绍Web应用容器的一些原理。 结合JavaEE的基础只是，介绍核心思想和实现机制 介绍容器中其他组件，系列一。 前言 1、Servlet容器是如何工作的？ 创建一个Request对象，用可能会在Servlet中使用的信息填充该Request对象; 创建一个调用Servlet的response对象，用来向WEB客户端发送响应； 调用Servlet的service()方法,将Request对象和Response对象。 复习Servlet的生命周期 Servlet 通过调用 init () 方法进行初始化。 Servlet 调用 service() 方法来处理客户端的请求。 Servlet 通过调用 destroy() 方法终止（结束）。 最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的。 2、Catalina Catalina是一个成熟的软件，设计和开发的十分优雅，功能结构也是模块化的，主要分为以下两个模块： 连接器 :负责将请求和容器相关联，为每个接收到的HTTP请求创建一个request对象和一个response，然后将处理过程交给容器。 容器：从连接器中接受到request和response对象，并调用相应的Servlet的service()方法。 第一章 简单的Web服务器 通过复习Java套接字编程，搭建一个简单的Web服务器，同时复习了计算机网络的相关知识以及JavaIO中的输入输出流相关操作。 WEB服务器的工作原理 服务器利用相应的服务器套接字(ServerSocket)（又称连接套接字,欢迎套接字(WelcomeSocket)）对相应的IP地址和端口进行监听，等待客户端发送相关连接请求; 客户端的套接字(ClientSocket)提出连接请求，要连接的目标是ServerSocket。为此，CllientSocket必须首先描述它要连接的服务器的套接字，指出服务器端套接字的地址Host和端口号port，然后就向服务器端套接字提出连接请求; 当服务器端套接字(ServerSocket)监听到或者说接收到客户端套接字(ClientSocket)的连接请求时，就响应客户端套接字的请求，建立一个新的线程，把服务器端套接字的描述发 给客户端，一旦客户端确认了此描述，双方就正式建立连接。而服务器端套接字继续处于监听状态，继续接收其他客户端套接字的连接请求;服务器端套接字(ServerSocket)在收到请求时会调用accept()方法为客户端创建一个新的套接字连接套接字（connection socket），实质上和客户端建立连接之后进行通信的是连接套接字 在编写过程中遇到的问题：(待解决) 编写代码时抛出异常： java.net.SocketException: Software caused connection abort: socket write error. 通过debug初步分析的结果是在浏览器生成发送给客户端响应时的输出流的write()函数出现了问题,查阅资料显示为在write()时输出流被提前关闭 测试过程中存在的问题： IE浏览器测试成功，谷歌浏览器测试失败. 第二章 一个简单的Servlet容器 Servlet容器的搭建 基于第一章WEB服务器的搭建，（第一章的WEB服务器只能访问服务器端的静态资源），在能访问静态资源static resources的基础之上，能够处理Servlet对应的相关请求并调用Servlet对应的类的方法。 采用类加载器加载URI中对应的Servlet类名对应的类 + newInstance() 实例化的方法来实例化Servlet类 new关键字 和 newInstance实例化的区别 new关键字 和 newInstance实例化的区别 创建对象的方式不一样，前者是创建一个新对象,后者是使用类加载机制. new创建一个类的时候，这个类可以没有被加载。但是使用newInstance()方法的时候，就必须保证： 1、这个类已经加载； 2、这个类已经连接了。 newInstance: 弱类型。低效率。只能调用无参构造。 new: 强类型。相对高效。能调用任何public构造。 参考博客：newInstance() 和 new 有什么区别? ServletServerTwo相比ServletSeverOne 使用了外观模式Facade（结构型） 外观模式 概述：我们通过外观的包装，使应用程序只能看到外观对象，而不会看到具体的细节对象，这样无疑会降低应用程序的复杂度，并且提高了程序的可维护性。 为子系统中的一组接口提供一个一致的界面， Facade模式定义了一个高层接口，这个接口使得这一子系统更加容易使用。引入外观角色之后，用户只需要直接与外观角色交互，用户与子系统之间的复杂关系由外观角色来实现，从而降低了系统的耦合度 参考博客：设计模式（九）外观模式Facade（结构型） 连接器 连接器：用于更好地创建Request(HttpServletRequest)对象和Response(HttpServletResponse)对象，并传递给service()方法 Servlet,GenericServlet,HttpServlet之间的异同：Servlet简介与Servlet和HttpServlet运行的流程 StringManager 每个包对应的StringManager实例，用于读取当前包下的存放异常和错误消息的properties文件。 每个包对应一个StringManager实例，采用单例模式进行单例对象的获取，以包名package name作为入参。 单例模式复习：通过将构造方法私有化，使用公有化的方法来调用构造方法来保证单例。 通过将包名package name作为键值对的键，在StringManager对应的HashTable中去查找，使用StringManager的getManager()方法来获取到StringManger实例。 获取到StringManager实例后，再使用getString(String key)，以错误码作为入参来获取具体的错误信息。 HttpConnector 等待Http请求 为每个请求创建相应的HttpProcessor实例 调用process()方法 HttpProcessor 主要任务 创建一个HttpRequest对象 同时引入HTTP请求的Header和Cookie和请求参数以及相关方法。 创建一个HttpResponse对象 解析HTTP请求的第一行内容和请求头信息，填充HttpRequest对象 将Socket作为入参，调用process()，并根据URI判断请求类型并作出相应的处理，把创建的HttpRequest和HttpResponse对象传递给不同类型的处理器。 使用StringManager来发送错误消息 解析HTTP请求： 读取套接字的输入流；调用输入流的read()方法，从请求行中获取到了方法、URI、HTTP协议版本等信息； 解析请求行： 会使用SocketInputStream的相关方法来解析请求头并提取出URI，再根据URI格式解析出URI中的相关信息（其中主要包括Pathname、查询字符串Search和JSessionID），再为这些相关信息创建相应的对象，存储在HttpRequest中 JSESSIONID，用于携带会话标识符，会话标识符通常是作为Cookie嵌入的，但是当浏览器禁用了Cookie时，也可以将会话标识符嵌入到查询字符串中 解析请求头： 在读取了请求头的名称和值后，调用HttpRequest的addHeader()方法，将其添加到HttpRequest对象的HashMap中。 由于请求头中可能包含一些属性设置信息。例如Content-type,Content-length,Cookies等，则进行相应的处理后再填充到HttpRequest对象中。 解析Cookie 什么是Cookie？ Cookie : Cookie实际上是一小段的文本信息（键/值的形式）。客户端请求服务器，如果服务器需要记录该用户状态，就使用response向客户端浏览器颁发一个Cookie。客户端浏览器会把Cookie保存起来。当浏览器再请求该网站时，浏览器把请求的网址连同该Cookie一同提交给服务器。服务器检查该Cookie，以此来辨认用户状态。服务器还可以根据需要修改Cookie的内容。 产生原因： 由于HTTP是一种无状态的协议，服务器单从网络连接上无从知道客户身份。故需要引入一种来跟踪客户端的会话信息。 参考博文：Cookie/Session机制详解 核心函数parseCookieHeader() : 由于Cookie中存放的值是键/值的形式，例如： 其中各个键值对之间是以分号进行分隔的，键和值之间又是以=号进行连接，所以该函数便是充分利用字符串的格式特性来对Cookie使用while()循环来进行解析。 源码如下： public static Cookie[] parseCookieHeader(String header) { if (header != null &amp;&amp; header.length() &gt;= 1) { ArrayList cookies = new ArrayList(); //注意循环体中的59和61，其实是ASCII码 while(header.length() &gt; 0) { int semicolon = header.indexOf(59); if (semicolon &lt; 0) { semicolon = header.length(); } if (semicolon == 0) { break; } String token = header.substring(0, semicolon); if (semicolon &lt; header.length()) { header = header.substring(semicolon + 1); } else { header = &quot;&quot;; } try { int equals = token.indexOf(61); if (equals &gt; 0) { String name = token.substring(0, equals).trim(); String value = token.substring(equals + 1).trim(); cookies.add(new Cookie(name, value)); } } catch (Throwable var7) { ; } } return (Cookie[])cookies.toArray(new Cookie[cookies.size()]); } else { return new Cookie[0]; } } 获取参数 常见的获取参数的方法有getParameterMap(),getParameter(),getParameterNames(),getParameterValues()，这些方法都是在对请求报文中的参数进行了解析的基础之上展开的，都会调用parseParameter()方法。 GET请求报文中的参数位于URI中的查询字符串； POST请求报文的参数则会存储在HashMap中，而且是一个比较特殊的HashMap(ParameterMap)，其中ParameterMap被设计成了一种相对安全的机制（只有在locked的布尔值为false时，才能对HashMap进行相应的修改）,被设计成这种机制是因为HashMap本身的线程安全问题。 ","link":"https://blog.shunzi.tech/post/tomcat-one/"},{"title":"适配器模式","content":" 该系列博文主要是介绍一些常用的设计模式。 此处仅使用了Java语言来进行示例，设计模式是一种思想，不受预言限制 结合一些具体的例子对适配器模式进行讲解。 适配器模式 将一个类的接口转换成客户希望的另一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 OO设计原则： 面向接口编程 封装变化 多用组合少用继承 对修改关闭，对扩展开放 public class AdapterDemo { public static void main(String[] args) { PowerA powerA = new PowerAImplA(); startA(powerA); PowerB powerB = new PowerBImpl(); //startA(powerB); 报错 //由于 startA 的参数为 PowerA 类型,此时需要适配 PowerAAdapter pAAdapter = new PowerAAdapter(powerB); startA(pAAdapter); } public static void startA(PowerA powerA){ // ...... powerA.insert(); // ...... } //与startA的内容大部分重复，故仅适用 startA() ，从而需要一个相应的适配器，使得 PowerB 能使用 startA() public static void startB(PowerB powerB){ // ...... powerB.connect(); // ...... } } //适配器Adapter核心代码 class PowerAAdapter implements PowerA{ private PowerB powerB;// 要进行适配的接口 public PowerAAdapter(PowerB powerB) { this.powerB = powerB; } @Override public void insert() { powerB.connect(); } } /** * 电源A接口 */ interface PowerA{ public void insert(); } class PowerAImplA implements PowerA{ @Override public void insert() { // TODO Auto-generated method stub System.out.println(&quot;电源A接口插入，开始工作。&quot;); } } /** * 电源B接口 */ interface PowerB{ public void connect(); } class PowerBImpl implements PowerB{ public void connect(){ System.out.println(&quot;电源B接口已连接。&quot;); } } ","link":"https://blog.shunzi.tech/post/Adapter/"},{"title":"Java.String","content":" 该博文主要是介绍字符串处理过程中一些常用的函数和操作。 结合Java本身封装的字符串库来讲解 对于部分操作的效率进行对比，探讨性能问题 Java.String String 类的两种赋值方式 String 可以表示出一个字符串,根据String的源码我们会发现String类实际上是使用字符数组char[]存储的. 如下为String的源码实现 public final class String implements java.io.Serializable, Comparable&lt;String&gt;, CharSequence { /** The value is used for character storage. */ private final char value[]; /** Cache the hash code for the string */ private int hash; // Default to 0 /** use serialVersionUID from JDK 1.0.2 for interoperability */ private static final long serialVersionUID = -6849794470754667710L; public String() { this.value = &quot;&quot;.value; } public String(String original) { this.value = original.value; this.hash = original.hash; } public String(char value[]) { this.value = Arrays.copyOf(value, value.length); } public String(char value[], int offset, int count) { if (offset &lt; 0) { throw new StringIndexOutOfBoundsException(offset); } if (count &lt;= 0) { if (count &lt; 0) { throw new StringIndexOutOfBoundsException(count); } if (offset &lt;= value.length) { this.value = &quot;&quot;.value; return; } } // Note: offset or count might be near -1&gt;&gt;&gt;1. if (offset &gt; value.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } this.value = Arrays.copyOfRange(value, offset, offset+count); } public String(int[] codePoints, int offset, int count) { if (offset &lt; 0) { throw new StringIndexOutOfBoundsException(offset); } if (count &lt;= 0) { if (count &lt; 0) { throw new StringIndexOutOfBoundsException(count); } if (offset &lt;= codePoints.length) { this.value = &quot;&quot;.value; return; } } // Note: offset or count might be near -1&gt;&gt;&gt;1. if (offset &gt; codePoints.length - count) { throw new StringIndexOutOfBoundsException(offset + count); } final int end = offset + count; // Pass 1: Compute precise size of char[] int n = count; for (int i = offset; i &lt; end; i++) { int c = codePoints[i]; if (Character.isBmpCodePoint(c)) continue; else if (Character.isValidCodePoint(c)) n++; else throw new IllegalArgumentException(Integer.toString(c)); } // Pass 2: Allocate and fill in char[] final char[] v = new char[n]; for (int i = offset, j = 0; i &lt; end; i++, j++) { int c = codePoints[i]; if (Character.isBmpCodePoint(c)) v[j] = (char)c; else Character.toSurrogates(c, v, j++); } this.value = v; } public String(byte bytes[], int offset, int length, String charsetName) throws UnsupportedEncodingException { if (charsetName == null) throw new NullPointerException(&quot;charsetName&quot;); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charsetName, bytes, offset, length); } public String(byte bytes[], int offset, int length, Charset charset) { if (charset == null) throw new NullPointerException(&quot;charset&quot;); checkBounds(bytes, offset, length); this.value = StringCoding.decode(charset, bytes, offset, length); } public String(byte bytes[], String charsetName) throws UnsupportedEncodingException { this(bytes, 0, bytes.length, charsetName); } public String(byte bytes[], Charset charset) { this(bytes, 0, bytes.length, charset); } public String(byte bytes[], int offset, int length) { checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length); } public String(byte bytes[]) { this(bytes, 0, bytes.length); } public String(StringBuffer buffer) { synchronized(buffer) { this.value = Arrays.copyOf(buffer.getValue(), buffer.length()); } } public String(StringBuilder builder) { this.value = Arrays.copyOf(builder.getValue(), builder.length()); } 常见的两种String赋值方式 String str1 = new String(&quot;name&quot;); String str2 = &quot;name&quot;; 两种赋值方式分析 //会创建两个字符串对象，一个在常量池中创建便于下一次使用，一个在堆内存中创建 String str1 = new String(&quot;name&quot;); //最多创建一个字符串对象，有可能不用创建对象（如果常量池中已经存在相应的常量） String str2 = &quot;name&quot;;//推荐使用 内存分析 String类编译期与运行期分析 首先明确对象之间 &quot;==&quot; 是比较两个对象的地址。 public class StringDemo { public static void main(String[] args) { // 情况一 结果为true String aString = &quot;a1&quot;; String a1String = &quot;a&quot; + 1; System.out.println(aString == a1String); // 情况二 结果为false String bString = &quot;b1&quot;; int bb = 1; String b1String = &quot;b&quot; + bb; System.out.println(bString == b1String); // 情况三 结果为true String cString = &quot;c1&quot;; final int cc = 1; String c1String = &quot;c&quot; + cc; System.out.println(cString == c1String); // 情况四 结果为false String dString = &quot;dd&quot;; final int dd = getDD(); String d1String = dString + dd; System.out.println(dString == d1String); } public static int getDD(){ return 1; } } 情况二为false，因为 b1String 在编译期时是还没有完全确定下来的，因为 bb 是一个变量，只有在执行期才能加载到。 情况四为false，注意与情况三进行对比，虽然使用了final关键字，但是根据函数返回值确定下来的，而函数要执行仍然是在运行期的时候确定的，编译期仍然无法确定。 String 类字符与字符串操作方法 字符与字符串操作 字节与字符串操作 是否以指定内容开头或结尾 ) String 类替换操作 String 类字符串截取操作 字符串拆分操作 String 类字符串查找操作 String 类其他操作方法 更多请见相关api文档 常见用例 System.out.println(&quot;abcde&quot;.charAt(3));//d System.out.println(&quot;abcde&quot;.toCharArray()[2]);//c byte[] bytes = &quot;abcde&quot;.getBytes(); for(int i = 0; i &lt; bytes.length; i++){ System.out.println(bytes[i]); } System.out.println(new String(bytes).toString()); System.out.println(new String(bytes,0,2).toString());//ab System.out.println(new String(bytes,&quot;utf-8&quot;).toString()); System.out.println(&quot;abcde&quot;.startsWith(&quot;ab&quot;));//true System.out.println(&quot;abcde&quot;.startsWith(&quot;cd&quot;,2));//true System.out.println(&quot;abcde&quot;.endsWith(&quot;de&quot;));//true System.out.println(&quot;abc,de&quot;.replace(&quot;,&quot;, &quot;:&quot;).toString());//abc:de System.out.println(&quot;abcde&quot;.replace(&quot;ab&quot;, &quot;AB&quot;));//ABcde //匹配正则表达式中26个字母替换为相应的字符串 System.out.println(&quot;abcde&quot;.replaceAll(&quot;[a-z]&quot;, &quot;*&quot;));//***** System.out.println(&quot;abcde&quot;.replaceFirst(&quot;[a-z]&quot;, &quot;*&quot;));//*bcde //包括起始位置 System.out.println(&quot;abcde&quot;.substring(2));//cde //包括起始位置，不包括结束位置 System.out.println(&quot;abcde&quot;.substring(1, 3));//bc String[] value = &quot;ab-cde&quot;.split(&quot;-&quot;);//传入的参数为正则表达式 String[] value2 = &quot;ab-c-de&quot;.split(&quot;-&quot;,3);//拆分成三个字符串数组 for(String s:value){ System.out.print(s); } System.out.println(); for(String s:value2){ System.out.print(s); } System.out.println(&quot;abcde&quot;.contains(&quot;abc&quot;));//true System.out.println(&quot;abcde&quot;.indexOf(&quot;a&quot;));//0 参数虽然为整形，但会转为unicode字符 System.out.println(&quot;abcde&quot;.indexOf(&quot;bc&quot;));//0 返回首地址 System.out.println(&quot;abcded&quot;.lastIndexOf(&quot;d&quot;));//5 System.out.println(&quot;abcdede&quot;.lastIndexOf(&quot;de&quot;));//5 System.out.println(&quot;abcde&quot;.isEmpty());//false System.out.println(&quot;abcde&quot;.length());//5 System.out.println(&quot;abcde&quot;.toUpperCase());//ABCDE System.out.println(&quot;ABCDE&quot;.toLowerCase());//abcde System.out.println(&quot; abcde &quot;.trim());//abcde System.out.println(&quot;abc&quot;.concat(&quot;de&quot;));//abcde 源码 public char charAt(int index) { if ((index &lt; 0) || (index &gt;= value.length)) { throw new StringIndexOutOfBoundsException(index); } return value[index]; } public char[] toCharArray() { // Cannot use Arrays.copyOf because of class initialization order issues char result[] = new char[value.length]; System.arraycopy(value, 0, result, 0, value.length); return result; } // 使用平台默认的编码转换成字节数组，默认gbk，根据虚拟机 public byte[] getBytes(String charsetName) throws UnsupportedEncodingException { if (charsetName == null) throw new NullPointerException(); return StringCoding.encode(charsetName, value, 0, value.length); } public String(byte bytes[]) { this(bytes, 0, bytes.length); } public String(byte bytes[], int offset, int length) { checkBounds(bytes, offset, length); this.value = StringCoding.decode(bytes, offset, length); } public String(byte bytes[], String charsetName) throws UnsupportedEncodingException { this(bytes, 0, bytes.length, charsetName); } public boolean startsWith(String prefix) { return startsWith(prefix, 0); } public boolean startsWith(String prefix, int toffset) { char ta[] = value;//调用该方法的对象的value int to = toffset;//偏移量 char pa[] = prefix.value;//前缀的value int po = 0;//前缀的偏移量 int pc = prefix.value.length;//前缀的总数 // Note: toffset might be near -1&gt;&gt;&gt;1. if ((toffset &lt; 0) || (toffset &gt; value.length - pc)) { return false; } while (--pc &gt;= 0) { if (ta[to++] != pa[po++]) { return false; } } return true; } public boolean endsWith(String suffix) { return startsWith(suffix, value.length - suffix.value.length); } public String replace(char oldChar, char newChar) { if (oldChar != newChar) { int len = value.length; int i = -1; char[] val = value; /* avoid getfield opcode */ while (++i &lt; len) { if (val[i] == oldChar) { break; } } if (i &lt; len) { char buf[] = new char[len]; for (int j = 0; j &lt; i; j++) { buf[j] = val[j]; } while (i &lt; len) { char c = val[i]; buf[i] = (c == oldChar) ? newChar : c; i++; } return new String(buf, true); } } return this; } public String replaceFirst(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceFirst(replacement); } public String replaceAll(String regex, String replacement) { return Pattern.compile(regex).matcher(this).replaceAll(replacement); } public String replace(CharSequence target, CharSequence replacement) { return Pattern.compile(target.toString(), Pattern.LITERAL).matcher( this).replaceAll(Matcher.quoteReplacement(replacement.toString())); } public String substring(int beginIndex) { if (beginIndex &lt; 0) { throw new StringIndexOutOfBoundsException(beginIndex); } int subLen = value.length - beginIndex; if (subLen &lt; 0) { throw new StringIndexOutOfBoundsException(subLen); } return (beginIndex == 0) ? this : new String(value, beginIndex, subLen); } public String substring(int beginIndex, int endIndex) { if (beginIndex &lt; 0) { throw new StringIndexOutOfBoundsException(beginIndex); } if (endIndex &gt; value.length) { throw new StringIndexOutOfBoundsException(endIndex); } int subLen = endIndex - beginIndex; if (subLen &lt; 0) { throw new StringIndexOutOfBoundsException(subLen); } return ((beginIndex == 0) &amp;&amp; (endIndex == value.length)) ? this : new String(value, beginIndex, subLen); } public String[] split(String regex, int limit) { /* fastpath if the regex is a (1)one-char String and this character is not one of the RegEx's meta characters &quot;.$|()[{^?*+\\\\&quot;, or (2)two-char String and the first char is the backslash and the second is not the ascii digit or ascii letter. */ char ch = 0; if (((regex.value.length == 1 &amp;&amp; &quot;.$|()[{^?*+\\\\&quot;.indexOf(ch = regex.charAt(0)) == -1) || (regex.length() == 2 &amp;&amp; regex.charAt(0) == '\\\\' &amp;&amp; (((ch = regex.charAt(1))-'0')|('9'-ch)) &lt; 0 &amp;&amp; ((ch-'a')|('z'-ch)) &lt; 0 &amp;&amp; ((ch-'A')|('Z'-ch)) &lt; 0)) &amp;&amp; (ch &lt; Character.MIN_HIGH_SURROGATE || ch &gt; Character.MAX_LOW_SURROGATE)) { int off = 0; int next = 0; boolean limited = limit &gt; 0; ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(); while ((next = indexOf(ch, off)) != -1) { if (!limited || list.size() &lt; limit - 1) { list.add(substring(off, next)); off = next + 1; } else { // last one //assert (list.size() == limit - 1); list.add(substring(off, value.length)); off = value.length; break; } } // If no match was found, return this if (off == 0) return new String[]{this}; // Add remaining segment if (!limited || list.size() &lt; limit) list.add(substring(off, value.length)); // Construct result int resultSize = list.size(); if (limit == 0) { while (resultSize &gt; 0 &amp;&amp; list.get(resultSize - 1).length() == 0) { resultSize--; } } String[] result = new String[resultSize]; return list.subList(0, resultSize).toArray(result); } return Pattern.compile(regex).split(this, limit); } public String[] split(String regex) { return split(regex, 0); } public int indexOf(int ch, int fromIndex) { final int max = value.length; if (fromIndex &lt; 0) { fromIndex = 0; } else if (fromIndex &gt;= max) { // Note: fromIndex might be near -1&gt;&gt;&gt;1. return -1; } if (ch &lt; Character.MIN_SUPPLEMENTARY_CODE_POINT) { // handle most cases here (ch is a BMP code point or a // negative value (invalid code point)) final char[] value = this.value; for (int i = fromIndex; i &lt; max; i++) { if (value[i] == ch) { return i; } } return -1; } else { return indexOfSupplementary(ch, fromIndex); } } public int indexOf(String str) { return indexOf(str, 0); } public int indexOf(String str, int fromIndex) { return indexOf(value, 0, value.length, str.value, 0, str.value.length, fromIndex); } public int indexOf(int ch) { return indexOf(ch, 0); } public boolean contains(CharSequence s) { return indexOf(s.toString()) &gt; -1; } public int lastIndexOf(int ch, int fromIndex) { if (ch &lt; Character.MIN_SUPPLEMENTARY_CODE_POINT) { // handle most cases here (ch is a BMP code point or a // negative value (invalid code point)) final char[] value = this.value; int i = Math.min(fromIndex, value.length - 1); for (; i &gt;= 0; i--) { if (value[i] == ch) { return i; } } return -1; } else { return lastIndexOfSupplementary(ch, fromIndex); } } public int lastIndexOf(int ch) { return lastIndexOf(ch, value.length - 1); } public boolean isEmpty() { return value.length == 0; } public int length() { return value.length; } public String trim() { int len = value.length; int st = 0; char[] val = value; /* avoid getfield opcode */ while ((st &lt; len) &amp;&amp; (val[st] &lt;= ' ')) { st++; } while ((st &lt; len) &amp;&amp; (val[len - 1] &lt;= ' ')) { len--; } return ((st &gt; 0) || (len &lt; value.length)) ? substring(st, len) : this; } public String concat(String str) { int otherLen = str.length(); if (otherLen == 0) { return this; } int len = value.length; char buf[] = Arrays.copyOf(value, len + otherLen); str.getChars(buf, len); return new String(buf, true); } ","link":"https://blog.shunzi.tech/post/Java.String/"},{"title":"操作系统引论","content":" 该系列博文主要是针对操作系统课程中的一些知识点进行梳理。 更多地是介绍相关概念，后续会结合部分代码介绍 操作系统系列之一 第一章 操作系统引论 1.1 操作系统的目标及作用 1.1.1 操作系统的概念 An Operating System is a program that managers the computer hardware provides a basis for application programs acts as an intermediary between the computer user and the computer hardware OS is a resource allocator Manages all resources Decides between conflicting requests for efficient and fair resource use OS is a control program Controls execution of programs to prevent errors and improper use of the computer 操作系统定义：操作系统是位于硬件层(HAL)之上，所有其它系统软件层之下的一个系统软件。其主要作用是管理好这些设备，提高它们的利用率和系统的吞吐量，并为用户和应用程序提供一个统一的接口，便于用户使用。 1.1.2 操作系统的位置 位于硬件层(HAL)之上，所有其它系统软件层之下的一个系统软件 1.1.3 操作系统的目标 有效性:操作系统允许以更有效的方式使用计算机系统资源。（提高系统资源利用率、提高系统的吞吐量） 方便性:操作系统使计算机更易于使用。 可扩充性:在操作系统中,允许有效地开发，测试和引进新的系统功能。 开放性:实现应用程序的可移植性和互操作性,要求具有统一的开放的环境。 1.1.4 操作系统的作用 OS 看做是用户与计算机硬件系统之间的接口。 含义：OS 处于用户与计算机硬件系统之间，用户通过OS来使用计算机系统。 OS作为计算机系统资源的管理者 从资源管理的观点看，则可把OS视为计算机系统资源的管理者。 处理机管理 存储器管理 I/O设备管理 文件管理 OS实现了对计算机资源的抽象 对于一个完全无软件的计算机系统(即裸机)，它向用户提供的是实际硬件接口(物理接口)，用户必须对物理接口的实现细节有充分的了解，并利用机器指令进行编程，因此很难使用。 通过OS的I／Ｏ管理实现了对计算机资源的抽象，帮助用户对硬件资源的使用和管理。 1.2 操作系统的发展历程 *(了解) 操作系统的产生 手工操作阶段 成批处理阶段 执行系统阶段 操作系统的完善 多道批处理系统 分时系统 实时处理系统 通用操作系统 操作系统的发展 计算机网络的出现 微型计算机的普及 极强的计算和处理能力 微内核操作系统 多核操作系统 1.2.1 无操作系统的计算机系统 1.2.1.1 人工操作方式（20世纪50年代） 人工控制 1.2.1.2 脱机输入/输出方式 在一台外围机的控制下，把纸带(卡片)上的数据(程序)输入到磁带上。当CPU需要这些程序和数据时，再从磁带上将其高速地调入内存。 1.2.2 单道批处理系统 系统对作业的处理都是成批进行的，且在内存中始终仅存一道作业运行，运行结束或出错，才自动调另一道作业运行，故称为单道批处理系统。 单道批处理系统主要特征:自动性、顺序性、单道性。 优点：减少人工操作，解决了作业的自动接续。 缺点：平均周转时间长，没有交互能力。 1.2.3 多道批处理系统 在内存中存放多道作业运行，运行结束或出错，自动调度内存中的另一道作业运行。多道程序。 主要特征：多道性、无序性、调度性（进程调度和作业调度）。 优点： 提高CPU的利用率。 提高内存和I/O设备利用率。 增加系统吞吐率。 缺点：平均周转时间长，没有交互能力。 需要解决的问题： 处理机管理：分配和控制CPU。 存储器管理：为每道程序分配必要的内存空间。 I/O设备管理：I/O设备的分配与操纵。 文件管理：文件的存取、共享和保护。 作业管理：如何组织作业运行。 1.2.4 分时操作系统 (重要) 分时系统是指在一台主机上连接了多个带有显示器和键盘的终端，同时允许多个用户通过自己的终端，以交互方式使用计算机，共享主机中的资源。 分时系统能很好地将一台计算机提供给多个用户同时使用，提高计算机的利用率。还可以满足用户对人---机交互的需求。 它被经常应用于查询系统中，满足许多查询用户的需要。 特点： 多路性：一个主机与多个终端相连； 交互性：以对话的方式为用户服务； 独占性：每个终端用户仿佛拥有一台虚拟机。 及时性：用户的请求能在很短的时间内获得响应。 1.2.5 实时操作系统 （重要） 所谓实时系统：是计算机及时响应外部事件的请求，在规定的时间内完成对该事件的处理，并控制所有实时设备和实时任务协调一致的运行。 实时系统的特征： 多路性：能对多个对象进行控制。 独立性：独立运行，不混淆，不破坏。 交互性：仅限于访问系统中某些特定的专用服务程序。 可靠性：高可靠性，应具有过载防护能力。 及时性：不同的系统要求不一样，控制对象必须在截止时间内完成。 1.2.6 微机操作系统 配置在微型机上的操作系统称为微机操作系统。 1．单用户单任务操作系统 2．单用户多任务操作系统 3．多用户多任务操作系统 1.3 操作系统的基本特性 （重要） 现代OS的四个基本特征： 并发性 共享性 虚拟性 异步性 并发是最重要的特征，其它特征都以并发为前提。 1.3.1 并发性 并行性：是指两个或多个事件在同一时刻发生。 并发性：是指两个或多个事件在同一时间间隔内发生。 在多道程序环境下，并发性是指在一段时间内，宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。 1.3.2 共享 共享：是指系统中的资源可供内存中多个并发执行的进程共同使用。 1.3.2.1 互斥共享 把在一段时间内只允许一个进程访问的资源，称为临界资源。 系统中的临界资源可以提供给多个进程使用，但一段时间内仅允许一个进程使用，称为互斥共享方式。 1.3.2.2 同时访问方式 一段时间内，多个进程可以同时使用这个资源。 从微观上看，多个进程交替互斥地使用系统中的某个资源。例如磁盘。 1.3.3 虚拟性 虚拟：是指通过某种技术把一个物理实体变为（映射为）若干个逻辑上的对应物，用于实现虚拟的技术称为虚拟技术。 虚拟处理机技术：分时实现 虚拟设备技术：SPOOLING技术（第五章） 虚拟磁盘技术: 虚拟存储器技术：虚拟存储器（第四章） 1.3.4 异步性 （易错） 执行结果不确定，程序不可再现。 异步性，多道程序环境下程序（进程）以异步的方式执行，每道程序在何时执行、各自执行的顺序、完成时间都是不确定的，也是不可预知的。 CSDN同步异步解析 1.4 操作系统的主要功能 处理机管理（CPU） 存储器管理 设备管理 文件管理 操作系统与用户之间的接口 1.4.1 处理机管理功能 主要功能 ：创建和撤销进程（或者线程），对诸进程的运行进行协调，实现进程之间的信息交换，以及按照一定的算法把处理机分配给进程。 1.4.1.1 进程控制 进程控制的主要功能：是为作业创建进程、撤消已结束的进程，以及控制进程在运行过程中的状态转换。 1.4.1.2 进程同步和互斥 进程同步的主要任务是为多个进程的运行进行协调。 1.4.1.2.1 进程互斥方式 这是指诸进程（线程）在对临界资源进行访问时，应采用互斥方式 1.4.1.2.2 进程同步方式 指进程相互合作去完成共同的任务时，诸进程之间的协调。 1.4.1.3进程通信 进程通信——是进程之间的信息交换。 通信方式：消息队列、管道、套接字等。 1.4.1.4 调度 作业调度(高级调度)：从后备队列中按照一定的算法选择出若干个作业，为他们分配资源，调入内存，建立进程，插入就绪队列中。 进程调度(低级调度)：是从进程的就绪队列中按照一定的算法选择一个新进程，把处理机分配给它，使进程投入执行。 1.4.2 存储器管理功能 主要任务：为多道程序的运行提供良好的环境，方便用户使用存储器，提高存储器的利用率以及能从逻辑上扩充内存。 内存分配 内存保护 地址映射 内存扩充 1.4.2.1 内存分配 1.4.2.1.1 静态分配方式 每个作业运行之前分配好内存空间，在作业的整个运行期间不再改变。 1.4.2.1.2 动态分配方式 每个作业在运行前或运行中，均可申请新的附加内存空间，以适应程序和数据的动态增涨。 1.4.2.2 内存保护 确保每道用户程序都只在自己的内存空间内运行，彼此互不干扰。 绝不允许用户程序访问操作系统的程序和数据，也不允许用户程序转移到非共享的其它用户程序中去执行。 1.4.2.3 地址映射 地址空间：目标程序或装入程序限定的空间，称为“地址空间”。单元的编号称为逻辑地址，又称为相对地址。 内存空间： 由内存中的一系列单元所限定的地址范围称为“内存空间”，其中的地址称为“物理地址” 地址映射：运行时，将地址空间中的逻辑地址转换为内存空间中与之对应的物理地址，称为地址映射 。 1.4.2.4 内存扩充 借助于虚拟存储技术:从逻辑上去扩充内存容量，使用户所感觉到的内存容量比实际内存容量大得多。 请求调入功能； 置换功能； 1.4.3 设备管理功能 设备管理的主要任务： 完成用户进程提出的I/O请求； 为用户进程分配其所需的I/O设备； 提高CPU和I/O设备的利用率； 提高I/O速度； 方便用户使用I/O设备。 设备管理具有的功能： 缓冲管理：有效地缓和CPU和I/O设备速度不匹配的矛盾 ，提高CPU的利用率。 设备分配：根据I/O请求，分配其所需的设备。 设备处理：设备处理程序又称为设备驱动程序。 1.4.4 文件管理功能 对用户文件和系统文件进行管理，以方便用户使用，并保证文件的安全性。 主要功能： 文件存储空间的管理:对诸多文件及文件的存储空间，实施统一的管理。 目录管理 文件的读／写管理和保护 1.4.5 方便用户使用的用户接口 计算机用户需要的用户命令 由OS实现的所有用户命令所构成的集合常被人们称为OS的Interface(用户接口)；有时也称为命令接口。 应用软件需要的System Call(系统调用) 由OS实现的所有系统调用所构成的集合被人们称为程序接口或应用编程接口(Application Programming Interface,API)。 用户接口 命令接口——用户可通过该接口向作业发出命令以控制作业的运行。 程序接口 ——由一组系统调用组成，每一个系统调用都是一个能完成特定功能的子程序，每当应用程序要求OS提供某种服务（功能）时，便调用具有相应功能的系统调用。 图形接口（GUI） 图形用户接口采用了图形化的操作界面，用非常容易识别的各种图标（icon）来将系统的各项功能、各种应用程序和文件，直观、逼真地表示出来。用户可用鼠标或通过菜单和对话框，来完成对应用程序和文件的操作。 1.5 操作系统的体系结构 操作系统是一个大型系统软件，其结构已经历了四代的变革： 第一代的OS是无结构的； 第二代OS采用了模块式结构； 第三代是层次式结构 现代OS结构是微内核结构； 1.5.1 无结构操作系统 在早期开发操作系统时，设计者只是把他的注意力放在功能的实现和获得高的效率上，缺乏首尾一致的设计思想。 OS是为数众多的一组过程的集合，各过程之间可以相互调用，在操作系统内部不存在任何结构，因此，有人把它称为整体系统结构。 缺陷： 设计出的操作系统既庞大又杂乱，缺乏清晰的程序结构。 编制出的程序错误很多，给调试工作带来很多困难；增加了维护人员的负担。 1.5.2 模块化OS结构 使用分块结构的系统包含若干module（模块）；其中，每一块实现一组基本概念以及与其相关的基本属性。 块与块之间的相互关系： 所有各块的实现均可以任意引用其它各块所提供的概念及属性。 优点： 提高了OS设计的正确性、可理解性和可维护性。 增强了0S的可适应性。 加速了OS的开发过程。 缺点： 对模块的划分及对接口的规定要精确描述很困难 从功能观点来划分模块时，未能将共享资源和独占资源加以区别； 1.5.3 分层式OS结构 使用分层系统结构包含若干layer（层）；其中，每一层实现一组基本概念以及与其相关的基本属性。 层与层之间的相互关系： 所有各层的实现不依赖其以上各层所提供的概念及其属性，只依赖其直接下层所提供的概念及属性； 每一层均对其上各层隐藏其下各层的存在。 1.5.4 微内核OS结构 客户/服务器模式(Client-Server Model) 所谓微内核技术，是指精心设计的、能实现现代OS核心功能的小型内核，它与一般的OS(程序)不同， 它更小更精炼，它不仅运行在核心态，而且开机后常驻内存， 它不会因内存紧张而被换出内存。 微内核所提供的功能: 操作系统的另一部分是内核，用来处理客户和服务器之间的通信， 即由内核来接收客户的请求，再将该请求送至相应的服务器；同时它也接收服务器的应答， 并将此应答回送给请求客户。 优点： 提高了系统的可扩展性 增强了系统的可靠性 可移植性好 提供了对分布式系统的支持 缺点 运行效率有所降低：消息传递开销+模式切换开销 ","link":"https://blog.shunzi.tech/post/os-one/"},{"title":"进程的描述与控制","content":" 该系列博文主要是针对操作系统课程中的一些知识点进行梳理。 更多地是介绍相关概念，后续会结合部分代码介绍 操作系统系列之二 第二章 进程的描述与控制 2.1 程序执行 2.1.1 程序顺序执行 特征： 顺序性：处理机的操作严格按照程序所规定的顺序执行。 封闭性：程序运行时独占全机资源，程序一旦开始执行，其执行结果不受外界因素影响。 可再现性：只要程序执行时的环境和初始条件相同，都将获得相同的结果。 （不论它是从头到尾不停顿地执行，还是“停停走走”地执行） 2.1.2 程序并发执行 特征： 间断性：程序并发执行是，由于共享系统资源，这些程序形成相互制约的关系，具有“执行-暂停-执行”特征。 失去封闭性：程序并发执行时，多个程序共享系统资源，因而这些资源的状态将由多个程序来改变，从而导致程序的运行失去封闭性。 不可再现性：程序并发执行，由于失去了封闭性，从而也失去了可再现性。 2.2 进程的描述 2.2.1 进程的定义和特征 2.2.1.1 典型的进程定义有： 进程是程序的一次执行。 进程是一个程序及其数据在处理机上顺序执行时所发生的活动。 进程是程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位 2.2.1.2 进程的结构 为使程序（含数据）能独立运行，应为之配置一进程控制块，即PCB（程序控制块） 由程序段、相关的数据段和PCB三部分便构成了进程实体（又称进程映像）。 所谓创建进程，实质上是创建进程实体中的PCB；而撤消进程，实质上是撤消进程的PCB。 PCB本质为一个结构体（eg:Linux中的PCB）： struct task_struct{ ... unsigned short uid; // 用户标识 int pid; // 进程标识 int processor;//标识用户正在使用的CPU,以支持对称多处理机方式 ... volatile long state; // 标识进程的状态 long prority; // 表示进程的优先级 unsighed long rt_prority;//表示实时进程的优先级，对于普通进程无效 long counter//进程动态优先级计数器，用于进程轮转调度算法 unsigned long flags; unsigned long policy;//进程调度策略 ... struct task_struct *next_task, *prev_task; struct task_struct *next_run,*prev_run; struct task_struct *p_opptr,*p_pptr,*p_cptr,*pysptr,*p_ptr; ... }; 2.2.1.3 进程的特征 (重要) 动态性：（状态在不断地发生改变） 进程的实质是进程实体的一次执行过程，因此，动态性是进程的最基本的特征。 动态性表现：“它由创建而产生，由调度而执行，由撤消而消亡”。可见，进程实体有一定的生命期。 程序是一组有序指令的集合，其本身并不具有运动的含义，因而是静态的。 并发性： 多个进程实体同存于内存中，且能在一段时间内同时运行。 独立性： 进程实体是一个能独立运行、独立分配资源和独立接受调度的基本单位。 异步性： 进程按各自独立的、不可预知的速度向前推进，或说进程实体按异步方式运行。 2.2.2 进程的基本状态及转换 （重要） 进程的三种基本状态： 就绪（Ready）状态：当进程已分配到除CPU以外的所有必要资源后，只要再获得CPU，便可立即执行。 执行（Running）状态：进程已获得CPU，其程序正在执行。单处理机系统中，只有一个进程处于执行状态，多处理机系统中，则有多个进程处于执行状态。 阻塞（Block）状态：正在执行的进程由于发生某事件而暂时无法继续执行时，便放弃处理机而处于暂停状态，把这种暂停状态称为阻塞状态，有时也称为等待状态。根据阻塞原因，系统中设置多个阻塞队列。 三种基本状态的转换 进程状态转换过程既有主动原因，也有被动原因 创建状态和终止状态 创建状态：如果进程所需资源尚不能得到满足，如无足够的内存空间，创建工作将无法完成，进程不能被调度，此时的进程状态为创建状态。 终止状态：一个进程到达了自然结束点，或者出现了无法克服的错误，或是被操作系统所终结，或是被其他有终止权的进程所终结，它将进入终止状态。 2.2.3 挂起操作和进程状态的转换 挂起状态 使执行的进程暂停执行，静止下来，我们把这种静止状态成为挂起状态 引入挂起状态的原因： 终端用户的请求用户；（发现程序运行有问题，将其暂停） 父进程请求；(挂起子进程，协调子进程的活动) 负荷调节的需要。当实时系统中的工作负荷较重，把一些不重要的进程挂起，以保证系统能正常运行。 操作系统的需要。操作系统有时希望挂起某些进程，以便检查运行中的资源使用情况或进行记账。 进程挂起状态的转换 有挂起状态的进程转换 挂起状态的思考 就绪/挂起（静止就绪）可以从活动就绪因为挂起操作转变为静止就绪状态，也可以从执行状态因为某些原因挂起转变为静止就绪状态 就绪/挂起状态无法直接进入执行状态 挂起状态无法直接进入执行状态 挂起进程无法自我激活，但是进程可以自我挂起（只要已知进程ID等有效信息） 2.2.4 进程管理中的数据结构 2.2.4.1 进程控制块的作用 独立运行基本单位的标志。 能实现间断性运行方式。（保护CPU现场） 提供进程管理所需要的信息。（OS通过PCB对进程- 实施控制和管理。） 提供进程调度所需要的信息。（提供进程状态、优先级等信息） 实现与其它进程的同步与通信。（消息队列指针，信号量等） 2.2.4.2 进程控制块的内容 进程标识符信息 处理器状态信息 进程调度信息 进程控制信息 进程标识符信息:进程标识符用于惟一地标识一个进程。一个进程通常有两种标识符： 内部标识符。为每一个进程赋予一个惟一的数字标识符，通常是进程的序号(Pid)。设置内部标识符主要是为了方便操作系统使用。 外部标识符。它由创建者提供(进程的名字)，通常是由字母、数字组成，往往是由用户（进程）在访问该进程时使用。 处理器状态信息:主要是由处理机的各种寄存器中的内容组成的。 通用寄存器，又称为用户可视寄存器。 指令计数器（PC），其中存放了要访问的下一条指令的地址。 程序状态字PSW，其中含有状态信息，如条件码、执行方式、中断屏蔽标志等 用户栈指针(SP)，用于存放系统调用参数及调用地址。栈指针指向该栈的栈顶。 这些都是中断和进程切换时需要保护的内容！ 进程调度信息:在PCB中还存放一些与进程调度和进程切换有关的信息。 进程状态。指明进程的当前状态。 进程优先级。 进程调度所需的其它信息。如：进程已等待CPU的时间总和、进程已执行的时间总和等； 事件。是指进程由执行状态转变为阻塞状态所等待发生的事件，即阻塞原因。 进程控制信息 程序和数据的地址，是指进程的程序和数据所在的内存或外存地址。 进程同步和通信机制，指实现进程同步和通信时必需的机制，如消息队列指针、信号量等，他们可能全部或部分的放在PCB中。 资源清单。进程所需的全部资源及已经分配到该进程的资源的清单； 链接指针。本进程所在队列的下一个进程的PCB的首地址。 2.2.4.3 进程控制块的组织方式 链接方式：把具有同一状态的PCB，用其中的链接字链接成一个队列。 索引方式：相同状态进程的PCB组织在一张表格中，系统根据所有进程的状态建立几张索引表，系统分别记载各PCB表格的起始地址。 多级队列：按照进程状态不同分别组织PCB队列，同一状态进程PCB按照优先级高低（或者到达的先后顺序）用链接指针连接起来。 2.3 进程控制 进程控制： 创建一个新进程，终止进程 进程运行中的状态转换。 进程控制一般是由OS内核中的一组原语来实现的。 2.3.1 原语 什么是原语？ 原语是由若干条指令组成的，用于完成一定功能的一个过程。是“原子操作”，即一个操作中的所有动作要么全做，要么全不做，换言之，是一个不可分割的基本单位，在执行过程中不允许被中断。 2.3.2 进程的创建 2.3.2.1 进程图（Process Graph） 进程图是用于描述一个进程的家族关系的有向树。 子进程可以继承父进程所拥有的资源。 当子进程被撤消时，应将其从父进程那里获得的资源归还给父进程。 在撤消父进程时，也必须同时撤消其所有的子进程。 2.3.2.2 引起创建进程的事件 导致一个进程去创建另一个进程的典型事件，可有以下四类： （1）用户登录。分时系统中，用户通过终端登录成功后，系统将为用户建立一个进程。 （2）作业调度。将作业调入内存后，创建进程。 （3）提供服务。例如：I/O请求，打印进程等。 （4）应用请求。基于应用进程的需求，由它自己创建一个新进程，以便使新进程以并发运行方式完成特定任务。 调用进程创建原语步骤： （1）申请空白PCB。 （2）为新进程分配资源。 （3）初始化进程控制块。 ① 初始化标识信息。 ② 初始化处理机状态信息。使程序计数器指向程序的入口地址，使栈指针指向栈顶； ③ 初始化处理机控制信息：进程的状态、优先级。 （4）将新进程插入就绪队列，启动调度。 2.3.3 进程的终止 （1）正常结束。 （2）异常结束： ① 越界错误。 ② 保护错。 ③ 非法指令。 ④ 特权指令错。 ⑤ 运行超时。 ⑥ 等待超时。 ⑦ 算术运算错、被0除： ⑧ I/O故障。 （3）外界干预 外界干预并非指在本进程运行中出现了异常事件，而是指进程因外界的请求而终止运行。 ① 操作员或操作系统干预。由于某种原因，例如，发生了死锁，由操作员或操作系统终止该进程； ② 父进程请求终止该进程； ③ 当父进程终止时，OS也将他的所有子孙进程终止。 （4） 进程的终止过程 根据被终止进程的PID找到它的PCB，从中读出该进程的状态。 若被终止进程正处于执行状态，应立即终止该进程的执行，重新进行调度。 若该进程还有子孙进程，立即将其所有子孙进程终止。 将被终止进程所拥有的全部资源，归还给其父进程，或者归还给系统。 将被终止进程的PCB从所在队列中移出。 2.3.4 进程的阻塞与唤醒 引起进程阻塞和唤醒的事件 1）向系统请求共享资源失败。 2）等待某种操作的完成：如I/O操作时进程进入阻塞状态，I/O完成后，被中断处理程序唤醒。 3）新数据尚未到达。处理数据的进程A阻塞，输入数据的进程B完成后去唤醒A。 4）无新工作可做, 如此时使用sleep（）进入休眠 进程阻塞过程 ① 正在执行的进程，当发现上述某事件时，由于无法继续执行，于是进程便通过调用阻塞原语block把自己阻塞； ② 把进程控制块中的现行状态由“执行”改为阻塞，并将PCB插入阻塞队列； ③ 转调度程序进行重新调度，将处理机分配给另一就绪进程，并进行切换。 进程唤醒过程 ① 当被阻塞进程所期待的事件出现时，则由有关进程（比如，用完并释放了该I/O设备的进程）调用唤醒原语wakeup()，将等待该事件的进程唤醒。 ② 唤醒原语执行的过程是：首先把被阻塞的进程从等待该事件的阻塞队列中移出，将其PCB中的现行状态由阻塞改为就绪，然后再将该PCB插入到就绪队列中。 2.3.5 进程的挂起和激活 进程的挂起：当出现了引起进程挂起的事件时（比如，用户进程请求将自己挂起，或父进程请求将自己的某个子进程挂起），系统将利用挂起原语suspend()将指定进程挂起或处于阻塞状态的进程挂起。 进程挂起的过程： 挂起原语的执行过程是： 1、首先检查被挂起进程的状态，若处于活动就绪状态，便将其改为静止就绪；对于活动阻塞状态的进程，则将之改为静止阻塞； 2、然后将被挂起进程的PCB复制到指定的内存区域。 进程的激活过程 ① 当发生激活进程的事件时，例如，父进程或用户进程请求激活指定进程，系统将利用激活原语active()将指定进程激活。 ② 系统利用激活原语active()将指定进程激活： 激活原语检查该进程的现行状态，若是静止就绪，便将之改为活动就绪； 若为静止阻塞，便将之改为活动阻塞。 进程控制原语可能引起的调度 时机：假如采用的是抢占调度策略，则每当有新进程进入就绪队列时，都应检查是否要进行重新调度。 创建、终止（自己）、挂起（自己）、激活、阻塞、唤醒都可能会产生新的调度。 2.4 进程的同步 2.4.1 进程的同步的基本概念 进程同步机制的主要任务，是对多个相关进程在执行次序上进行协调，使并发执行的诸进程之间能按照一定的规则（或时序）共享系统资源，并能很好的相互合作，从而使程序的执行具有可再现性。 间接相互制约关系 并发执行的进程在使用共享资源时的关系，资源的互斥。 直接相互制约关系 多个进程为完成同一项任务而相互合作的关系。 2.4.2 ","link":"https://blog.shunzi.tech/post/os-two/"},{"title":"Java基本常识","content":" 该博文主要是简单记录Java的一些基本知识 更多地是介绍相关概念，后续会结合部分代码介绍 会结合部分内存虚拟机的知识加深概念理解 Java体系架构 JavaSE : Java Standard Edition JavaEE : Java Enterprise Edition JavaME : Java Micro Edition Java语言的特点 面向对象 跨平台（提供了在不同平台下运行的解释环境） 健壮性（吸收了C/C++的特点） 安全性较高（自动回收垃圾，强制类型检查，取消指针） Java跨平台的原理 Java源代码 .java 编译为 Java字节码 .class（跨平台） Java字节码 .class 执行在 Java虚拟机 JVM 虚拟机JVM 基于不同操作系统（MAC、Linux、Windows) .class 文件由不同操作系统的虚拟机解释给操作系统。 JVM Java虚拟机 .class 对应的字节码在虚拟机上解释器再次进行“解释”和即使编译器上进行即时编译，再到达运行期系统，再到操作系统，再到硬件 JVM 可以理解成一个可运行Java字节码的虚拟计算系统，包含一个解释器组件，可以实现Java字节码和计算机操作系统之间的通信，对于不同的运行平台，有不同的JVM JVM屏蔽了底层运行平台的差别，实现了“一次编译，随处运行”。 垃圾回收器 不再使用的内存空间应当进行回收 - 垃圾回收 JVM 提供了一种系统线程跟踪存储空间的分配情况。 并在JVM的空闲时，检查并释放那些可以被释放的存储空间，自动运行。 JavaSE 的组成概念图 数组内存结构分析 栈内存（先进后出，栈内存大小是固定的） 存储确定大小的数据类型：临时变量，基本数据类型，引用类型。由于固定大小，所以存取速度较快。 堆内存 存储不确定大小的值（譬如值可能发生改变），需要临时地动态地进行分配，速度相对于栈较慢。 譬如: String[] name = {&quot;AA&quot;,&quot;bb&quot;,&quot;CC&quot;}; name 这个数组名会在栈内存中开辟空间，name 对应的是数组的地址 而 name 数组所对应的值则会在堆内存中开辟空间 多维数组 数组动态扩展,当数组要满时进行扩展 int[] array = new int[3]; int newlen = array.length*3/2 + 1; array = Arrays.copyOf(array,newlen); 对象与内存分析 new 关键字深入 new 关键字表示创建一个对象 new 关键字表示实例化对象 new 关键字表示申请内存空间 对象内存分析 class Dog{ String name; int age; } 栈内存中将储存对象的内存地址 给对象的属性赋值 在内存中创建多个对象 给多个对象属性赋值 声明两个对象，一个实例化，一个没有实例化 对象之间的赋值（只要类型相同就可以进行赋值） 分别实例化两个对象 此时对象之间的赋值（此时虚拟机将进行自动垃圾回收 GC ） 此时栈中没有对象指向堆中这一块内存，位于堆中的这一块内存将再也无法获取到，成为垃圾。 构造方法 构造方法是在类中定义的，构造方法的定义格式：方法名称与类名称相同，无返回类型的声明。 对象在实例化时，常常是（以无参构造方法为例，有参构造方法同理）： Type name = new Type(); 此时的 Type() 其实就是调用了相应的构造方法。 方法重载 ovrloading method 在类中可以创建多个方法，它们具有相同的名字，但具有不同的参数和不同的定义。 匿名对象 匿名对象就是定义一个没有名称的对象 该对象的特点是只能使用一次 该对象会直接在堆中开辟内存空间 该对象被使用后会成为垃圾对象，被GC回收 class Dog{ private String name; private int age; public say(); } // 该对象只能使用一次 new Dog().say(); 值传递与引用传递 示例：值传递 int x = 10; method(x); System.out.println(&quot;x=&quot;+x);//10 public static void method(int mx){ mx = 20; } 内存分析 示例：引用传递 Cat c = new Cat(); method(c); System.out.println(&quot;Cat age = &quot; + c.age);//5 public static void method(Cat cat){ cat.age = 5; } class Cat{ int age = 2; } 内存分析 示例：String 传递 String name = &quot;小白&quot;； method(name); System.out.println(name);//小白 public static void method(String sname){ sname = &quot;小红&quot;; } 内存分析： 执行 method() 之后 示例：String 传递 Cat c = new Cat(); method(c); System.out.println(&quot;Cat name = &quot; + c.name); public static void method(Cat cat){ cat.name = &quot;小黑&quot;； } class Cat{ String name = &quot;小白&quot;; } 内存分析 执行method()后 static关键字 java虚拟机内存结构 static关键字修饰的静态变量与方法就存储在方法区，static关键字修饰的成分不属于对象，直接属于类，而虚拟机中的方法区则主要是存放类的相关信息。线程共享，存储已经被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等等。 对于之前String类定义时，当使用直接赋值的方式时所创建的字符串常量不仅要在堆内存中进行创建，还要在方法区中的常量池中进行创建，每一个类对应一个常量池，对应相应的内存区域。 声明为static的方法的限制 static方法只能调用static方法 static方法只能访问和调用static数据 static方法不能以任何形式调用this和super 本质：static关键字修饰的属性和方法都是属于类的，并不属于对象，所以static修饰的属性和方法实在对象之前进行创建和加载的，故不能调用和访问属于对象的相关属性（非static修饰的属性和方法）。最好是使用类直接去调用static方法，虽然对象也可以调用static方法，但是static是属于类的 main方法 public 公共的 虚拟机需要调用 static 静态的 虚拟机需要调用 void 无返回值 虚拟机调用无需返回值 String[] args 表示参数为字符串数组 长度默认为0（不传参） 但实际可以传参 对象数组 foreach 与可变参数 jdk1.5 增强for循环 foreach for(类型变量名称：数组或集合){ //输出操作 } 可变参数，可用数组代替 返回值类型 方法名称(数据类型...参数名称){ //同一个类型的参数传入，数量可变 //但如果有其他类型的参数，需要将可变参数放在最末 } 代码块 普通代码块 直接写在方法中的代码块就是普通代码块 构造块 构造块是在类中定义的代码块,在构造对象时调用，先于构造方法执行 静态块 在类中使用static声明的构造块就是静态块,先于构造块执行。因为静态块是属于类的，而不属于对象，所以在加载的时候最先执行的是静态块。在类加载时执行，只执行一次 public class Demo{ //构造块 { String str = &quot;构造块&quot;; System.out.println(str);//此时无法输出，只有在new之后即构造之后才会执行 } public Demo(){ System.out.println(&quot;我是构造方法&quot;); } //静态块 static{ System.out.println(&quot;我是静态块&quot;); } public static void main(String[] args){ Code code1 = new Code(); Code code2 = new Code(); // 普通代码块 { String str =&quot;abc&quot;; System.out.println(str); } } } 单例设计模式 保证一个类仅有一个实例，并提供一个访问它的全局访问点。 构造方法私有化 声明一个本类对象 给外部提供一个静态方法获取对象实例 两种实现方式 饿汉式 先生成对象（从一开始就创建对象） 懒汉式 后生成对象（需要用到的时候才创建对象，涉及到多线程安全问题，需要改进） 应用：工具类一般设计为单例模式 public static void main(String[] args) { Singleton1 singleton1 = Singleton1.getInstance(); singleton1.print(); Singleton1 singleton12 = Singleton1.getInstance(); System.out.println(singleton1 == singleton12);//因为是同一个对象，输出为true Singleton2 singleton2 = Singleton2.getInstance(); singleton2.print(); } /** * 单例设计模式 * 饿汉式 */ class Singleton1{ // 定义一个对象并实例化 private static Singleton1 singleton1 = new Singleton1(); // 构造方法私有化 private Singleton1(){ } // 获取实例 的静态方法 public static Singleton1 getInstance(){ return singleton1; } public void print(){ System.out.println(&quot;饿汉式-单例设计模式&quot;); } } /** * 单例设计模式 * 懒汉式（多线程访问时会有安全问题） */ class Singleton2{ // 定义一个对象并实例化 private static Singleton2 singleton2 = null; // 构造方法私有化 private Singleton2(){ } // 获取实例 的静态方法 public static Singleton2 getInstance(){ if(singleton2 == null){ singleton2 = new Singleton2(); } return singleton2; } public void print(){ System.out.println(&quot;懒汉式-单例设计模式&quot;); } } 继承 继承是面向对象三大特征之一 被继承的类成为父类（超类，基类），继承父类的类成为子类（派生类） 继承是指一个对象直接使用另一个对象的属性和方法 通过继承可以实现代码重用 继承的限制 Java只能实现单继承，一个类只能有一个父类 允许多层继承，即一个子类只能有一个父类，父类又可以有自己的父类 继承只能继承非私有的属性和方法（非私有属性修饰符：public、default、protected） 构造方法不能被继承（实例化子类时会先调用父类的构造方法） 子类的实例化过程 在子类进行实例化操作的时候，首先会让父类进行实例化操作，即调用父类的构造方法，再实例化子类。 父类无默认的构造方法时，子类必须显式地调用父类的构造方法，利用super关键字（表示父类的引用），super(param)调用父类构造方法语句必须是第一句（子类构造函数中的第一句）。 方法的重写 方法的重写（overridingng method） 区分overloading 子类不想原封不动地继承父类的方法时，即想对从父类出继承来的方法进行一些自定义地改写，这就需要方法的重写，也称为方法的覆盖 方法的重写的特性 发生重写的两个方法返回值、方法名、参数列表必须完全一致 子类抛出的异常不能超过父类相应方法抛出的异常（子类异常不能大于父类异常） 子类方法的访问级别不能低于父类的相应方法的访问级别（子类访问级别不能低于父类的访问级别）子类继承后的方法的限定符范围需要大于父类。 属性的重写（意义不大） super关键字 调用父类中的属性、方法、构造方法 final关键字 final修饰类，表示为最终类，不能被继承 final修饰方法，方法不能再被子类重写 final修饰一个变量，变量将变为常量，不能在改变，一直保持初值。 可以在在声明的时候赋值。 可以在构造方法中赋值。 final 修饰一个类型的类，表示内存地址不能变 final 修饰基本数据类型，表示栈中的值不变 final 修饰一个对象的时候，表示栈中对象的地址不能变，但地址对应的对象是可以改变的。 抽象类 很多具有相同特征和行为的对象可以抽象为一个类；很多具有相同特征和行为的类可以抽象成一个抽象类。使用abstract关键字声明 抽象类的规则 抽象类可以没有抽象方法，有抽象方法的类必须是抽象类 非抽象类继承抽象类必须实现所有的抽象方法 抽象方法可以有方法实现和属性 抽象类不能被实例化 抽象类不能声明为final 抽象方法：只有声明，没有实现 接口 接口是一组行为的规范、定义、没有实现 面向对象设计法则：基于接口编程 interface 接口名称{ 全局常量; 抽象方法; } 接口可以继承多个接口 一个类可以实现多个接口 抽象类实现接口可以不实现方法 接口中的所有方法的访问权限都是public 接口中定义的属性都是常量 接口不能被实例化 多态 方法的重载和重写 重载发生在同一个类中，返回值相同，方法名相同，参数列表不同。 重写发生在子类和父类里，两种不同的形态 对象的多态性 对象多态性是从继承关系中的多个类而来 向上转型:将子类实例转为父类实例，可以自动转换 //char的容量比int小，所以可以自动完成 int i = 'a'; 父类 父类对象 = 子类实例;//自动转换 Person man = new Man();//父类的引用执行子类对象 Person woman = new Woman(); 子类 子类对象 = （子类）父类实例; 强制转换 Man m = (Man)man;//强制转换 Man m1 = (Man)woman;//报类型转换异常，转换失败 //整形（4个字节）比char（2个字节）要大，所以需要强制转换 char c = (char) 97; 方法的重载与重写就是方法的多态性表现 多个子类就是父类中的多种形态 父类引用可以指向子类对象、自动转换 子类对象指向父类引用需要强制转换（类型不对会报异常） 在实际开发中尽量使用父类引用(更利于扩展) 强制转换 在Java中强制类型转换分为基本数据类型和引用数据类型两种 子类可以自动转型为父类，但是父类强制转换为子类时只有当引用类型真正的身份为子类时才会强制转换成功，否则失败。 instanceof关键字 格式：对象 instanceof 类型 返回 booleann类型值 判断一个对象是否为某个例的实例，是就返回true，不是就返回false 父类设计法则 父类通常情况下都设计为抽象类或接口，其中优先考虑接口，如接口不能满足才考虑抽象类，接口优先 一个具体的类尽可能不去继承另一个具体类，这样的好处是无需检查对象是否为父类的对象 package com.review; public class InstanceofKeyWordDemo { public static void main(String[] args) { Person man = new Man(); say((Man)man);//父类对象需要强制转换为子类对象 Woman woman = new Woman(); say(woman); } // //static 和 返回类型 的位置不可交换 // public static void say(Man man){ // man.say(); // } // // public static void say(Woman woman){ // woman.say(); // } public static void say(Person person){ person.say(); // person.getAngry(); //由于父类没有该方法，故无法调用 // Woman woman = (Woman)person; //会报出类型转换异常,因为传入的person可能为man // 判断person对象是否为woman的实例 if(person instanceof Woman){ Woman woman = (Woman)person; woman.getAngry(); } } } abstract class Person{ private String name; public void setName(String name){ this.name = name; } public String getName(){ return name; } public abstract void say();//抽象方法 } class Man extends Person{ @Override public void say() { // TODO Auto-generated method stub System.out.println(&quot;I am a man!!!&quot;); } } class Woman extends Person{ @Override public void say() { // TODO Auto-generated method stub System.out.println(&quot;I am a woman!!!!&quot;); } // 本类中进行扩展的方法 public void getAngry(){ System.out.println(&quot;I am angry!!!&quot;); } } 抽象类应用 模板方法设计模式 定义一个操作中的算法的骨架，而将一些可变部分的实现延迟到子类中。模板方法模式使得子类可以不改变一个算法的结构即可重新定义该算法的某些特定的步骤。 接口应用 策略设计模式 定义了一系列的算法，将每一种算法封装起来并可以相互替换使用，策略模式让算法独立于使用它的客户应用而独立变化。 OO设计原则： 面向接口编程（面向抽象编程） 封装变化 多用组合，少用继承 Object类 Object类 是类层次结构的根类 每个类都使用Object作为超类。所有对象（包括数组都实现这个类的方法） 所有类都是Object类的子类 toString方法 //Object 类的 toString 方法返回一个字符串 //该字符串由类名（对象是该类的一个实例）、at 标记符“@”和此对象哈希码的无符号十六进制表示组成。 public String toString() { return getClass().getName() + &quot;@&quot; + Integer.toHexString(hashCode()); } equals()方法 public boolean equals(Object obj) { return (this == obj); } equals public boolean equals(Object obj)指示其他某个对象是否与此对象“相等”。 equals 方法在非空对象引用上实现相等关系： 自反性：对于任何非空引用值 x，x.equals(x) 都应返回 true。 对称性：对于任何非空引用值 x 和 y，当且仅当 y.equals(x) 返回 true 时，x.equals(y) 才应返回 &gt; true。 传递性：对于任何非空引用值 x、y 和 z，如果 x.equals(y) 返回 true，并且 y.equals(z) 返回 true，那么 x.equals(z) 应返回 true。 一致性：对于任何非空引用值 x 和 y，多次调用 x.equals(y) 始终返回 true 或始终返回 false，前提是对象上 equals 比较中所用的信息没有被修改。 对于任何非空引用值 x，x.equals(null) 都应返回 false。 Object 类的 equals 方法实现对象上差别可能性最大的相等关系；即，对于任何非空引用值 x 和 &gt;y，当且仅当 x 和 y 引用同一个对象时，此方法才返回 true（x == y 具有值 true）。 注意：当此方法被重写时，通常有必要重写 hashCode 方法，以维护 hashCode 方法的常规协定，该协定声明相等对象必须具有相等的哈希码。 参数： obj - 要与之比较的引用对象。 返回： 如果此对象与 obj 参数相同，则返回 true；否则返回 false。 另请参见： hashCode(), Hashtable equals 和 == 区别 语义上：==指的是内存引用一样。equals是指的是逻辑相等。逻辑相等具体的意思由编写者决定。 默认情况下(继承自Object类)，equals和==是一样的，除非被覆写(override)了。 最典型equals已经被override的例子是String； String中的字符串文本相等则视为逻辑相等（s1.equals(s2)==true）。 简单工厂模式 简单工厂模式是由一个公告对象决定创建出哪一种产品类的实例。简单工厂模式是工厂模式家族中最简单实用的模式。 /** * 工厂设计模式： * */ public class FactoryDemo { public static void main(String[] args) { // ClothDoll clothDoll = new ClothDoll(); // System.out.println(clothDoll.getInfo()); // // BarbieDoll barbieDoll = new BarbieDoll(); // System.out.println(barbieDoll.getInfo()); Doll clothDoll = DollFactory.getInstance(&quot;ClothDoll&quot;); if(null != clothDoll) System.out.println(clothDoll.getInfo()); Doll barbieDoll = DollFactory.getInstance(&quot;Barbie&quot;); if(null != barbieDoll) System.out.println(barbieDoll.getInfo()); } } //工厂类 降低耦合性（主函数与具体类之间的耦合） 解耦合 转移依赖关系 /** * 避免主类直接调用具体类来定义对象，然而被调用类可能发生了一些改变， * 那么对于main类的代码将会产生影响，可维护性下降，因为太过于依赖具体的类， * 故想让主类不与具体类产生依赖关系，所以选择引入工厂模式，也就是一个中间类， * 中间类中根据条件产生相应的对象，依赖具体的类，而主类只是需要调用工程类的 * 相应方法并传入不同的参数就可以产生相应的对象。 */ class DollFactory{ public static Doll getInstance(String name){ // 根据条件生产不同的对象 if(&quot;ClothDoll&quot;.equals(name)){ return new ClothDoll(); }else if(&quot;Barbie&quot;.equals(name)){ return new BarbieDoll(); }else{ return null; } } } interface Doll{ String getInfo(); } class ClothDoll implements Doll{ public String getInfo(){ return &quot;我是布娃娃，我怕脏&quot;; } } class BarbieDoll implements Doll{ public String getInfo(){ return &quot;我是芭比娃娃,我美得不可思议&quot;; } } 静态代理模式 把很多对象都具有的相同动作抽象出来，创建一个代理类。 /** * 代理模式（Proxy）：为其它对象提供一种代理以控制对这个对象的访问 */ public class ProxyDemo { public static void main(String[] args) { MiaiPerson person = new MiaiPerson(&quot;小白&quot;); //创建代理对象 Matchmaker matchmaker = new Matchmaker(person); matchmaker.miai(); } } /** * 主题接口 */ interface Subject{ public void miai(); } /** * 被代理的类 */ class MiaiPerson implements Subject{ private String name; public MiaiPerson(String name){ this.name = name; } public void miai(){ System.out.println(name+&quot;正在相亲中...&quot;); } } /** * 代理类 */ class Matchmaker implements Subject{ private Subject target;//要代理的目标对象 public Matchmaker(Subject target){ this.target = target; } public void before(){ System.out.println(&quot;为代理人匹配如意郎君。&quot;); } public void after(){ System.out.println(&quot;本次相亲结束&quot;); } @Override public void miai() { before(); // 真正执行相亲的方法 target.miai(); after(); } } 适配器模式 将一个类的接口转换成客户希望的另一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。 OO设计原则： 面向接口编程 封装变化 多用组合少用继承 对修改关闭，对扩展开放 public class AdapterDemo { public static void main(String[] args) { PowerA powerA = new PowerAImplA(); startA(powerA); PowerB powerB = new PowerBImpl(); //startA(powerB); 报错 PowerAAdapter pAAdapter = new PowerAAdapter(powerB); startA(pAAdapter); } public static void startA(PowerA powerA){ // ...... powerA.insert(); // ...... } //与startA的内容大部分重复 // public static void startB(PowerB powerB){ // // ...... // powerB.connect(); // // ...... // } } class PowerAAdapter implements PowerA{ private PowerB powerB;// 要进行适配的接口 public PowerAAdapter(PowerB powerB) { this.powerB = powerB; } @Override public void insert() { powerB.connect(); } } /** * 电源A接口 */ interface PowerA{ public void insert(); } class PowerAImplA implements PowerA{ @Override public void insert() { // TODO Auto-generated method stub System.out.println(&quot;电源A接口插入，开始工作。&quot;); } } /** * 电源B接口 */ interface PowerB{ public void connect(); } class PowerBImpl implements PowerB{ public void connect(){ System.out.println(&quot;电源B接口已连接。&quot;); } } 内部类 内部类的基本概念 在一个类的内部定义的类 class Outer{ class Inner{ } } 编译会产生两个class文件 eg.Dog.class eg.DogChildDog.class(ChildDog.class(ChildDog.class(符号表示子类) 在外部创建内部类对象 内部类除了可以在外部类中产生实例化对象，也可以在外部类的外部来实例化，那么，根据内部类生成的*.class文件：OuterInner.class,Inner.class,Inner.class,符号在程序运行时将替换为“.”，所以内部类的访问可以通过“外部类.内部类”的形式表示 Outer out = new Outer();//产生外部类实例 Outer.Inner in = null;//声明内部类对象 in = out.new Inner();//实例化内部类对象 方法内部类 内部类除了可以作为一个类的成员以外 ，还可以把类放在方法里定义 class Outer{ public void doSomething(){ class Inner{ public void seeOuter(){ } } } } 示例代码 public class InnerClassDemo1 { public static void main(String[] args) { Dog dog = new Dog(&quot;二妞&quot;); dog.say(); Dog.ChildDog childDog = null;//声明内部类引用 // 该方式一般不使用 childDog = dog.new ChildDog();//创建内部类的实例 childDog.talk(); dog.childTalk(); } } class Dog{ private String name; public Dog(String name){ this.name = name; } public void say(){ System.out.println(&quot;我是一只狗，主人叫我&quot; + name); } //内部类(成员内部类) class ChildDog{ public void talk(){ System.out.println(&quot;我是一只小狗狗，我妈是&quot; + name); } } public void childTalk(){ ChildDog childDog = new ChildDog(); childDog.talk(); } } 注意： 方法内部类只能在定义该内部类的方法内实例化，不可以在此方法外对其实例化。 方法内部类对象不能使用该内部类所在方法的非final局部变量 原因：局部变量位于内存中栈的位置，故方法中的局部变量就是该方法的域，两个大括号之间。而在方法中定义的类的作用域也只能是方法的域，但如果实现外部的接口，则该类创建的对象的作用域将变大，而局部变量的属性只在方法的域中，在外部的时候将无法正常使用。所以需要加final关键字使其变为常量 public class InnerClassDemo2 { } class Dog{ private String name; public Dog(String name){ this.name = name; } public void say(){ System.out.println(&quot;我是一只狗，主人叫我&quot; + name); } // 在方法里面声明一个内部类 public void childTalk(final String childName){ final int x = 10; class ChildDog{ public void talk(){ System.out.println(&quot;我是一只狗狗，我妈妈是&quot;+ name); System.out.println(&quot;x=&quot; + x); System.out.println(&quot;我的名字是&quot; + childName); } } ChildDog childDog = new ChildDog(); childDog.talk(); } } 静态内部类 静态的含义是该内部类可以像其他静态成员一样，没有外部类对象时，也能够访问它。静态嵌套类仅能访问外部类的静态成员和方法。 class Outer{ static class Inner{} } class Test{ public static void main(String[] args){ } } 匿名内部类 匿名内部类就是没有名字的内部类 匿名内部类的三种情况 继承式的匿名内部类 接口式的匿名内部类 参数式的匿名内部类 public class InnerClassDemo4 { public static void main(String[] args) { // 1、继承式匿名内部类 Dog dog = new Dog(&quot;小白&quot;){ public void say(){ System.out.println(&quot;我是一只womanDog&quot;); } }; dog.say(); // 2.接口式匿名内部类 Child child = new Child() { @Override public void talk() { // TODO Auto-generated method stub System.out.println(&quot;我是一只小狗狗&quot;); } }; child.talk(); // 3、参数式匿名内部类 childTalk(new Child() { @Override public void talk() { // TODO Auto-generated method stub System.out.println(&quot;我是一只小狗狗&quot;); } }); } public static void childTalk(Child child){ child.talk(); } } class Dog{ private String name; public Dog(String name){ this.name = name; } public void say(){ System.out.println(&quot;我是一只狗，主人叫我&quot; + name); } } class WomanDog extends Dog{ public WomanDog(String name){ super(name); } public void say(){ System.out.println(&quot;我是一只womanDog&quot;); } } interface Child{ public void talk(); } 注意： 不能有构造方法，只能有一个实例 不能定义任何静态成员、静态方法 不能是public protected private static 修饰 一定是在new的后面，用其隐含实现一个接口或实现一个类 匿名内部类为局部的，所以局部内部类的所有限制都对其生效。 内部类的作用 每个内部类都能独立地继承自一个（接口的）实现，所以无论外部类是否已经继承了某个（接口的）实现，对于内部类没有影响。如果没有内部类提供的可以继承多个具体的或抽象的类的能力，一些设计与编程问题就很难解决。从这个角度看，内部类使得多重继承的解决方案变得完整。接口除了解决了部分问题，而内部类有效地实现了&quot;多重继承&quot;类没有影响。如果没有内部类提供的可以继承多个具体的或抽象的类的能力，一些设计与编程问题就很难解决。从这个角度看，内部类使得多重继承的解决方案变得完整。接口除了解决了部分问题，而内部类有效地实现了&quot;多重继承&quot; ","link":"https://blog.shunzi.tech/post/java-one/"},{"title":"四分之一","content":" 大一结束的暑假写的一些总结和规划 更多地是胡言乱语吧，随笔记录 应该会有后续吧 小时候一直都在幻想自己长大后的光彩模样，也渐渐认清自己到底什么样。不断长大，经历越来越多的事情，走到不同阶段，不同的心态，不同的为人处世。也慢慢地也从一切的按部就班的所谓的寒窗生涯到了疲于选择的咸鱼日常。 之前的一切按部就班完全是建立在自己想要未来一切无所拘束的奢望的基础上的，毕业之后觉得解脱了，觉得自己身上全是年轻，可以做自己很多想做的事，比如和自己喜欢的人在一起，比如生活也渐渐有了自己的角色，存在感也逐渐爆棚～。慢慢地不免沉溺于所谓的虚荣，对外在的有利条件要求太多，而内心又不够充实，好比已经被虫蛀空了树干的大树，结局也很显然就算再枝繁叶茂，也只不过是昙花一现，最终暴露在人们眼前的其实是腐烂与不堪。 在这个四分之一的起点，难免走弯路、犯错、碌碌无为，并不能一味地否定自己，招摇地说自己走在错的路上，其实就是因为四分之一起点的原因呢，一个曾经没有睁眼的人，现在要自己走了，睁眼之后免不了自己不断去尝试，浅尝辄止也罢，深陷其中也罢，都是在慢慢地习惯自己去看，自己去闯，也渐渐褪去之前的依赖和稚气。 从零到四分之一的这个结点，有很多前所未有的经历和体验，有的是张狂，有的是压抑，有的是担当，有的是逃避，有的是放纵，有的是克制……没心没肺的大笑，凌晨马路旁的歌声，酒瓶的清脆撞击，让人肚子疼的三俗笑话，不分昼夜的卧谈会，正经事面前的不假思索，处心积虑的邂逅，假装镇定的寒暄，天花乱坠的计划和打算，随波逐流的盲从，分身乏术的慵懒，逃避选择的恣意妄为，信誓旦旦的作死flag，麻烦面前的随性而为，玩笑里的一本正经，自私自利的套路和当局者的如痴如醉，人过境迁的清醒和旁观者的谈笑自若；不担当不退却，热衷于言听计从，不自大不谦逊，仍不忘怒刷存在感。也有无数温柔待人的暖心细节，也还是经历过落寞郁闷致死的漫漫长夜……还算丰富，还算精彩，但也还是有那么一些不甘。的确不如那些计划安排的都好好的人一样能够从容自若地跟着自己的节奏，或是忙忙碌碌的充实，或是闲情逸致的欢愉，又或是随性到底的洒脱，郁郁而终的悔恨……但至少也有一波未平一波又起的波折，比起百无聊赖，跌宕起伏的起承转合倒是还能让人时刻对生活有一种期待与好奇，也就没了当初的愤世嫉俗、自我伤非，反而倒显得多了一些从容，多了一些成熟。 以前的不服输不甘示弱死要面子在经历了这么一年之后不说完全戒除，至少有了改观。当年那位与众人舌战群儒喋喋不休自命不凡的少年现在学会了宽容、隐忍、砥砺德行，相信每个人都和自己一样，有自己的想法，有自己的为人处世的方式，每个人都有自己的不同，而不再和以前一样非要把自己的主观意志强加给对方，很多事本没有对错，有的只是不同的方式和不同的结果。也渐渐明白不是所有都能通过自己感天动地的努力就能如愿以偿的，有的时候其实只是剑走偏锋，也不是所有的收获都得通过最后的一锤定音来真实反映的，其实对于自己，没必要那么功利地要求自己去得到一个他人艳羡自我满足于虚荣带来的快乐的结果，可能一路走来尽收眼底的匆匆逝去的一棵树一阵风才是最好的收获。就好比自己喜欢一个人的时候觉得自己要和她一起走过看过一路的风雨和彩虹，其实无意中会发现有那么一个在慢慢成长的自己，学会体谅学会担当学会照顾学会自立学会爱人，结果历来都是平分秋色，一拍即合或是形同陌路，但在这个追逐的过程里一直都是一位满腔热血，无畏风雨，顶天立地的渐渐长大的少年的身影！ 对于家人，自己就好像常在港口往来的帆船，曾经的十多年一直躲在避风港里无数次幻想出海后的天地，到了出海的那一天还是对未知的好奇要超过离开港湾的不舍，慢慢的一个人去经历风雨，桅杆折断，逆风航行，经历了一些之前未经历过的苦难后也开始怀念小港的安静与舒服，渐渐也体会到了灯塔守护人的良苦用心，无论风雨，灯塔照旧，就好像一直在等，等着一条伤痕累累需要回到港口休整然后继续出发的小船。出海后的天地很广阔，十多年一直向往未知和自由的船儿也渐渐喜欢上了弄潮时的惊险、平静时的安宁，喜欢上了潮起潮落，喜欢上了这充满无数未知惊喜的海，也就慢慢地忘了帆后一直闪着的灯光，也忘了灯塔守护人的坚持……相比之前而言，自己在对家人的态度还是有了很大改观，慢慢地理解到他们作为长辈的深思熟虑与责任担当，也知道自己未来也会接过他们手中的接力棒，或是主动关心，或是主动说一句以前难以启齿的想念，又或是说一些父母最想听到的儿女的日常，种种种种……在自己长大的同时也不忘回头看看身后默默守护灯塔的家人，除了寒暄，这次回家一定要尝试只有很小的时候才会有的拥抱，不为别的，就只为了把这么多年憋在心里没有表达出来的爱意至少在今后的日子里能够充分闪耀！ 可能在总结了这么一番之后，外在的东西并不会改变多少，但至少知道自己还是在长大，心智也渐渐成熟，可能对于外在还是存在几分苛求，但相信自己也会慢慢地努力，不再有当年困难面前畏缩不前的胆小懦弱手足无措，而是迎难而上的不惧风雨沉着处事。也还有很多没尝试过的一些长大路上必须经历的事，但总会有身体力行的那一天的，现在的话做好心理准备，抓住机会，慢慢地扛起属于自己的担子，不再想东想西，而是有一些真正意义上会倾尽自己心血的事，一些自己要真正坚持、真正去拼去努力得到的所谓的“奢求”，当然也不忘身边的人以及身后的人，慢慢蜕去扎人的刺，用坦诚和爱来慢慢抚平之前因为自己带给周围人的“创伤”。 总之，小时候床头挂的牛头配饰上面写的四个大字，可能就是我这毕生追寻的信仰了！——顶天立地 ","link":"https://blog.shunzi.tech/post/One quarter/"}]}