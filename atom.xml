<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>YouDieInADream</title>
    <updated>2021-12-04T14:20:18.312Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, YouDieInADream</rights>
    <entry>
        <title type="html"><![CDATA[ChameleonDB: a Key-value Store for Optane Persistent Memory]]></title>
        <id>https://blog.shunzi.tech/post/ChameleonDB/</id>
        <link href="https://blog.shunzi.tech/post/ChameleonDB/">
        </link>
        <updated>2021-09-28T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 EuroSys2021 - ChameleonDB: a Key-value Store for Optane Persistent Memory</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 EuroSys2021 - ChameleonDB: a Key-value Store for Optane Persistent Memory</li>
</ul>
</blockquote>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>Optane DC PM 的出现使得利用其高带宽和低延迟构建持久性 KV 存储成为了热点。Optane Pmem 本质上是一种具有两个不同属性的混合存储设备，这是Optane Pmem面临的一个主要挑战。
<ul>
<li>一方面，是一个高速的<strong>字节寻址</strong>的设备，类似于 DRAM</li>
<li>另一方面，对 Optane 的<strong>写操作以 256bytes 进行</strong>，更像一个块设备。</li>
</ul>
</li>
<li>现有的基于持久性内存的 KV 存储设计没有考虑到后面的因素，导致出现了<strong>较高的写放大以及受限的读写吞吐量</strong>。与此同时，直接重用原有的为块设备设计的 KV 存储，如 LSM，也将因为<strong>字节寻址的特性而导致更高的读延迟</strong>。</li>
<li>本文中我们提出了 ChameleonDB，一个考虑且利用了 PM 的两个特性的基于混和 内存/存储 场景的设计。使用 LSM Tree 以低写放大率有效地接受写入，且使用了一个 DRAM 中的 HASH 表来 bypass LSM 的多层结构从而快速读。与此同时，ChameleonDB 可能选择在后台维护LSM多级结构，以在系统崩溃后实现较短的恢复时间。ChameleonDB 的混合结构被设计成能够吸收写负载的突发写入，从而避免长尾延迟。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>现有的 KV 存储设计在 PM 场景中面临主要三个挑战：</li>
</ul>
<h3 id="challenge-1-optane-pmem-is-a-block-device">Challenge 1: Optane Pmem is a Block Device</h3>
<ul>
<li>在 PM 发布之前就有大量研究人员提出了该设备场景下的 KV 存储。这些设计都假设 PM 是一个稍微慢一点且持久的 DRAM，相应地，这样的设计通常构建一个持久化哈希表或持久化树来索引存储日志中的 KV 项。当新的KV项目按照到达顺序批量写入日志时，索引上的相应更新(通常)在非连续的内存位置单独进行(由哈希函数或树结构确定)。
<ul>
<li>HASH:
<ul>
<li>Level hashing</li>
<li>CCEH</li>
</ul>
</li>
<li>tree-structured
<ul>
<li>WB+ Tree</li>
<li>FAST&amp;FAIR</li>
</ul>
</li>
</ul>
</li>
<li>不幸的是，这些前面的假设和最终商业 PM 相关性能特性的研究表现不一致。</li>
<li>据报道，Optane Pmem的写单元大小为256B，为了理解这种性能特征的含义，我们将特定大小的数据写入与256B单元大小对齐的随机选择的地址。在实验中，我们将写大小从8B改变为128KB，并使用不同的线程数，以便能够达到内存的峰值带宽。测试结果如下所示，当写大小小于 256B 时，写吞吐量比256B及其以上的写吞吐量要小很多，更有意思的是，256B 的吞吐量几乎是 128B 的两倍，128B 又几乎是 64B 的两倍，这强有力地证明了设备 256B 访问单元的特性。<strong>任何非连续的小于该大小的写入都要进行一个 RMW 读后写的操作来生成一个 256B 的写，从而导致写入放大和并减少了有效的内存带宽</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210428155831.png" alt="20210428155831" loading="lazy"></li>
<li>这样的特性造成之前基于假设的 KV 存储的设计遭受了写性能的损失，原则上类似于对其他块设备(如机械硬盘和 SSD)的小写操作。特别的是，利用持久性的 HASH 表以及树结构的 KV 存储，在 key 插入、rehash、rebalance等过程中每个对索引的更新都是小写操作，比如 16bytes，远小于 256byte，此时写的放大高达 16。由于没有充分考虑设备的写单元，这些设计无法提供高写性能。</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210629140723.png" alt="20210629140723" loading="lazy"></figure>
<h3 id="challenge-2-optane-pmem-is-of-high-speed">Challenge 2: Optane Pmem is of High Speed</h3>
<ul>
<li>有大量的工作都是在解决基于块设备的 KV 存储中的小写问题，如 LevelDB/RocksDB/Cassandra/LSM-trie/PebblesDB 都是学术界以及工业界中的例子。他们聚合了最近的更新操作并批量顺序写这些数据到磁盘，根据 key 比较的结果或者hash函数的结果。由于维护一个大的已排序列表的开销太大，所以要维护多个和指数级长的已排序列表。每个列表位于其单独的 level，从 L0 开始是最短的列表，每个 k+1 层 level 都有着 k 层的 r 倍容量。r 的值在不同的 KV 存储中不同，例如 LevelDB/RocksDB r=10，LSM-trie r=8，在 KV 项被写入到存储之后，初始在 L0，然后向下移动直到最后一层。</li>
<li>LSM 树结构有两种主要的压缩方案，它们对写放大和读性能有不同的影响。leveling 和 size-tiering。
<ul>
<li>leveling: 在相邻两层的 KV 项被归并排序然后重新插入到更下面的一层。随着下层的 Key 的个数达到了上层的很多倍之后，每个压缩的写入放大倍数可以大到10(以LevelDB为例)。</li>
<li>size-tiering: 每一层包含多个具有重叠的键范围的子 levels，在子 Level 之间进行键的归并排序操作，结果将被写入到更低一层的新的子 level 中。这样的话，该压缩的写入放大总是为 1。确实该策略可以显著减少额外的写操作，但也会显著增加子 level 的个数，然后增加了在存储中查询一个 Key 的开销。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210303230248.png" alt="20210303230248" loading="lazy"></li>
</ul>
</li>
<li>而基于 LSM 的设计似乎是在Optane Pmem上部署的一个很好的候选方案，它为块设备设计了内置的多级结构，不幸的是，<strong>它与Optane的高读性能不兼容</strong>。因为它的多层设计，读一个 Key 需要检索很多层，从 L0 开始，直到这个 Key 被找到或者找到最后一层都没找到这个 Key。我们的目标是每次搜索 Key 只有一个磁盘读取，在 DRAM 中为每个 KV 项所在的块维护了 BloomFilter，从而在从该层读取磁盘上的数据之前先判断是否该 Key 存在于这个 Block 中。<strong>相比于毫秒级别的磁盘访问时间，对于 filter 的纳秒级的操作几乎可以忽略。只要实际只发生一次磁盘读取，这样的设计就能在磁盘上的 KV 项上实现最好的读取延迟。然而，当存储设备是Optane Pmem 时，情况就大不相同了，它的读延迟本身是纳秒级的，只有 DRAM 读延迟的3倍左右</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210629140829.png" alt="20210629140829" loading="lazy"></li>
<li>为了理解 Optane 高速访问的影响，我们选择构建了一个基于 HASH 的 KV 存储， 7 层 LSM-trie，分别在 SATA SSD、PCIe SSD 和 Optane PMem 上。在不同的层级上读取 Key 并报告其读延迟，如下图所示。 从表中读取项的时间(在图中表示为“Table read”)是高度一致的，无论键驻留在哪个级别，因为只需要读取一次磁盘(或Pmem)。如图2(a)和2(b)所示，当存储在 SSD 上运行时，花费在过滤器(在图中表示为“Filter Check”)上的时间只占很小的一部分。因此，在 Bloom flters 帮助下的磁盘上的 KV 存储，使用多层次结构不会损害读性能。然而如图 c 所示，当 Optane Pmem 使用的时候，花费在过滤器上的时间就变得很长，相比于 Pmem 的读取时间。KV 对在较低的层级时，它不断增加，最终成为不可接受的。这个观察结果表明，<strong>多级结构成为实现稳定的的低读延迟的主要障碍。同时，同样的结构对于允许批写以适应块设备也是必不可少的</strong></li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210428225223.png" alt="20210428225223" loading="lazy"></figure>
<h3 id="challenge-3-optane-pmem-is-non-volatile">Challenge 3: Optane Pmem is Non-volatile</h3>
<ul>
<li>为了避免前面提到的挑战，研究者已经提出了当 Pmem 仅用于存储KV项作为存储日志时，将索引结构移动到 DRAM 上。因为 KV 项批量写到日志中没有任何写放大，然后所有的索引的读和更新都发生在 DRAM 上，这样的设计提供了高吞吐和低延迟。（<strong>索引位于 DRAM 上</strong>）
<ul>
<li>ASPLOS’20 FlatStore</li>
<li>FAST'19 uDepot</li>
<li>SOSP'19 KVell</li>
</ul>
</li>
<li>然而，让整个索引或者绝大部分索引都在易失的内存中就一定程度上消除了 Optane Pmem 作为持久性内存的优势，Pmem 本身就是承诺快速故障恢复的介质。对于一个存储了几十亿的 KV 对的 KV 存储，DRAM 中的索引可能占据的空间超过 100GB，而且有限的 DRAM 空间本身就是被多种系统和应用共享，一旦索引数据在丢失，从存储日志中重建这么大的索引是会花费特别长的时间的。与此同时，快速的恢复和重启是很重要的，特别是在虚拟化的环境中，一个 KV 存储可能宿主在虚拟机或者容器中，这些虚拟机容器本身的启动时间就只有几秒甚至不到秒的级别。所以<strong>把最近的对于索引结构的更新周期性地保存在 PMem 中，并使Pmem上的索引有组织并可以很快使用 对于 LSM Tree 的设计是非常有用的</strong>。</li>
<li><strong>（目标：利用 PMem 进行快速故障恢复，减少 DRAM 消耗，主要是快速构建索引）</strong></li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li><strong>in-DRAM index</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210629140940.png" alt="20210629140940" loading="lazy"></li>
</ul>
<h3 id="solution">Solution</h3>
<ul>
<li>虽然现有的KV存储设计不能同时实现Optane Pmem上期望的多个目标(<strong>高写吞吐量、低读延迟、高动态工作负载下良好的读尾延迟、小DRAM占用空间、快速恢复和重启</strong>)，但我们提出了一个KV存储设计，名为ChameleonDB，可以在一个系统中实现这个目标。为了证明 ChameleonDB 的实力，下图中和基于 HASH 位于 Pmem 的索引存储以及基于 HASH 位于 DRAM 的索引存储进行了相应的四个性能维度的对比。和写吞吐高度相关的写放大，读延迟，内存占用，恢复时间。
<ul>
<li>PmemLSM 对应传统的带有 Bloom 过滤器的 LSM 树状 KV 存储设计，它有很长的读延迟。（<strong>因为 LSM 和布隆过滤器</strong>）</li>
<li>DRAM_HASH 对应内存中维护索引，PMem 中维护日志，会有比较大的内存占用和很长的恢复时间。（<strong>因为内存 HASH 表内存占用，索引重建开销大</strong>）</li>
<li>Pmem-Hash 对应了持久的 HASH 表设计，会有比较大的写放大，相应地导致较低的写吞吐。（<strong>因为 HASH 操作均为小写，基于 PMem 的特性，存在写放大</strong>）</li>
<li>相反，ChameleonDB 在四个维度都表现得很好。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210429141642.png" alt="20210429141642" loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="contribution">Contribution</h4>
<ul>
<li>我们分析了现有的在 PMem 上构建的 KV 存储设计的缺点，证明了他们中还没有方案可以同时实现低读延迟、低 DRAM 消耗、快速重启以及高吞吐。特别的，我们揭示了在Optane Pmem上部署基于LSM 的 KV 存储的困境，据我们所知，这尚未在开放文献中进行讨论。</li>
<li>我们提出了 ChameleonDB，一个新的为 PMem 设计的 KV 存储，一定程度上讲，该设计是一个混合设计。利用了其中一个存储(pmems-hash、pmems-lsm 和 dram-hash)的各自优势来解决其他存储可能存在的问题。特别的，<strong>使用了一个多层的结构来高效地持久化索引上的更新操作，使用了一个内存中的 HASH 表来加速小规模的最近更新的索引的读操作，然后使用了在 Pmem 上的哈希表来遍历存储中的大部分 Key，它还提供了一种操作模式，用于减少长读尾延迟</strong>。</li>
<li>我们实现了 ChameleonDB 并进行了测试对比，和其他几种结构的最先进的方案进行了对比。实验表明 ChameleonDB 成功地同时实现了前面提到的目标，且和其他存储相比要在一个或者多个维度上表现得更好。</li>
</ul>
<h2 id="design">Design</h2>
<ul>
<li>
<p><strong>整体结构</strong></p>
<ul>
<li>KV 分离</li>
<li>HASH 分区</li>
<li>DRAM-NVM 混合索引<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210629141059.png" alt="20210629141059" loading="lazy"></li>
</ul>
</li>
<li>
<p>ChameleonDB 是一个 <strong>Value 存储在存储日志上的 KV 存储</strong>，同时 <strong>keys（或者他们的 hash 值）以及对应值的地址信息被存储在相应的持久性索引</strong>中，如下图所示。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210429162043.png" alt="20210429162043" loading="lazy"></p>
</li>
<li>
<p>KV 项根据请求到达顺序被批量写入到 Value Log 中，持久性索引是一个有多个 shards 的高度并行的结构，每个 shard 有自己的多层结构以及自己的 compaction 操作。Keys 根据相应的 hash 值分布在这些 shards<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210629145203.png" alt="20210629145203" loading="lazy"></p>
</li>
</ul>
<h3 id="a-multi-shard-structure">A Multi-shard Structure</h3>
<ul>
<li>ChameleonDB 的索引被组织成多个 shard 的结构，每一个 shard 覆盖了相等范围的 HASH key space。一个 shard 是一个多层的类 LSM 的结构，如下图所示。</li>
<li>每一个 Level 会有很多个子 levels，被称之为 tables，<strong>每一个又被组织成固定大小的 hashtable，使用线性探测的方法来解决 HASH 冲突</strong>。和其他基于 LSM 的 KV 存储一样，每个 shard 有一个内存中的 Memtable 来聚合 KV 项。当 Memtable 满了之后，即负载因子达到了阈值，这时候执行刷回，从 DRAM 到 PMem，最终在 Pmem 上组织成持久的不可变的 L0 level 上的 Tables。每一层能够容纳的最大的 Table 数量是由层级之间的比例系数 r 来确定的，除了最后一层只包含一个 Table。下图所示的例子中的 r 为 4，因此在有四个 Memtables 被 flush 到 Level0 之后然后变满，从而触发 Compaction<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210429162627.png" alt="20210429162627" loading="lazy"></li>
<li>Leveling 和 Size-Tiering 两种策略各有优缺，ChameleonDB在不同的LSM级别上使用这两种压缩方案来提供低写放大和低读延迟。在每一个 shard，size_tiering 主要用于压缩上层，就是在 PMem 上的除了最后一层以外的层次，leveling compaction 用于压缩最底层。这种混合压缩策略在 SIGMOD’18 的 Dostoevsky 中也有应用被称之为 lazy leveling，在写放大和读延迟之间达到平衡，并且比单独使用两种方案中的任何一种性能更好</li>
<li>基于 LSM 的 KV 存储的 Compaction 的策略通常都是在两个相邻的层次之间进行的。以如下所示的图中的例子来说，L0 满了触发到 L1 的 compaction，然后级联 compaction 触发到 L2 的 compaction。</li>
<li>在 ChameleonDB 中，<strong>我们引入了 Direct Compaction 算法通过允许在多个 levels 之间进行 compaction 而减少了压缩开销</strong>。为了完成如下图 a 所示的 compaction，Direct Compaction 首先触发了一个包含 L0,L1,L2 的 compaction，如图 b 所示。同样地，最后一层的 compaction 会在 L0 满且上层 level 都有 r-1 个 tables 的时候执行。在 shard 执行了最后一层的 compaction 之后，shard 上层的所有表都被清除因为其数据项已经被移动到了最后一层。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210429163911.png" alt="20210429163911" loading="lazy"></li>
<li><strong>随着单个 shard 中每个 table 的大小（包括内存中的 Memtable）超过了 256B（PMem 的访问单元），需要按照 256B 大小进行对齐，flushing/compacting table 可以完整利用 PMem 的写带宽，解决了前面描述的第一个挑战。在此之后，每一次 flush memtable 之后，ChameleonDB 持久化 LSM 数据结构的修改，故障恢复只需要重启恢复没有持久化的 Memtable 数据，从而解决挑战三</strong>。</li>
</ul>
<h3 id="the-auxiliary-bypass-index-abi-in-a-shard">The Auxiliary Bypass Index (ABI) in a Shard</h3>
<ul>
<li>因为每一个 shard 是一个多层的结构，一个 get 操作因为需要一层一层检查 tables 直到找到目标的 Key 或者到达最后一层，会有比较严重的读延迟，如下图 a 所示。因为 ChameleonDB 在上层使用了 size-tiering 的压缩策略来减小写放大，levels（以及子 level）的数量已经显著增加，而<strong>在 LSM 中使用的比较多的布隆过滤器，没办法为这种较长的读延迟提供高效的解决方案，因为检查 filters 本身就会占据 PMem 读延迟的大约 50%</strong>，这就是我们为什么需要一个新的解决方案来减小读延迟，也就是 ChameleonDB 辅助旁路索引  Auxiliary Bypass Index（ABI）</li>
<li><strong>每个 shard 有自己的 ABI，本质是一个内存中的 HASH 表，索引了所有的上层 Keys，通过使用 ABI，在查询 Keys 的时候最多查询三个表，一个 Memtable，一个 ABI，一个最后一层的 table</strong>，如图 b 所示。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210429171653.png" alt="20210429171653" loading="lazy"></li>
<li><strong>ABI 包含且只包含存在于上层的 KV 项，当 Keys 被持久化到 L0 的时候，也需要被插入到 ABI 中，在数据项被合并到最后一层的时候，相应地需要从 ABI 中移除对应的索引</strong>。为了实现这样的效果，ChameleonDB 在执行 MemTable 到 L0 的刷回的时候把数据项添加到 ABI，在最后一层的 Compaction 执行之后删除 ABI 中所有的数据项，因为在最后一层的 Compaction 之后所有的上层数据都会被清除。PinK 也尝试着把上层以一个多层的 LSM 结构给固定住，而 ABI 将上层的 keys 组织成一个 hash table，在 DRAM 中以 O(1) 的时间复杂度进行访问。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507171610.png" alt="20210507171610" loading="lazy"></li>
<li>除了减小尾延迟以外，ABI 也能有助于加速最后一层的 Compaction，ABI 包含一个 shard 的上层的所有数据项，Direct Compaction 是把上层所有的 tables 给压缩到最后一层的 table，<strong>不再需要把所有的持久化的上层 Tables 读取出来然后压缩到最后一层，而是直接合并已经存在于 DRAM 中的数据项到最后一层，如下图所示，从而减小最后一层 Compaction 的开销。通过使用 ABI，我们避免了检查多个 levels，从而解决上述挑战2</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507172253.png" alt="20210507172253" loading="lazy"></li>
<li><strong>直接合并 ABI 和 Last Level，这里的前提是 ABI 的数据和 Upper Level 的数据一致，也就是说都是存储的 索引</strong></li>
</ul>
<h3 id="trade-restart-time-for-the-put-performance">Trade Restart Time for the Put Performance</h3>
<ul>
<li>ChameleonDB shards 的多层结构帮忙减少了重启时间，因为只有 Memtable 需要在重启过程中恢复数据，同时 Memtables 只包含了非常小比例的数据，比如对于一个 4 层结构，且系数为 4 的存储大概只包含了 1/256。然而，维护一个多层 LSM 结构消耗了读带宽、CPU 时钟周期、以及写带宽，因为 Compaction 操作需要与 put 请求一起不断进行，这会降低 put 性能。对于传统的 LSM 设计，比如 LevelDB\RocksDB，多层的结构不得不总是被维护，否则，读延迟会大打折扣，因为越来越多的memtable被刷新到0，而没有被压缩就会产生很多级别，导致遍历次数变多。除此以外，DRAM 的消耗也会快速增加，因为 Memtable 变得越来越大来容纳更多的 KV 项。相反，<strong>ChameleonDB 的结构中，上层通常用于快速恢复，提供了一个不用妥协读延迟和 DRAM 消耗来维护多层结构的机遇</strong>。（加 HASH 索引）</li>
<li>在系统特别稳定以及系统很少故障的场景中，ChameleonDB 可以通过暂时暂停多级结构的维护来换取更高的 put 性能，而无需在密集的 put 工作负载期间进行上层压缩，从而降低系统崩溃时重启时间延长的风险。这样的策略我们称之为 <strong>Write-Intensive Mode</strong></li>
<li>在 Write-Intensive Mode 下，Memtables 当满了的时候不再被刷回到 L0，L0 到 L1 的压缩不会被触发，同样地，<strong>所有的压缩到其他高层也不会进行。只有当 ABI 已满时，才会触发清除 ABI 的最后一级压缩，如果这时候一个系统故障发生了，重启时间可能会很长因为 ABI 中所有的项必须从存储日志中恢复</strong>。实验表明该模式下的重启时间仍然比 DRAM-Hash 要小，DRAM-HASH 的整个索引都需要在重启过程中重建。此外，ChameleonDB 重启时间的长短可以将 Write-Intensive Mode 作为一个用户选项来进行控制。</li>
<li><strong>简而言之，Write-Intensive Mode 就是不使用上层的索引，停止维护多级结构，ABI 满了则进行合并</strong></li>
</ul>
<h3 id="handling-put-bursts">Handling Put Bursts</h3>
<ul>
<li>读操作尾延迟通常用来测量 QoS，后台压缩任务对于尾延迟有比较高的影响因为该操作消耗 Optane Pmem 读写带宽，在一个 Put 操作爆发期间，读操作的尾延迟可能显著增加，因为后台压缩任务被触发。</li>
<li>为了提供更好的服务质量，我们在 ChameleonDB 中引入了一个动态的 <strong>Get-Protect Mode</strong> 来监控读操作的尾延迟并调整 Compaction 的时机。具体而言，<strong>就是一个读操作尾延迟到达一个阈值的时候，ChameleonDB 挂起所有的上层 Compactions，包括 flush 操作，延缓最后一层压缩的执行</strong>。挂起上层压缩的影响就是重启时间可能因为不止要恢复 Memtable 还有 ABI 来扫描整个 Log 从而变得很长，和 WriteIntensive Mode 类似，当 ABI 满了之后，所有的数据项需要被压缩到最后一层，假设 DRAM 空间被限制了，且不能容纳一个二级 ABI。然而，最后一层的压缩需要读取最后一层的数据并和 ABI 中的数据进行合并，并写回 Optane Pmem，该操作的开销可能非常大，且显著影响尾延迟，因此 <strong>Get-Protect Mode  下只是将 ABI 中的内容 dump 到 Pmem 上作为一个新的级别，而没有进行合并。这将增加get 请求检查的 levels 的数量。然而，根据我们的实验结果，这种副作用相对于最后一级压缩的成本是适度的</strong>。此外，我们限制了可以转储到 Optane Pmem 的 ABI 的数量(默认为一个)。Dumped 的 tables 将在 put burst subsides 之后被合并到最后一层。当尾延迟低于预先设定的阈值时，将取消 Get-Protect Mode。</li>
<li><strong>简而言之，Get-Protect Mode 就是挂起 Compaction，ABI 满了则 dump 到 PMem</strong></li>
</ul>
<h3 id="implementation-details">Implementation Details</h3>
<h4 id="randomized-load-factors">Randomized Load Factors</h4>
<ul>
<li>哈希表通常用作一种可扩展的结构，其大小随插入的项而变化。一旦一个表被认为是满的，它将被展开，其中的项目将被重新散列到新的位置。重新散列是一个耗时的过程。因此，在ChameleonDB的一个分片中，我们为上层的键使用一个固定大小的散列表，以避免频繁地进行重散列，因为散列表的大小变化很大。</li>
<li>ChameleonDB的分片中的每个表(或子层)也是一个散列表。它通过线性探测来解决碰撞问题。确定此类哈希表是否已满的常用方法是在插入新项时探测它的次数。但是，使用探测计数来确定表是否已满可能会与 ChameleonDB 中的压缩操作发生冲突。例如，由于插入了一个探测数大于阈值的项，一个级别中的4个表中的项可能无法插入到比它大4倍的新表中。另一方面，允许无限的探测时间也不是一个可接受的选择，因为在最坏的情况下，一个 get 请求可能需要扫描整个表，导致太长的 get 延迟。</li>
<li>在ChameleonDB中，我们限制了MemTable的加载因子，从而控制所有持久性表的加载因子(因为它们的所有项都来自MemTable)。例如，当我们将MemTable的负载因子限制为不超过75%时，表的四个槽中就有一个是空的。当MemTable的负载因子达到预定义的阈值时，认为它已经满了。当到达一个空槽时，对哈希表的扫描就会停止，<strong>因此降低负载因子来增加表中的空槽可以改善获取延迟，但代价是降低空间效率和更频繁的压缩。对所有分片使用统一的负载因子阈值将导致压缩突发，因为插入通常会均匀地分布到 shards 上。在压缩爆发期间，所有shards 都需要进行上层压缩，甚至更糟的是，最后一级压缩的KV存储会经历一个显著的性能退化期。为了缓解压缩突发，ChameleonDB为 shards 使用随机加载因子，从而错开不同 shards 的压缩时间。</strong></li>
</ul>
<h4 id="dram-footprint">DRAM footprint</h4>
<ul>
<li>使用ABI改善获取延迟的一个主要问题是它的 DRAM 占用。众所周知，每个分片中的级别大小呈指数级增长。<strong>尽管ABI包含了所有上层的条目，但它只占总指数的中等比例</strong>(例如，1/r ，层次之间的比率为 r )。</li>
</ul>
<h4 id="write-amplifcation">Write Amplifcation</h4>
<ul>
<li>ChameleonDB中的写放大(不包括对存储日志的写)与级别数(表示为l)、级间比r和负载因子(表示为f)有关。写哈希表的写放大为1/f。例如，一个负载因子为75%、大小为1MB的哈希表只有0.75MB的用户数据，因此写入这个表的写入放大倍数是1/0.75。由于ChameleonDB使用大小分层(size-tiering)来进行中层压缩，使用水平分层(leveling )来进行最后一级压缩，ChameleonDB的写入放大倍数为(l−1 + r)/f。</li>
</ul>
<h4 id="kv-items-in-the-storage-log">KV items in the storage log</h4>
<ul>
<li>存储日志中的每个条目都有{key, value_size, value}的形式，其中，key 和 value_size 的大小固定在8个字节，而 value 的大小(也就是 value_size)是可变的。日志项首先在DRAM中缓冲以形成批处理。然后，当批处理的大小达到预定义的阈值(例如，4KB)时，将批处理附加到日志的尾部。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>为了观察和理解 ChameleonDB 在真实系统中的性能行为，并了解它是否实现了所有的设计目标(高写吞吐量、低读延迟、响应请求峰值对读尾延迟的影响，以及快速恢复和重新启动)，我们在英特尔Optane持久性存储器(Pmem)上开发了 ChameleonDB 原型。</li>
</ul>
<h3 id="setup">Setup</h3>
<ul>
<li>ChameleonDB以它的App Direct模式运行，通过调用Persistent的开源库中的函数，通过load和store指令访问内存，通过调用 PMDK 中开源库的函数。</li>
<li>所有实验都在两台服务器上进行8核Intel Xeon Silver 4215处理器，64GB DRAM和两个128GB Optane Pmem。两个Optane Pmem内存条连接到同一个插座，并以交错模式运行。在我们的实验中，KV存储运行在处理器上，本地访问Optane Pmem，以获得更高的访问效率。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507225125.png" alt="20210507225125" loading="lazy"></li>
<li>对比对象：
<ul>
<li>DRAM-HASH：DRAM 中维护 HASH 索引（robin-hood hash table）</li>
<li>Pmem-HASH：PMem 中的持久性 HASH 索引 CCEH</li>
<li>Pmem-LSM-F：带布隆过滤器且位于 PMem 的 LSM</li>
<li>Pmem-LSM-NF：不带布隆过滤器且位于 PMem 的 LSM</li>
<li>Pmem-LSM-PinK：除了最后一层以外所有层驻留在 DRAM 上的 LSM，且不使用布隆过滤器（<strong>该对照组是最公平的对照组，资源开销相近</strong>）</li>
</ul>
</li>
<li><strong>写性能</strong>：
<ul>
<li>PMem-hash 最差，因为在 Optane 上进行了大量的随机写，与 256B 不匹配，写放大严峻</li>
<li>ChameleonDB, Pmem-LSM-NF, and Pmem-LSM-PinK 性能相近，比 Pmem-LSM-F 好很多，这三个方案性能优势相近，因为写入都是大而顺序的，且都没有布隆过滤器的开销。其中 ChameleonDB 和 Pmem-LSM-PinK 在 Compaction 期间使用了一些 DRAM 读代替 PMem 读。三个方案的性能差距较小。因为在 compaction 中发挥了 Pmem 顺序读的优势，与更昂贵的 CPU 使用和哈希操作期间随机 dram 访问 KV 项和压缩相比，读的性能影响大大降低。DRAM-HASH 占用内存更多，恢复更慢，大规模数据场景瓶颈就越明显。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204527.png" alt="20210507204527" loading="lazy"></li>
</ul>
</li>
<li>延迟除 PMem-HASH 以外是比较接近的。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210630110740.png" alt="20210630110740" loading="lazy"></li>
<li>读方面，Pmem-LSM-NF 最差，DRAM-HASH 最高，其次为本文的方案。ChameleonDB 比 PinK 好是因为 PinK 在 DRAM 维护了多层结构 LSM 需要进行多次检查（level by level）。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204550.png" alt="20210507204550" loading="lazy"></li>
<li>延迟和吞吐的比较结果一致。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210630111312.png" alt="20210630111312" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204635.png" alt="20210507204635" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204648.png" alt="20210507204648" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204710.png" alt="20210507204710" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210507204752.png" alt="20210507204752" loading="lazy"></li>
</ul>
<h2 id="related-work">Related Work</h2>
<h3 id="persistent-hashing">Persistent Hashing</h3>
<ul>
<li><strong>PFHT</strong>：是一种布谷鸟哈希变体，它通过在一次写操作中最多允许一次 displacement 来优化以减少在服务写请求期间的内存写操作</li>
<li><strong>Level Hashing</strong>：应用两级散列方案，以便每个键可以有三个桶作为插入的候选桶，这有助于提高负载因子，来代替双重哈希</li>
<li><strong>CCEH</strong>：CCEH是一种可扩展的散列，它使用线性探测策略，因此成功的插入只需要一次内存写操作</li>
<li>这些工作设计的写优化的 HASH 都是试图减少 PM 上的写入，但是现阶段在 Optane 上的就地更新会有严峻写放大，ChameleonDB 则聚合了数据。</li>
</ul>
<h3 id="key-value-stores-for-persistent-memory">Key-value stores for Persistent Memory</h3>
<ul>
<li><strong>SLM-DB</strong>：使用一个全局 B+tree 作为索引，数据追加写日志。因为全局索引所以只能存在 PM 上，但又造成小写。而本文的方案使用 HASH 表，占用更小，可以放在内存中</li>
<li><strong>FlatStore</strong>：维护一个易失的全局索引，数据丢 PM，故障恢复时间长。本文只需要恢复一小部分。</li>
<li>准确理解存储设备的性能特征对于在设备上进行有效的KV-store设计至关重要。</li>
<li><strong>HiKV</strong> 假设了延迟方面 PM 写比 DRAM 差，读相近，但其实最近研究发现 PM 写延迟更接近 DRAM，读延迟更差。本文的设计通过只在 DRAM 中进行小的写操作和使用基于 DRAM 哈希表的旁路索引减少读操作来适应这些真正的性能特征。</li>
<li><strong>Bullet</strong> 使用交叉引用日志(CRLs)技术将持久内存写移出关键路径，解决了优化的持久内存和 DRAM 常驻 KV 存储之间的性能差距。本文则是使用不同的方式来消除这个性能 gap，使用内存哈希表来减少对 Optane Pmem 的访问，然后批量执行小写。</li>
<li><strong>NoveLSM</strong> 和 <strong>MatrixKV</strong> 则主要是利用 PM 来提升写性能，NoveLSM 维护更大的 Memtable 在 PM 上来直接在 PM 上执行小写，MatrixKV 则是把 L0 放在 PM 上来优化 compaction 使其更细粒度。但是本文的方案比这两个方案都要好，因为适配了 256B 访问单元避免了小写，使用了共享数据结构来减少 LSM 的层级数，从而细粒度压缩并减小放大，同时使用了 size-tiering 和 leveling compactions 来减小写放大并保证低延迟。</li>
</ul>
<h3 id="key-value-stores-for-fast-ssd">Key-Value Stores for Fast SSD</h3>
<ul>
<li><strong>KVell</strong>：利用无共享结构和异步 I/O 解决方案来充分利用磁盘 I/O 带宽，避免 CPU 瓶颈，从而提升性能</li>
<li><strong>Zhang et al.</strong>：提出了 FPGA 加速 compaction 的 LSM KV 存储来减小 CPU 瓶颈</li>
<li><strong>PinK</strong>：消除了上层的 BF 并使用基于 FPGA 的 SSD 消除了 compaction 过程中的 CPU 的开销</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[X-Engine: An Optimized Storage Engine for Large-scale E-commerce Transaction Processing]]></title>
        <id>https://blog.shunzi.tech/post/X-Engine/</id>
        <link href="https://blog.shunzi.tech/post/X-Engine/">
        </link>
        <updated>2021-08-30T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 SIGMOD19: X-Engine: An Optimized Storage Engine for Large-scale E-commerce Transaction Processing</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>该篇文章来自于 SIGMOD19: X-Engine: An Optimized Storage Engine for Large-scale E-commerce Transaction Processing</li>
</ul>
</blockquote>
<!--more-->
<h2 id="x-engine">X-Engine</h2>
<h3 id="abstract">Abstract</h3>
<ul>
<li>X-Engine，这是在阿里巴巴构建的 POLARDB 的一个写优化存储引擎，它使用了带有 LSM 树(日志结构的合并树)的分层存储架构，利用 FPGA 加速压缩等硬件加速，以及一套优化，包括事务中的异步写、多级流水线和压缩期间的增量缓存替换。评估结果表明，X-Engine 在事务性工作负载下的性能优于其他存储引擎。</li>
<li><strong>简单概括</strong>：
<ul>
<li>大量的优化都是通过利用多核来异步并发执行一些操作，并使用流水线的相关技术来争取把 CPU 和磁盘打满，并一定程度上进行协同。<strong>但开启大量线程的代价就是极高的 CPU 占用</strong>，所以后面又提出了使用 FPGA 来加速 compaction，通过将 compaction 任务进行拆分，使得 FPGA 可以参与。</li>
<li>其他的几个优化在 RocksDB 上本身也已经得到了应用。
<ul>
<li>L0 层内 compaction 来提高 L0 的有序性加速查询和 compaction</li>
<li>细粒度缓存 KV Cache 和 Block Cache 的协同；细粒度缓存效率对点查询的优化效率更高，粗粒度缓存在 compaction 过程中进行直接替换来代替缓存失效淘汰</li>
<li>元数据索引的多版本 CoW 更新来代替直接 Update 整个元数据</li>
</ul>
</li>
<li>还有就是
<ul>
<li>并发内存跳表，跳表内热数据的多版本优化，数据按版本链式保存</li>
<li>事务的分阶段处理，流水线优化</li>
<li>compaction 过程的数据重用，结合上面的元数据 CoW</li>
<li>内部各个操作的优先级调度，主要是对 compaction 调度</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="introduction">Introduction</h3>
<ul>
<li>下图是双 11 负载的变化情况，概括下来就是在特定时间点请求可能呈海啸式增长。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210828173902.png" alt="20210828173902" loading="lazy"></li>
<li>以前处理这种负载的方式为使用一个共享的架构，来在多个 DB 实例中进行分布式的事务处理，然后在峰值来临之前对 DB 实例进行扩展，但是这样由于需要大量实例，它需要大量的资金和工程成本。而本文提出的方法则是 <strong>提升单机的存储引擎容量来解决该问题</strong>，从而使得需要的实例数减少，给定固定成本的可实现吞吐量增加。</li>
<li>针对这种洪泛式的负载，需要快速将数据从内存移动到持久化存储，同时还需要处理高并发的电商事务处理。这种在线促销的负载主要表现为数个小时的事务陡增，包含大量的写入。即便内存大小不断增大，但是比起负载还是小很多，所以必须在 RAM/SSD/HDD 混和存储场景下来重复利用存储资源。X-Engine 根据数据的冷热来进行数据的分层放置，甚至使用 NVM。</li>
<li>LSM 比较适合这种分层结构存储，除了 LSM 以外，还有一些加速写的数据结构，LSM、VLDB12 LogBase、以及优化的树结构，甚至混和结构 bLSM，但是对于我们的场景都不够高效；另一些则通过交换点和范围查询的性能来提高写的性能，因为写不适合混合读和写的电子商务工作负载。</li>
<li>大多数数据库负载，热记录都在一个稳定的时间内表现出了很强的空间局部性，记录的空间位置随时间变化很快。这是因为随着时间的推移，在不同的类别或记录上有不同的促销活动。例如，全天都有针对不同类别或品牌的秒杀促销活动(销售热门商品，你必须抓住它们可供购买的那一秒)，以刺激需求，吸引顾客购买随时间变化的不同商品。这意味着数据库缓存中的热记录会不断变化，任何记录的温度都可能从冷/暖到热或从热到冷/暖。如果我们将数据库缓存视为一个储存库，而将底层(大型)数据库视为海洋，则这种现象将导致当前(即热记录vs .冷记录)在非常深的海洋中快速地向任何方向移动(例如，存储在X-Engine上的巨大数据库)。存储引擎需要确保新出现的热记录能够尽可能快地从深水中检索并有效缓存。</li>
<li><strong>Contributions</strong>
<ul>
<li>利用线程级别的并行在内存中处理大量请求，从事务中解耦写请求来异步执行，将长写路径分解为流水线中的多个阶段，以增加总体吞吐量</li>
<li>利用重定义的 LSM 结构来在分层存储器件上在不同的层次之间移动数据，并优化 compaction 算法</li>
<li>把 compaction offload 到 FPGA</li>
<li>CoW 更新的多版本元数据索引来加速点查询，而不管数据的温度</li>
</ul>
</li>
</ul>
<h3 id="system-overview">System Overview</h3>
<ul>
<li>电商场景数据热度变化频繁，且包含大量的写请求，考虑使用 LSM，但是传统的 LSM 不适合该负载，所以我们做了大量的优化。X-Engine 可以部署在 POLARFS 上，POLARFS 使用了大量的新技术如 RDMA NVMe 等</li>
<li>下图展示了 X-Engine 的架构，把每个 Table 分区到了多个子表，维护了一个 LSM 树，为每个子表关联 metasnapshot 和 索引，每个数据库实例有个 REDO LOG，每个 LSM 树由驻留在内存中的热数据层和 NVM/SSD/HDD 的 温/冷 数据层组成，之后会被分区到不同层。数据的热温冷代表着理想的访问频率，然后相应地被放到对应的层。热数据层包含一个 active memetable 和多个 immutable memtable，都是跳表，存储最近插入的数据，并缓冲热记录。温冷数据层组织数据在一个类似树的结构中，每一层的树存储有序的 extents，一个 extents 打包了对应的数据块、索引块和过滤器块，我们正在探索机器学习技术来识别适当的数据温度，这在附录C中有简要的讨论，它的全部细节超出了这项工作的范围。</li>
<li>X-Engine 利用 redo logs、metasnapshots、和 index 来支持事务多版本并发控制，每一个 metasnapshot 有一个元数据索引来追踪所有的 memtables 和所有层次的 extents，树的一个或多个相邻层组成一个层，分别存储在NVM、SSD和HDD上。在X-Engine中，表被划分为几个子表。每个子表都有自己的热、热和冷数据层(即LSM树)。X-Engine 以面向行格式存储记录。我们设计了一个多版本memtables 来存储不同版本的记录，以支持MVCC(在第3.2.1节介绍)。在磁盘上，元数据索引跟踪存储在区段中的记录的所有版本。我们将在3.1节详细介绍数据结构。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830115307.png" alt="20210830115307" loading="lazy"></li>
<li><strong>The read path</strong>: 读路径是从存储器中检索记录的过程。原来的lsm树设计没有很好的读取性能。查找首先搜索memtable。在memtable中，它必须一个一个地遍历每一层。在最坏的情况下，查找必须以扫描所有级别结束，直到最大级别得出查询记录不存在的结论。为了加速这一过程，提出了一个 manifest file 来定位包含查询键的目标SST。在每个 SST 内还应用了 Bloom 过滤器，以促进早期终止</li>
<li>为了为电子商务交易中常见的点查找获得良好的响应时间，我们<strong>优化了区段的设计，引入了跟踪所有 memtable 和区段的元数据索引，以及一组缓存以促进快速查找</strong>。我们还提出了<strong>压缩中的增量缓存替换方法，以减少压缩导致的不必要的缓存迁出</strong>。我们引入快照以确保查询读取记录的正确版本</li>
<li><strong>The write path</strong>: 写路径包括物理访问路径和在存储引擎中插入或更新记录的相关过程。在lsm树KV存储中，到达的键值对被附加或插入到活动memtable中。一旦完全填充，活动memtable就会切换为不可变的，等待刷新到磁盘。同时，创建一个新的空活动memtable。为了支持高并发事务处理，存储引擎需要通过在持久存储(例如SSD)中记录新记录，并将它们高速插入memtable。在这个过程中，<strong>我们区分了长延迟的磁盘 I/O 和低延迟的内存访问，并将它们组织在一个多级流水线中，以减少每个线程的空闲状态，提高总体吞吐量</strong>(章节3.2.3)。为了实现高水平的并发性，我们进一步<strong>将写的提交与事务处理分离开来，并分别优化它们的线程级并行性</strong>。</li>
<li><strong>Flush and Compaction</strong>: LSM 树依赖于刷新和压缩操作来合并数据，这些数据可能会淹没从memtable到磁盘的主内存，并保持合并数据的有序顺序。不可变memtable被刷新到Level0，在此期间记录被排序并打包到排序序列表(SSTs)中。每个SST都独占一个 Key 范围，因此一个级别可以包含多个SST。当 Level-i 的 sst 规模达到某一阈值时，与Leveli+1键值域重叠的sst合并。这个合并过程在某些系统中称为压缩，因为它还会删除标记为要删除的记录。最初的压缩算法从这两层读取 sst，合并它们，然后将合并后的结果写回 lsm 树。<strong>这个过程有几个缺点:它消耗大量的cpu和磁盘 I/O;同一条记录从LSMtree读取和写入多次，导致写入放大;它会使正在合并的记录的缓存内容无效，即使它们的值保持不变</strong></li>
<li>在X-Engine中，我们首先<strong>优化不可变memtables的刷新</strong>。对于压缩，我们<strong>应用数据重用来减少要合并的区段数量，异步I/O与磁盘I/O重叠，以及FPGA加载来减少CPU消耗</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830152307.png" alt="20210830152307" loading="lazy"></li>
</ul>
<h3 id="detailed-design">Detailed Design</h3>
<ul>
<li>X-Engine 应用 MVCC 和两阶段锁来实现 Snapshot isolation 和 Read Commited 的隔离级别来保证 ACID，同一记录的不同版本被存储为具有自动递增版本 ID 的单独元组。X-Engine 将这些版本的最大值跟踪为 LSN (日志序列号)。每个传入事务使用它所看到的LSN 作为快照。事务只读取比它自己的 LSN 更小的最大版本的元组，并为它所写的每个元组添加行锁，以避免写冲突。</li>
<li>下图展示了事务处理的流程。这个过程包括一个读/写阶段和一个提交阶段。事务的所有读请求都是通过访问lsm树的读路径在读/写阶段提供的。在此阶段，<strong>将在事务中插入或更新的记录写入事务缓冲区</strong>。接下来，在<strong>提交阶段，将从事务缓冲区写入存储的任务记录分发到多个写任务队列。通过记录相应的记录并将它们插入到lsm树中</strong>，引入了一个多级流水线来处理所有这些写任务。下面我们将详细介绍X-Engine的数据结构、读路径、写路径、刷新和压缩<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830152521.png" alt="20210830152521" loading="lazy"></li>
</ul>
<h4 id="the-read-path">The read path</h4>
<ul>
<li>我们从数据结构的设计开始，包括区段、缓存和索引。对于每个数据结构，我们将介绍它如何促进读取路径中的快速查找。</li>
</ul>
<h5 id="extent">Extent</h5>
<ul>
<li>图4显示了区段的布局，由数据块、元数据和块索引组成。记录以面向行的样式存储在数据块中。元数据 schema 跟踪每个列的类型。块索引保留每个数据块的偏移量。在生产系统的当前部署中，我们在 LSM 树的所有级别上将区段的总大小调优为2 MB。因为许多电子商务事务以高度倾斜的方式访问记录，所以保持这种大小的区段允许在压缩期间重用许多区段(详见第3.3.2节)。这种设计还有助于压缩期间的增量缓存替换(在3.1.4节中引入的优化)<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830153138.png" alt="20210830153138" loading="lazy"></li>
<li>我们在每个区段中使用版本存储模式数据 schema，以加速 DDL(数据定义语言)操作。通过这种设计，当向表中添加新列时，我们只需要在新版本的新区段上强制执行这个新列，而不需要修改任何现有的区段。当查询读取带有不同版本模式的区段时，它将与最新版本保持一致，并将带有旧模式的记录的空属性的缺省值。这种快速 DDL 特性对于在线电子商务业务非常重要，因为在线电子商务业务经常根据需求的变化调整数据库模式的设计。</li>
</ul>
<h5 id="cache">Cache</h5>
<ul>
<li>图5说明了XEngine中的数据库缓存。我们专门为点查找优化了行缓存，点查找是阿里巴巴电子商务交易中的大多数查询。行缓存使用LRU缓存替换策略缓存记录，而不管记录在 LSM 树中的哪个级别。 因此，即使是最大级别的记录也可以被缓存，只要查询访问它。一旦点查找错过了 memtables，查询的键就被散列到行缓存中相应的匹配槽中。因此，在点查询的行缓存中检索记录只需要 O(1)时间。当随机查找访问记录时，行缓存的影响较小。</li>
<li>我们在行缓存中只保存最新版本的记录，由于时间局部性，这些记录被访问的机会最大。为了实现这一点，我们<strong>在刷新期间用行缓存中的新记录替换旧版本的记录，从而减少因刷新而导致的缓存丢失</strong></li>
<li><strong>块缓存以数据块为单位缓冲数据。它为每个没有从行缓存c查询或范围查询请求提供服务</strong>。表缓存包含子表头的元数据信息，这些子表头通向相应的区段。找到区段后，我们使用Bloom筛选器筛选不匹配的键。然后，我们搜索索引块来定位记录，并最终从它的数据块中检索它。</li>
<li>这些缓存对于温度变化后减少记录缓存丢失非常重要。由于记录的空间局部性，行缓存中出现的热记录和已有的热记录可能来自相同的区段甚至相同的数据块。因此，<strong>表和块缓存有助于提高缓存未命中后的总体缓存命中率，并可能有助于减少行缓存中替换的延迟</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830153356.png" alt="20210830153356" loading="lazy"></li>
</ul>
<h5 id="multi-version-metadata-index">Multi-version Metadata Index</h5>
<ul>
<li>图 6 说明了 X-Engine 中多版本元数据索引的结构。<strong>每个子表的LSM 树都有其关联的元数据索引</strong>，该索引从表示子表的根节点开始。<strong>索引的每次修改都会创建一个新的元快照，它指向所有相关的级别和memtable，而不需要修改现有元快照的节点(即copy-on-write方法</strong>)。</li>
<li>在图6中， extent i 最初是 Level0 的一部分，且被缓存(用红色表示)。当重用此区段的压缩完成时，MetaSnapshot v 旁边会创建一个新的 MetaSnapshot v+1，链接到新合并的 Level1 (颜色较深)。Level1 的元数据只需要指向extent i，而不需要在磁盘中实际移动它(用紫色表示)，从而保持所有缓存内容的完整性。<strong>利用这种写时复制方法，事务可以以只读方式访问它们想要的任何版本，而不需要在数据访问期间锁定索引</strong>。我们使用垃圾收集来删除过时的元快照。RocksDB 等其他存储引擎也探索过类似的设计。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830154135.png" alt="20210830154135" loading="lazy"></li>
</ul>
<h5 id="incremental-cache-replacement">Incremental cache replacement</h5>
<ul>
<li>在lsm树中，由于压缩合并了磁盘中的许多区段，它通常会导致大量缓存驱逐，降低了查找时的缓存命中率，并导致明显的性能下降和不稳定的响应时间。即使缓存记录的值没有改变，如果它们的区段与压缩中涉及的其他区段共享重叠的键范围，则它们可能已经在磁盘中移动了。</li>
<li>为了解决这个问题，我们建议在块缓存中进行增量替换，而不是从缓存中删除所有 compacted 区段。在压缩期间，我们检查要合并的区段的数据块是否被缓存。如果是这样，我们<strong>将缓存中的旧块替换为相同位置的新合并块，而不是简单地清除所有旧块。这种方法通过保持块缓存中一些块的更新和不移动来减少缓存丢失</strong>。</li>
</ul>
<h4 id="the-write-path">The write path</h4>
<ul>
<li>在本节中，我们从优化接收每个子表的 LSM 树的传入记录的memtable结构开始。接下来，我们介绍如何设计写任务队列和写路径中的多阶段流水线，它们由X-Engine中所有子表的 LSM 树共享。</li>
</ul>
<h5 id="multi-version-memtable">Multi-version memtable</h5>
<ul>
<li>我们实现 memtable 作为一个无锁的skiplist，像许多其他系统一样，以实现良好的查找和插入性能[9]。然而，在查询热记录时，基于skiplist的memtable的最先进实现存在性能问题。对单个记录的频繁更新会生成多个版本。如果一个热记录与只关注最新版本的查询的谓词匹配，则查询可能必须扫描许多旧版本来定位所请求的版本。电子商务平台上的在线促销活动，在消费者下单购买热门商品时，放大了这些多余的访问。</li>
<li>在X-Engine中，我们<strong>垂直地将相同记录的新版本附加到原始节点旁，形成一个新的链表</strong>。图7显示了提议的结构，其中蓝色节点存储具有不同键的记录，黄色节点存储同一记录的多个版本。<strong>不同的键被组织在一个跳表中，而每个键的多个版本存储在一个链表中。热记录上的更新会增加相应的链表节点</strong>。此外，通常被传入事务引用的新版本被保持在唯一键的skiplist的底层，如图7所示，其中版本99是最新的版本。<strong>该设计减少了扫描不必要的老版本的数据造成的开销</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830160234.png" alt="20210830160234" loading="lazy"></li>
</ul>
<h5 id="asynchronous-writes-in-transactions">Asynchronous writes in transactions</h5>
<ul>
<li>在InnoDB这样的存储引擎中，传统的一线程一事务方法在写效率上有很大的缺陷。在这种方法中，用户线程从头到尾执行事务。虽然这种方法很容易实现写的执行和事务的并发控制，而用户线程自己不写日志，线程必须等待磁盘IOs写日志的长延迟完成</li>
<li>在X-Engine中，我们选择了另一种方法，将<strong>写的提交与相应的事务解耦，并将它们分组以进行批处理</strong>。如图3所示，我们首先将写任务分配到多个无锁写任务队列中。在此之后，大多数线程可以异步返回来处理其他事务，这样在多级流水线(将在下面介绍)中，每个队列只留下一个线程来参与提交写任务。通过这种方式，事务中的写入是异步的。在高并发工作负载中，这种方法允许更多的线程处理来自并发事务的写任务。队列的最佳数量取决于机器中可用的I/O带宽，以及每个无锁队列上方的多个线程之间的争用。我们发现，在32核机器中，每个队列中有8个线程会使I/O带宽饱和，由于竞争，为每个队列分配更多的线程会降低吞吐量。此外，在解耦过程之后，我们将同一个队列中的写任务分组在一起，并批量处理它们。与单个事务提交相比，批处理提交可以显著提高I/O，从而提高吞吐量。在下面的讨论中，我们将优化批处理的效率<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830180411.png" alt="20210830180411" loading="lazy"></li>
</ul>
<h5 id="multi-staged-pipeline">Multi-staged pipeline</h5>
<ul>
<li>写路径是访问主内存和磁盘的多个操作的一个长序列，在其执行过程中计算工作负载会发生变化。这使得通过计算隐藏内存访问和磁盘 I/O 具有挑战性</li>
<li>为了解决这个问题，我们将写路径分解为多个阶段。图3的右半部分显示了四阶段流水线的概述，其中各个阶段交替访问主内存和磁盘。
<ul>
<li>在第一阶段，<strong>日志缓冲</strong>，线程从事务缓冲区收集每个写请求的wal (write-ahead logs)到内存日志缓冲区，并计算它们相应的CRC32错误检测代码。这个阶段涉及重要的计算和仅访问主内存。</li>
<li>在第二阶段，<strong>日志刷回</strong>，线程将缓冲区中的日志刷新到磁盘。刷新日志后，日志序列号在日志文件中向前移动。然后，这些线程将已经写完日志的写任务推入下一阶段</li>
<li><strong>写 memtable</strong>。在这里，多个线程并行地在活动 memtable 中追加记录。这个阶段只访问主内存。所有这样的写都可以在失败后从 WAL 中恢复。</li>
<li>最后一个阶段，<strong>提交</strong>，完成所有任务的事务最终由多个线程并行提交，并释放它们使用的资源(如锁)。</li>
</ul>
</li>
<li>在这个流水线中，<strong>我们根据每个阶段的需求，分别为每个阶段调度线程，使每个阶段的吞吐量与其他阶段的吞吐量相匹配，以达到总吞吐量的最大化</strong>。虽然前三个阶段是内存密集型的，但第一和第二阶段访问主内存中的不同数据结构，而第二阶段写入磁盘。因此，重叠它们可以提高主存储器和磁盘的利用率。</li>
<li>此外，我们还<strong>分别限制了每个阶段的线程数。由于较强的数据依赖关系在前两个阶段,每个阶段我们只安排一个线程(例如,图3的线程 a)</strong>,其他阶段,我们分配多个线程并行处理(例如,线程b, c, d,如图3所示)。所有线程从阶段拉取任务进行处理。从前两个阶段提取任务的操作是先发制人的，只允许第一个到达的线程处理该阶段。其他阶段则是理想并行，允许多个线程并行工作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830161423.png" alt="20210830161423" loading="lazy"></li>
</ul>
<h4 id="flush-and-compaction">Flush and Compaction</h4>
<h5 id="fast-flush-of-warm-extents-in-level0">Fast flush of warm extents in Level0</h5>
<ul>
<li>依赖 flush 来避免 OOM，在即将到来的交易高峰时，风险是很大的。在 X-engine中，每个刷新操作将其不可变的memtable转换为extent，将它们追加到Level0，并离开，而不与现有记录合并。然而，这个过程会留下一组未排序的区段。查询现在必须访问所有的区段来寻找潜在的匹配。这个过程中涉及的磁盘 I/O 开销很大。尽管Level0的大小可能不到整个存储空间的1%，但它包含的记录只比最近插入到memtable中的记录稍微老一点。由于电子商务工作负载中的强大时间局部性，传入查询很可能需要这些记录。因此，我们将 Level0 中的区段称为 warm extents</li>
<li>我们<strong>引入了 Level0 内部的压缩来积极地合并 Level0 中的暖区，而不将合并的区推入下一个 Level1</strong>。这种方法将温记录保存在lsm 树的第一级，防止查询深入树中检索这些记录。另一方面，由于 Level0 与其他级别相比规模较小，Level0 内部的压缩只需要访问一小部分区段，不像其他在更深级别中广泛合并区段的压缩。由于这种对 CPU 和 I/O 的轻量级消耗，可以频繁地执行level0内部的压缩。</li>
</ul>
<h5 id="accelerating-compactions">Accelerating compactions</h5>
<ul>
<li>
<p>压缩涉及昂贵的合并操作。我们对压缩应用了三种优化: 数据重用、异步 I/O 和 FPGA 加载。</p>
</li>
<li>
<p><strong>数据重用</strong>。我们在压缩过程中重用区段和数据块，以减少合并两个相邻级别(即Leveli和Leveli+1)所需的 I/O 数量。为了增加重用的机会并使其有效，我们将区段的大小减少到 2 MB，并进一步将区段划分为多个 16 KB 的数据块。<strong>如果压缩所涉及的一个区段的键范围与其他区段的键范围不重叠，则只需更新其相应的元数据索引(如第3.1.3节所介绍的)就可以重用它，而不需要在磁盘上实际移动它</strong>。我们在图8中展示了一个示例，其中Level1的3个区段(键范围为[1,35]、[80,200]和[210,280])被压缩为来自Level2的5个区段(键范围为[1,30]、[50,70]、[100,130]、[150,170]和[190,205])。下面我们列出了不同的重用情况：</p>
<ul>
<li>Level1 的 Extent[210,280]和Level2的Extent[50,70]被直接重用</li>
<li>Level1 的 Extent[1,35] 与 Level2的extent[1,30]重叠。但前者只有一个数据块[1,25]与后者的数据块重叠。因此，数据块[32,35]被重用</li>
<li>Level1 extent[80,200]与Level2中的多个extent重叠。它的第二个数据块与Level2中的三个区段重叠。然而，这个数据块中的键是稀疏的，因为在135到180之间没有键。因此，我们将其分为两个数据块 [106,135] 和[180,200]，并分别将其与Level2的extent[100, 130]和[190,205]进行合并。Extent[150,170]被直接重用<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210830162544.png" alt="20210830162544" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>Asynchronous I/O</strong>：<strong>在区段级别，压缩操作由三个不相交的阶段组成</strong>:</p>
<ul>
<li>(1)从存储器中检索两个输入区段，</li>
<li>(2)合并它们，</li>
<li>(3)将合并的区段(一个或多个)写回存储器。</li>
</ul>
</li>
<li>
<p>第一和第三阶段是I/O阶段，而第二阶段是计算密集型阶段。我们在第一和第三阶段发出异步I/O请求。第二阶段实现为第一I/O阶段的回调函数。当多个压缩并行运行时，第二阶段的执行与其他阶段的执行重叠，以隐藏I/O</p>
</li>
<li>
<p><strong>FPGA oﬀloading</strong>：我们引入 FPGA 来加速压缩，并减少cpu上的资源消耗。<strong>通过上面介绍的两个优化，在cpu上运行的压缩仍然会消耗多个线程。因为压缩工作在lsm树的两个连续级别的独立区段对上，所以压缩任务在这些区段对的粒度上理论上能并行的</strong>。因此，它可以被分割成多个小任务。我们把这样小的任务放到 FPGA 上，并以流的方式处理它们。每个压缩区段都被传输回磁盘。通过这样的加载，<strong>CPU 线程从合并区段的沉重负担中得到释放。因此，我们能够分配更多的线程来处理并发事务</strong>。这种基于 FPGA 的压实加速的细节超出了本文的范围，将另行探讨。</p>
</li>
</ul>
<h5 id="scheduling-compactions">Scheduling compactions</h5>
<ul>
<li>LSM 树依赖于压缩来按已排序的顺序保存记录，并从存储中删除标记为要删除的记录。如果不及时删除已删除的记录，查找查询可能不得不遍历存储中的许多无效记录，从而严重损害读性能。压缩还有助于合并Level0中的子级别，以降低该级别的查找成本。在X-Engine中，我们<strong>引入了基于规则的压缩调度</strong>，以利用这些好处。我们根据压实操作的层次来区分压实:
<ul>
<li>intra-Level0 压缩(合并Level0中的子层)</li>
<li>minor 压缩(合并两个相邻的层，除了最大的层)</li>
<li>major 压缩(合并最大的层和它上面的层)</li>
<li>self-major 压缩(合并最大的层以减少碎片和删除记录)。</li>
</ul>
</li>
<li>当一个级别的总大小或区段总数达到预定义的阈值时，就会触发压缩。<strong>所有触发的压缩作业都被放入一个优先队列</strong>。确定优先级的规则对配置是开放的，这进一步取决于数据库之上的不同应用程序的需求。</li>
<li>下面，我们将展示一个为阿里巴巴的一个在线应用程序量身定制的配置示例。在这个应用程序中，<strong>有频繁的删除。当应该删除的记录数量达到其阈值时，就会触发压缩以删除这些记录，并且这种压缩具有最高优先级，以防止存储空间的浪费。删除之后，Level0 内部的压缩被优先排序，以帮助加速查找 Level0 中最近插入的记录。Minor、major 和 self-major compaction 分别按此顺序排列</strong>:
<ul>
<li>(1) Compactions for deletions.</li>
<li>(2) Intra-Level0 compactions.</li>
<li>(3) Minor compaction.</li>
<li>(4) Major compaction.</li>
<li>(5) Self-major compactions.</li>
</ul>
</li>
<li>可以对不同子表以及不同 LSM 树的压缩进行并发执行调度。这种并行性是理想的，因为子表之间的数据独立。在每个子表中，一次只执行一个压缩。虽然可以对同一lsm -树执行多个压缩，而不会损坏数据或导致任何数据竞争，但我们有足够的机会在子表级别进行并发压缩，以提高性能。</li>
</ul>
<h2 id="related-work">Related Work</h2>
<ul>
<li></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Other]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-Three-Other/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-Three-Other/">
        </link>
        <updated>2021-08-20T09:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Other</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Other</li>
</ul>
</blockquote>
<!-- more -->
<h3 id="other-prediction-metrics">OTHER PREDICTION METRICS</h3>
<ul>
<li>并非所有细粒度策略都预测重用距离或二分标签，当然可以使用不同的预测目标捕获过去的行为。例如，Kharbutli 和 Solihin[2005] 死块预测器的一个组件可以<strong>预测一个缓存行在缓存中被重用的最大次数</strong>。作为这类解决方案的一个例子，我们现在详细讨论 <strong>EVA 策略</strong> [Beckmann和Sanchez, 2017]，它引入了一种新的预测目标，称为 EVA，这是少数几个使用历史信息来指导老化过程的细粒度解决方案之一。</li>
</ul>
<h4 id="economic-value-addedeva">ECONOMIC VALUE ADDED(EVA)</h4>
<ul>
<li>贝克曼和桑切斯认为，只有当我们对未来拥有完美的知识时，用预期时间最长的候选人替代重用是最优的 [Belady, 1966]，但这一策略对于面临未来固有不确定性的实际解决方案是不够的 [Beckmann和桑切斯，2017]。因此，实际的解决方案需要平衡两个相互竞争的目标:
<ul>
<li>(1) <strong>最大限度地提高给定行的命中缓存的概率</strong>，</li>
<li>(2) <strong>限制该行消耗缓存空间的持续时间</strong>。</li>
</ul>
</li>
<li>仅仅基于重用距离的解决方案只考虑了权衡的一方面。</li>
<li>为了解决这一限制，Beckmann 和 Sanchez[2017] 提出了一个称为经济附加值(EVA)的新指标，将这两个因素合并为一个单一指标。<strong>EVA 被定义为候选对象可能产生的命中数与其在缓存中的平均占用率的比值</strong>。方程 (4.1) 表明 EVA 是对给定缓存行计算的。我们看到一个缓存行的 EVA 有两个组成部分。首先，该行将因其预期的未来命中次数而获得奖励(具有较高重用概率的该行将具有较高的EVA值)。其次，这一行会因为它所消耗的缓存空间而受到惩罚。这个惩罚是通过对每个候选对象在缓存中花费的时间进行收费来计算的，收费的速率是一行的平均命中率(缓存命中率除以其大小)，这是消耗缓存空间的长期机会成本。因此，EVA 指标通过计算该行命中的几率是否值得它所消耗的缓存空间来获取缓存行的成本效益权衡。$$ EVA = Expected_hits - (Cache_hit_rate / Cache_size) * Expected_time$$</li>
<li>候选人的 EVA 是从他们的年龄推断出来的，并随着候选人的年龄进行修正。例如，左边的图4.6 显示了 EVA 是如何随着候选应用程序年龄的变化而变化的，该应用程序遍历一个小数组和一个大数组(右边显示了同一应用程序的重用距离分布)。首先，EVA 较高，因为访问很有可能来自一个小数组，这意味着替换策略可以赌候选者会很快命中。但是，当年龄超过短数组的长度时，EVA 急剧下降，因为从大数组缓存行的代价要高得多。此时，低 EVA 使大数组中的行很可能被逐出。缓存来自大数组的行所带来的代价随着行的老化和接近大数组的重用距离而降低。这种惩罚反映在年龄从 50 岁到 250 岁的 EVA 逐渐增加，在这一点上 EVA 替换策略保护缓存行免受大数组替代，即使它们是老的。因此，在面临不确定性时，EVA 替代策略会随着候选人年龄的增长而对他们有更多的了解。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821160012.png" alt="20210821160012" loading="lazy"></li>
<li>当然，<strong>另一种减少不确定性的方法是将缓存行分为不同的类别</strong>。例如，如果我们将小数组和大数组划分为不同的类别，图 4.7 显示了与年龄相关的 EVA，我们可以看到每个类别的 EVA 曲线要简单得多。理论上，可以扩展 EVA 以支持分类，但实现的复杂性将这种扩展限制在少数类。在下一节中，我们将讨论依赖于许多细粒度类的替换策略，但学习每个类的更简单的指标。相比之下，EVA 以细粒度对年龄进行排序，但这限制了 EVA 使用更少的类。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821160331.png" alt="20210821160331" loading="lazy"></li>
<li>EVA 替换策略通过记录命中和驱逐的年龄分布并使用轻量级软件运行时处理有关这些驱逐的信息来计算 EVA 曲线。为了预测一条缓存行的 EVA，将其年龄作为索引，建立一个代表 EVA 曲线的驱逐优先级数组。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Classification]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-Two-Classification/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-Two-Classification/">
        </link>
        <updated>2021-08-20T08:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Classification</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Classification</li>
</ul>
</blockquote>
<!-- more -->
<h3 id="classification-based-policies">CLASSIFICATION-BASED POLICIES</h3>
<ul>
<li><strong>基于分类的策略学习了传入行的二分类：缓存访问是否可能导致未来的命中？</strong> 对缓存友好的行，预期会导致缓存命中的行以更高的优先级插入缓存，以便它们有足够的机会接收缓存命中，而不喜欢缓存的行(不期望缓存命中的行)则以较低的优先级插入，这样它们就可以在不浪费缓存资源的情况下被快速清除。</li>
<li>与其他类替换策略相比，基于分类的策略有几个优点。与混合替换策略对给定时间段内的所有行进行统一决策相比，<strong>基于分类的替换策略可以插入一些高优先级的行和一些低优先级的行</strong>。与基于重用距离的策略(目标是数值的重用距离预测)相比，<strong>基于分类的策略解决了一个更简单的二分预测问题</strong>。</li>
<li>正如我们将看到的，基于分类的政策起源于两种不同的文献。<strong>基于采样的死块预测器(SDBP)</strong> [Khan等人，2010] (章节4.2.1) 建立在死块预测文献的基础上，而<strong>基于签名的命中预测 (SHiP)</strong> [Wu等人，2011a] (章节4.2.2)则来源于缓存替换策略文献。有趣的是，这两种解决方案都得到了概念上相似的想法，而且事后看来，这两篇论文共同统一了这两个领域。</li>
<li>现在我们将讨论这些和其他最近的基于分类的策略。虽然这些政策之间存在显著差异，但它们都有两个特点。
<ul>
<li>首先，它们<strong>都包含一个二分预测器</strong>，该预测器可以学习过去的缓存行为，以指导各个行的插入优先级。</li>
<li>其次，他们<strong>都从最先进的粗粒度政策中借鉴了 Promotion、老化 和 驱逐模式，这有助于解释不准确的预测</strong>。</li>
</ul>
</li>
<li>在描述这些策略时，我们将考虑以下设计问题。
<ul>
<li>哪种缓存解决方案是策略学习？</li>
<li>预测机制是什么，预测的粒度是什么？</li>
<li>是什么老化机制确保不准确的预测最终被淘汰？</li>
</ul>
</li>
</ul>
<h4 id="sampling-based-dead-block-prediction-sdbp">SAMPLING BASED DEAD BLOCK PREDICTION (SDBP)</h4>
<ul>
<li>许多研究发现，由于LLC中的大部分块都是死块(它们在被驱逐之前不会再次重用)，<strong>死块预测可以用来指导缓存替换和死块的早期旁路（旁路即为不缓存）</strong> [Khan等人，2010,Lai和Falsafi, 2000]。Lai 和 Falsafi[2000] 引入了使用死块预测器将数据预取到 L1 中的死块的思想。他们的 reftrace 预测器预测，如果指令地址的 trace 导致最后一次访问一个块，那么同样的 trace 也会导致最后一次访问其他块。为了降低为所有缓存块维护指令跟踪的成本，Khan 等人[2010] <strong>引入了基于采样的死块预测器(SDBP)，它对程序计数器(PC)的缓存行为进行采样，以确定传入的块是否可能 dead</strong>。来自已知插入死块的 PC 的未来缓存访问被 Bypass，这样他们就不会污染缓存。来自没有插入死块的 PC 的访问将使用一些基本策略(即随机或 LRU 替换策略)插入缓存。</li>
<li>值得注意的是，<strong>SDBP 从使用一小部分缓存访问填充的解耦采样器学习</strong>(参见图4.2)。如果一个块被从采样器中移除而不被重用，则对相应的 PC 进行负向训练；否则，预测器被积极训练。解耦采样器有几个优点：
<ul>
<li>首先，只使用所有缓存访问的一小部分样本来训练预测器，这导致了功率和空间效率高的预测器设计和采样器中易管理的元数据(为每个采样器条目维护 PC)。</li>
<li>其次，采样器的替换策略不必与缓存的替换策略相匹配。Khan 等人[2010] 对采样器使用 LRU 策略，他们对主缓存使用随机替换。</li>
<li>最后，采样器的结合性独立于缓存的结合性，这允许开销更小的采样器设计。Khan 等人使用 12 路采样器作为 16 路缓存。表 4.1 总结了SDBP的关键操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115233.png" alt="20210821115233" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115325.png" alt="20210821115325" loading="lazy"></li>
</ul>
</li>
<li>因此，为了回答我们在本节开始时概述的问题:
<ul>
<li>(1) SDBP 学习基于 LRU 的采样器的缓存决策;</li>
<li>(2) 它预测死块(缓存厌恶与缓存友好) 使用一个倾斜的预测器设计，以 PC 的粒度进行这些预测;</li>
<li>(3) SDBP 绕过所有预测为缓存厌恶的传入块，使用基线替换策略管理剩余的行，因此假阳性(预测为缓存友好的缓存厌恶块)使用基线替换策略老化，而假阴性(缓存友好的块被预测为缓存厌恶)没有任何机会重用。</li>
</ul>
</li>
</ul>
<h4 id="signature-based-hit-prediction-ship">SIGNATURE BASED HIT PREDICTION (SHIP)</h4>
<ul>
<li>像 SDBP 一样，<strong>SHiP</strong> [Wu et al.， 2011a] 学习了底层替换策略的淘汰行为，但 SHiP 背后的主要观点是，<strong>重用行为与将该行插入缓存的 PC 更紧密相关，而不是与最后访问该行的 PC</strong>。因此，在缓存淘汰时，SHiP 的预测器被 PC 训练，PC 首先在缓存丢失时插入该行，预测器只在缓存 MISS 时被咨询(在命中时，行被提升到最高优先级而不咨询预测器)。</li>
<li>更具体地说，SHiP 训练一个预测器，它可以学习给定的签名是否具有近或远的重引用间隔。<strong>在缓存中采样一些集合以维护签名和训练预测器。在一个采样集中的缓存命中，与这条缓存行相关联的签名被训练为正，以指示一个近重引用，而在驱逐一条从未被重用的行时，该签名被训练为负，以指示一个远重引用</strong>。当插入新行时，将使用传入行的签名来咨询预测器，并确定传入行的重新引用间隔(对所有访问执行预测，而不仅仅是对采样集)。一旦插入到缓存中，行就使用一个简单的 RRIP 策略进行管理(参见表4.2)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115906.png" alt="20210821115906" loading="lazy"></li>
<li>签名的选择对 SHiP 的有效性至关重要。Jaleel 等人 [2010b] 评估了一个程序计数器签名(PC)、一个内存区域签名和一个指令序列历史签名，他们发现 <strong>PC 签名性能最好</strong>。</li>
<li>SHiP 基于 DRRIP [Jaleel et al.，2010b] 创建细粒度策略。<strong>DRRIP 对 epoch 中的所有缓存行进行统一的重新引用预测，而 SHiP 进行更细粒度的预测：它通过将每个引用与一个唯一的签名关联起来，将传入行的分类到不同的组中。</strong> 假设具有相同签名的缓存行具有相似的重引用行为，但允许具有不同签名的缓存行在同一 epoch 内具有不同的重引用行为。</li>
<li>现在我们回答本节开始时提到的问题。
<ul>
<li>(1) 最初，SHiP 从 SRRIP 中学习，但一旦 SHiP 的预测器被训练，进一步的训练更新来自 SHiP 自身的重用和驱逐行为。</li>
<li>(2) SHiP 使用基于 PC 的预测器，其中与一行相关联的 PC 就是插入一行缓存 miss 的 PC，其中每个 PC 与一个饱和计数器相关联。</li>
<li>(3) SHiP 依靠 RRIP 政策来老化所有行。</li>
</ul>
</li>
</ul>
<h4 id="hawkeye">HAWKEYE</h4>
<ul>
<li>为了避免基于启发式的解决方案(如LRU)的病态，Hawkeye [Jain和Lin, 2016] 构建了 Belady 的 MIN 解[Belady, 1966]，这是有趣的两个原因。
<ul>
<li>首先，Belady 的 MIN 对于任何引用序列都是最优的，因此基于 MIN 的解决方案可能适用于任何访问模式。</li>
<li>其次，Belady 的 MIN 算法是一种不切实际的算法，因为它替换了将来重复使用最多的行；因此，它依赖于未来的知识。</li>
</ul>
</li>
<li>Hawkeye 的核心见解是，<strong>虽然无法预测未来，但可以将 Belady 的 MIN 算法应用到过去的内存引用中。此外，如果一个程序过去的行为是它未来行为的一个很好的预测器，那么通过学习过去的最优解，Hawkeye 可以训练一个预测器，它应该在未来访问中表现良好</strong>。</li>
<li>为了理解模拟过去事件的 MIN 需要多少历史信息，Jain 和 Lin[2016] 通过限制其未来的窗口来研究 MIN 的性能。图 4.3 显示，虽然 MIN 需要一个很长的窗口到未来(SPECint 2006的 8x 缓存大小)，但它不需要一个无边界窗口。因此，要将 MIN 应用到过去的事件，我们需要一个缓存大小为 8x 的历史记录。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821124850.png" alt="20210821124850" loading="lazy"></li>
<li>由于维持缓存大小 8x 的历史信息开销巨大，Hawkeye 只计算几个抽样集的最优解，并<strong>引入 OPTgen 算法，为这些抽样集计算与 Belady 的 MIN 策略相同的答案</strong>。<strong>OPTgen 确定如果使用 MIN 策略将被缓存的行</strong>。OPTgen 背后的关键观点是，<strong>当一行再次被重用时，可以准确地确定该行的最佳缓存决策</strong>。因此，在每次重用时，OPTgen 都会回答以下问题: 这一行是否会在 MIN 情况下是缓存命中还是缓存丢失？这种洞察力使得 OPTgen 能够使用少量的硬件预算和简单的硬件操作，以 O(n) 复杂性再现 Belady 的解决方案。</li>
<li>图4.4 显示了 Hawkeye 替换策略的总体设计。OPTgen 训练 Hawkeye 预测器，这是一种基于 PC 的预测器，它可以了解 PC 插入的行是倾向于缓存友好型还是缓存反对型。当 OPTgen 确定一条行将被 MIN 命中时，该行对应的 PC 被正训练，否则被负训练。每次使用传入行的 PC 进行缓存插入和提升时，都会参考预测器。被预测为缓存友好的行将以高优先级插入，而被预测为缓存厌恶的行将以低优先级插入(参见表4.3)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821125001.png" alt="20210821125001" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821125202.png" alt="20210821125202" loading="lazy"></li>
<li>回答本节开头的问题:
<ul>
<li>(1) Hawkeye 从最优缓存解决方案中学习，而不是从 LRU 或 SRRIP 中学习；</li>
<li>(2) Hawkeye 使用基于 PC 的预测器学习最优解;</li>
<li>(3) Hawkeye 还依赖于 RRIP 的老化机制，以高优先级插入 age line。为了纠正不准确的预测，Hawkeye 还对预测器进行负训练，当预测到不喜欢缓存的行被取消而没有被重用时。</li>
</ul>
</li>
<li>Hawkeye 在概念上做出了一个有趣的贡献: <strong>它将缓存替换描述为一个有监督的学习问题</strong>，这令人惊讶，因为不像分支预测，程序执行最终提供每个分支的正确结果，硬件缓存不提供这样的标记数据。通过对过去事件应用最优解决方案，Hawkeye 提供了标记数据，这<strong>表明缓存替换领域可能会从监督学习的大量研究中受益</strong>。</li>
</ul>
<h4 id="perceptron-based-prediction">PERCEPTRON-BASED PREDICTION</h4>
<ul>
<li>预测性策略在很大程度上取决于预测者的准确性。<strong>SDBP、SHiP 和 Hawkeye 都使用基于 PC 的预测器，准确率约为 70 - 80%</strong>。Jiménez 和 Teran 旨在通过使用更好的特征和更好的预测模型来提高预测精度 [Jiménez和Teran, 2017, Teran等人，2016]。例如,<strong>感知器预测 [Teran et al., 2016] 使用简单的人工神经网络 [Rosenblatt, 1962]，以增加使用带有更丰富的特性的 PC</strong> ，如 (1) PCs 历史信息，(2) 来自内存地址的位，(3) 数据的压缩表示，和(4) 一个块被访问的次数。每个特征被用来索引一个不同的饱和计数器表，然后求和并与一个阈值进行比较，生成一个二分预测。一小部分的访问被采样以使用感知器更新策略更新感知器预测器：如果预测是错误的，或者如果总和没有超过某个大小，那么计数器在访问时减少，在驱逐时增加。图 4.5 对比了感知器预测器(右)和先前基于 PC 的预测器(左)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821140306.png" alt="20210821140306" loading="lazy"></li>
<li>Multiperspective Reuse Predictor [Jiménez和Teran, 2017] 探索了一组广泛的特性，这些特性捕获了程序的各种属性，生成了一个从多个角度获得信息的预测器。特征被参数化，包含了关于每个训练输入的 LRU 堆栈位置、每个特征被哈希的 PC 位和 PC 历史的长度的更丰富的信息。这些参数一起创建了一个大的特征空间，从而导致更高的预测精度。</li>
</ul>
<h4 id="evicted-address-filtereaf">EVICTED ADDRESS FILTER(EAF)</h4>
<ul>
<li><strong>EAF 策略</strong> [Seshadri等人，2012] <strong>单独预测每个缺失块的重用行为，允许比 PC 更细粒度的差异</strong>。关键的观察是，如果一个高重用的块被过早地从缓存中逐出，那么它将在逐出后不久被访问，而低重用的块在逐出后很长一段时间内都不会被访问。因此，<strong>EAF 策略使用 bloom 过滤器</strong> [bloom, 1970] (一种概率性的、空间效率高的结构，用于确定集合中某个元素的存在或缺失) <strong>来跟踪一小组最近被驱逐的地址</strong>。在<strong>缓存丢失时，如果新行出现在 bloom 过滤器(也称为逐出地址过滤器)中，那么预测该行是重用友好的，并以高优先级插入</strong>; 否则，<strong>根据双峰插入策略 the Bimodal Insertion policy 插入新行</strong>(参见3.1.2节)。当 <strong>bloom 过滤器满时，它被重置</strong>。EAF 在概念上类似于一个小型的候选缓存，它跟踪最近从主缓存中删除的行。表 4.4 总结了 EAF 策略的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821154831.png" alt="20210821154831" loading="lazy">
<ul>
<li><strong>回顾一下 BIP，双峰插入策略，本质是使缓存行以较低的一个概率插入到 MRU 的位置。BIP 保持 LIP 的抗抖动，因为它在大多数时间插入 LRU 位置，但它也可以通过偶尔保留较新的行来适应阶段变化。</strong></li>
</ul>
</li>
<li>从概念上讲，EAF 策略延长了缓存行的生存期，超过了淘汰时间，因此一行的生存期从它的插入开始，但当该行从 bloom 过滤器中删除时结束。随着生命周期的延长，对于具有较长重用间隔的行，EAF 观察重用变得可行，从而获得更好的抗扫描能力和抗抖动能力，从而获得更好的性能。</li>
<li><strong>重用检测器 (ReD)</strong> [crc, 2017, Albericio等人，2013] 提出了类似的想法，因为它<strong>绕过 LLC 或地址重用表中没有击中的任何缓存行，跟踪最近的缓存丢失。因此，ReD 只在第二次重用时将行插入缓存中</strong>。为了避免在第一次看到所有行时绕过它们，ReD 还使用基于 PC 的预测器来预测在第一次访问后可能被重用的行。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Reuse Distance]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-One-Reuse-Distance/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-One-Reuse-Distance/">
        </link>
        <updated>2021-08-20T07:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Reuse Distance</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Reuse Distance</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="fine-grained-replacement-policies">Fine-Grained Replacement Policies</h2>
<ul>
<li>细粒度策略在插入时区分高速缓存行，这种区分通常基于类似高速缓存行的前一个生存期的回收信息。例如，如果细粒度策略得知某一行在其之前的生命周期中没有被重用而被逐出，那么它可以以低优先级将该行插入到缓存中。相比之下，粗粒度策略(如LRU)只有在从MRU位置迁移到LRU位置后才会驱逐一行，所以该方案强制缓存行驻留在缓存很长一段时间，消耗宝贵的缓存空间来确定行应该驱逐。因此，<strong>通过学习以前缓存行的行为，细粒度策略可以更有效地使用缓存</strong>。</li>
<li>根据用于预测插入优先级的指标，我们将细粒度策略分为三大类。
<ul>
<li>第一类(第4.1节) 包含<strong>预测传入行预期重用时间间隔</strong>的解决方案。</li>
<li>第二类(第4.2节) 包含<strong>预测二分缓存决策</strong>的解决方案 (<strong>缓存友好 vs. 缓存不友好</strong>)。</li>
<li>第三类比其他两类要小得多，包括<strong>引入新的预测指标的策略</strong> [Beckmann and Sanchez, 2017, Kharbutli and Solihin, 2005]。</li>
</ul>
</li>
<li>细粒度解决方案有其他几个设计维度。首先，由于要记住跨多个缓存生存期的单个行的缓存行为可能很麻烦，所以这些策略<strong>学习一组行的缓存行为</strong>。例如，<strong>许多解决方案根据加载行的指令的地址 (PC) 对行进行分组，因为由同一 PC 加载的行往往具有类似的缓存行为</strong>。最近的解决方案着眼于更复杂的方法来分组缓存行 [Jiménez 和 Teran, 2017, Teran et al.， 2016]。第二个设计维度是用于学习缓存行为的历史记录量。</li>
<li>细粒度替换策略起源于两个看起来不同的上下文中。
<ul>
<li><strong>其中一个使用预测来识别死块（即在被移除之前不会被使用的块），然后就可以重新用于其他用途</strong>。例如，识别死块的最早动机之一是将它们用作预取缓冲区 [Hu等人，2002,Lai等人，2001]。另一个动机是关闭已死的缓存行 [Abella等人，2005,Kaxiras等人，2001]。</li>
<li><strong>第二个工作推广了混合重引用间隔策略</strong> [Jaleel et al.， 2010b]，<strong>使它们更有 learning 的基础</strong>。</li>
</ul>
</li>
<li>尽管它们的起源不同，这两种研究方向已经趋同为概念上相似的解决方案。</li>
</ul>
<blockquote>
<ul>
<li>PC 程序计数器是计算机处理器中的寄存器，它包含当前正在执行的指令的地址（位置）。当每个指令被获取，程序计数器的存储地址加一。在每个指令被获取之后，程序计数器指向顺序中的下一个指令。当计算机重启或复位时，程序计数器通常恢复到零。</li>
</ul>
</blockquote>
<h3 id="reuse-distance-prediction-policies">REUSE DISTANCE PREDICTION POLICIES</h3>
<ul>
<li>基于重用距离的策略预估块的重用距离，重用距离可以根据连续引用块之间的访问次数或循环次数来定义。对重用距离的完美预测足以模拟 Belady 的最优解决方案，但<strong>由于重用距离值的高变化，很难精确预测重用距离。因此，现实的解决方案估计重用距离分布或其他聚合的重用距离统计</strong>。</li>
</ul>
<h4 id="expiration-based-dead-block-predictors">EXPIRATION-BASED DEAD BLOCK PREDICTORS</h4>
<ul>
<li>许多死块预测器使用<strong>过去的驱逐信息</strong>来估计平均重用距离统计数据，它们驱逐未在预期重用距离内重用的行 [Abella et al.， 2005, Faldu和Grot, 2017, Hu et al.， 2002, Kharbutli 和 Solihin, 2005, Liu et al.， 2008, Takagi 和 Hiraki, 2004]。</li>
<li>Hu 等人了解了缓存块的存活时间，其中<strong>存活时间定义为一个块在缓存中保持存活的周期数</strong> [Hu等人，2002]。当插入一个块时，它的生命周期预计与最近的一次的生命周期相近。如果块在缓存中停留了两倍的生命周期而没有收到缓存命中，则这样的块被定义为死亡块并将从缓存中移除。</li>
<li>Abella 等人使用了类似的死块预测策略来关闭 L2 高速缓存行，但他们没有使用周期数，而是<strong>根据连续引用之间的缓存访问次数来定义重用距离</strong> [Abella等人，2005]。</li>
<li>Kharbutli 和 Solihin[2005] <strong>使用计数器来跟踪每条缓存行的访问间隔，其中一行的访问间隔被定义为在对该行的后续访问之前被访问的其他缓存行的数量</strong>。而且，如果访问间隔超过一定的阈值，它们就会预测一个缓存行是死的。该阈值由访问间隔预测器(AIP)预测，该预测器跟踪所有过去驱逐的访问间隔，并记住基于 PC 的表中所有此类间隔的最大值。</li>
<li>Faldu 和 Grot [2017] 展示了 Leeway 策略，<strong>该策略使用了 live distance 的概念，即在缓存行生命周期中观察到的最大栈距离，以识别死块</strong>。缓存块的活动距离是从块的前几代学习的，当一个块超过它的活动距离时，它被预测为死块。使用每个 PC 的活距离预测表(LDPT)来保存前面的活距离。LDPT 为传入的块预测活动距离，任何在 LRU 堆栈中移动超过其预测活动距离的块都被预测为死块。<strong>为了避免跨生存期和跨同一 PC 访问的块的活动距离的高可变性，Leeway 部署了两个更新策略</strong>，它们控制基于工作负载特征调整 LDPT 中的活动距离值的速率。第一种策略是激进的，支持 bypassing；第二种策略是保守的，有利于缓存。Leeway 使用 Set Dueling [Qureshi et al.， 2007] 来选择这两种策略。</li>
</ul>
<h4 id="reuse-distance-ordering">REUSE DISTANCE ORDERING</h4>
<ul>
<li>Keramidas 等人 [2007] 使用重用距离预测来代替驱逐预计在未来被重用最多的缓存行。他们的策略学习每个 LOAD 指令 (PC) 的重用距离，对于每个传入行，它<strong>计算一个估计的访问时间 (ETA)，这是当前时间和预期的重用间隔的总和。然后，它根据 ETA 值来排序缓存行，并驱逐 ETA 值最大的行</strong>。</li>
<li>为了防止 ETA 预测不可用的情况，该方案<strong>还跟踪行衰减，这是对一行在缓存中未被访问的时间的估计，如果该行的衰减间隔大于其 ETA，它将驱逐该行</strong>。</li>
<li>更具体地说，图4.1显示了替换策略有两个候选行:
<ul>
<li>(1) ETA最大的行(ETA行)；</li>
<li>(2) 衰减时间最大的行(LRU行)</li>
</ul>
</li>
<li>选择两个值中最大的行进行驱逐。因此，该策略在有条件时依赖 ETA，否则转变回 LRU。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821112316.png" alt="20210821112316" loading="lazy"></li>
</ul>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="">[1] Chip Multiprocessor Architecture: Techniques to Improve Throughput and Latency</a>
<ul>
<li>Kunle Olukotun, Lance Hammond, and James Laudon</li>
<li>2007</li>
</ul>
</li>
</ul>
<h3 id="基于重用距离的缓存策略的其他相关研究">基于重用距离的缓存策略的其他相关研究</h3>
<ul>
<li><a href="https://research.cs.wisc.edu/multifacet/papers/sigmetrics13_caches_reuse.pdf">Reuse-based Online Models for Caches</a></li>
<li><a href="https://www.usenix.org/sites/default/files/conference/protected-files/inflow16_slides_lavaee.pdf">Cache Replacement Based on Reuse-Distance Prediction</a></li>
<li><a href="http://iccd.et.tudelft.nl/Proceedings/2007/Papers/5.1.5.pdf">ElCached: Elastic	Multi-Level	Key-Value	Cache</a></li>
<li><a href="https://www.cs.rochester.edu/~cding/Documents/Publications/TR741.pdf">Reuse Distance Analysis 2001</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/2818374"></a></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3380732"></a></li>
<li><a href="">一种基于重用距离预测与流检测的高速缓存替换算法</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Hybrid]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-Three-Hybrid/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-Three-Hybrid/">
        </link>
        <updated>2021-08-20T06:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Hybrid</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Hybrid</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="hybrid-policies">Hybrid Policies</h2>
<ul>
<li>混合策略 [Jaleel et al.， 2010b, Qureshi et al.， 2006, 2007] 认识到不同的工作负载，甚至相同工作负载中的不同阶段，可以从不同的替代策略中受益。例如，如果程序在小工作集和大工作集之间交替，<strong>当工作集较小时，它将受益于最近友好策略，当工作集较大时，它将受益于抗抖动策略</strong>。因此，混合策略评估应用程序当前工作集的需求，并从多个竞争策略中动态选择。混合策略面临的两个主要挑战是 (1) 准确识别最有利的策略，以及 (2 以较低的硬件成本管理多个策略。我们现在讨论解决这些挑战的两种解决方案。</li>
</ul>
<h3 id="adaptive-replacement-cache-arc">ADAPTIVE REPLACEMENT CACHE (ARC)</h3>
<ul>
<li>自适应替换缓存(ARC) [Megiddo 和 Modha, 2003] 通过动态平衡基于最近次数和基于频率的驱逐，结合了最近次数和频率的优势。特别是，ARC 保持跟踪两个额外的标签目录，L1 和 L2，它们一起记住的元素是基线缓存所能容纳的两倍。L1 目录维护最近使用过的、只访问一次的页面，L2 目录维护最近访问过的、至少访问过两次的页面。ARC 的目标是动态地为 L1 和 L2 选择适当的缓存量(见图3.10)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820231629.png" alt="20210820231629" loading="lazy"></li>
<li>更具体地说，ARC 将基线缓存目录分为两个列表 T1 和 T2，分别用于最近和经常引用的条目。T1 和 T2 使用幽灵列表(分别为 B1 和 B2)进行扩展，幽灵列表分别跟踪 T1 和 T2 中最近被逐出的缓存项。幽灵链表只包含标签元数据，而不是实际数据，当相应的数据从缓存中被移除时，条目就会被添加到幽灵链表中。T1 和 B1 共同构成基于最近的 L1 目录，T2 和 B2 共同构成基于频率的L2 目录。</li>
<li>ARC 动态调制 T1 和 T2 专用的缓存空间。一般来说，B1 中的命中会增加 T1 的大小(专用于最近访问的元素的缓存的比例)，B2 中的命中会增加 T2 的大小(专用于至少访问过两次的元素的缓存的比例)。来自 T1 和 T2 的驱逐项分别被添加到 B1 和 B2。</li>
</ul>
<h3 id="set-dueling">SET DUELING</h3>
<ul>
<li>ARC 等混合策略的一个问题是维护附加标记目录的巨大开销。Qureshi 等人[2006] 引入了 Set Dueling，这是一种以较低的硬件成本对不同策略的行为进行抽样的精确机制。Set Dueling 建立在这样的观察之上:一些随机选择的集合可以准确地表示整个缓存上不同替换策略的行为。Qureshi 等人[2006] 用数学方法表明，对于 1-4MB(1024–4096 sets) 的缓存，64 个集合就足以捕获整个缓存的行为。现在，我们通过讨论两个具有代表性的策略来更详细地讨论 Set Dueling。</li>
</ul>
<h4 id="dynamic-insertionpolicydip">Dynamic InsertionPolicy(DIP)</h4>
<ul>
<li>Dynamic InsertionPolicy(DIP) 动态插入策略(DIP)通过动态调制传入缓存行的插入位置，结合了最近友好策略和抗抖动策略的优点 [Qureshi等人，2007]。特别地，DIP 在最近友好的 LRU(表3.1) 和抗抖动的双模态插入策略(表3.6)之间交替使用。</li>
<li>为了在两个策略中进行选择，DIP 使用 Set Dueling 动态跟踪每个策略的命中率。图 3.11 显示 DIP 专用于 LRU (图3.11中的集0、5、10和15) 和 BIP(图3.11中的集3、6、9和12)。这些专用集被称为 Set Dueling 监视器(SDM)，而其余集被称为跟随集。策略选择(PSEL)饱和计数器通过识别接收更多缓存命中的 SDM 来决定获胜策略。特别地，当专用于 LRU 的 SDM 接收到命中时，PSEL 增加，当专用于 BIP 的 SDM 接收到命中时，PSEL 减少( k 位 PSEL 计数器初始化为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2^{k-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>)。获胜的策略由 PSEL 的 MSB 确定。如果 PSEL 的 MSB 为 0，跟随集使用 LRU 策略；否则跟踪集将使用 BIP。因此，Set Dueling 不需要任何单独的存储结构，除了 PSEL 计数器。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821094915.png" alt="20210821094915" loading="lazy"></li>
</ul>
<h4 id="dynamic-re-reference-interval-policy-drrip">Dynamic Re-Reference Interval Policy (DRRIP)</h4>
<ul>
<li>DRRIP 建立在 DIP 的基础上，增加了抗扫描的能力。特别是，DRRIP 使用set dueling 创建了 SRRIP 和 BRRIP 的混合，SRRIP 是 LRU 的抗扫描版本(表3.7)，BRRIP 是 BIP 的抗扫描版本(表3.8)。</li>
<li>正如我们将在第 4 章中看到的，Set Dueling 背后的洞察力对许多后续细粒度策略产生了很大的影响，这些策略使用 Set 采样的概念来有效地跟踪元数据，以确定细粒度缓存行为。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Frequency]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-Two-Frequency/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-Two-Frequency/">
        </link>
        <updated>2021-08-20T05:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Frequency</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Frequency</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="frequency-based-policies">FREQUENCY-BASED POLICIES</h2>
<ul>
<li>基于频率的策略不依赖于最近的访问频率，而是使用访问频率来识别受害者，因此访问频率较高的行优先缓存在访问频率较低的行之上。这种方法不太容易受到扫描的干扰，并且在较长一段时间内考虑重用行为，而不仅仅是最后一次使用。</li>
<li>最简单的基于频率的策略是最少频繁使用(LFU)策略[Coffman and Denning, 1973]，它将频率计数器与每条缓存行关联起来。当该行插入缓存时，频率计数器初始化为 0，每次访问该行时，频率计数器都增加。在缓存丢失时，频率最低的候选对象将被逐出。表 3.9 总结了这些操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820170200.png" alt="20210820170200" loading="lazy"></li>
<li>不幸的是，基于频率的策略不能很好地适应应用程序阶段中的更改，因为即使不再访问前一阶段中具有高频率计数的行，它们仍然会缓存到新的阶段中。为了解决这个问题，有几个解决方案[Lee等人，2001年，O Neil等人，1993年，Robinson和Devarakonda, 1990年，Shasha和Johnson, 1994] 用最近的信息增加频率信息，让旧的行优雅地老化。这里我们讨论两种这样的解决方案。</li>
</ul>
<h4 id="frequency-based-replacement-fbr">Frequency-Based Replacement (FBR)</h4>
<ul>
<li>FBR [Robinson和Devarakonda, 1990] 指出，<strong>基于频率的方法容易受到短时局域性突发的高计数器值的误导</strong>。因此，FBR 通过选择性地增加频率计数器从频率计数中剔除局部性。特别是，FBR 不增加 LRU 堆栈的顶部部分的频率计数器，这被称为新部分。因此，时间局部性的短突发不会影响频率计数器。图 3.8 说明了这个策略。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820170345.png" alt="20210820170345" loading="lazy"></li>
<li>不幸的是，这种策略有一个缺点，即一旦新部分的行老化，即使是经常使用的行也会很快被淘汰，因为它们没有足够的时间来建立频率计数。因此，FBR 进一步限制替换在一个旧的部分中使用最少的行，其中包括最近没有被访问的行(LRU堆栈的底部)。堆栈的其余部分称为中间部分，它为经常使用的行提供了足够的时间来建立它们的频率值。</li>
<li>表 3.10 总结了 FBR 的操作策略。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820222332.png" alt="20210820222332" loading="lazy"></li>
</ul>
<h4 id="least-recentlyfrequentlyused-lrfu">Least Recently/FrequentlyUsed (LRFU)</h4>
<ul>
<li>LRFU 策略 [Lee等人，2001] 建立在这样的观察之上：LRU 和 LFU 策略代表了结合了最近和频率信息的一系列政策的极端点(见图3.9)。LRFU 使用一种名为联合近期和频率(Combined Recency and Frequency, CRF)的新指标，通过允许近期和频率之间的灵活权衡来探索这个频谱。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820222453.png" alt="20210820222453" loading="lazy"></li>
<li>与基于频率的策略一样，<strong>LRFU 将每个过去对块的引用计算进去</strong>，但与基于频率的策略不同的是，<strong>LRFU 通过一个权重函数来衡量每个引用的相对贡献</strong>。特别地，LRFU 为每个区块计算 CRF 值，<strong>CRF 值是权重函数 F(x) 对每个过去的参考的总和，其中 x 是过去的引用距离当前时间的距离</strong>。因此，对于纯粹基于频率的策略仿真，权重函数可以对所有过去的引用给予同等的优先级，而对于基于最近的策略仿真，权重函数可以对最后的引用给予较高的优先级。</li>
<li>LRFU采用式3.1中的权重函数，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 是经验选择的参数。权重函数为老的缓存行提供指数级的低优先级，这使得 LRFU 保留了 FBR 的好处，同时支持优雅的老化。$$F(x) = (\frac{1}{p})^{{\lambda}x}$$</li>
<li>表 3.11 总结了块 b 在不同决策点上的 LFRU 策略操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820225822.png" alt="20210820225822" loading="lazy"></li>
<li>LRFU 的性能很大程度上依赖于此，因此随后开发的 ALRFU 策略动态调整了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 的值 [Lee et al.， 1999]。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Recency]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-One-Recency/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-One-Recency/">
        </link>
        <updated>2021-08-20T04:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Recency</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Recency</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="粗粒度缓存">粗粒度缓存</h2>
<ul>
<li>缓存替换文献跨越了50年，首先是在操作系统页面替换的上下文中，然后是在硬件缓存的上下文中。在此期间，大多数研究都集中在更智能的粗粒度策略的开发上，这可能可以用这些策略的简单性来解释:每个缓存行都与少量的替换状态相关联，对所有新插入的行进行统一初始化，然后在重用这些行时使用简单的操作进行操作。</li>
<li>在本章中，我们将讨论粗粒度策略的关键进展。我们将粗粒度策略划分为三个类，根据它们在插入到缓存后区分行的方式。
<ul>
<li>第一类包括绝大多数粗粒度策略，使用最近信息来排序行(基于最近的策略)。</li>
<li>第二个类使用频率来排序行(基于频率的策略)。</li>
<li>最后，第三类(混合策略)动态地选择不同的粗粒度替换策略。</li>
</ul>
</li>
<li>粗粒度策略设计中的一个运行主题是识别三种常见的缓存访问模式，即最近友好访问、抖动访问[Denning, 1968]和扫描[Jaleel et al.， 2010b]。当应用程序的工作集小到足以容纳缓存时，就会出现最近友好访问，这样大多数内存引用的重用距离小于缓存的大小。相比之下，当应用程序的工作集超过缓存的大小时，就会发生抖动访问，这样大多数内存引用的重用距离都大于缓存的大小。最后，<strong>扫描是一个永不重复的流访问序列</strong>。正如我们将在本章中看到的，几乎所有粗粒度替换策略都专门针对这些访问模式或这三种访问模式的混合进行了优化。</li>
</ul>
<h3 id="recency-based-policies">RECENCY-BASED POLICIES</h3>
<ul>
<li>基于最近的策略根据最近的信息对行进行优先排序。LRU策略[Mattson et al.， 1970]是这些策略中最简单和最广泛使用的。在缓存回收中，LRU策略只是从一组给定的候选缓存行中删除最老的行。为了找到最老的行，LRU策略在概念上维护一个最近栈，其中栈的顶部代表 MRU 行，堆栈的底部代表 LRU 行。通过为每一行关联一个计数器并更新它，可以精确地或近似地维护这个最近堆栈，如表3.1所示<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100316.png" alt="20210820100316" loading="lazy"></li>
<li>当引用存在时间局域性时，即最近使用的数据可能在不久的将来被重用时，LRU的性能很好。但是对于两种类型的访问模式，它的性能很差。
<ul>
<li>首先，当应用程序的工作集大小超过缓存大小时，它可能是悲观的，导致一种称为抖动的现象[Denning, 1968] (见图3.1)。</li>
<li>其次，LRU在有扫描的情况下表现很差，因为它缓存了最近使用的扫描，代价是更有可能被重用的旧行。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100424.png" alt="20210820100424" loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="variants-of-lru">VARIANTS OF LRU</h4>
<ul>
<li>我们现在描述一些显著的 LRU 变体，用于检测和容纳抖动或流访问。</li>
</ul>
<h5 id="most-recently-used-mru">Most Recently Used (MRU)</h5>
<ul>
<li>MRU 策略通过取消新行以保留旧行来解决问题。因此，当工作集大于缓存时，它能够保留工作集的一部分。例如, 图3.2显示了抖动, 访问模式如图3.1所示,系统提高了 LRU 年代缓存命中率的一小部分工作集在这个例子中没有驱逐行a .表3.2显示, MRU 和 LRU的系统最新政策是相同的,除了 MRU 就清除系统的缓存行最新位置而不是LRU的位置。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100725.png" alt="20210820100725" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100846.png" alt="20210820100846" loading="lazy"></li>
<li><strong>然而 MRU 对于抖动访问是理想的，但它在最近友好访问出现时表现很差，而且它对应用程序工作集的变化适应性很差，因为它不太可能缓存来自新工作集的任何行</strong>。</li>
</ul>
<h5 id="early-evictionlrueelru">Early EvictionLRU(EELRU)</h5>
<ul>
<li>EELRU政策也避免了抖动[Smaragdakis等人，1999]。其<strong>主要思想是检测工作集大小超过缓存大小的情况，在这种情况下，有几行会提前被逐出。因此，早期驱逐会丢弃一些随机选择的行，这样剩余的行就可以通过 LRU 策略进行有效管理</strong>。更具体地说，当工作集与缓存匹配时，EELRU 会收回 LRU 行，但当它观察到在一个比主内存大的粗略循环模式中接触了太多行时，它会驱逐 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 最近使用的行</li>
<li>图 3.3 显示了EELRU在最近轴的三个区域之间的区别。最近轴的左端点表示MRU线，右端点表示LRU线。LRU内存区域由最近使用的行组成，位置e和M分别表示早期驱逐区域和晚期驱逐区域的开始。在 miss 时，EELRU 策略要么淘汰最近最少使用的位置(晚区域)的行，要么淘汰 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 位置(早区域)的行。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820104234.png" alt="20210820104234" loading="lazy"></li>
<li>为了决定是使用早期驱逐还是 LRU 驱逐，<strong>EELRU 跟踪每个地区收到的命中次数。如果该分布是单调递减的，则 EELRU 假设没有振荡，并将 LRU 行逐出</strong>。但是，如果分布显示晚期区域比早期区域的命中次数更多，那么 EELRU 将从早期区域删除，这使得晚期区域的行在缓存中保留的时间更长。表3.3总结了 EELRU 的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820104648.png" alt="20210820104648" loading="lazy"></li>
</ul>
<h5 id="segmented-lru-seg-lru">Segmented LRU (Seg-LRU)</h5>
<ul>
<li>分段 LRU 通过<strong>优先保留至少访问过两次的行来处理扫描访问</strong>[Karedla et al.， 1994]。Seg-LRU将LRU堆栈划分为两个逻辑段(见图3.4)，即<strong>适用段段和受保护段</strong>。新进来的缓存行被添加到试用段，当他们收到再次命中，被提升到保护段。因此，受保护段中的行至少被访问了两次，而扫描访问永远不会提升到受保护段。在驱逐中，从试用部分中选择最近使用最少的一行。</li>
<li>表 3.4 总结了 seg-lru 的操作。在预备段的 MRU 位置插入新的行，在缓存命中时，行被提升到保护段的 MRU 位置。因为受保护的部分是有限的，提升到受保护的部分可能会迫使受保护部分的 LRU 线迁移到试用部分的 MRU 末端，使这一行在从试用部分被驱逐之前有另一个机会获得 hit。因此，Seg-LRU 可以适应工作集的变化，因为老的缓存行最终被降级到试用段。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105040.png" alt="20210820105040" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105221.png" alt="20210820105221" loading="lazy"></li>
<li>Seg-LRU 策略的一个变体赢得了第一次缓存替换锦标赛[Gao和Wilkerson, 2010]。</li>
</ul>
<h4 id="beyond-lru-insertion-and-promotion-policies">BEYOND LRU: INSERTION AND PROMOTION POLICIES</h4>
<ul>
<li>Qureshi 等人[2007]观察到，通过修改插入策略，同时保持驱逐策略不变(将缓存行逐出LRU位置)，可以实现基于最近的策略的变体。例如，可以使用 <strong>LRU Insertion policy (LIP)</strong> [Qureshi et al.， 2007] 来模拟 MRU (表3.2)，该策略在 LRU 位置插入新行，而不是 MRU 位置(见表3.5)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105811.png" alt="20210820105811" loading="lazy"></li>
<li>这种见解促进了使用新的插入和 promotion 策略的替换策略的设计。通过以不同的方式解释最近的信息，这些策略可以处理比 LRU 更广泛的访问模式类。在本节中，我们将讨论这方面的一些值得注意的解决方案。由于应用程序通常由许多不同的访问模式组成，这些策略本身都不够，通常被用作混合解决方案的一部分，我们将在3.3节中讨论。</li>
</ul>
<h5 id="bimodal-insertion-policy-bip">Bimodal Insertion Policy (BIP)</h5>
<ul>
<li>由于像 LIP(和MRU) 这样的抗抖动策略不能适应在阶段变化期间工作集的变化，<strong>BIP</strong> [Qureshi等人，2007] 修改LIP，<strong>使缓存行偶尔(以低概率)插入到 MRU 位置</strong>。BIP 保持 LIP 的抗抖动，因为它在大多数时间插入 LRU 位置，但它也可以通过偶尔保留较新的行来适应阶段变化。表3.6显示，BIP 插入到 MRU 位置的概率为 e，设为 1/32 或 1/64。e 取值为 1 表示在 MRU 位置插入(模拟 LRU 插入策略)，e 取值为 0 表示在 LRU 位置插入(模拟LIP插入策略)。因此，从实现的角度来看，<strong>BIP 的参数统一了 LRU 和 MRU 插入光谱中不同位置的所有插入策略</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820110509.png" alt="20210820110509" loading="lazy"></li>
</ul>
<h5 id="static-re-reference-interval-prediction-srrip">Static Re-Reference Interval Prediction (SRRIP)</h5>
<ul>
<li>Jaleel等人[2010b]认为<strong>缓存替换可以被认为是一个重新引用区间预测 (RRIP) 问题</strong>，传统的 LRU 链可以被认为是一个 RRIP 链; LRU 链上一行的位置代表了自上次使用以来的时间，而 RRIP 链上一行的位置代表了它被预测重新引用的顺序[Jaleel et al.， 2010b]。特别是，预测在 RRIP 链首的行将被最快的重新引用(最短的重新引用间隔)，而在 RRIP 链尾的行将被最远的重新引用(最大的重新引用间隔)。当缓存丢失时，位于 RRIP 链尾部的行将被替换。</li>
<li>在这个视图中，我们可以看到，LRU策略预计，新的缓存行将有 near immediate re-reference 间隔（插入在 RRIP 链的头），而 thrash-resistant LIP 策略预测，新行将有一个遥远的 re-reference 间隔（插入在 RRIP 的尾部）。图 3.5 用一个 2bit 重引用预测值(RRPV)表示的RRIP 链说明了这一点：00 对应最近的重参考区间预测，11 对应遥远的重参考区间预测。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820111229.png" alt="20210820111229" loading="lazy"></li>
<li>Jaleel等人[2010b]表明，与预测位于 RRIP 链末端的重新引用间隔不同，<strong>预测中间的重新引用间隔有很大的好处，它允许替换策略适应不同访问模式的混合</strong>。特别是，扫描对未重用数据的引用会破坏最近友好策略(如 LRU)，因为它们会丢弃应用程序的工作集，而不会产生任何缓存命中。<strong>为了适应最近友好访问和扫描的混合，Jaleel等人[2010b] 提出了SRRIP</strong>，它<strong>给进入的行一个中间的重新引用间隔</strong>，然后如果它们被重用，将它们提升到链首。因此，SRRIP 为最近友好的策略增加了抗扫描的能力，因为它防止具有远重引用间隔的行(扫描)驱逐具有近重引用间隔的行。</li>
<li>在大多数情况下，SRRIP 为每个缓存块关联一个 m 位值来存储它的 Re-Reference Prediction value (RRPV)，但是 Jaleel 等人[2010b] 发现一个 2 位的RRPV值就足以提供抗扫描能力。表 3.7 总结了2位 RPRV 值对SRRIP的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820115551.png" alt="20210820115551" loading="lazy"></li>
<li><strong>和 LRU 一样，当所有块的重引用间隔大于可用缓存时，SRRIP会对缓存进行震荡。</strong> 为了添加 thrash-resistance scan-resistant 策略)，Jaleel et al. [2010b] 提出 <strong>BimodalRRIP (BRRIP)</strong>, BIP[Qureshi et al., 2007] 的一个变种，大多数缓存块都插入了一个远端重引用间隔预测(即 RRPV 为 3)，它很少插入一个中间重引用间隔预测(即 RRPV 为 2 )的缓存块。. BRRIP 业务的 RRPV 汇总于表 3.8。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820120340.png" alt="20210820120340" loading="lazy"></li>
<li>由于应用程序可以在最近友好型和抖动型工作集之间交替使用，因此 SRRIP 和 BRRIP 本身都是不够的。在3.3节中，我们将讨论可以解决所有三种常见访问模式(最近友好、抖动和扫描)的 SRRIP 和 BRRIP 的混合版本，为许多应用程序带来性能改进。</li>
</ul>
<h5 id="protecting-distance-based-policy-pdp">Protecting Distance-Based Policy (PDP)</h5>
<ul>
<li><strong>PDP</strong> 策略[Duong et al.， 2012]是 RRIP 的一种推广，因为它<strong>动态估计保护距离(PD)，所有的缓存行在 PD 访问时都受到保护，然后才能被驱逐。PD 是一个单独的值，用于插入到缓存中的所有行，但它会根据应用程序的动态行为不断更新</strong>。</li>
<li>为了估计 PD, PDP 计算重用距离分布(RDD)，这是在程序执行的最近间隔内观察到的重用距离分布。使用 RDD, <strong>PD 被定义为覆盖缓存中大多数行的重用距离</strong>，这样大多数行在 PD 或更小的距离被重用。例如，图 3.6 显示了 436.cactusADM 的 RDD。这里 PD 设置为 64。使用小型专用处理器很少重新计算 PD。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820152114.png" alt="20210820152114" loading="lazy"></li>
<li>更具体地说，在插入或提升时，每条缓存行都被赋一个值来表示其剩余的 PD (RPD)，即它仍然受到保护的访问的次数；这个值被初始化为 PD。每次访问一个集合后，该集合中每一行的 RPD 值都是通过递减的 RPD 值 (在0处饱和) 来老化的。只有当该行的 RPD 大于 0 时，该行才会受到保护。未受保护的行被选为受害者进行逐出。</li>
</ul>
<h5 id="pseudolru-置换gippr-的起源插入和-promotion">PseudoLRU 置换(GIPPR) 的起源插入和 Promotion</h5>
<ul>
<li>受 RRIP 的启发，Jiménez[2013] 观察到在插入和 Promote 的选择上有很多自由度，因此他们使用 <strong>插入/Promote 向量(IPV)</strong> 的概念概括了插入和 Promote 策略的修改。当块被插入到缓存中，或者从最近栈的不同位置提升时，IPV 则指示块在最近栈中的新位置。特别是，k 路 set-associative 缓存，IPV 是一个 k+1 个项组成的向量的，其中每个项的整数从 0 到 k-1，<strong>其中第 I 个位置的值表示位置 I 的块在被重新引用时应该提升到的新位置</strong>。IPV 中的第 k 个项表示应该插入一个新的传入块的位置。如果最近堆栈中的新位置小于旧位置，则块向下移动以腾出空间；否则，块会向上移动以腾出空间。</li>
<li>图3.7显示了两个样本IPVs，第一个代表LRU，第二个代表更复杂的插入和提升策略。</li>
<li>虽然 IPVs 的广义概念非常强大，但可能的 IPVs 数量呈指数增长，(k-way 缓存的可能的 IPVs 有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>k</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">k^{k+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>)，因此 Jiménez[2013] 使用离线遗传搜索来为 SPEC 2006 基准开发良好的 IPVs。遗传搜索得到的IPV如图3.7底部所示。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820153014.png" alt="20210820153014" loading="lazy"></li>
<li>正如不存在适用于所有工作负载的单一插入策略或 RRIP 策略一样，每个工作负载的最佳 IPV 也是不同的。因此，Jiménez[2013] 提出了一种混合解决方案，使用 set dueling (见章节3.3) 来考虑多个 IPVs。</li>
</ul>
<h4 id="extended-lifetime-recency-based-policies">EXTENDED LIFETIME RECENCY-BASED POLICIES</h4>
<ul>
<li>延长生存期策略是基于最近的策略的一个特殊类，它通过将一些缓存行存储在辅助缓冲区或受害缓存中人为地延长它们的生存期。这里的<strong>关键动机是将驱逐决定推迟到更晚的时候，以便做出更明智的决定</strong>。此策略<strong>允许粗粒度策略稍微增大缓存命中的重用窗口，使其大于缓存的大小</strong>。</li>
</ul>
<h5 id="shepherd-cache">Shepherd Cache</h5>
<ul>
<li>Shepherd Cache (SC) 模仿了 Belady 的最优策略 [Rajan and Govindarajan, 2007]。由于 Belady 的策略需要未来访问的知识，Rajan 和 Govindarajan 在一个叫做 Shepherd Cache 的辅助缓存的帮助下模拟了这个未来的前瞻性。特别地，缓存在逻辑上分为两个组件，<strong>模拟最佳替换的主缓存(MC)<strong>和</strong>使用简单的FIFO替换策略的 SC</strong>。SC 通过提供一个前视窗口来支持 MC 的最佳替换。新行最初在 SC 中进行缓冲，直到新行离开 SC 时，才从 MC 决定替换受害者。当新行在 SC 中时，将收集关于替换候选人在 MC 中重用顺序的信息。</li>
<li>例如，由于 Belady 的策略驱逐了在未来被重用最多的行，所以早期被重用的行就不太可能被驱逐。当从 SC 中移除新行(由于 SC 中的其他插入)时，将通过从前瞻性窗口中尚未重用的 MC 中选择一个候选者或最后重用的候选者来选择一个替换候选者; 如果 MC 中的所有行在 SC 行重用之前被重用，那么 SC 行将替换自己。尽管 SC 和 MC 在逻辑上是分离的，Rajan 和 Govindarajan[2007] 通过组织缓存避免了数据从一个组件到另一个组件的任何移动，这样两个逻辑组件可以被组织为一个单一的物理结构。</li>
<li>因此，SC 通过在 SC的帮助下延长 MC 缓存中的行的生命周期，模拟了一种更好的替换方案, SC 的权衡是 MC 中的替换以高 lookahead 接近真正的最优，而更高的 lookahead 是以降低 MC 容量为代价的。不幸的是，Jain 和 Lin[2016] 的后续工作表明，为了接近 belady 的最优策略的行为，该策略需要提前 8x 缓存的大小。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Taxonomy]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Taxonomy/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Taxonomy/">
        </link>
        <updated>2021-08-20T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Taxonomy</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Taxonomy</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="缓存替换策略的分类">缓存替换策略的分类</h2>
<ul>
<li>为了组织这本书和几十年来研究过的许多思想，我们提出了缓存替换问题的解决方案的分类。我们的分类法建立在这样一个观察之上:缓存替换策略解决了一个预测问题，其<strong>目标是预测是否应该允许任何给定的行留在缓存中</strong>。该决定在缓存行生命周期的多个点重新计算，从该行插入缓存时开始，到该行从缓存中删除时结束。</li>
<li>因此，在我们的分类法中，我们首先根据插入决策的粒度将缓存替换策略分为两大类。在第一类策略中，我们称之为<strong>粗粒度策略</strong>，当所有的行插入缓存时，它们被<strong>相同地对待</strong>，并且<strong>只根据它们在缓存中的行为来区分行</strong>。例如，由于一个缓存行驻留在缓存中，它的优先级可能会在每次重用时增加。相比之下，<strong>细粒度策略在插入到缓存时区分行</strong>(除了观察它们驻留在缓存中的行为)。为了在插入时进行区分，<strong>细粒度策略通常依赖于关于缓存访问行为的历史信息</strong>。例如，如果细粒度策略了解到某个特定指令加载的行在过去往往会被逐出而不会被重用，那么它可以以低优先级插入该行。</li>
<li>为了更好地理解我们的分类，了解替换策略通常是通过将少量替换状态与每个缓存行关联来实现的，这一点很有用。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819182902.png" alt="20210819182902" loading="lazy"></li>
</ul>
<h3 id="缓存生命周期中的相关概念">缓存生命周期中的相关概念</h3>
<ul>
<li><strong>Insertion Policy</strong>：当新行插入缓存时，替换策略如何<strong>初始化</strong>它的替换状态?</li>
<li><strong>Promotion Policy</strong>：当缓存行在缓存中命中的时候，替换策略如何<strong>更新</strong>该行的替换状态?</li>
<li><strong>Aging Policy</strong>：当插入或提升<strong>竞争行</strong>时，替换策略如何更新该行的替换状态?</li>
<li><strong>Eviction Policy</strong>：给定固定数量的选择，哪一行执行替换策略驱逐？</li>
<li>从缓存线的生命周期来看，我们可以看到<strong>粗粒度策略在插入时对所有缓存行的处理是相同的</strong>，所以它们主要依赖于聪明的老化和 promotion 策略来操作替换优先级。相反，<strong>细粒度策略使用更智能的插入策略</strong>。当然，细粒度策略还需要适当的老化和 promotion 来更新替换状态，因为在插入时并不能预测所有行为</li>
</ul>
<h3 id="粗粒度缓存">粗粒度缓存</h3>
<ul>
<li><strong>粗粒度策略在第一次插入缓存时对所有行的处理都是相同的</strong>，它们通过<strong>观察驻留在缓存中的行的重用行为来区分对缓存友好的行和对缓存不友好的行</strong>。根据用于区分缓存驻留行的指标，我们将粗粒度策略进一步划分为三个类。
<ul>
<li>第一个类包括绝大多数粗粒度策略，描述了基于最近访问时间对高速缓存行进行排序的策略。</li>
<li>第二个类包括基于访问频率对高速缓存行进行排序的策略。</li>
<li>最后，第三类中的策略随着时间的推移监视缓存行为，以便在给定的时间段内动态地选择最佳粗粒度策略。</li>
</ul>
</li>
</ul>
<h4 id="recency-based-policies">Recency-Based Policies</h4>
<ul>
<li>基于最近的策略基于每一行在其生命周期内的最新引用来排序行。为了达到这个顺序，基于最近的策略维护了一个概念性的 recency stack，该栈提供了引用行的相对顺序。不同的政策以不同的方式利用最新信息。例如，常用的LRU(最近最少使用)策略(及其变体)优先驱逐最近最少使用的行，而MRU(最近最常使用)等策略优先驱逐最近使用的行；其他政策利用中间解决方案。</li>
<li>我们进一步划分基于最近行为的生命周期定义的基于最近行为的策略。
<ul>
<li>第一个类，包括绝大多数基于 recency-based 的策略，描述了当一行被从缓存中移除时结束该行生命周期的策略。我们认为这种策略有 <strong>固定的生命周期</strong>。</li>
<li>第二个类包括一些策略，这些策略通过引入一个临时结构来<strong>延长缓存行的生命周期</strong>，使其超越回收，该结构为具有较长重用距离的行提供第二次接收缓存命中的机会。我们说，这样的政策有一个<strong>延长的寿命</strong>。</li>
</ul>
</li>
</ul>
<h4 id="frequency-basedpolicies">Frequency-BasedPolicies</h4>
<ul>
<li>基于频率的策略维护频率计数器，以<strong>根据它们被引用的频率对行进行排序</strong>。不同的替换策略使用不同的策略来更新和解释这些频率计数器。例如，<strong>一些策略单调地增加计数器，而另一些策略则随着时间的推移使计数器老化</strong>(降低其值)。另一个例子是，一些策略驱逐频率最低的行，而另一些策略驱逐频率满足预定义标准的行</li>
</ul>
<h4 id="hybrid-policies">Hybrid Policies</h4>
<ul>
<li><strong>由于不同的粗粒度策略适用于不同的缓存访问模式，因此混合策略可以在几个预先确定的粗粒度策略中动态选择，以适应程序执行中的阶段变化</strong>。特别是，混合策略在评估期间观察一些候选粗粒度策略的命中率，并使用此反馈来为未来访问选择最佳的粗粒度策略。自适应策略是有利的，因为它们有助于克服单个粗粒度策略的问题。在第三章中,我们将看到,最先进的混合政策调节粗粒度之间使用不同的插入优先的政策,我们注意到,尽管插入优先级的变化随着时间的推移,这些策略仍是粗粒度的,因为在一个时期,所有的处理是完全相同的。</li>
</ul>
<h3 id="细粒度缓存">细粒度缓存</h3>
<ul>
<li><strong>细粒度策略在将行插入缓存时区分行</strong>。它们通过使用来自高速缓存行以前生命周期的信息来进行这些区分。例如，如果一行在过去没有收到命中，那么它可以以低优先级插入。由于记住所有缓存行的过去行为是不可行的，细粒度策略通常记住一组缓存行的行为。例如，许多策略组合成一组最后被相同的加载指令访问的缓存行。</li>
<li>当然，所有细粒度策略都需要考虑的一个问题是在插入时用于区分这些组的指标。基于使用的度量，我们将细粒度策略分为两大类:
<ul>
<li>(1) 基于分类的策略与每一组可能的预测之一相关联，即缓存友好型和缓存厌恶型;</li>
<li>(2) 基于重用距离的策略尝试预测每组缓存线的详细重用距离信息。</li>
</ul>
</li>
<li>这两类定义了一个频谱的极端，频谱一端的策略只有两个可能的预测值，而频谱另一端的政策有许多可能的预测值;许多策略将会在四个或八个可能值中预测其中之一。最后，Beckmann 和 Sanchez 提出了新的度量标准 [Beckmann和Sanchez, 2017]，我们将在第三类中讨论</li>
</ul>
<h4 id="classification-based-policies">Classification-Based Policies</h4>
<ul>
<li>基于分类的策略将传入的缓存行分为两类: 友好缓存行或反对缓存线。其<strong>主要思想是优先清除不支持缓存的行，以便为支持缓存的行留下更多的空间</strong>，因此，支持缓存的行会以比反对缓存的行更高的优先级插入。<strong>在缓存友好的行之间维护二级排序，通常基于最近次数</strong>。基于分类的策略被广泛认为是最先进的，因为 (1) 它们可以利用过去行为的长期历史来为未来做出更明智的决定，(2) 它们可以容纳所有类型的缓存访问模式。</li>
</ul>
<h4 id="reuse-distance-based-policies">Reuse Distance-Based Policies</h4>
<ul>
<li>基于重用距离的策略预测传入行的详细重用距离信息。<strong>超过预期重用距离而没有接收到命中的行将从缓存中删除</strong>。这些策略可以被视为基于最近的策略或基于频率的策略的预测变体，因为<strong>基于最近的和基于频率的策略都通过监视缓存线在缓存驻留时的重用行为隐式地估计重用距离</strong>(分别通过最近和频率)。利用历史信息对重用距离的显式预测有其独特的优点和缺点</li>
</ul>
<h3 id="设计考量">设计考量</h3>
<ul>
<li>替换策略的主要目标是提高缓存命中率，许多设计因素有助于实现更高的命中率：
<ul>
<li>粒度: 在插入时，行的粒度是多少?是否所有的高速缓存行都被相同的处理，或者它们是否根据历史信息分配不同的优先级</li>
<li>历史: 替换策略在做决策时利用了多少历史信息?</li>
<li>访问模式: 替换策略对特定访问模式的专门化程度如何?它对访问模式的更改或不同访问模式的混合是否健壮?</li>
</ul>
</li>
<li>图 2.2 总结了我们完整的分类，并显示了替换策略的不同类别是如何处理这些设计因素的。一般来说，当我们移到图的右侧(通常与更新的策略相对应)时，趋势是<strong>趋向于使用较长的历史记录并能够适应各种访问模式的更细粒度的解决方案</strong>。在细粒度预测缓存行为的重要性可以通过观察最近的cache Replacement Championship(2017)的前四个解决方案都是细粒度的来衡量。细粒度预测为最新的细粒度策略提供了两个优势。
<ul>
<li>首先，它们允许细粒度策略只将缓存空间专用于最有可能从缓存中受益的行;相比之下，粗粒度策略倾向于重复将缓存资源分配给没有产生任何命中的行。</li>
<li>其次，它们允许细粒度策略动态地确定不同组的行的访问模式;相比之下，粗粒度策略假设整个缓存遵循统一的缓存访问模式。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820095251.png" alt="20210820095251" loading="lazy"></li>
</ul>
</li>
<li>在细粒度的政策中,趋势是使用更长的历史作为state-ofthe-art细粒度策略(Jain and Lin, 2016)从历史是8样本信息缓存的大小(见第4章)。悠久的历史的应用让策略检测缓存友好长期重用行,否则就会被一长串厌恶缓存的中间访问所混淆。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Introduction]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Introduction/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Introduction/">
        </link>
        <updated>2021-08-20T02:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Introduction</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Introduction</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>这本书总结了CPU数据缓存的缓存替换策略。重点是算法问题，因此作者首先定义了一个分类法，将以前的策略分为两大类，他们称之为<strong>粗粒度策略</strong>和<strong>细粒度策略</strong>。然后每一类又分为三个子类，描述了解决缓存替换问题的不同方法，并总结了每一类的重要工作。然后探讨了更丰富的因素，包括优化缓存miss率之外的指标的解决方案、针对多核设置定制的解决方案、考虑与预取器的交互的解决方案以及考虑新的内存技术的解决方案。本书最后讨论了未来工作的趋势和挑战。这本书假定读者将对计算机架构和缓存有一个基本的了解，将对整个领域的学者和从业者有用。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>几十年来，移动数据的延迟大大超过了执行一条指令的延迟，所以缓存，既减少了内存延迟又减少了内存流量，是所有现代微处理器的重要组成部分。因为缓存的大小和延迟之间有一个一般的权衡，大多数微处理器维持一个缓存的层次结构，较小的低延迟缓存由较大的高延迟缓存提供下层支持，后者最终由 DRAM 提供。对于每一个缓存，有效性可以通过它的命中率来衡量，我们定义为 s/r，其中 s 是缓存服务的内存请求的数量，r 是向缓存发出的内存请求的总数。</li>
<li>有几种方法可以提高缓存命中率。<strong>一种方法是增加缓存的大小</strong>，通常以增加延迟为代价。<strong>第二种方法是增加缓存的关联性</strong>，这就增加了缓存行可以映射到的可能缓存位置的数量。在一种极端情况下，直接映射缓存(关联性为1)将每条缓存行映射到缓存中的单个位置。在另一个极端，完全关联缓存允许缓存线放置在缓存的任何位置。不幸的是，随着关联性的增加，功耗和硬件复杂度都会增加。<strong>第三种方法，也是本书的主题，是选择一个好的缓存替换策略</strong>，它回答了这样一个问题:<strong>当一个新行要插入到缓存中，哪一行应该被移除以为新行腾出空间</strong>?</li>
<li>写一本关于缓存替换的书似乎有些奇怪，因为Lazslo Belady在50多年前就提出了一个可证明的最优策略。但Belady的政策是不可实现的，因为它依赖于未来的知识，它抛弃了在未来会被重复使用最多的缓存行。因此，多年来，研究人员已经探索了几种不同的方法来解决缓存替换问题，通常依赖于考虑访问频率、访问近期和最近的预测技术的启发式方法。</li>
<li>此外，还有一个问题是缓存替换策略与死块预测器之间的关系，死块预测器试图预测缓存中不再需要的行。我们现在知道，缓存行的生命周期有多个决策点，从插入该行开始，随着时间的推移，直到最终退出该行，我们知道在这些不同的决策点执行操作有不同的技术。基于这个观点，我们认为死块预测器是缓存替换策略的一种特殊情况，我们发现缓存替换策略的空间非常丰富。</li>
<li>最后，缓存在软件系统中也很普遍。事实上，第一个替代策略是为操作系统的分页系统开发的，虽然软件缓存和硬件缓存之间存在技术上的差异，但我们希望本书中的一些想法也能对软件缓存的开发人员有用。</li>
<li><strong>Scope</strong>：本书主要介绍CPU数据缓存的硬件cache替换策略。虽然我们讨论的大多数研究都是在一级缓存的背景下进行的，智能缓存替换的好处是最明显的，但一般的想法通常适用于缓存层次结构的其他级别。</li>
<li><strong>Roadmap</strong>：在第2章中，我们首先定义一个二维分类法。主要维度描述替换决策的粒度。第二个维度描述了用于做出替换决策的度量。然后第3章和第4章使用我们的分类法来描述现有的替换策略。第5章介绍了使缓存替换复杂化的其他考虑因素，包括数据预取、共享缓存、可变丢失成本、压缩和新技术。在第六章中，我们总结了2017年举行的Cache replace锦标赛的结果，总结了最近的Cache replace趋势，然后后退一步，总结未来研究的更大趋势和挑战。</li>
</ul>
<h3 id="相关介绍">相关介绍</h3>
<h4 id="参考资料">参考资料</h4>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/102293437">知乎 - CPU Cache 的基本原理</a></li>
</ul>
<h4 id="概念理解延申">概念理解延申</h4>
<h5 id="提高缓存命中率的方法二增加缓存的关联性">提高缓存命中率的方法二：增加缓存的关联性</h5>
<p><strong>什么是缓存的关联性</strong></p>
<ul>
<li>
<p>在Cache中，地址映射是指把主存地址空间映射到Cache地址空间，在将主存块复制到Cache中的时候遵循一定的映射规则，标志位为1时候，表示其Cache映射的主存块数据有效。（注意与地址变换的区别，地址变换是指CPU在访存的时候，将主存地址按照映射规则换算成Cache地址的过程 ）。 地址映射有三种方式：直接映射，全相联映射，组相联映射</p>
</li>
<li>
<p><strong>直接映射</strong></p>
<ul>
<li>优点：硬件设计上会更加简单，因此成本上也会较低</li>
<li>缺点：会出现 cache 颠簸（cache thrashing）<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150201.png" alt="20210819150201" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819122855.png" alt="20210819122855" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>两路组相连</strong>：两路组相连缓存较直接映射缓存最大的差异就是：第一个地址对应的数据可以对应 2 个 cache line，而直接映射缓存一个地址只对应一个cache line。 <strong>组相联的本质就是组内是全相联，组间是直接相连</strong></p>
<ul>
<li>两路组相连缓存的硬件成本相对于直接映射缓存更高。因为其每次比较tag的时候需要比较多个cache line对应的tag（某些硬件可能还会做并行比较，增加比较速度，这就增加了硬件设计复杂度）</li>
<li>两路组相联缓存可以有助于降低 cache 颠簸可能性<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150037.png" alt="20210819150037" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150219.png" alt="20210819150219" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>全相联</strong>：所有的cache line都在一个组内，因此地址中不需要set index部分。因为，只有一个组让你选择，间接来说就是你没得选。我们根据地址中的tag部分和所有的cache line对应的tag进行比较（硬件上可能并行比较也可能串行比较）。</p>
<ul>
<li>在全相连缓存中，任意地址的数据可以缓存在任意的cache line中。所以，这可以最大程度的降低cache颠簸的频率。但是硬件成本上也是更高。</li>
<li>由于Cache比较电路的设计和实现比较困难，这种方式只适合于小容量Cache采用。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819174545.png" alt="20210819174545" loading="lazy"></li>
</ul>
</li>
</ul>
<p><strong>缓存关联性如何影响缓存效率？</strong></p>
<ul>
<li>直接相连也就是对应的缓存关联性较差，对应的会造成缓存的频繁写入淘汰，因为映射关系固定，而增加了相联性之后，因为映射关系不再固定，缓存缺失的概率降低，命中率得到提升</li>
</ul>
<h4 id="béládys-algorithm">Bélády's algorithm</h4>
<ul>
<li>https://en.wikipedia.org/wiki/Cache_replacement_policies#B%C3%A9l%C3%A1dy's_algorithm</li>
<li>最有效的缓存算法是总是丢弃那些在未来很长一段时间内不需要的信息。这个最优结果被称为 Bélády 的最优算法/简单地说最优替换策略或千里眼算法。由于通常不可能预测未来需要多少信息，这在实践中通常是不可实现的。只有经过实验才能计算出实际的最小值，并且可以比较实际选择的缓存算法的有效性。</li>
</ul>
]]></content>
    </entry>
</feed>