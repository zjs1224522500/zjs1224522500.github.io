<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>YouDieInADream</title>
    <updated>2021-11-03T02:52:49.132Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, YouDieInADream</rights>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Other]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-Three-Other/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-Three-Other/">
        </link>
        <updated>2021-08-20T09:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Other</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Other</li>
</ul>
</blockquote>
<!-- more -->
<h3 id="other-prediction-metrics">OTHER PREDICTION METRICS</h3>
<ul>
<li>并非所有细粒度策略都预测重用距离或二分标签，当然可以使用不同的预测目标捕获过去的行为。例如，Kharbutli 和 Solihin[2005] 死块预测器的一个组件可以<strong>预测一个缓存行在缓存中被重用的最大次数</strong>。作为这类解决方案的一个例子，我们现在详细讨论 <strong>EVA 策略</strong> [Beckmann和Sanchez, 2017]，它引入了一种新的预测目标，称为 EVA，这是少数几个使用历史信息来指导老化过程的细粒度解决方案之一。</li>
</ul>
<h4 id="economic-value-addedeva">ECONOMIC VALUE ADDED(EVA)</h4>
<ul>
<li>贝克曼和桑切斯认为，只有当我们对未来拥有完美的知识时，用预期时间最长的候选人替代重用是最优的 [Belady, 1966]，但这一策略对于面临未来固有不确定性的实际解决方案是不够的 [Beckmann和桑切斯，2017]。因此，实际的解决方案需要平衡两个相互竞争的目标:
<ul>
<li>(1) <strong>最大限度地提高给定行的命中缓存的概率</strong>，</li>
<li>(2) <strong>限制该行消耗缓存空间的持续时间</strong>。</li>
</ul>
</li>
<li>仅仅基于重用距离的解决方案只考虑了权衡的一方面。</li>
<li>为了解决这一限制，Beckmann 和 Sanchez[2017] 提出了一个称为经济附加值(EVA)的新指标，将这两个因素合并为一个单一指标。<strong>EVA 被定义为候选对象可能产生的命中数与其在缓存中的平均占用率的比值</strong>。方程 (4.1) 表明 EVA 是对给定缓存行计算的。我们看到一个缓存行的 EVA 有两个组成部分。首先，该行将因其预期的未来命中次数而获得奖励(具有较高重用概率的该行将具有较高的EVA值)。其次，这一行会因为它所消耗的缓存空间而受到惩罚。这个惩罚是通过对每个候选对象在缓存中花费的时间进行收费来计算的，收费的速率是一行的平均命中率(缓存命中率除以其大小)，这是消耗缓存空间的长期机会成本。因此，EVA 指标通过计算该行命中的几率是否值得它所消耗的缓存空间来获取缓存行的成本效益权衡。$$ EVA = Expected_hits - (Cache_hit_rate / Cache_size) * Expected_time$$</li>
<li>候选人的 EVA 是从他们的年龄推断出来的，并随着候选人的年龄进行修正。例如，左边的图4.6 显示了 EVA 是如何随着候选应用程序年龄的变化而变化的，该应用程序遍历一个小数组和一个大数组(右边显示了同一应用程序的重用距离分布)。首先，EVA 较高，因为访问很有可能来自一个小数组，这意味着替换策略可以赌候选者会很快命中。但是，当年龄超过短数组的长度时，EVA 急剧下降，因为从大数组缓存行的代价要高得多。此时，低 EVA 使大数组中的行很可能被逐出。缓存来自大数组的行所带来的代价随着行的老化和接近大数组的重用距离而降低。这种惩罚反映在年龄从 50 岁到 250 岁的 EVA 逐渐增加，在这一点上 EVA 替换策略保护缓存行免受大数组替代，即使它们是老的。因此，在面临不确定性时，EVA 替代策略会随着候选人年龄的增长而对他们有更多的了解。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821160012.png" alt="20210821160012" loading="lazy"></li>
<li>当然，<strong>另一种减少不确定性的方法是将缓存行分为不同的类别</strong>。例如，如果我们将小数组和大数组划分为不同的类别，图 4.7 显示了与年龄相关的 EVA，我们可以看到每个类别的 EVA 曲线要简单得多。理论上，可以扩展 EVA 以支持分类，但实现的复杂性将这种扩展限制在少数类。在下一节中，我们将讨论依赖于许多细粒度类的替换策略，但学习每个类的更简单的指标。相比之下，EVA 以细粒度对年龄进行排序，但这限制了 EVA 使用更少的类。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821160331.png" alt="20210821160331" loading="lazy"></li>
<li>EVA 替换策略通过记录命中和驱逐的年龄分布并使用轻量级软件运行时处理有关这些驱逐的信息来计算 EVA 曲线。为了预测一条缓存行的 EVA，将其年龄作为索引，建立一个代表 EVA 曲线的驱逐优先级数组。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Classification]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-Two-Classification/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-Two-Classification/">
        </link>
        <updated>2021-08-20T08:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Classification</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Classification</li>
</ul>
</blockquote>
<!-- more -->
<h3 id="classification-based-policies">CLASSIFICATION-BASED POLICIES</h3>
<ul>
<li><strong>基于分类的策略学习了传入行的二分类：缓存访问是否可能导致未来的命中？</strong> 对缓存友好的行，预期会导致缓存命中的行以更高的优先级插入缓存，以便它们有足够的机会接收缓存命中，而不喜欢缓存的行(不期望缓存命中的行)则以较低的优先级插入，这样它们就可以在不浪费缓存资源的情况下被快速清除。</li>
<li>与其他类替换策略相比，基于分类的策略有几个优点。与混合替换策略对给定时间段内的所有行进行统一决策相比，<strong>基于分类的替换策略可以插入一些高优先级的行和一些低优先级的行</strong>。与基于重用距离的策略(目标是数值的重用距离预测)相比，<strong>基于分类的策略解决了一个更简单的二分预测问题</strong>。</li>
<li>正如我们将看到的，基于分类的政策起源于两种不同的文献。<strong>基于采样的死块预测器(SDBP)</strong> [Khan等人，2010] (章节4.2.1) 建立在死块预测文献的基础上，而<strong>基于签名的命中预测 (SHiP)</strong> [Wu等人，2011a] (章节4.2.2)则来源于缓存替换策略文献。有趣的是，这两种解决方案都得到了概念上相似的想法，而且事后看来，这两篇论文共同统一了这两个领域。</li>
<li>现在我们将讨论这些和其他最近的基于分类的策略。虽然这些政策之间存在显著差异，但它们都有两个特点。
<ul>
<li>首先，它们<strong>都包含一个二分预测器</strong>，该预测器可以学习过去的缓存行为，以指导各个行的插入优先级。</li>
<li>其次，他们<strong>都从最先进的粗粒度政策中借鉴了 Promotion、老化 和 驱逐模式，这有助于解释不准确的预测</strong>。</li>
</ul>
</li>
<li>在描述这些策略时，我们将考虑以下设计问题。
<ul>
<li>哪种缓存解决方案是策略学习？</li>
<li>预测机制是什么，预测的粒度是什么？</li>
<li>是什么老化机制确保不准确的预测最终被淘汰？</li>
</ul>
</li>
</ul>
<h4 id="sampling-based-dead-block-prediction-sdbp">SAMPLING BASED DEAD BLOCK PREDICTION (SDBP)</h4>
<ul>
<li>许多研究发现，由于LLC中的大部分块都是死块(它们在被驱逐之前不会再次重用)，<strong>死块预测可以用来指导缓存替换和死块的早期旁路（旁路即为不缓存）</strong> [Khan等人，2010,Lai和Falsafi, 2000]。Lai 和 Falsafi[2000] 引入了使用死块预测器将数据预取到 L1 中的死块的思想。他们的 reftrace 预测器预测，如果指令地址的 trace 导致最后一次访问一个块，那么同样的 trace 也会导致最后一次访问其他块。为了降低为所有缓存块维护指令跟踪的成本，Khan 等人[2010] <strong>引入了基于采样的死块预测器(SDBP)，它对程序计数器(PC)的缓存行为进行采样，以确定传入的块是否可能 dead</strong>。来自已知插入死块的 PC 的未来缓存访问被 Bypass，这样他们就不会污染缓存。来自没有插入死块的 PC 的访问将使用一些基本策略(即随机或 LRU 替换策略)插入缓存。</li>
<li>值得注意的是，<strong>SDBP 从使用一小部分缓存访问填充的解耦采样器学习</strong>(参见图4.2)。如果一个块被从采样器中移除而不被重用，则对相应的 PC 进行负向训练；否则，预测器被积极训练。解耦采样器有几个优点：
<ul>
<li>首先，只使用所有缓存访问的一小部分样本来训练预测器，这导致了功率和空间效率高的预测器设计和采样器中易管理的元数据(为每个采样器条目维护 PC)。</li>
<li>其次，采样器的替换策略不必与缓存的替换策略相匹配。Khan 等人[2010] 对采样器使用 LRU 策略，他们对主缓存使用随机替换。</li>
<li>最后，采样器的结合性独立于缓存的结合性，这允许开销更小的采样器设计。Khan 等人使用 12 路采样器作为 16 路缓存。表 4.1 总结了SDBP的关键操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115233.png" alt="20210821115233" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115325.png" alt="20210821115325" loading="lazy"></li>
</ul>
</li>
<li>因此，为了回答我们在本节开始时概述的问题:
<ul>
<li>(1) SDBP 学习基于 LRU 的采样器的缓存决策;</li>
<li>(2) 它预测死块(缓存厌恶与缓存友好) 使用一个倾斜的预测器设计，以 PC 的粒度进行这些预测;</li>
<li>(3) SDBP 绕过所有预测为缓存厌恶的传入块，使用基线替换策略管理剩余的行，因此假阳性(预测为缓存友好的缓存厌恶块)使用基线替换策略老化，而假阴性(缓存友好的块被预测为缓存厌恶)没有任何机会重用。</li>
</ul>
</li>
</ul>
<h4 id="signature-based-hit-prediction-ship">SIGNATURE BASED HIT PREDICTION (SHIP)</h4>
<ul>
<li>像 SDBP 一样，<strong>SHiP</strong> [Wu et al.， 2011a] 学习了底层替换策略的淘汰行为，但 SHiP 背后的主要观点是，<strong>重用行为与将该行插入缓存的 PC 更紧密相关，而不是与最后访问该行的 PC</strong>。因此，在缓存淘汰时，SHiP 的预测器被 PC 训练，PC 首先在缓存丢失时插入该行，预测器只在缓存 MISS 时被咨询(在命中时，行被提升到最高优先级而不咨询预测器)。</li>
<li>更具体地说，SHiP 训练一个预测器，它可以学习给定的签名是否具有近或远的重引用间隔。<strong>在缓存中采样一些集合以维护签名和训练预测器。在一个采样集中的缓存命中，与这条缓存行相关联的签名被训练为正，以指示一个近重引用，而在驱逐一条从未被重用的行时，该签名被训练为负，以指示一个远重引用</strong>。当插入新行时，将使用传入行的签名来咨询预测器，并确定传入行的重新引用间隔(对所有访问执行预测，而不仅仅是对采样集)。一旦插入到缓存中，行就使用一个简单的 RRIP 策略进行管理(参见表4.2)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821115906.png" alt="20210821115906" loading="lazy"></li>
<li>签名的选择对 SHiP 的有效性至关重要。Jaleel 等人 [2010b] 评估了一个程序计数器签名(PC)、一个内存区域签名和一个指令序列历史签名，他们发现 <strong>PC 签名性能最好</strong>。</li>
<li>SHiP 基于 DRRIP [Jaleel et al.，2010b] 创建细粒度策略。<strong>DRRIP 对 epoch 中的所有缓存行进行统一的重新引用预测，而 SHiP 进行更细粒度的预测：它通过将每个引用与一个唯一的签名关联起来，将传入行的分类到不同的组中。</strong> 假设具有相同签名的缓存行具有相似的重引用行为，但允许具有不同签名的缓存行在同一 epoch 内具有不同的重引用行为。</li>
<li>现在我们回答本节开始时提到的问题。
<ul>
<li>(1) 最初，SHiP 从 SRRIP 中学习，但一旦 SHiP 的预测器被训练，进一步的训练更新来自 SHiP 自身的重用和驱逐行为。</li>
<li>(2) SHiP 使用基于 PC 的预测器，其中与一行相关联的 PC 就是插入一行缓存 miss 的 PC，其中每个 PC 与一个饱和计数器相关联。</li>
<li>(3) SHiP 依靠 RRIP 政策来老化所有行。</li>
</ul>
</li>
</ul>
<h4 id="hawkeye">HAWKEYE</h4>
<ul>
<li>为了避免基于启发式的解决方案(如LRU)的病态，Hawkeye [Jain和Lin, 2016] 构建了 Belady 的 MIN 解[Belady, 1966]，这是有趣的两个原因。
<ul>
<li>首先，Belady 的 MIN 对于任何引用序列都是最优的，因此基于 MIN 的解决方案可能适用于任何访问模式。</li>
<li>其次，Belady 的 MIN 算法是一种不切实际的算法，因为它替换了将来重复使用最多的行；因此，它依赖于未来的知识。</li>
</ul>
</li>
<li>Hawkeye 的核心见解是，<strong>虽然无法预测未来，但可以将 Belady 的 MIN 算法应用到过去的内存引用中。此外，如果一个程序过去的行为是它未来行为的一个很好的预测器，那么通过学习过去的最优解，Hawkeye 可以训练一个预测器，它应该在未来访问中表现良好</strong>。</li>
<li>为了理解模拟过去事件的 MIN 需要多少历史信息，Jain 和 Lin[2016] 通过限制其未来的窗口来研究 MIN 的性能。图 4.3 显示，虽然 MIN 需要一个很长的窗口到未来(SPECint 2006的 8x 缓存大小)，但它不需要一个无边界窗口。因此，要将 MIN 应用到过去的事件，我们需要一个缓存大小为 8x 的历史记录。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821124850.png" alt="20210821124850" loading="lazy"></li>
<li>由于维持缓存大小 8x 的历史信息开销巨大，Hawkeye 只计算几个抽样集的最优解，并<strong>引入 OPTgen 算法，为这些抽样集计算与 Belady 的 MIN 策略相同的答案</strong>。<strong>OPTgen 确定如果使用 MIN 策略将被缓存的行</strong>。OPTgen 背后的关键观点是，<strong>当一行再次被重用时，可以准确地确定该行的最佳缓存决策</strong>。因此，在每次重用时，OPTgen 都会回答以下问题: 这一行是否会在 MIN 情况下是缓存命中还是缓存丢失？这种洞察力使得 OPTgen 能够使用少量的硬件预算和简单的硬件操作，以 O(n) 复杂性再现 Belady 的解决方案。</li>
<li>图4.4 显示了 Hawkeye 替换策略的总体设计。OPTgen 训练 Hawkeye 预测器，这是一种基于 PC 的预测器，它可以了解 PC 插入的行是倾向于缓存友好型还是缓存反对型。当 OPTgen 确定一条行将被 MIN 命中时，该行对应的 PC 被正训练，否则被负训练。每次使用传入行的 PC 进行缓存插入和提升时，都会参考预测器。被预测为缓存友好的行将以高优先级插入，而被预测为缓存厌恶的行将以低优先级插入(参见表4.3)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821125001.png" alt="20210821125001" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821125202.png" alt="20210821125202" loading="lazy"></li>
<li>回答本节开头的问题:
<ul>
<li>(1) Hawkeye 从最优缓存解决方案中学习，而不是从 LRU 或 SRRIP 中学习；</li>
<li>(2) Hawkeye 使用基于 PC 的预测器学习最优解;</li>
<li>(3) Hawkeye 还依赖于 RRIP 的老化机制，以高优先级插入 age line。为了纠正不准确的预测，Hawkeye 还对预测器进行负训练，当预测到不喜欢缓存的行被取消而没有被重用时。</li>
</ul>
</li>
<li>Hawkeye 在概念上做出了一个有趣的贡献: <strong>它将缓存替换描述为一个有监督的学习问题</strong>，这令人惊讶，因为不像分支预测，程序执行最终提供每个分支的正确结果，硬件缓存不提供这样的标记数据。通过对过去事件应用最优解决方案，Hawkeye 提供了标记数据，这<strong>表明缓存替换领域可能会从监督学习的大量研究中受益</strong>。</li>
</ul>
<h4 id="perceptron-based-prediction">PERCEPTRON-BASED PREDICTION</h4>
<ul>
<li>预测性策略在很大程度上取决于预测者的准确性。<strong>SDBP、SHiP 和 Hawkeye 都使用基于 PC 的预测器，准确率约为 70 - 80%</strong>。Jiménez 和 Teran 旨在通过使用更好的特征和更好的预测模型来提高预测精度 [Jiménez和Teran, 2017, Teran等人，2016]。例如,<strong>感知器预测 [Teran et al., 2016] 使用简单的人工神经网络 [Rosenblatt, 1962]，以增加使用带有更丰富的特性的 PC</strong> ，如 (1) PCs 历史信息，(2) 来自内存地址的位，(3) 数据的压缩表示，和(4) 一个块被访问的次数。每个特征被用来索引一个不同的饱和计数器表，然后求和并与一个阈值进行比较，生成一个二分预测。一小部分的访问被采样以使用感知器更新策略更新感知器预测器：如果预测是错误的，或者如果总和没有超过某个大小，那么计数器在访问时减少，在驱逐时增加。图 4.5 对比了感知器预测器(右)和先前基于 PC 的预测器(左)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821140306.png" alt="20210821140306" loading="lazy"></li>
<li>Multiperspective Reuse Predictor [Jiménez和Teran, 2017] 探索了一组广泛的特性，这些特性捕获了程序的各种属性，生成了一个从多个角度获得信息的预测器。特征被参数化，包含了关于每个训练输入的 LRU 堆栈位置、每个特征被哈希的 PC 位和 PC 历史的长度的更丰富的信息。这些参数一起创建了一个大的特征空间，从而导致更高的预测精度。</li>
</ul>
<h4 id="evicted-address-filtereaf">EVICTED ADDRESS FILTER(EAF)</h4>
<ul>
<li><strong>EAF 策略</strong> [Seshadri等人，2012] <strong>单独预测每个缺失块的重用行为，允许比 PC 更细粒度的差异</strong>。关键的观察是，如果一个高重用的块被过早地从缓存中逐出，那么它将在逐出后不久被访问，而低重用的块在逐出后很长一段时间内都不会被访问。因此，<strong>EAF 策略使用 bloom 过滤器</strong> [bloom, 1970] (一种概率性的、空间效率高的结构，用于确定集合中某个元素的存在或缺失) <strong>来跟踪一小组最近被驱逐的地址</strong>。在<strong>缓存丢失时，如果新行出现在 bloom 过滤器(也称为逐出地址过滤器)中，那么预测该行是重用友好的，并以高优先级插入</strong>; 否则，<strong>根据双峰插入策略 the Bimodal Insertion policy 插入新行</strong>(参见3.1.2节)。当 <strong>bloom 过滤器满时，它被重置</strong>。EAF 在概念上类似于一个小型的候选缓存，它跟踪最近从主缓存中删除的行。表 4.4 总结了 EAF 策略的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821154831.png" alt="20210821154831" loading="lazy">
<ul>
<li><strong>回顾一下 BIP，双峰插入策略，本质是使缓存行以较低的一个概率插入到 MRU 的位置。BIP 保持 LIP 的抗抖动，因为它在大多数时间插入 LRU 位置，但它也可以通过偶尔保留较新的行来适应阶段变化。</strong></li>
</ul>
</li>
<li>从概念上讲，EAF 策略延长了缓存行的生存期，超过了淘汰时间，因此一行的生存期从它的插入开始，但当该行从 bloom 过滤器中删除时结束。随着生命周期的延长，对于具有较长重用间隔的行，EAF 观察重用变得可行，从而获得更好的抗扫描能力和抗抖动能力，从而获得更好的性能。</li>
<li><strong>重用检测器 (ReD)</strong> [crc, 2017, Albericio等人，2013] 提出了类似的想法，因为它<strong>绕过 LLC 或地址重用表中没有击中的任何缓存行，跟踪最近的缓存丢失。因此，ReD 只在第二次重用时将行插入缓存中</strong>。为了避免在第一次看到所有行时绕过它们，ReD 还使用基于 PC 的预测器来预测在第一次访问后可能被重用的行。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Fine-Grained Reuse Distance]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Fine-Grained-One-Reuse-Distance/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Fine-Grained-One-Reuse-Distance/">
        </link>
        <updated>2021-08-20T07:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Reuse Distance</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Fine-Grained Reuse Distance</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="fine-grained-replacement-policies">Fine-Grained Replacement Policies</h2>
<ul>
<li>细粒度策略在插入时区分高速缓存行，这种区分通常基于类似高速缓存行的前一个生存期的回收信息。例如，如果细粒度策略得知某一行在其之前的生命周期中没有被重用而被逐出，那么它可以以低优先级将该行插入到缓存中。相比之下，粗粒度策略(如LRU)只有在从MRU位置迁移到LRU位置后才会驱逐一行，所以该方案强制缓存行驻留在缓存很长一段时间，消耗宝贵的缓存空间来确定行应该驱逐。因此，<strong>通过学习以前缓存行的行为，细粒度策略可以更有效地使用缓存</strong>。</li>
<li>根据用于预测插入优先级的指标，我们将细粒度策略分为三大类。
<ul>
<li>第一类(第4.1节) 包含<strong>预测传入行预期重用时间间隔</strong>的解决方案。</li>
<li>第二类(第4.2节) 包含<strong>预测二分缓存决策</strong>的解决方案 (<strong>缓存友好 vs. 缓存不友好</strong>)。</li>
<li>第三类比其他两类要小得多，包括<strong>引入新的预测指标的策略</strong> [Beckmann and Sanchez, 2017, Kharbutli and Solihin, 2005]。</li>
</ul>
</li>
<li>细粒度解决方案有其他几个设计维度。首先，由于要记住跨多个缓存生存期的单个行的缓存行为可能很麻烦，所以这些策略<strong>学习一组行的缓存行为</strong>。例如，<strong>许多解决方案根据加载行的指令的地址 (PC) 对行进行分组，因为由同一 PC 加载的行往往具有类似的缓存行为</strong>。最近的解决方案着眼于更复杂的方法来分组缓存行 [Jiménez 和 Teran, 2017, Teran et al.， 2016]。第二个设计维度是用于学习缓存行为的历史记录量。</li>
<li>细粒度替换策略起源于两个看起来不同的上下文中。
<ul>
<li><strong>其中一个使用预测来识别死块（即在被移除之前不会被使用的块），然后就可以重新用于其他用途</strong>。例如，识别死块的最早动机之一是将它们用作预取缓冲区 [Hu等人，2002,Lai等人，2001]。另一个动机是关闭已死的缓存行 [Abella等人，2005,Kaxiras等人，2001]。</li>
<li><strong>第二个工作推广了混合重引用间隔策略</strong> [Jaleel et al.， 2010b]，<strong>使它们更有 learning 的基础</strong>。</li>
</ul>
</li>
<li>尽管它们的起源不同，这两种研究方向已经趋同为概念上相似的解决方案。</li>
</ul>
<blockquote>
<ul>
<li>PC 程序计数器是计算机处理器中的寄存器，它包含当前正在执行的指令的地址（位置）。当每个指令被获取，程序计数器的存储地址加一。在每个指令被获取之后，程序计数器指向顺序中的下一个指令。当计算机重启或复位时，程序计数器通常恢复到零。</li>
</ul>
</blockquote>
<h3 id="reuse-distance-prediction-policies">REUSE DISTANCE PREDICTION POLICIES</h3>
<ul>
<li>基于重用距离的策略预估块的重用距离，重用距离可以根据连续引用块之间的访问次数或循环次数来定义。对重用距离的完美预测足以模拟 Belady 的最优解决方案，但<strong>由于重用距离值的高变化，很难精确预测重用距离。因此，现实的解决方案估计重用距离分布或其他聚合的重用距离统计</strong>。</li>
</ul>
<h4 id="expiration-based-dead-block-predictors">EXPIRATION-BASED DEAD BLOCK PREDICTORS</h4>
<ul>
<li>许多死块预测器使用<strong>过去的驱逐信息</strong>来估计平均重用距离统计数据，它们驱逐未在预期重用距离内重用的行 [Abella et al.， 2005, Faldu和Grot, 2017, Hu et al.， 2002, Kharbutli 和 Solihin, 2005, Liu et al.， 2008, Takagi 和 Hiraki, 2004]。</li>
<li>Hu 等人了解了缓存块的存活时间，其中<strong>存活时间定义为一个块在缓存中保持存活的周期数</strong> [Hu等人，2002]。当插入一个块时，它的生命周期预计与最近的一次的生命周期相近。如果块在缓存中停留了两倍的生命周期而没有收到缓存命中，则这样的块被定义为死亡块并将从缓存中移除。</li>
<li>Abella 等人使用了类似的死块预测策略来关闭 L2 高速缓存行，但他们没有使用周期数，而是<strong>根据连续引用之间的缓存访问次数来定义重用距离</strong> [Abella等人，2005]。</li>
<li>Kharbutli 和 Solihin[2005] <strong>使用计数器来跟踪每条缓存行的访问间隔，其中一行的访问间隔被定义为在对该行的后续访问之前被访问的其他缓存行的数量</strong>。而且，如果访问间隔超过一定的阈值，它们就会预测一个缓存行是死的。该阈值由访问间隔预测器(AIP)预测，该预测器跟踪所有过去驱逐的访问间隔，并记住基于 PC 的表中所有此类间隔的最大值。</li>
<li>Faldu 和 Grot [2017] 展示了 Leeway 策略，<strong>该策略使用了 live distance 的概念，即在缓存行生命周期中观察到的最大栈距离，以识别死块</strong>。缓存块的活动距离是从块的前几代学习的，当一个块超过它的活动距离时，它被预测为死块。使用每个 PC 的活距离预测表(LDPT)来保存前面的活距离。LDPT 为传入的块预测活动距离，任何在 LRU 堆栈中移动超过其预测活动距离的块都被预测为死块。<strong>为了避免跨生存期和跨同一 PC 访问的块的活动距离的高可变性，Leeway 部署了两个更新策略</strong>，它们控制基于工作负载特征调整 LDPT 中的活动距离值的速率。第一种策略是激进的，支持 bypassing；第二种策略是保守的，有利于缓存。Leeway 使用 Set Dueling [Qureshi et al.， 2007] 来选择这两种策略。</li>
</ul>
<h4 id="reuse-distance-ordering">REUSE DISTANCE ORDERING</h4>
<ul>
<li>Keramidas 等人 [2007] 使用重用距离预测来代替驱逐预计在未来被重用最多的缓存行。他们的策略学习每个 LOAD 指令 (PC) 的重用距离，对于每个传入行，它<strong>计算一个估计的访问时间 (ETA)，这是当前时间和预期的重用间隔的总和。然后，它根据 ETA 值来排序缓存行，并驱逐 ETA 值最大的行</strong>。</li>
<li>为了防止 ETA 预测不可用的情况，该方案<strong>还跟踪行衰减，这是对一行在缓存中未被访问的时间的估计，如果该行的衰减间隔大于其 ETA，它将驱逐该行</strong>。</li>
<li>更具体地说，图4.1显示了替换策略有两个候选行:
<ul>
<li>(1) ETA最大的行(ETA行)；</li>
<li>(2) 衰减时间最大的行(LRU行)</li>
</ul>
</li>
<li>选择两个值中最大的行进行驱逐。因此，该策略在有条件时依赖 ETA，否则转变回 LRU。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821112316.png" alt="20210821112316" loading="lazy"></li>
</ul>
<h3 id="参考文献">参考文献</h3>
<ul>
<li><a href="">[1] Chip Multiprocessor Architecture: Techniques to Improve Throughput and Latency</a>
<ul>
<li>Kunle Olukotun, Lance Hammond, and James Laudon</li>
<li>2007</li>
</ul>
</li>
</ul>
<h3 id="基于重用距离的缓存策略的其他相关研究">基于重用距离的缓存策略的其他相关研究</h3>
<ul>
<li><a href="https://research.cs.wisc.edu/multifacet/papers/sigmetrics13_caches_reuse.pdf">Reuse-based Online Models for Caches</a></li>
<li><a href="https://www.usenix.org/sites/default/files/conference/protected-files/inflow16_slides_lavaee.pdf">Cache Replacement Based on Reuse-Distance Prediction</a></li>
<li><a href="http://iccd.et.tudelft.nl/Proceedings/2007/Papers/5.1.5.pdf">ElCached: Elastic	Multi-Level	Key-Value	Cache</a></li>
<li><a href="https://www.cs.rochester.edu/~cding/Documents/Publications/TR741.pdf">Reuse Distance Analysis 2001</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/2818374"></a></li>
<li><a href="https://dl.acm.org/doi/abs/10.1145/3380732"></a></li>
<li><a href="">一种基于重用距离预测与流检测的高速缓存替换算法</a></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Hybrid]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-Three-Hybrid/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-Three-Hybrid/">
        </link>
        <updated>2021-08-20T06:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Hybrid</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Hybrid</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="hybrid-policies">Hybrid Policies</h2>
<ul>
<li>混合策略 [Jaleel et al.， 2010b, Qureshi et al.， 2006, 2007] 认识到不同的工作负载，甚至相同工作负载中的不同阶段，可以从不同的替代策略中受益。例如，如果程序在小工作集和大工作集之间交替，<strong>当工作集较小时，它将受益于最近友好策略，当工作集较大时，它将受益于抗抖动策略</strong>。因此，混合策略评估应用程序当前工作集的需求，并从多个竞争策略中动态选择。混合策略面临的两个主要挑战是 (1) 准确识别最有利的策略，以及 (2 以较低的硬件成本管理多个策略。我们现在讨论解决这些挑战的两种解决方案。</li>
</ul>
<h3 id="adaptive-replacement-cache-arc">ADAPTIVE REPLACEMENT CACHE (ARC)</h3>
<ul>
<li>自适应替换缓存(ARC) [Megiddo 和 Modha, 2003] 通过动态平衡基于最近次数和基于频率的驱逐，结合了最近次数和频率的优势。特别是，ARC 保持跟踪两个额外的标签目录，L1 和 L2，它们一起记住的元素是基线缓存所能容纳的两倍。L1 目录维护最近使用过的、只访问一次的页面，L2 目录维护最近访问过的、至少访问过两次的页面。ARC 的目标是动态地为 L1 和 L2 选择适当的缓存量(见图3.10)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820231629.png" alt="20210820231629" loading="lazy"></li>
<li>更具体地说，ARC 将基线缓存目录分为两个列表 T1 和 T2，分别用于最近和经常引用的条目。T1 和 T2 使用幽灵列表(分别为 B1 和 B2)进行扩展，幽灵列表分别跟踪 T1 和 T2 中最近被逐出的缓存项。幽灵链表只包含标签元数据，而不是实际数据，当相应的数据从缓存中被移除时，条目就会被添加到幽灵链表中。T1 和 B1 共同构成基于最近的 L1 目录，T2 和 B2 共同构成基于频率的L2 目录。</li>
<li>ARC 动态调制 T1 和 T2 专用的缓存空间。一般来说，B1 中的命中会增加 T1 的大小(专用于最近访问的元素的缓存的比例)，B2 中的命中会增加 T2 的大小(专用于至少访问过两次的元素的缓存的比例)。来自 T1 和 T2 的驱逐项分别被添加到 B1 和 B2。</li>
</ul>
<h3 id="set-dueling">SET DUELING</h3>
<ul>
<li>ARC 等混合策略的一个问题是维护附加标记目录的巨大开销。Qureshi 等人[2006] 引入了 Set Dueling，这是一种以较低的硬件成本对不同策略的行为进行抽样的精确机制。Set Dueling 建立在这样的观察之上:一些随机选择的集合可以准确地表示整个缓存上不同替换策略的行为。Qureshi 等人[2006] 用数学方法表明，对于 1-4MB(1024–4096 sets) 的缓存，64 个集合就足以捕获整个缓存的行为。现在，我们通过讨论两个具有代表性的策略来更详细地讨论 Set Dueling。</li>
</ul>
<h4 id="dynamic-insertionpolicydip">Dynamic InsertionPolicy(DIP)</h4>
<ul>
<li>Dynamic InsertionPolicy(DIP) 动态插入策略(DIP)通过动态调制传入缓存行的插入位置，结合了最近友好策略和抗抖动策略的优点 [Qureshi等人，2007]。特别地，DIP 在最近友好的 LRU(表3.1) 和抗抖动的双模态插入策略(表3.6)之间交替使用。</li>
<li>为了在两个策略中进行选择，DIP 使用 Set Dueling 动态跟踪每个策略的命中率。图 3.11 显示 DIP 专用于 LRU (图3.11中的集0、5、10和15) 和 BIP(图3.11中的集3、6、9和12)。这些专用集被称为 Set Dueling 监视器(SDM)，而其余集被称为跟随集。策略选择(PSEL)饱和计数器通过识别接收更多缓存命中的 SDM 来决定获胜策略。特别地，当专用于 LRU 的 SDM 接收到命中时，PSEL 增加，当专用于 BIP 的 SDM 接收到命中时，PSEL 减少( k 位 PSEL 计数器初始化为 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mn>2</mn><mrow><mi>k</mi><mo>−</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">2^{k-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>)。获胜的策略由 PSEL 的 MSB 确定。如果 PSEL 的 MSB 为 0，跟随集使用 LRU 策略；否则跟踪集将使用 BIP。因此，Set Dueling 不需要任何单独的存储结构，除了 PSEL 计数器。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210821094915.png" alt="20210821094915" loading="lazy"></li>
</ul>
<h4 id="dynamic-re-reference-interval-policy-drrip">Dynamic Re-Reference Interval Policy (DRRIP)</h4>
<ul>
<li>DRRIP 建立在 DIP 的基础上，增加了抗扫描的能力。特别是，DRRIP 使用set dueling 创建了 SRRIP 和 BRRIP 的混合，SRRIP 是 LRU 的抗扫描版本(表3.7)，BRRIP 是 BIP 的抗扫描版本(表3.8)。</li>
<li>正如我们将在第 4 章中看到的，Set Dueling 背后的洞察力对许多后续细粒度策略产生了很大的影响，这些策略使用 Set 采样的概念来有效地跟踪元数据，以确定细粒度缓存行为。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Frequency]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-Two-Frequency/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-Two-Frequency/">
        </link>
        <updated>2021-08-20T05:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Frequency</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Frequency</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="frequency-based-policies">FREQUENCY-BASED POLICIES</h2>
<ul>
<li>基于频率的策略不依赖于最近的访问频率，而是使用访问频率来识别受害者，因此访问频率较高的行优先缓存在访问频率较低的行之上。这种方法不太容易受到扫描的干扰，并且在较长一段时间内考虑重用行为，而不仅仅是最后一次使用。</li>
<li>最简单的基于频率的策略是最少频繁使用(LFU)策略[Coffman and Denning, 1973]，它将频率计数器与每条缓存行关联起来。当该行插入缓存时，频率计数器初始化为 0，每次访问该行时，频率计数器都增加。在缓存丢失时，频率最低的候选对象将被逐出。表 3.9 总结了这些操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820170200.png" alt="20210820170200" loading="lazy"></li>
<li>不幸的是，基于频率的策略不能很好地适应应用程序阶段中的更改，因为即使不再访问前一阶段中具有高频率计数的行，它们仍然会缓存到新的阶段中。为了解决这个问题，有几个解决方案[Lee等人，2001年，O Neil等人，1993年，Robinson和Devarakonda, 1990年，Shasha和Johnson, 1994] 用最近的信息增加频率信息，让旧的行优雅地老化。这里我们讨论两种这样的解决方案。</li>
</ul>
<h4 id="frequency-based-replacement-fbr">Frequency-Based Replacement (FBR)</h4>
<ul>
<li>FBR [Robinson和Devarakonda, 1990] 指出，<strong>基于频率的方法容易受到短时局域性突发的高计数器值的误导</strong>。因此，FBR 通过选择性地增加频率计数器从频率计数中剔除局部性。特别是，FBR 不增加 LRU 堆栈的顶部部分的频率计数器，这被称为新部分。因此，时间局部性的短突发不会影响频率计数器。图 3.8 说明了这个策略。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820170345.png" alt="20210820170345" loading="lazy"></li>
<li>不幸的是，这种策略有一个缺点，即一旦新部分的行老化，即使是经常使用的行也会很快被淘汰，因为它们没有足够的时间来建立频率计数。因此，FBR 进一步限制替换在一个旧的部分中使用最少的行，其中包括最近没有被访问的行(LRU堆栈的底部)。堆栈的其余部分称为中间部分，它为经常使用的行提供了足够的时间来建立它们的频率值。</li>
<li>表 3.10 总结了 FBR 的操作策略。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820222332.png" alt="20210820222332" loading="lazy"></li>
</ul>
<h4 id="least-recentlyfrequentlyused-lrfu">Least Recently/FrequentlyUsed (LRFU)</h4>
<ul>
<li>LRFU 策略 [Lee等人，2001] 建立在这样的观察之上：LRU 和 LFU 策略代表了结合了最近和频率信息的一系列政策的极端点(见图3.9)。LRFU 使用一种名为联合近期和频率(Combined Recency and Frequency, CRF)的新指标，通过允许近期和频率之间的灵活权衡来探索这个频谱。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820222453.png" alt="20210820222453" loading="lazy"></li>
<li>与基于频率的策略一样，<strong>LRFU 将每个过去对块的引用计算进去</strong>，但与基于频率的策略不同的是，<strong>LRFU 通过一个权重函数来衡量每个引用的相对贡献</strong>。特别地，LRFU 为每个区块计算 CRF 值，<strong>CRF 值是权重函数 F(x) 对每个过去的参考的总和，其中 x 是过去的引用距离当前时间的距离</strong>。因此，对于纯粹基于频率的策略仿真，权重函数可以对所有过去的引用给予同等的优先级，而对于基于最近的策略仿真，权重函数可以对最后的引用给予较高的优先级。</li>
<li>LRFU采用式3.1中的权重函数，其中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 是经验选择的参数。权重函数为老的缓存行提供指数级的低优先级，这使得 LRFU 保留了 FBR 的好处，同时支持优雅的老化。$$F(x) = (\frac{1}{p})^{{\lambda}x}$$</li>
<li>表 3.11 总结了块 b 在不同决策点上的 LFRU 策略操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820225822.png" alt="20210820225822" loading="lazy"></li>
<li>LRFU 的性能很大程度上依赖于此，因此随后开发的 ALRFU 策略动态调整了<span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathdefault">λ</span></span></span></span> 的值 [Lee et al.， 1999]。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Coarse-Grained Recency]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Coarse-Grained-One-Recency/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Coarse-Grained-One-Recency/">
        </link>
        <updated>2021-08-20T04:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Recency</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Coarse-Grained Recency</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="粗粒度缓存">粗粒度缓存</h2>
<ul>
<li>缓存替换文献跨越了50年，首先是在操作系统页面替换的上下文中，然后是在硬件缓存的上下文中。在此期间，大多数研究都集中在更智能的粗粒度策略的开发上，这可能可以用这些策略的简单性来解释:每个缓存行都与少量的替换状态相关联，对所有新插入的行进行统一初始化，然后在重用这些行时使用简单的操作进行操作。</li>
<li>在本章中，我们将讨论粗粒度策略的关键进展。我们将粗粒度策略划分为三个类，根据它们在插入到缓存后区分行的方式。
<ul>
<li>第一类包括绝大多数粗粒度策略，使用最近信息来排序行(基于最近的策略)。</li>
<li>第二个类使用频率来排序行(基于频率的策略)。</li>
<li>最后，第三类(混合策略)动态地选择不同的粗粒度替换策略。</li>
</ul>
</li>
<li>粗粒度策略设计中的一个运行主题是识别三种常见的缓存访问模式，即最近友好访问、抖动访问[Denning, 1968]和扫描[Jaleel et al.， 2010b]。当应用程序的工作集小到足以容纳缓存时，就会出现最近友好访问，这样大多数内存引用的重用距离小于缓存的大小。相比之下，当应用程序的工作集超过缓存的大小时，就会发生抖动访问，这样大多数内存引用的重用距离都大于缓存的大小。最后，<strong>扫描是一个永不重复的流访问序列</strong>。正如我们将在本章中看到的，几乎所有粗粒度替换策略都专门针对这些访问模式或这三种访问模式的混合进行了优化。</li>
</ul>
<h3 id="recency-based-policies">RECENCY-BASED POLICIES</h3>
<ul>
<li>基于最近的策略根据最近的信息对行进行优先排序。LRU策略[Mattson et al.， 1970]是这些策略中最简单和最广泛使用的。在缓存回收中，LRU策略只是从一组给定的候选缓存行中删除最老的行。为了找到最老的行，LRU策略在概念上维护一个最近栈，其中栈的顶部代表 MRU 行，堆栈的底部代表 LRU 行。通过为每一行关联一个计数器并更新它，可以精确地或近似地维护这个最近堆栈，如表3.1所示<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100316.png" alt="20210820100316" loading="lazy"></li>
<li>当引用存在时间局域性时，即最近使用的数据可能在不久的将来被重用时，LRU的性能很好。但是对于两种类型的访问模式，它的性能很差。
<ul>
<li>首先，当应用程序的工作集大小超过缓存大小时，它可能是悲观的，导致一种称为抖动的现象[Denning, 1968] (见图3.1)。</li>
<li>其次，LRU在有扫描的情况下表现很差，因为它缓存了最近使用的扫描，代价是更有可能被重用的旧行。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100424.png" alt="20210820100424" loading="lazy"></li>
</ul>
</li>
</ul>
<h4 id="variants-of-lru">VARIANTS OF LRU</h4>
<ul>
<li>我们现在描述一些显著的 LRU 变体，用于检测和容纳抖动或流访问。</li>
</ul>
<h5 id="most-recently-used-mru">Most Recently Used (MRU)</h5>
<ul>
<li>MRU 策略通过取消新行以保留旧行来解决问题。因此，当工作集大于缓存时，它能够保留工作集的一部分。例如, 图3.2显示了抖动, 访问模式如图3.1所示,系统提高了 LRU 年代缓存命中率的一小部分工作集在这个例子中没有驱逐行a .表3.2显示, MRU 和 LRU的系统最新政策是相同的,除了 MRU 就清除系统的缓存行最新位置而不是LRU的位置。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100725.png" alt="20210820100725" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820100846.png" alt="20210820100846" loading="lazy"></li>
<li><strong>然而 MRU 对于抖动访问是理想的，但它在最近友好访问出现时表现很差，而且它对应用程序工作集的变化适应性很差，因为它不太可能缓存来自新工作集的任何行</strong>。</li>
</ul>
<h5 id="early-evictionlrueelru">Early EvictionLRU(EELRU)</h5>
<ul>
<li>EELRU政策也避免了抖动[Smaragdakis等人，1999]。其<strong>主要思想是检测工作集大小超过缓存大小的情况，在这种情况下，有几行会提前被逐出。因此，早期驱逐会丢弃一些随机选择的行，这样剩余的行就可以通过 LRU 策略进行有效管理</strong>。更具体地说，当工作集与缓存匹配时，EELRU 会收回 LRU 行，但当它观察到在一个比主内存大的粗略循环模式中接触了太多行时，它会驱逐 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 最近使用的行</li>
<li>图 3.3 显示了EELRU在最近轴的三个区域之间的区别。最近轴的左端点表示MRU线，右端点表示LRU线。LRU内存区域由最近使用的行组成，位置e和M分别表示早期驱逐区域和晚期驱逐区域的开始。在 miss 时，EELRU 策略要么淘汰最近最少使用的位置(晚区域)的行，要么淘汰 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>e</mi><mrow><mi>t</mi><mi>h</mi></mrow></msup></mrow><annotation encoding="application/x-tex">e^{th}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.849108em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">t</span><span class="mord mathdefault mtight">h</span></span></span></span></span></span></span></span></span></span></span></span> 位置(早区域)的行。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820104234.png" alt="20210820104234" loading="lazy"></li>
<li>为了决定是使用早期驱逐还是 LRU 驱逐，<strong>EELRU 跟踪每个地区收到的命中次数。如果该分布是单调递减的，则 EELRU 假设没有振荡，并将 LRU 行逐出</strong>。但是，如果分布显示晚期区域比早期区域的命中次数更多，那么 EELRU 将从早期区域删除，这使得晚期区域的行在缓存中保留的时间更长。表3.3总结了 EELRU 的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820104648.png" alt="20210820104648" loading="lazy"></li>
</ul>
<h5 id="segmented-lru-seg-lru">Segmented LRU (Seg-LRU)</h5>
<ul>
<li>分段 LRU 通过<strong>优先保留至少访问过两次的行来处理扫描访问</strong>[Karedla et al.， 1994]。Seg-LRU将LRU堆栈划分为两个逻辑段(见图3.4)，即<strong>适用段段和受保护段</strong>。新进来的缓存行被添加到试用段，当他们收到再次命中，被提升到保护段。因此，受保护段中的行至少被访问了两次，而扫描访问永远不会提升到受保护段。在驱逐中，从试用部分中选择最近使用最少的一行。</li>
<li>表 3.4 总结了 seg-lru 的操作。在预备段的 MRU 位置插入新的行，在缓存命中时，行被提升到保护段的 MRU 位置。因为受保护的部分是有限的，提升到受保护的部分可能会迫使受保护部分的 LRU 线迁移到试用部分的 MRU 末端，使这一行在从试用部分被驱逐之前有另一个机会获得 hit。因此，Seg-LRU 可以适应工作集的变化，因为老的缓存行最终被降级到试用段。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105040.png" alt="20210820105040" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105221.png" alt="20210820105221" loading="lazy"></li>
<li>Seg-LRU 策略的一个变体赢得了第一次缓存替换锦标赛[Gao和Wilkerson, 2010]。</li>
</ul>
<h4 id="beyond-lru-insertion-and-promotion-policies">BEYOND LRU: INSERTION AND PROMOTION POLICIES</h4>
<ul>
<li>Qureshi 等人[2007]观察到，通过修改插入策略，同时保持驱逐策略不变(将缓存行逐出LRU位置)，可以实现基于最近的策略的变体。例如，可以使用 <strong>LRU Insertion policy (LIP)</strong> [Qureshi et al.， 2007] 来模拟 MRU (表3.2)，该策略在 LRU 位置插入新行，而不是 MRU 位置(见表3.5)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820105811.png" alt="20210820105811" loading="lazy"></li>
<li>这种见解促进了使用新的插入和 promotion 策略的替换策略的设计。通过以不同的方式解释最近的信息，这些策略可以处理比 LRU 更广泛的访问模式类。在本节中，我们将讨论这方面的一些值得注意的解决方案。由于应用程序通常由许多不同的访问模式组成，这些策略本身都不够，通常被用作混合解决方案的一部分，我们将在3.3节中讨论。</li>
</ul>
<h5 id="bimodal-insertion-policy-bip">Bimodal Insertion Policy (BIP)</h5>
<ul>
<li>由于像 LIP(和MRU) 这样的抗抖动策略不能适应在阶段变化期间工作集的变化，<strong>BIP</strong> [Qureshi等人，2007] 修改LIP，<strong>使缓存行偶尔(以低概率)插入到 MRU 位置</strong>。BIP 保持 LIP 的抗抖动，因为它在大多数时间插入 LRU 位置，但它也可以通过偶尔保留较新的行来适应阶段变化。表3.6显示，BIP 插入到 MRU 位置的概率为 e，设为 1/32 或 1/64。e 取值为 1 表示在 MRU 位置插入(模拟 LRU 插入策略)，e 取值为 0 表示在 LRU 位置插入(模拟LIP插入策略)。因此，从实现的角度来看，<strong>BIP 的参数统一了 LRU 和 MRU 插入光谱中不同位置的所有插入策略</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820110509.png" alt="20210820110509" loading="lazy"></li>
</ul>
<h5 id="static-re-reference-interval-prediction-srrip">Static Re-Reference Interval Prediction (SRRIP)</h5>
<ul>
<li>Jaleel等人[2010b]认为<strong>缓存替换可以被认为是一个重新引用区间预测 (RRIP) 问题</strong>，传统的 LRU 链可以被认为是一个 RRIP 链; LRU 链上一行的位置代表了自上次使用以来的时间，而 RRIP 链上一行的位置代表了它被预测重新引用的顺序[Jaleel et al.， 2010b]。特别是，预测在 RRIP 链首的行将被最快的重新引用(最短的重新引用间隔)，而在 RRIP 链尾的行将被最远的重新引用(最大的重新引用间隔)。当缓存丢失时，位于 RRIP 链尾部的行将被替换。</li>
<li>在这个视图中，我们可以看到，LRU策略预计，新的缓存行将有 near immediate re-reference 间隔（插入在 RRIP 链的头），而 thrash-resistant LIP 策略预测，新行将有一个遥远的 re-reference 间隔（插入在 RRIP 的尾部）。图 3.5 用一个 2bit 重引用预测值(RRPV)表示的RRIP 链说明了这一点：00 对应最近的重参考区间预测，11 对应遥远的重参考区间预测。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820111229.png" alt="20210820111229" loading="lazy"></li>
<li>Jaleel等人[2010b]表明，与预测位于 RRIP 链末端的重新引用间隔不同，<strong>预测中间的重新引用间隔有很大的好处，它允许替换策略适应不同访问模式的混合</strong>。特别是，扫描对未重用数据的引用会破坏最近友好策略(如 LRU)，因为它们会丢弃应用程序的工作集，而不会产生任何缓存命中。<strong>为了适应最近友好访问和扫描的混合，Jaleel等人[2010b] 提出了SRRIP</strong>，它<strong>给进入的行一个中间的重新引用间隔</strong>，然后如果它们被重用，将它们提升到链首。因此，SRRIP 为最近友好的策略增加了抗扫描的能力，因为它防止具有远重引用间隔的行(扫描)驱逐具有近重引用间隔的行。</li>
<li>在大多数情况下，SRRIP 为每个缓存块关联一个 m 位值来存储它的 Re-Reference Prediction value (RRPV)，但是 Jaleel 等人[2010b] 发现一个 2 位的RRPV值就足以提供抗扫描能力。表 3.7 总结了2位 RPRV 值对SRRIP的操作。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820115551.png" alt="20210820115551" loading="lazy"></li>
<li><strong>和 LRU 一样，当所有块的重引用间隔大于可用缓存时，SRRIP会对缓存进行震荡。</strong> 为了添加 thrash-resistance scan-resistant 策略)，Jaleel et al. [2010b] 提出 <strong>BimodalRRIP (BRRIP)</strong>, BIP[Qureshi et al., 2007] 的一个变种，大多数缓存块都插入了一个远端重引用间隔预测(即 RRPV 为 3)，它很少插入一个中间重引用间隔预测(即 RRPV 为 2 )的缓存块。. BRRIP 业务的 RRPV 汇总于表 3.8。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820120340.png" alt="20210820120340" loading="lazy"></li>
<li>由于应用程序可以在最近友好型和抖动型工作集之间交替使用，因此 SRRIP 和 BRRIP 本身都是不够的。在3.3节中，我们将讨论可以解决所有三种常见访问模式(最近友好、抖动和扫描)的 SRRIP 和 BRRIP 的混合版本，为许多应用程序带来性能改进。</li>
</ul>
<h5 id="protecting-distance-based-policy-pdp">Protecting Distance-Based Policy (PDP)</h5>
<ul>
<li><strong>PDP</strong> 策略[Duong et al.， 2012]是 RRIP 的一种推广，因为它<strong>动态估计保护距离(PD)，所有的缓存行在 PD 访问时都受到保护，然后才能被驱逐。PD 是一个单独的值，用于插入到缓存中的所有行，但它会根据应用程序的动态行为不断更新</strong>。</li>
<li>为了估计 PD, PDP 计算重用距离分布(RDD)，这是在程序执行的最近间隔内观察到的重用距离分布。使用 RDD, <strong>PD 被定义为覆盖缓存中大多数行的重用距离</strong>，这样大多数行在 PD 或更小的距离被重用。例如，图 3.6 显示了 436.cactusADM 的 RDD。这里 PD 设置为 64。使用小型专用处理器很少重新计算 PD。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820152114.png" alt="20210820152114" loading="lazy"></li>
<li>更具体地说，在插入或提升时，每条缓存行都被赋一个值来表示其剩余的 PD (RPD)，即它仍然受到保护的访问的次数；这个值被初始化为 PD。每次访问一个集合后，该集合中每一行的 RPD 值都是通过递减的 RPD 值 (在0处饱和) 来老化的。只有当该行的 RPD 大于 0 时，该行才会受到保护。未受保护的行被选为受害者进行逐出。</li>
</ul>
<h5 id="pseudolru-置换gippr-的起源插入和-promotion">PseudoLRU 置换(GIPPR) 的起源插入和 Promotion</h5>
<ul>
<li>受 RRIP 的启发，Jiménez[2013] 观察到在插入和 Promote 的选择上有很多自由度，因此他们使用 <strong>插入/Promote 向量(IPV)</strong> 的概念概括了插入和 Promote 策略的修改。当块被插入到缓存中，或者从最近栈的不同位置提升时，IPV 则指示块在最近栈中的新位置。特别是，k 路 set-associative 缓存，IPV 是一个 k+1 个项组成的向量的，其中每个项的整数从 0 到 k-1，<strong>其中第 I 个位置的值表示位置 I 的块在被重新引用时应该提升到的新位置</strong>。IPV 中的第 k 个项表示应该插入一个新的传入块的位置。如果最近堆栈中的新位置小于旧位置，则块向下移动以腾出空间；否则，块会向上移动以腾出空间。</li>
<li>图3.7显示了两个样本IPVs，第一个代表LRU，第二个代表更复杂的插入和提升策略。</li>
<li>虽然 IPVs 的广义概念非常强大，但可能的 IPVs 数量呈指数增长，(k-way 缓存的可能的 IPVs 有 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msup><mi>k</mi><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow></msup></mrow><annotation encoding="application/x-tex">k^{k+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8491079999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8491079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.03148em;">k</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span></span></span></span></span></span></span></span>)，因此 Jiménez[2013] 使用离线遗传搜索来为 SPEC 2006 基准开发良好的 IPVs。遗传搜索得到的IPV如图3.7底部所示。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820153014.png" alt="20210820153014" loading="lazy"></li>
<li>正如不存在适用于所有工作负载的单一插入策略或 RRIP 策略一样，每个工作负载的最佳 IPV 也是不同的。因此，Jiménez[2013] 提出了一种混合解决方案，使用 set dueling (见章节3.3) 来考虑多个 IPVs。</li>
</ul>
<h4 id="extended-lifetime-recency-based-policies">EXTENDED LIFETIME RECENCY-BASED POLICIES</h4>
<ul>
<li>延长生存期策略是基于最近的策略的一个特殊类，它通过将一些缓存行存储在辅助缓冲区或受害缓存中人为地延长它们的生存期。这里的<strong>关键动机是将驱逐决定推迟到更晚的时候，以便做出更明智的决定</strong>。此策略<strong>允许粗粒度策略稍微增大缓存命中的重用窗口，使其大于缓存的大小</strong>。</li>
</ul>
<h5 id="shepherd-cache">Shepherd Cache</h5>
<ul>
<li>Shepherd Cache (SC) 模仿了 Belady 的最优策略 [Rajan and Govindarajan, 2007]。由于 Belady 的策略需要未来访问的知识，Rajan 和 Govindarajan 在一个叫做 Shepherd Cache 的辅助缓存的帮助下模拟了这个未来的前瞻性。特别地，缓存在逻辑上分为两个组件，<strong>模拟最佳替换的主缓存(MC)<strong>和</strong>使用简单的FIFO替换策略的 SC</strong>。SC 通过提供一个前视窗口来支持 MC 的最佳替换。新行最初在 SC 中进行缓冲，直到新行离开 SC 时，才从 MC 决定替换受害者。当新行在 SC 中时，将收集关于替换候选人在 MC 中重用顺序的信息。</li>
<li>例如，由于 Belady 的策略驱逐了在未来被重用最多的行，所以早期被重用的行就不太可能被驱逐。当从 SC 中移除新行(由于 SC 中的其他插入)时，将通过从前瞻性窗口中尚未重用的 MC 中选择一个候选者或最后重用的候选者来选择一个替换候选者; 如果 MC 中的所有行在 SC 行重用之前被重用，那么 SC 行将替换自己。尽管 SC 和 MC 在逻辑上是分离的，Rajan 和 Govindarajan[2007] 通过组织缓存避免了数据从一个组件到另一个组件的任何移动，这样两个逻辑组件可以被组织为一个单一的物理结构。</li>
<li>因此，SC 通过在 SC的帮助下延长 MC 缓存中的行的生命周期，模拟了一种更好的替换方案, SC 的权衡是 MC 中的替换以高 lookahead 接近真正的最优，而更高的 lookahead 是以降低 MC 容量为代价的。不幸的是，Jain 和 Lin[2016] 的后续工作表明，为了接近 belady 的最优策略的行为，该策略需要提前 8x 缓存的大小。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Taxonomy]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Taxonomy/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Taxonomy/">
        </link>
        <updated>2021-08-20T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Taxonomy</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Taxonomy</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="缓存替换策略的分类">缓存替换策略的分类</h2>
<ul>
<li>为了组织这本书和几十年来研究过的许多思想，我们提出了缓存替换问题的解决方案的分类。我们的分类法建立在这样一个观察之上:缓存替换策略解决了一个预测问题，其<strong>目标是预测是否应该允许任何给定的行留在缓存中</strong>。该决定在缓存行生命周期的多个点重新计算，从该行插入缓存时开始，到该行从缓存中删除时结束。</li>
<li>因此，在我们的分类法中，我们首先根据插入决策的粒度将缓存替换策略分为两大类。在第一类策略中，我们称之为<strong>粗粒度策略</strong>，当所有的行插入缓存时，它们被<strong>相同地对待</strong>，并且<strong>只根据它们在缓存中的行为来区分行</strong>。例如，由于一个缓存行驻留在缓存中，它的优先级可能会在每次重用时增加。相比之下，<strong>细粒度策略在插入到缓存时区分行</strong>(除了观察它们驻留在缓存中的行为)。为了在插入时进行区分，<strong>细粒度策略通常依赖于关于缓存访问行为的历史信息</strong>。例如，如果细粒度策略了解到某个特定指令加载的行在过去往往会被逐出而不会被重用，那么它可以以低优先级插入该行。</li>
<li>为了更好地理解我们的分类，了解替换策略通常是通过将少量替换状态与每个缓存行关联来实现的，这一点很有用。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819182902.png" alt="20210819182902" loading="lazy"></li>
</ul>
<h3 id="缓存生命周期中的相关概念">缓存生命周期中的相关概念</h3>
<ul>
<li><strong>Insertion Policy</strong>：当新行插入缓存时，替换策略如何<strong>初始化</strong>它的替换状态?</li>
<li><strong>Promotion Policy</strong>：当缓存行在缓存中命中的时候，替换策略如何<strong>更新</strong>该行的替换状态?</li>
<li><strong>Aging Policy</strong>：当插入或提升<strong>竞争行</strong>时，替换策略如何更新该行的替换状态?</li>
<li><strong>Eviction Policy</strong>：给定固定数量的选择，哪一行执行替换策略驱逐？</li>
<li>从缓存线的生命周期来看，我们可以看到<strong>粗粒度策略在插入时对所有缓存行的处理是相同的</strong>，所以它们主要依赖于聪明的老化和 promotion 策略来操作替换优先级。相反，<strong>细粒度策略使用更智能的插入策略</strong>。当然，细粒度策略还需要适当的老化和 promotion 来更新替换状态，因为在插入时并不能预测所有行为</li>
</ul>
<h3 id="粗粒度缓存">粗粒度缓存</h3>
<ul>
<li><strong>粗粒度策略在第一次插入缓存时对所有行的处理都是相同的</strong>，它们通过<strong>观察驻留在缓存中的行的重用行为来区分对缓存友好的行和对缓存不友好的行</strong>。根据用于区分缓存驻留行的指标，我们将粗粒度策略进一步划分为三个类。
<ul>
<li>第一个类包括绝大多数粗粒度策略，描述了基于最近访问时间对高速缓存行进行排序的策略。</li>
<li>第二个类包括基于访问频率对高速缓存行进行排序的策略。</li>
<li>最后，第三类中的策略随着时间的推移监视缓存行为，以便在给定的时间段内动态地选择最佳粗粒度策略。</li>
</ul>
</li>
</ul>
<h4 id="recency-based-policies">Recency-Based Policies</h4>
<ul>
<li>基于最近的策略基于每一行在其生命周期内的最新引用来排序行。为了达到这个顺序，基于最近的策略维护了一个概念性的 recency stack，该栈提供了引用行的相对顺序。不同的政策以不同的方式利用最新信息。例如，常用的LRU(最近最少使用)策略(及其变体)优先驱逐最近最少使用的行，而MRU(最近最常使用)等策略优先驱逐最近使用的行；其他政策利用中间解决方案。</li>
<li>我们进一步划分基于最近行为的生命周期定义的基于最近行为的策略。
<ul>
<li>第一个类，包括绝大多数基于 recency-based 的策略，描述了当一行被从缓存中移除时结束该行生命周期的策略。我们认为这种策略有 <strong>固定的生命周期</strong>。</li>
<li>第二个类包括一些策略，这些策略通过引入一个临时结构来<strong>延长缓存行的生命周期</strong>，使其超越回收，该结构为具有较长重用距离的行提供第二次接收缓存命中的机会。我们说，这样的政策有一个<strong>延长的寿命</strong>。</li>
</ul>
</li>
</ul>
<h4 id="frequency-basedpolicies">Frequency-BasedPolicies</h4>
<ul>
<li>基于频率的策略维护频率计数器，以<strong>根据它们被引用的频率对行进行排序</strong>。不同的替换策略使用不同的策略来更新和解释这些频率计数器。例如，<strong>一些策略单调地增加计数器，而另一些策略则随着时间的推移使计数器老化</strong>(降低其值)。另一个例子是，一些策略驱逐频率最低的行，而另一些策略驱逐频率满足预定义标准的行</li>
</ul>
<h4 id="hybrid-policies">Hybrid Policies</h4>
<ul>
<li><strong>由于不同的粗粒度策略适用于不同的缓存访问模式，因此混合策略可以在几个预先确定的粗粒度策略中动态选择，以适应程序执行中的阶段变化</strong>。特别是，混合策略在评估期间观察一些候选粗粒度策略的命中率，并使用此反馈来为未来访问选择最佳的粗粒度策略。自适应策略是有利的，因为它们有助于克服单个粗粒度策略的问题。在第三章中,我们将看到,最先进的混合政策调节粗粒度之间使用不同的插入优先的政策,我们注意到,尽管插入优先级的变化随着时间的推移,这些策略仍是粗粒度的,因为在一个时期,所有的处理是完全相同的。</li>
</ul>
<h3 id="细粒度缓存">细粒度缓存</h3>
<ul>
<li><strong>细粒度策略在将行插入缓存时区分行</strong>。它们通过使用来自高速缓存行以前生命周期的信息来进行这些区分。例如，如果一行在过去没有收到命中，那么它可以以低优先级插入。由于记住所有缓存行的过去行为是不可行的，细粒度策略通常记住一组缓存行的行为。例如，许多策略组合成一组最后被相同的加载指令访问的缓存行。</li>
<li>当然，所有细粒度策略都需要考虑的一个问题是在插入时用于区分这些组的指标。基于使用的度量，我们将细粒度策略分为两大类:
<ul>
<li>(1) 基于分类的策略与每一组可能的预测之一相关联，即缓存友好型和缓存厌恶型;</li>
<li>(2) 基于重用距离的策略尝试预测每组缓存线的详细重用距离信息。</li>
</ul>
</li>
<li>这两类定义了一个频谱的极端，频谱一端的策略只有两个可能的预测值，而频谱另一端的政策有许多可能的预测值;许多策略将会在四个或八个可能值中预测其中之一。最后，Beckmann 和 Sanchez 提出了新的度量标准 [Beckmann和Sanchez, 2017]，我们将在第三类中讨论</li>
</ul>
<h4 id="classification-based-policies">Classification-Based Policies</h4>
<ul>
<li>基于分类的策略将传入的缓存行分为两类: 友好缓存行或反对缓存线。其<strong>主要思想是优先清除不支持缓存的行，以便为支持缓存的行留下更多的空间</strong>，因此，支持缓存的行会以比反对缓存的行更高的优先级插入。<strong>在缓存友好的行之间维护二级排序，通常基于最近次数</strong>。基于分类的策略被广泛认为是最先进的，因为 (1) 它们可以利用过去行为的长期历史来为未来做出更明智的决定，(2) 它们可以容纳所有类型的缓存访问模式。</li>
</ul>
<h4 id="reuse-distance-based-policies">Reuse Distance-Based Policies</h4>
<ul>
<li>基于重用距离的策略预测传入行的详细重用距离信息。<strong>超过预期重用距离而没有接收到命中的行将从缓存中删除</strong>。这些策略可以被视为基于最近的策略或基于频率的策略的预测变体，因为<strong>基于最近的和基于频率的策略都通过监视缓存线在缓存驻留时的重用行为隐式地估计重用距离</strong>(分别通过最近和频率)。利用历史信息对重用距离的显式预测有其独特的优点和缺点</li>
</ul>
<h3 id="设计考量">设计考量</h3>
<ul>
<li>替换策略的主要目标是提高缓存命中率，许多设计因素有助于实现更高的命中率：
<ul>
<li>粒度: 在插入时，行的粒度是多少?是否所有的高速缓存行都被相同的处理，或者它们是否根据历史信息分配不同的优先级</li>
<li>历史: 替换策略在做决策时利用了多少历史信息?</li>
<li>访问模式: 替换策略对特定访问模式的专门化程度如何?它对访问模式的更改或不同访问模式的混合是否健壮?</li>
</ul>
</li>
<li>图 2.2 总结了我们完整的分类，并显示了替换策略的不同类别是如何处理这些设计因素的。一般来说，当我们移到图的右侧(通常与更新的策略相对应)时，趋势是<strong>趋向于使用较长的历史记录并能够适应各种访问模式的更细粒度的解决方案</strong>。在细粒度预测缓存行为的重要性可以通过观察最近的cache Replacement Championship(2017)的前四个解决方案都是细粒度的来衡量。细粒度预测为最新的细粒度策略提供了两个优势。
<ul>
<li>首先，它们允许细粒度策略只将缓存空间专用于最有可能从缓存中受益的行;相比之下，粗粒度策略倾向于重复将缓存资源分配给没有产生任何命中的行。</li>
<li>其次，它们允许细粒度策略动态地确定不同组的行的访问模式;相比之下，粗粒度策略假设整个缓存遵循统一的缓存访问模式。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210820095251.png" alt="20210820095251" loading="lazy"></li>
</ul>
</li>
<li>在细粒度的政策中,趋势是使用更长的历史作为state-ofthe-art细粒度策略(Jain and Lin, 2016)从历史是8样本信息缓存的大小(见第4章)。悠久的历史的应用让策略检测缓存友好长期重用行,否则就会被一长串厌恶缓存的中间访问所混淆。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Cache Replacement Policies - Introduction]]></title>
        <id>https://blog.shunzi.tech/post/CRP-Introduction/</id>
        <link href="https://blog.shunzi.tech/post/CRP-Introduction/">
        </link>
        <updated>2021-08-20T02:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Introduction</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>Cache Replacement Policies - Introduction</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="abstract">Abstract</h2>
<ul>
<li>这本书总结了CPU数据缓存的缓存替换策略。重点是算法问题，因此作者首先定义了一个分类法，将以前的策略分为两大类，他们称之为<strong>粗粒度策略</strong>和<strong>细粒度策略</strong>。然后每一类又分为三个子类，描述了解决缓存替换问题的不同方法，并总结了每一类的重要工作。然后探讨了更丰富的因素，包括优化缓存miss率之外的指标的解决方案、针对多核设置定制的解决方案、考虑与预取器的交互的解决方案以及考虑新的内存技术的解决方案。本书最后讨论了未来工作的趋势和挑战。这本书假定读者将对计算机架构和缓存有一个基本的了解，将对整个领域的学者和从业者有用。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>几十年来，移动数据的延迟大大超过了执行一条指令的延迟，所以缓存，既减少了内存延迟又减少了内存流量，是所有现代微处理器的重要组成部分。因为缓存的大小和延迟之间有一个一般的权衡，大多数微处理器维持一个缓存的层次结构，较小的低延迟缓存由较大的高延迟缓存提供下层支持，后者最终由 DRAM 提供。对于每一个缓存，有效性可以通过它的命中率来衡量，我们定义为 s/r，其中 s 是缓存服务的内存请求的数量，r 是向缓存发出的内存请求的总数。</li>
<li>有几种方法可以提高缓存命中率。<strong>一种方法是增加缓存的大小</strong>，通常以增加延迟为代价。<strong>第二种方法是增加缓存的关联性</strong>，这就增加了缓存行可以映射到的可能缓存位置的数量。在一种极端情况下，直接映射缓存(关联性为1)将每条缓存行映射到缓存中的单个位置。在另一个极端，完全关联缓存允许缓存线放置在缓存的任何位置。不幸的是，随着关联性的增加，功耗和硬件复杂度都会增加。<strong>第三种方法，也是本书的主题，是选择一个好的缓存替换策略</strong>，它回答了这样一个问题:<strong>当一个新行要插入到缓存中，哪一行应该被移除以为新行腾出空间</strong>?</li>
<li>写一本关于缓存替换的书似乎有些奇怪，因为Lazslo Belady在50多年前就提出了一个可证明的最优策略。但Belady的政策是不可实现的，因为它依赖于未来的知识，它抛弃了在未来会被重复使用最多的缓存行。因此，多年来，研究人员已经探索了几种不同的方法来解决缓存替换问题，通常依赖于考虑访问频率、访问近期和最近的预测技术的启发式方法。</li>
<li>此外，还有一个问题是缓存替换策略与死块预测器之间的关系，死块预测器试图预测缓存中不再需要的行。我们现在知道，缓存行的生命周期有多个决策点，从插入该行开始，随着时间的推移，直到最终退出该行，我们知道在这些不同的决策点执行操作有不同的技术。基于这个观点，我们认为死块预测器是缓存替换策略的一种特殊情况，我们发现缓存替换策略的空间非常丰富。</li>
<li>最后，缓存在软件系统中也很普遍。事实上，第一个替代策略是为操作系统的分页系统开发的，虽然软件缓存和硬件缓存之间存在技术上的差异，但我们希望本书中的一些想法也能对软件缓存的开发人员有用。</li>
<li><strong>Scope</strong>：本书主要介绍CPU数据缓存的硬件cache替换策略。虽然我们讨论的大多数研究都是在一级缓存的背景下进行的，智能缓存替换的好处是最明显的，但一般的想法通常适用于缓存层次结构的其他级别。</li>
<li><strong>Roadmap</strong>：在第2章中，我们首先定义一个二维分类法。主要维度描述替换决策的粒度。第二个维度描述了用于做出替换决策的度量。然后第3章和第4章使用我们的分类法来描述现有的替换策略。第5章介绍了使缓存替换复杂化的其他考虑因素，包括数据预取、共享缓存、可变丢失成本、压缩和新技术。在第六章中，我们总结了2017年举行的Cache replace锦标赛的结果，总结了最近的Cache replace趋势，然后后退一步，总结未来研究的更大趋势和挑战。</li>
</ul>
<h3 id="相关介绍">相关介绍</h3>
<h4 id="参考资料">参考资料</h4>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/102293437">知乎 - CPU Cache 的基本原理</a></li>
</ul>
<h4 id="概念理解延申">概念理解延申</h4>
<h5 id="提高缓存命中率的方法二增加缓存的关联性">提高缓存命中率的方法二：增加缓存的关联性</h5>
<p><strong>什么是缓存的关联性</strong></p>
<ul>
<li>
<p>在Cache中，地址映射是指把主存地址空间映射到Cache地址空间，在将主存块复制到Cache中的时候遵循一定的映射规则，标志位为1时候，表示其Cache映射的主存块数据有效。（注意与地址变换的区别，地址变换是指CPU在访存的时候，将主存地址按照映射规则换算成Cache地址的过程 ）。 地址映射有三种方式：直接映射，全相联映射，组相联映射</p>
</li>
<li>
<p><strong>直接映射</strong></p>
<ul>
<li>优点：硬件设计上会更加简单，因此成本上也会较低</li>
<li>缺点：会出现 cache 颠簸（cache thrashing）<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150201.png" alt="20210819150201" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819122855.png" alt="20210819122855" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>两路组相连</strong>：两路组相连缓存较直接映射缓存最大的差异就是：第一个地址对应的数据可以对应 2 个 cache line，而直接映射缓存一个地址只对应一个cache line。 <strong>组相联的本质就是组内是全相联，组间是直接相连</strong></p>
<ul>
<li>两路组相连缓存的硬件成本相对于直接映射缓存更高。因为其每次比较tag的时候需要比较多个cache line对应的tag（某些硬件可能还会做并行比较，增加比较速度，这就增加了硬件设计复杂度）</li>
<li>两路组相联缓存可以有助于降低 cache 颠簸可能性<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150037.png" alt="20210819150037" loading="lazy"><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819150219.png" alt="20210819150219" loading="lazy"></li>
</ul>
</li>
<li>
<p><strong>全相联</strong>：所有的cache line都在一个组内，因此地址中不需要set index部分。因为，只有一个组让你选择，间接来说就是你没得选。我们根据地址中的tag部分和所有的cache line对应的tag进行比较（硬件上可能并行比较也可能串行比较）。</p>
<ul>
<li>在全相连缓存中，任意地址的数据可以缓存在任意的cache line中。所以，这可以最大程度的降低cache颠簸的频率。但是硬件成本上也是更高。</li>
<li>由于Cache比较电路的设计和实现比较困难，这种方式只适合于小容量Cache采用。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210819174545.png" alt="20210819174545" loading="lazy"></li>
</ul>
</li>
</ul>
<p><strong>缓存关联性如何影响缓存效率？</strong></p>
<ul>
<li>直接相连也就是对应的缓存关联性较差，对应的会造成缓存的频繁写入淘汰，因为映射关系固定，而增加了相联性之后，因为映射关系不再固定，缓存缺失的概率降低，命中率得到提升</li>
</ul>
<h4 id="béládys-algorithm">Bélády's algorithm</h4>
<ul>
<li>https://en.wikipedia.org/wiki/Cache_replacement_policies#B%C3%A9l%C3%A1dy's_algorithm</li>
<li>最有效的缓存算法是总是丢弃那些在未来很长一段时间内不需要的信息。这个最优结果被称为 Bélády 的最优算法/简单地说最优替换策略或千里眼算法。由于通常不可能预测未来需要多少信息，这在实践中通常是不可实现的。只有经过实验才能计算出实际的最小值，并且可以比较实际选择的缓存算法的有效性。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[UniKV: Toward High-Performance and Scalable KV Storage in Mixed Workloads via Unified Indexing]]></title>
        <id>https://blog.shunzi.tech/post/UniKV/</id>
        <link href="https://blog.shunzi.tech/post/UniKV/">
        </link>
        <updated>2021-06-24T13:21:02.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>ICDE20: UniKV: Toward High-Performance and Scalable KV Storage in Mixed Workloads via Unified Indexing</li>
<li>https://ieeexplore.ieee.org/document/9101876</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>ICDE20: UniKV: Toward High-Performance and Scalable KV Storage in Mixed Workloads via Unified Indexing</li>
<li>https://ieeexplore.ieee.org/document/9101876</li>
</ul>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>现有的 KV 存储有严重的读写放大问题，现有设计通常都是在做一些 R/W/SCAN 的权衡，很难同时实现比较高的性能。</li>
<li>本文设计的 UniKV，结合了 HASH 索引和 LSM Tree，利用数据局部性来区分 KV 对的索引管理，它还开发了多种技术来解决统一索引技术引起的问题，从而同时提高读、写和扫描的性能。实验表明，UniKV 在读写混合工作负载下的总吞吐量显著优于几个最先进的 KV 存储(例如，LevelDB、RocksDB、HyperLevelDB 和 PebblesDB)</li>
</ul>
<h2 id="intro">Intro</h2>
<ul>
<li>我们的见解是，哈希索引是一种经过充分研究的索引技术，它支持特定KV对的快速查找。然而，将散列索引和LSM-tree组合起来是具有挑战性的，因为它们每个都要做出不同的设计权衡。HASH 支持高性能读写，但是不支持 range，同时 HASH 索引保存在内存中，额外的内存开销也就导致了伸缩性的问题。而 LSM 是同时支持高效的写和 scan 的，并同时有良好的可扩展性，但是有比较严重的 compaction 开销和多层访问的开销。因此我们提出了问题：<strong>我们是否可以整合 HASH 和 LSM 来同时保证 read/write/scan/Scalability 的高效？</strong></li>
<li>现有 KV 负载常有数据局部性，也就给上述问题的解决提供了机会。UniKV 采用分层体系结构实现差异化数据索引，给最近写入的数据（可能会频繁访问的数据，即热数据）构建轻量级内存哈希索引，而维护大量的不频繁访问的数据（也就是冷数据）在一个完全有序的基于 LSM 的设计中。</li>
<li>为了高效利用 HASH 索引和 LSM 树而不用导致大量的内存和 I/O 开销，UniKV 设计了三种技术：
<ul>
<li>轻量级的两级 HASH 索引来保证内存开销尽可能小，同时加速热数据的访问</li>
<li>为了减少热KV对迁移到冷KV对引起的I/O开销，UniKV 设计了部分KV分离策略来优化迁移过程</li>
<li>为了在 大 KV 存储中实现高扩展性和高效的读写性能，UniKV 不像传统的基于 lsm 树的设计那样，允许冷 KV 对之间有多个级别;相反，它采用了一种横向扩展(scale-out)方法，通过动态范围分区方案将数据动态地分割为多个独立的分区。</li>
</ul>
</li>
</ul>
<h2 id="background-and-motivation">BACKGROUND AND MOTIVATION</h2>
<ul>
<li>背景方面主要介绍了造成 LSM 读写放大的原因：
<ul>
<li>写放大：compaction 读后排序覆盖写</li>
<li>读放大：L0 无序全量检索，一层一层地进行检索，BloomFilter 误报</li>
</ul>
</li>
<li>Motivation：
<ul>
<li><strong>内存哈希索引</strong>：内存 HASH 索引查询性能高，直接使用 HASH 函数计算就能找到对应的 pointer，然后根据 pointer 就能取出对应的数据。写性能也很高，虽然可能产生 HASH 冲突，但也有一些 HASH 冲突的解决办法。</li>
<li><strong>设计 tradeoffs</strong>：HASH 索引和 LSM 其实是在 R/W/扩展性 三个方面做了不同的权衡，
<ul>
<li>HASH 索引的读写较好，但是有一些限制
<ul>
<li>扩展性较差，对于大数量的 KV 存储，读写性能可能显著下降因为哈希冲突和有限的内存，性能可能比 LSM 还差。为了证明该结论，使用了基于 HASH 索引的 SkimpyStash 来和 LevelDB 对比<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210623164841.png" alt="20210623164841" loading="lazy"></li>
<li>除此以外，HASH 索引不支持高效的范围查询</li>
</ul>
</li>
<li>LSM 支持大量的数据存储而不使用额外的内存索引，在大数据量场景下比 HASH 表现更好，然后有着很严峻的读写放大问题，虽然有大量的方案在优化这一点，但是本质都是在 R/W 之间做 trade-off。</li>
</ul>
</li>
<li><strong>负载特征</strong>: 真实 KV 负载不仅仅是读写混合的，还有很强的数据访问倾斜性，即一小部分数据可能需要接受大量的请求。该结论也进行了实验证明，下图展示了 SSTable 的访问频率。X 轴代表了从低到高 Level 顺序编号的 SSTable，Y 轴代表每个 SSTable 的访问次数。平均来讲，更低 Level 的 SSTables 也就是更小 ID 的 SSTables 通常刚从内存中刷回，相比于更高层的 Table 有更高的访问频率。最后一层包含大约 70% 的 SSTables，但是他们只接受 9% 的请求。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210623165833.png" alt="20210623165833" loading="lazy"></li>
<li><strong>Main Idea</strong>：基于 HASH 和 LSM 各自设计的 trade-off，以及负载局部性的事实，我们的思想就是结合 HASH 索引和 LSM 来解决各自的限制。具体来讲，就是使用 HASH 索引来加速单个 Key 的查询，通常是很小比例的经常访问的数据，也就是热数据。与此同时，对于大量的不常访问的冷数据，我们仍然遵循原始的 LSM 设计来提供较高的 scan 性能。同时为了支持大数据量的存储的可扩展性，我们提出了动态范围分区的方法来以 scale out 的形式扩展 LSM。</li>
</ul>
</li>
</ul>
<h2 id="design">Design</h2>
<h3 id="架构概览">架构概览</h3>
<ul>
<li>两层结构：
<ul>
<li>第一层为 UnsortedStore，保存了从内存刷回的数据，无序。</li>
<li>第二层为 Sorted Store，保存了从 Unsorted Store 合并后的有序的数据</li>
</ul>
</li>
<li>利用数据的局部性，小比例的热数据保存在 UnsortedStore 中，直接使用一个内存 HASH 索引来支持快速的读写，同时保证剩余数量的冷数据在 SortedStore 中，以有序的形式来提供高效的 scan 和更好的扩展性。</li>
<li>使用了如下技术：
<ul>
<li>Differentiated indexing：两层使用不同的索引</li>
<li>Differentiated indexing：部分 KV 分离来优化两层之间的 Merge</li>
<li>Dynamic range partitioning：动态范围分区来以 scale out 的形式扩展存储</li>
<li>Scan optimizations and consistency：优化了 scan 和 crash recovery 的实现<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210623170709.png" alt="20210623170709" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="differentiated-indexing">Differentiated Indexing</h3>
<h4 id="data-management">Data Management</h4>
<ul>
<li>第一层直接追加写，无需有序，内存 HASH 表索引，从而快速查询。第二层在 LSM 中组织，Key 全局有序，KV 分离，Value 存储在单独的 Value Log。LSM 中存储 Key 和 Address。</li>
<li>移除了所有 SSTables 的 BloomFilters 来节省内存空间并减少计算开销。对于 UnsortedStore 中的数据直接使用内存 HASH 表就能查询到对应 Key 的位置信息，对于 SortedStore 中的数据，我们也可以有效地找到可能包含 KV 对的SSTable，通过二叉搜索，比较 SSTable 的键和存储在内存中的边界键，因为所有的键在 SotedStore 中有序。即便是查询不存在的键，也只会访问一个 SSTable，这只会增加一个额外不必要的从 SSTable 的 I/O 读取来确认不存在的 Key，因为我们可以通过使用索引块的元数据来直接决定 SSTable 哪个数据块内(通常是4KB) 需要读出，元数据又通常是缓存在内存中的。相比之下，现有的基于 LSM 树的 KV 存储可能需要对 SSTable 进行 7.6 次检查，由于 Bloom 过滤器的误报以及多层搜索，平均需要 2.3 次 I/O 才能进行键查找。</li>
<li>对于内存中的数据管理，UniKV 使用和传统 LSM 相似的方式，同时使用 WAL 确保数据的可靠性，KV 首先写入到磁盘上的日志中来保证崩溃一致性，然后插入到 Memtable，也就是对应的一个内存跳表中，Mmetable 满了之后，转换为 Immutable Memtable，然后再刷回到磁盘变成 UnsortedStore 中的一个数据文件，追加写入。</li>
</ul>
<h4 id="hash-indexing">Hash indexing</h4>
<ul>
<li>UnsortedStore 中的 Tables 以追加写的方式写入，使用内存哈希表索引，为了减小内存开销，我们构建了一个轻量级的两级 HASH，结合布谷鸟 HASH 和链式 HASH 来解决哈希冲突。如下图所示，每个桶存储使用了布谷鸟哈希的 KV 对索引项，所以可能因为哈希冲突而追加好几个 overflowed 的索引项。当我们构建索引项的时候，根据 n 个哈希函数的哈希结果找到对应的桶，直到找到一个空位置，最多使用 n 个哈希函数，如果不能在 n 个桶中找到对应的空桶，生成一个 overflowed 的索引追加到对应的桶后 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>n</mi></msub><mo>(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo>)</mo><mi mathvariant="normal">%</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">h_n(key) \% N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">n</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">%</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span></li>
<li>定位到桶之后，记录 KeyTag 和 SSTableID 到对应的索引项中，每个索引项包含三个信息 &lt;keyTag, SSTableID, pointer&gt;，keyTag 保存使用了不同的 HASH 函数计算出的哈希值的高位 2 Bytes，被用来在查询过程中快速过滤出索引项。使用 2Bytes 来存储 SSTable ID，如果 UnsortedStore 中一个 SSTable 2MB，那么我们可以索引 128GB 的数据。指针使用了 4Bytes 来执行相同 Bucket 的下一个索引项。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210623173549.png" alt="20210623173549" loading="lazy"></li>
</ul>
<h4 id="key-lookup">Key Lookup</h4>
<ul>
<li>键查询过程如下：
<ul>
<li>使用 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo>)</mo></mrow><annotation encoding="application/x-tex">h_{n+1}(key)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span></span></span></span> 计算 KeyTag，搜索候选的 buckets 从 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mi>n</mi></msub><mo>(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo>)</mo><mi mathvariant="normal">%</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">h_{n}(key) \% N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">%</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>h</mi><mn>1</mn></msub><mo>(</mo><mi>k</mi><mi>e</mi><mi>y</mi><mo>)</mo><mi mathvariant="normal">%</mi><mi>N</mi></mrow><annotation encoding="application/x-tex">h_{1}(key) \% N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord"><span class="mord mathdefault">h</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.03148em;">k</span><span class="mord mathdefault">e</span><span class="mord mathdefault" style="margin-right:0.03588em;">y</span><span class="mclose">)</span><span class="mord">%</span><span class="mord mathdefault" style="margin-right:0.10903em;">N</span></span></span></span>，直到找到了 KV 对，对于每一个候选的 Bucket，因为最新的 overflow 项被追加到了其尾部，因此，我们将 keyTag 与属于溢这个 bucket 的索引项从尾部开始进行比较，一旦找到了匹配的 keyTag，就根据对应的 SSTableID 检索出 SSTable 对应的元数据信息，并读出对应的 KV 对，注意查询的 KV 对可能因为哈希冲突而不存在于这个 SSTable，然后我们继续查询候选的 buckets，最终，如果 KV 对在 UnsortedStore 中没找到，那我们就从 SortedStore 上通过二分查找去找。</li>
</ul>
</li>
</ul>
<h4 id="memory-overhead">Memory overhead</h4>
<ul>
<li>我们分析了哈希索引的内存开销，UnsortedStore 中的每一个 KV 对消耗一个索引项，每个索引项消耗 8B 的内存，因此 UnsortedStore 中的每 1GB 数据，每个 KV 项 1KB 的话，将有大约 100w 索引项，消耗大约 10M 内存（在给定 Bucket 利用率大约为 80% 的情况下），内存的使用量小于 UnsortedStore 的数据大小的 1%。对于特别小的 KV 对，哈希索引可能导致较大的内存开销，一个解决方案就是差异化不同大小的 KV 项的索引的管理，例如对于小 KV 则使用经典的 LSM，对于大 KV 则使用 UniKV。</li>
<li>我们的哈希索引在设计上做了一个 trade-off，一方面，哈希冲突可能当我们分配 Buckets 给 KV 对时存在，因此我们需要在索引项中存储 Key 的信息以便在查询过程中区分。另一方面，存储完整的 Key 浪费了内存，为了平衡内存消耗和读性能，UniKV 使用了两个哈希来保留 HASH 值的 2B 作为 keyTag，从而显著减少了哈希冲突的概率。如果哈希冲突发生了，也可以通过比较存储在磁盘上的键来解决。</li>
</ul>
<h3 id="partial-kv-separation">Partial KV separation</h3>
<ul>
<li>因为 UnsortedStore 的 KV 对使用了内存哈希表来索引，引入了额外的内存开销。对应的 SSTables 的键的范围因为只能追加写的策略变得可能范围重叠，所以执行 scan 的时候不可避免地需要检索每个 SSTable。为了限制内存开销并保证 scan 性能，UniKV 限制了 UnsortedStore 的大小，当大小达到设定阈值 UnsortedLimit 时，UniKV 触发一个合并操作，从 UnsortedStore 合并到 SortedStore，该参数根据可用内存来配置。</li>
<li>合并操作可能产生大的 I/O 开销，因为 SortedStore 现有的 KV 对需要先读取然后合并排序后写回，因此如何减少合并开销是关键且有挑战性的。UniKV 提出了一个部分 KV 分离的策略，在 UnsortedStore 中不分离地保存，但是在 SortedStore 中分离保存。基本原理就是从内存中最近刷回的保存在 UnsortedStore 的数据因为局部性的原因更有可能是热数据，因此为了高效访问 KV 存储在了一起，然而 SortedStore 中的 KV 数据更可能是冷数据，因此数据量非常大，造成很大的合并开销。因此我们采用了 KV 分离来减少合并的开销。</li>
<li>下图描绘了该设计思路，当从 UnsortedStore 合并到 SortedStore 的时候，UniKV 批量合并 Keys，同时保存 value 到一个新的创建的追加写的日志文件中，还使用指针记录值位置，这些指针与相应的键保存在一起，每一个指针项包含四个属性：&lt;partition,logNumber,offset,length&gt; 分别代表 partition number，log file id, value location, length。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210624145132.png" alt="20210624145132" loading="lazy"></li>
</ul>
<h4 id="garbage-collection-gc-in-sortedstore">Garbage collection (GC) in SortedStore</h4>
<ul>
<li>SSTable 中的无效 Keys 在 Compaction 的时候会被删除，所以这里的 GC 是指删除 Value Log 中的无效 Values。GC 以日志文件为单元进行操作，当一个分区的大小超过预定义阈值的时候触发 GC，具体而言就是 GC 操作首先从日志文件的分区中识别并读出所有的有效值，然后把所有的有效值写回到一个新的日志文件，并生成新的指针来记录最新的 value 位置信息，最后删除无效的指针和老旧的文件。</li>
<li>有两个关键问题：
<ul>
<li>哪些分区被选择来 GC？</li>
<li>如何从日志文件中快速识别出有效的值并读取？</li>
</ul>
</li>
<li>不像之前的 KV 分离方案 WiscKey 一样严格按顺序执行 GC，UniKV 可以灵活地选择任意分区执行 GC 因为 KV 根据键范围被映射到了不同的分区而且分区之间的操作互相独立。我们采用了一个贪心方法来选择有最大数量的 KV 的分区来 GC，同时检查选择的分区的日志文件中的 value 的有效性，UniKV 只需要检查保存了有效的 Key 和最近的位置信息的 SortedStore 中的 Keys 和指针，因此对于每个 GC 操作只需要扫描 SortedStore 中的所有 Keys 和指针来获取所有的有效值，花费的时间主要取决于 SSTables 的总大小，注意 GC 和 compaction 操作是顺序执行的，所以 GC 操作也发生在数据加载的过程中，并且在度量写性能时还计算了 GC 成本。</li>
</ul>
<h3 id="dynamic-range-partitioning">Dynamic Range Partitioning</h3>
<ul>
<li>因为 SortedStore 随着数据量逐渐变大，如果只是像 LSM 在大规模的存储中简单地添加更多的层次，将会导致频繁的合并操作，相应地在写数据时将数据从低向高移动，读数据时触发多层访问。每个 GC 需要通过查询 LSM 从日志文件中读取所有的有效的数据并写回，所以 GC 的开销也会随着层级的增加变得很大。因此 UniKV 提出了动态范围分区的方法来以 scale out 形式扩展，该方法把不同范围的 KV 映射到独立管理的不同分区，每个分区有自己的 UnsortedStore 和 SortedStore。</li>
<li>工作原理如下图所示，初始地，UniKV 在一个分区写数据，一旦大小达到阈值，partitionSizeLimit，UniKV 就把分区根据范围平均分成两个分区，对于范围分区，关键特性是两个新分区的键不应该有重叠。为了实现这一点，UnsortedStore 和 SortedStore 中的 KV 对都需要被拆分。</li>
<li>为了分割 UnsortedStore 和 SortedStore 中的键，UniKV 首先锁住它们并暂停写请求。注意，锁粒度是一个分区，也就是说，UniKV 锁住整个分区，并在分割期间暂停对这个分区的所有写入。然后对所有键进行排序，以避免分区之间的重叠。它首先将所有内存中的 KV 对刷新到 UnsortedStore 中，并读取 UnsortedStore 和 SortedStore 中的所有 sstable 来执行合并排序，就像在基于 lsm 树的 KV 存储中一样。然后将排序后的键分成等量的两部分，并记录两部分之间的边界键 K。注意，这个边界键 K 作为分裂点。也就是说，密钥小于 K 的 KV 对形成一个分区 P1，其他的则形成另一个分区 P2。通过分隔点，UniKV 将 UnsortedStore 中的有效值分成两部分，并通过将它们附加到每个分区新创建的日志文件中，将它们写到相应的分区中。最后，UniKV 将值位置存储在指针中，这些指针与对应的键保存在一起，并将所有键和指针写回对应分区中的 SortedStore。注意 UniKV 释放锁，并在键分割后恢复处理写请求。</li>
<li>其次，为了拆分 SortedStore 中的值(这些值分别存储在多个日志文件中)，UniKV 采用了一个惰性拆分方案，该方案在使用后台线程进行 GC 期间拆分日志文件中的值。它的工作原理如下。P1 中的 GC 线程首先扫描 P1 的 SortedStore 中的所有 sstable。然后，它从 P1 和 P2 共享的旧日志文件中读出有效值，并将它们写回属于 P1 的新创建的日志文件。最后，它生成新的指针，这些指针存储有相应的键，以记录值的最新位置。P2 中的 GC 线程执行与 P1 中相同的过程。惰性拆分设计的主要好处是通过将其与 GC 操作集成在一起来显著减少拆分开销，从而避免较大的 I/O 开销。注意，对于范围分区，P2 中的最小键必须大于 P1 中的所有键。一旦一个分区达到其大小限制，这个范围分区过程就会重复。我们强调，每个分割操作都可以看作是一个压缩操作加上一个 GC 操作，但是它们必须按顺序执行。因此，在分区中分割键会引入额外的 I/O。分割后，每个分区都有自己的 UnsortedStore、SortedStore 和日志文件。</li>
<li>对于大型KV存储，初始分区可能会分裂多次，从而产生多个分区。为了在进行读写操作时有效地定位某个分区，我们在内存中记录每个分区的分区号和边界键，作为分区索引。我们还持久化地将分区索引存储在磁盘上的 manifest 中。另外，不同分区的键没有重叠，所以每个键只能存在于一个分区中。因此，可以通过首先定位一个分区来执行键查找，这可以通过检查边界键来有效地完成，然后只查询一个分区内的 KV 对。简而言之，动态范围分区方案通过将KV 对拆分为多个独立分区，以横向扩展 scale-out 的方式扩展存储。因此，该方案可以保证较高的读写性能，以及高效的扫描，即使是大型 KV 存储，使其具有良好的可扩展性</li>
</ul>
<h3 id="io-cost-analysis">I/O Cost Analysis</h3>
<ul>
<li>此处省略。原文分别分析了传统 LSM 和 UniKV 各自的读写开销。UniKV 相比之下开销更小。</li>
</ul>
<h3 id="scan-optimization">Scan Optimization</h3>
<ul>
<li>因为 KV 在 UniKV 的 SortedStore 中是分开存储的，所以 scan 操作可能会对 value 造成随机读。而 UnsortedStore 中的 SSTables 本身又是直接从内存刷回的所以是无序的，其键范围是重叠的，scan 操作可能就会读取每个 SSTable，因此也会造成额外的读开销。</li>
<li>为了优化 scan，UniKV 首先通过快速的查询分区键的边界来快速找到要 scan 的键对应的分区，可能会显著减少需要 scan 的数据量，UniKV 还使用了一些其他策略。</li>
<li>对于 UnsortedStore，UniKV 提出了一个 Size-based Merge 策略，即把 Unsorted Store 中的所有 SSTables 合并成一个大的 SSTable，使用一个后台线程来保证数据全局有序，该线程只有当 UnsortedStore 中的 SSTables 数量达到预定义的阈值 scanMergeLimit 时的才会执行。即便该操作会造成额外的 I/O 开销，如果 UnsortedStore 的大小是有限的话，该操作的 I/O 开销也会很小。该操作可以显著提升 scan 的性能因为高效的顺序读，特别是大数据量的 scan 操作。</li>
<li>对于 SortedStore，UniKV 利用 SSD 的 I/O 并行性来从日志文件中多线程并发获取 values。UniKV 还利用预读机制来预取 values 到 page cache，工作流程如下：首先从 SortedStore 中的 SSTables 获取给定 scan range 内的 keys 和 pointers，然后以第一个键的值开始向日志文件发出预读请求(via posix_fadvise)，最后根据指针读取出 values 并返回 KV 对。另一方面我们认为 UniKV 不会对 UnsortedStore 和 SortedStore 的扫描结果执行内存合并和排序，这是因为一个 scan 操作是通过如下三步执行的：
<ul>
<li>seek() 它从每个需要检查在 UnsortedStore 和 SortedStore的sstable 中定位开始键，并返回初始键的 KV 对直到找到</li>
<li>next() 找到比需要检查的每个 sstable 中最后一个返回的键大的下一个最小键，并返回最小键的值</li>
<li>重复步骤二直到返回的 KV 对的数量等于 scan 的长度</li>
</ul>
</li>
<li>通过上述优化，我们的实验表明 UniKV 的扫描性能与 LevelDB 相似，后者总是保持键在每个级别上的完全排序。</li>
</ul>
<h3 id="crash-consistency">Crash Consistency</h3>
<ul>
<li>三个地方需要保证崩溃一致性：Memtables 中缓冲的数据，内存哈希索引，SortedStore 的 GC。对于 KV 对直接使用 WAL 来保证，关于分区的元数据信息直接使用 manifest 来保证，一样也是 WAL 机制。</li>
<li>内存哈希表则使用 checkpointing 技术，当每一半的 UnsortedLimit sstable 从内存刷新到 UnsortedStore 时，它将哈希索引保存在磁盘文件中，因此重建哈希索引可以通过读最近保存的磁盘上的索引的副本来 replay。</li>
<li>GC 操作的一致性保证不太一样，因为需要重写现有的有效 KV 对，为了保证现有的有效的 KV 对的一致性，执行以下步骤：
<ul>
<li>识别出有效 KV 对</li>
<li>读取有效的 values 并写回到新的 log file</li>
<li>写所有的新的指向新的日志文件的指针和对应的 Key 到 SortedStore 的 SSTable 中</li>
<li>标记新的 SSTable 为有效，以 GC_done 的标志标记老的日志文件来允许其被删除</li>
<li>如果 crash，可以根据上面四个步骤 redo GC</li>
</ul>
</li>
<li>UniKV 实现了上述机制，实验表明恢复开销很小。</li>
</ul>
<h3 id="implementation-issues">Implementation Issues</h3>
<ul>
<li>基于 LevelDB v1.20 实现，修改了大约 8K 代码，大部分都是为了引入哈希索引、动态范围分区和部分 KV 分离、以及 GC 操作。因为 UnsortedStore 和 SortedStore 都是基于 SSTable 的，所以 UniKV 可以直接利用 SSTable 成熟稳定的代码。</li>
<li>对于 scan，UniKV 利用了多线程技术来并发获取 values。然而线程数量受限于一个进程的内存空间大小，使用太多线程可能触发频繁的上下文切换，UniKV 维护了一个 32 线程的线程池并从池中并行地分配线程来获取 values。Scan 过程中，UniKV 插入固定数量的 value 地址到 worker queue 中，然后可以唤醒睡眠线程来并行第读取 value。</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161445.png" alt="20210625161445" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161520.png" alt="20210625161520" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161538.png" alt="20210625161538" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161549.png" alt="20210625161549" loading="lazy"></figure>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161558.png" alt="20210625161558" loading="lazy"></figure>
<figure data-type="image" tabindex="6"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161647.png" alt="20210625161647" loading="lazy"></figure>
<figure data-type="image" tabindex="7"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210625161709.png" alt="20210625161709" loading="lazy"></figure>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Differentiated Key-Value Storage Management for Balanced I/O Performance]]></title>
        <id>https://blog.shunzi.tech/post/DiffKV/</id>
        <link href="https://blog.shunzi.tech/post/DiffKV/">
        </link>
        <updated>2021-06-18T13:21:02.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>ATC21 Differentiated Key-Value Storage Management for Balanced I/O Performance</li>
<li>https://www.cse.cuhk.edu.hk/~pclee/www/pubs/atc21diffkv.pdf</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>ATC21 Differentiated Key-Value Storage Management for Balanced I/O Performance</li>
<li>https://www.cse.cuhk.edu.hk/~pclee/www/pubs/atc21diffkv.pdf</li>
</ul>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>KV 存储读写放大严峻，现有设计都是在做 trade-off，不能同时实现高性能的读写以及 scan。所以提出了 DiffKV，在 KV 分离的基础上构建并仔细管理 Key Value 的顺序。Key 沿用 LSM-Tree，保持全序，同时以协调的方式管理部分有序的 Value，以保持高扫描性能。进一步提出细粒度 KV 分离，以大小区分 KV 对，实现混合工作负载下的均衡性能。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>为了减小 compaction 开销，现有方案大致有几个方向。
<ul>
<li>放松全局有序的要求来缓解 compaction 开销。<strong>但常伴随着 scan 性能的下降</strong>
<ul>
<li>Dostoevsky</li>
<li>PebblesDB</li>
<li>SlimDB</li>
</ul>
</li>
<li>基于 KV 分离，Key 有序，专门的存储区管理 Value。<strong>一是更适用于大 KV，二是还是有 scan 的性能损失（毕竟 Value 的顺序不保证了就导致随机读），三是还引入了垃圾回收的开销</strong>
<ul>
<li>WiscKey</li>
<li>HashKV</li>
<li>BadgerDB</li>
<li>Atlas</li>
<li>An Efficient Memory-Mapped Key-Value Store for Flash Storage</li>
<li>Titan</li>
<li>UniKV</li>
</ul>
</li>
</ul>
</li>
<li>简而言之，现有的LSM-tree优化仍然受到读写和扫描之间紧密的性能紧张关系的限制，包括：
<ul>
<li>(i) <strong>键和值的有序程度</strong></li>
<li>(ii) <strong>不同大小的KV对的管理</strong>。</li>
</ul>
</li>
</ul>
<h2 id="background-and-motivation">Background and Motivation</h2>
<ul>
<li>LSM-tree KV Store 基础结构此处省略。</li>
</ul>
<h3 id="现有优化方案">现有优化方案</h3>
<ul>
<li>主要还是分为两类:
<ul>
<li>Relaxing fully-sorted ordering</li>
<li>KV separation</li>
</ul>
</li>
</ul>
<h4 id="relaxing-fully-sorted-ordering">Relaxing fully-sorted ordering</h4>
<ul>
<li>以 PebblesDB 为例实现了分段 LSM。将一个 Level 分为了几个不相交的 Guards，每个 Guard 里的 SSTables 可以范围重叠。那么你会问了：<strong>为啥这样就能减小 compaction 开销呢？</strong> 因为 PebblesDB 只是读取了某一层中的 group 的 SSTables，然后进行了排序创建了新的 SSTables 写到下一层，compaction 过程不需要读取下一层的 SSTables 从而减小 compaction 开销和写放大。<strong>但是因为每个 Group 里的 SSTable 是重叠的，所以牺牲了一部分 scan 性能，虽然可以利用多线程来并行读，但是就导致了更多的 CPU 开销，所以提升也是有限的。</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210617231445.png" alt="20210617231445" loading="lazy"></li>
</ul>
<h4 id="kv-separation">KV separation</h4>
<ul>
<li>KV 分离则是将 Key 和 Location 信息存储在 LSM-tree，Value 单独地存储在一个日志里，Titan 中以多个 blob 文件形式来组织 Value。对于中大型 Value，因为 Key 和 Location 有着远小于 Value 的大小，在 LSM-tree 里存储的数据量就比较小，compaction 开销和写放大也相应比较小。除此以外，小的 LSM-tree 也减小了读放大，可以提升读性能。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618103203.png" alt="20210618103203" loading="lazy"></li>
<li>然而，由于 KV 分离将值写入一个附加日志，一个连续范围的键值现在分散在日志的不同位置。scan 操作也就会导致随机读取，因此导致比较差的 scan 性能，特别是小到中型 Value（OLTP 应用超过 90% 的 value 小于 1KB），除此以外，KV 分离还需要 GC 来回收空间，频繁的 GC 操作会导致额外的 I/O 开销。</li>
</ul>
<h3 id="trade-off-analysis">Trade-off Analysis</h3>
<ul>
<li>性能测试之前，先大概讲一下 <a href="https://docs.pingcap.com/zh/tidb/stable/titan-overview">Titan</a>
<ul>
<li>Titan 基于 KV 分离实现，此外，采用了多个小的 Blob Files 来代替大的只追加写的日志对 Value 进行管理，使用多线程来减小 GC 开销。</li>
<li>数据组织形式如下：
<ul>
<li>LSM-tree: Key - index(BlobFileID:offset:ValueSize)</li>
<li>BlobFile: Value (Value 的存储类似于原本 SSTable 的结构)
<ul>
<li>每条 BlobRecord 冗余存储了 Value 对应的 Key 以便反向索引，但也引入了写放大</li>
<li>KeyValue 有序存放，为了提升 scan 性能，甚至进行预取</li>
<li>支持 BlobRecord 粒度的 compression，支持多种算法</li>
</ul>
</li>
</ul>
</li>
<li>GC：
<ul>
<li>监听 LSM-tree 的 compaction 来统计每个 BlobFile 的 discardable 数据大小，触发的 GC 则选择对应 discardable 最大的 File 来作为 candidate</li>
<li>GC 选择了一些 candidates，当 discardable size 达到一定比例之后再 GC。使用 Sample 算法，随机取 BlobFile 中的一段数据 A，计其大小为 a，然后遍历 A 中的 key，累加过期的 key 所在的 blob record 的 size 计为 d，最后计算得出 d 占 a 比值 为 r，如果 r &gt;= discardable_ratio 则对该 BlobFile 进行 GC，否则不对其进行 GC。如果 discardable size 占整个 BlobFile 数据大小的比值已经大于或等于 discardable_ratio 则不需要对其进行 Sample<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618111817.png" alt="20210618111817" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li><strong>Write performance</strong>：加载 100GB 数据库，不同 value 大小（128B - 16KB），两种优化方案都能降低写放大，随着 value 大小的增加，写放大降低的越多，相应的写吞吐也就很大提升。证明以往的两种优化方向都是可以改善写放大并提升写性能的，特别是对于大 Value。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618104027.png" alt="20210618104027" loading="lazy"></li>
<li><strong>Read and scan performance</strong>：放松全局有序的要求导致查询性能出现了降级，而 KV 分离因为显著减小了 LSM-tree 的大小，读吞吐比 RocksDB 高很多。而对于 scan 两种方案都比 RocksDB 差很多，做了延迟分解，发现大部分扫描时间花在了 iteratively reading values，随着 value 的增大，scan 延迟的差距变小，因为访问更大的 Value 对应了更小的随机读开销。对于那些小 KV 为主的负载，两种优化方案都会受到很大限制。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618140321.png" alt="20210618140321" loading="lazy"></li>
<li>总结一下，现有的优化方案其实都是在做 reads/writes 的 trade-off，两种优化方向的本质都是在降低 Value 的有序程度来减小写放大并提升吞吐量，但相应地牺牲了 scan 的性能，特别是对于小到中型的 KV 对。</li>
</ul>
<h2 id="design">Design</h2>
<h3 id="system-overview">System Overview</h3>
<ul>
<li>DiffKV 利用了一个类似于 LSM-tree 的结构 vTree 来组织 Value 保证 Value 的部分有序，也是由多个 Level 组成，每个 level 只能以追加的方式写入，和 LSM-Tree 的不同在于，vTree 只存储那些在每一层中不一定按键完全排序的 Value，即允许部分有序来保证 scan 性能。</li>
<li>为了实现部分有序，vTree 也需要类似 compaction 的操作，称之为 merge，为了减小 merge 的开销，DiffKV 让 LSM-tree 的 compaction 和 vTree 的 merge 协调地执行来减少总体开销。</li>
<li>为了让 DiffKV 和现有设计兼容，内存组件和 WAL 都和原生 LSM 一样，流程是一样的。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618144116.png" alt="20210618144116" loading="lazy"></li>
</ul>
<h3 id="data-orgaization">Data Orgaization</h3>
<ul>
<li>vTree 分层组织，每层由 sorted groups 组成，每个 group 由很多 vTables 组成。（<strong>这里仿佛就跟 PebblesDB 的设计是类似的了</strong>）</li>
</ul>
<h4 id="vtable">vTable</h4>
<ul>
<li>大小固定，8MB 默认。一个 immutable Memtable 的 flush 可以生成很多个 vTables，取决于 value size 和 Memtable Size。</li>
<li>vTable 组成部分：
<ul>
<li>data area: 基于 Key 的顺序存储 values</li>
<li>metedata area: 记录必要的元数据，比如 vTable 的数据大小，该 table 中最小最大 Value（<strong>和 SSTable 不同的是不要求 BllomFilter</strong>，因为直接查索引就能知道）。元数据较小，存储开销也较小。</li>
</ul>
</li>
</ul>
<h4 id="sorted-group">Sorted Group</h4>
<ul>
<li>所有的 vTables 在 Group 中也是有序的。也就是无重叠范围，LSM-tree 的 SSTables 组成的集合也相当于一个 Sorted Group。DiffKV 一次 flush 对应生成一个 Sorted Group，从而保留每个 immutable Memtable 里顺序性，对应 Groups 数量表示了 vTree 的有序程度，Groups 数量增加，有序性相应下降，在一个极端情况下，如果所有的 sstable/vtable组成一个 Group，那么有序程度最大，因为所有的 KV 对都是完全排序的。</li>
<li><strong>（埋个坑：这里直接用 Groups 数量来表示有序程度会不会有些草率，其实有序与否更多是看重叠范围大小，当然 Groups 越多重叠的概率可能越大，但不确定这个有序程度是不是会对后面造成影响，拭目以待）</strong></li>
</ul>
<h4 id="vtree">vTree</h4>
<ul>
<li>vTree --- 1:N --- levels --- 1:N --- groups --- 1:N --- vTables --- 1:N --- values</li>
<li>全局有序的最小单元是 group，level 就可能存在具有重叠键范围的 groups 了，merge 操作不需要对 vTree 中连续两层中的所有值进行排序;与 lsm-tree 中的 compaction 相比，这减轻了I/O开销</li>
</ul>
<h3 id="compaction-triggered-merge">Compaction-Triggered Merge</h3>
<ul>
<li><strong>首先，为什么要 merge？</strong> 其实就是为了保证部分有序，或者说维持有序性，这样才可能加速 scan。</li>
<li>Merge 会读取一定数量的 vTables，通过查询 LSM-tree 中存储的最新位置信息来检查哪些 value 是有效的，每个 merge 还要更新最新的 location 信息到 LSM-tree 中</li>
<li>为了限制 vTree 中的合并开销，vTree 中的合并操作不是独立执行的，而是和 LSM-tree 中的压缩操作以协调的方式触发的。所以称该操作为 compaction-triggered merge</li>
<li><strong>举例说明：</strong> 假设 LSM tree 的 level 和 vTree 的 level 是关联的。
<ul>
<li>当 LSM-tree 发生 Level i -&gt; i+1 的 compaction 的时候，相应地触发 vTree 上相应的 value 从 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">vL_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">vL_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 的 merge。</li>
<li>Merge 有两个主要问题：
<ul>
<li><strong>选哪些 value 进行 merge</strong>？选择那些参加了 compaction 的 keys 对应的 value 进行 merge，称之为 compaction-related value</li>
<li><strong>如何把这些 value 写回到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">vL_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span></strong>？把这些 value 组织起来生成新的 vTables 并把这些 vTables 以追加写的形式写到下一层。如下图所示的棕色就是这些需要 merge 的 values，<span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>V</mi><mn>33</mn></msub></mrow><annotation encoding="application/x-tex">V_{33}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>V</mi><mn>13</mn></msub></mrow><annotation encoding="application/x-tex">V_{13}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span><span class="mord mtight">3</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, <span class="katex"><span class="katex-mathml"><math><semantics><mrow><msub><mi>V</mi><mn>45</mn></msub></mrow><annotation encoding="application/x-tex">V_{45}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span><span class="mord mtight">5</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 在排序后追加写入到下一层。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618151924.png" alt="20210618151924" loading="lazy"></li>
</ul>
</li>
</ul>
</li>
<li>生成的 vTables 中的所有数据都是有序的，也就是说 Merge 产生的 vTables 形成了一个单独的 sorted group。但是，我们要指出的是，合并操作并不需要重新组织 vTree 中 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">vL_i</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> 和 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">vL_{i+1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">i</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 两级的所有vtable。追加写的形式来表面重写，从而减小写放大，老的 vTables 也不会在 merge 过程中删除，因为可能还是包含一些有效的 value，交给 GC 来判断处理。</li>
<li><strong>compaction-triggered merge 带来的好处表现在两个方面</strong>：(<strong>协同设计的核心</strong>)
<ul>
<li><strong>只合并与 compaction 相关的值可以非常有效地识别哪些值仍然有效，因为在 compaction 期间本身就需要从 lsm-tree 中读出相应的键</strong>。相反如果 vTree 的 merge 独立执行，那就需要查询 LSM-tree 并比较 location 信息来判断有效性了，增加了较大的查询开销。</li>
<li>merge 操作过程中新生成的 vTables 伴随着有效 value 的位置信息的变化，LSM-tree 需要被更新来维护对应的最新的 value location 信息，因为<strong>只有 compaction-related values 被合并，更新 LSM 树中的值位置可以通过直接更新参与 compaction 的 KV 对来执行</strong>。因此，更新值位置的开销可以隐藏在 compaction 操作中，因为 compaction 本身需要重写 lsm 树中的 KV 对</li>
</ul>
</li>
</ul>
<h3 id="merge-optimizations">Merge Optimizations</h3>
<ul>
<li>Compaction-triggered merge 引入了有限的合并开销，该开销又主要是检查 value 的有效性并写回 value location 信息造成的。但是如果让每一次 compaction 操作都触发 merge 的话可能造成频繁的 merge，比如 vTree 中的每个级别只与 LSM-tree 中的一个级别相关，那么每个 compaction 操作都必须在 vTree 中触发合并操作，为了减小 merge 开销，提出了两个优化方案。</li>
</ul>
<h4 id="lazy-merge">Lazy merge</h4>
<ul>
<li>该策略用于限制 merge 频率和开销。核心思想是聚合多个 lower vTree levels 为单个 level，关联地聚合 LSM-tree 的多个 levels，如下图所示。聚合 vTree 的 0,...,n-2 为一个单独的 level，相应地聚合 LSM Tree 的 0,...,n-2 levels，因此任何发生在 0,...,n-2 之间的 compaction 都不会触发 merge，也就是说 vTree 的 0,...,n-2 Level Merge 操作被延迟到除非需要合并到 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>v</mi><msub><mi>L</mi><mrow><mi>n</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">vL_{n-1}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.891661em;vertical-align:-0.208331em;"></span><span class="mord mathdefault" style="margin-right:0.03588em;">v</span><span class="mord"><span class="mord mathdefault">L</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">n</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span></span></span></span> 的时候才会执行。</li>
<li>该策略显著减少合并次数和合并数据量，但是牺牲了较低层次的 value 的有序程度，然而，我们认为这种牺牲对扫描性能的影响是有限的。因为 LSM 中的大部分数据都是保存在最后几层，不同层次的数据分布不均匀意味着大多数值实际上是从 vTree 的最后两层进行扫描的，所以最后两层的值的有序程度才会更多地影响 scan 性能。也就是说，low level 对 scan 性能影响很小，频繁的在 low level 的合并操作不会对 scan 性能的提升有什么帮助但却引入了较大的 merge 开销。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618155342.png" alt="20210618155342" loading="lazy"></li>
</ul>
<h4 id="scan-optimized-merge">Scan-optimized merge</h4>
<ul>
<li>该策略用于调整 value 的有序程度，来保证较高的 scan 性能。原本的合并策略中，上层的 value 被重新组织写入到下层，下层的 value 其实是没有参与 merge 的。这种策略减小了写放大，但是导致了太多的 sorted group，即可能重叠的现象更严峻。因此我们的核心思想是找到和其他 vTables 有重叠范围的 vTables，让这些 vTables 参与 merge 而不管其位于哪一层。这样就能保证有序的程度较高。</li>
<li>下图描述了核心思想，在普通的 compaction-triggered merge 之后，首先在下层检查包含 compaction-related values 的 vTables，目标是找出满足如下两个条件的 vTables 集合：
<ul>
<li>集合中至少一个 vTable 有重叠的键范围</li>
<li>vTables 的数量，也就是 set size，大于预先定义的阈值，max_sorted_run</li>
</ul>
</li>
<li>如果存在上述的 SET，scan 性能势必会降低，因为这些 vTable 没有被排序。所以添加一个 scan optimization tag 给这些 tables，所以他们将能总是参与到下一次 Merge 并增加有序性。</li>
<li>为了找到这样的一组 vTables，首先遍历每个包含 compaction-related values 的  vTable 的起始和终止 Key，对于每个检查的 vTable，统计有重叠键范围的 vTables 的数量，可以通过扫描排序后的键字符串来完成。如下图所示，考虑一个检查过的 vTable 【26-38】，扫描排序后的字符串，可以统计出在 Key 38 之前起始 keys 的个数，本例中有五个，然后结束 Key 在 26 之前的只有一个，然后相减，得到和 【26-38】 重叠的 tables 有四个，也就是该集合的 size 为 4，如果超过了阈值，那么就会给这些 tables 添加 tag，并在下一次合并中对他们进行合并。同时这个 tag 会被持久化到 mainfest file 中，持久化开销可忽略不计。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618161027.png" alt="20210618161027" loading="lazy"></li>
<li>Scan-optimized merge 该策略是一个原始策略上的 enhancement。原策略只是简单做追加写，而该策略进一步包含了下一层中确定的 tables 来进行合并，从而增加有序性。注意该策略引入了有限的合并开销，有两个原因：
<ul>
<li>允许每一层有多个 sorted groups（一整层不是全局有序的） <strong>（这里有点奇怪）</strong></li>
<li>不是标记了的 table 中的所有 values 都会参与 merge，而只是 compaction-related values 会参与。</li>
</ul>
</li>
</ul>
<h3 id="gc">GC</h3>
<ul>
<li>为了减小 GC 开销，又提出了基于无效 value 数量的 state-aware lazy approach</li>
</ul>
<h4 id="state-awareness">State awareness</h4>
<ul>
<li>DiffKV 在一个哈希表中记录每个 vTable 的无效 KV 的数量，每次当 vTable 参与一个 Merge 的时候，DiffKV 记录从 vTable 中检索到的值的数量，并在哈希表中更新旧 vTable中无效值的数量。它还为哈希表中任何新的 vTable 插入一个条目。对哈希表的更新是在合并操作期间执行的，因此开销是有限的。另外，哈希表中的每个条目只占用几个字节，所以哈希表的内存开销是有限的。</li>
</ul>
<h4 id="lazy-gc">Lazy GC</h4>
<ul>
<li>如果 vTable 有一定比例的无效 Values 且超过阈值 gc_threshold 的时候被选为 GC candidate，需要注意的是 DiffKV 不能立马回收候选的 vTables，相反只是标记一个 GC tag，延迟 GC 到下一次合并。具体的，如果带有 GC tag 的 vTable 被包含到一次合并中，那么该 table 包含的 values 将总是被重写到下一个 level。</li>
<li>该策略避免了查询 LSM 检查有效性的额外开销，也避免了更新 LSM 新的地址信息的开销，被延迟到和合并一起执行，所以查询和更新的开销可以被合并操作给隐藏。</li>
</ul>
<h3 id="discussion">Discussion</h3>
<ul>
<li><strong>Optimizing compaction at L0</strong>：提出一个简单的优化 selective compaction 来聚合 Level 0 中小的 SSTables，具体而言，我们会触发内部 compaction，简单地在 L0 处合并多个小 sstable 来生成一个新的大的，而不与 L1 处的 sstable 合并，这样的话，L0 中的 SSTables 的大小将和 L1 中的相当，且没有额外的 compaction 开销引入。</li>
<li><strong>Crash consistency</strong>：DiffKV 基于 Titan 实现，一致性保证和 Titan 以及 RocskDB 一样，使用了 WAL。同时 DiffKV 还未记录无效数据的 HASHTable 提供了一致性保证，随着 HashTable 在 compaction 之后被更新，DiffKV 将更新信息追加到 manifest 文件中。</li>
</ul>
<h2 id="fine-grained-kv-separation">Fine-grained KV Separation</h2>
<ul>
<li>对于大KV对，KV分离的好处是显著的，但对于小KV对就不是这样了。然而，不同值大小的混合工作负载也很常见;例如，在广义帕累托分布下，值的大小可能变化很大。在本节中，我们通过 value 大小来区分KV对，通过细粒度的KV分离来进一步增强DiffKV，从而实现混合工作负载下的均衡性能。</li>
</ul>
<h3 id="differentiated-value-management">Differentiated Value Management</h3>
<ul>
<li>使用两个参数分为三组，small medium large 。小 value 直接写 LSM，中 Value 写 vTree，大 value 写 vLog。</li>
<li>大 value 的在写 Memtable 之前就先分离了 KV，这样做的好处有两个方面：
<ul>
<li>直接将大 value 刷回到 vLog，并保留小的 Key 和地址信息在 Memtable，可以节省内存消耗，同时因为大的顺序 I/O 保证了较高的写性能。</li>
<li>因为大 value 被写回到了 disk，就不用再写 WAL 了，减小了 I/O 的次数</li>
</ul>
</li>
<li>需要注意的是，对于中小型KV对，以及大型KV对的键和值位置，仍然需要写入WAL中，以保证一致性。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618170142.png" alt="20210618170142" loading="lazy"></li>
</ul>
<h3 id="hotness-aware-vlogs">Hotness-aware vLogs</h3>
<h4 id="structure-of-vlogs">Structure of vLogs</h4>
<ul>
<li>就是一个简单的环形只允许追加的日志，由一组无序的 vTables 组成，vTable 和前面提到的 vTable 格式相同，唯一的不同是 value 是追加写入到无序的 vTables 的，所以在每个 table 内也是无序的，我们这样做的原因是因为 KV 分离对于大型 KV 对执行写入MemTable 之前,大型 KV 被立即刷新到磁盘,以免写 WAL,所以没有办法排序每个 table 中的 values。事实上也不需要，因为他们本身就能从大 I/O 中获益了而不用批量写入。</li>
</ul>
<h4 id="gc-for-vlogs">GC for vLogs</h4>
<ul>
<li>为了减小 GC 开销，使用了一个热点感知的设计，采用一种简单而有效的无参数冷热分离方案。如下图所示，使用两个 vLogs，分别对应冷热 vLogs，存储了对应的冷热数据，每个vLog 都有自己的写边界，我们分别称它们为 写头 和 GC 头。为了实现冷热分离，用户写入的数据被追加到 hot vLog 的 write head，而来自于 GC 写的数据（比如有效数据需要在 GC 过程中进行写入）被追加写到 cold vLog 的 GC head。其基本原理是，GC 回收的值通常比最近写入的用户数据访问频率更低，因此可以将它们视为冷数据，这种设计的一个好处是实现简单，因为实现冷热识别不需要参数。显然，我们还可以应用其他的热点感知分类方案。</li>
<li>DiffKV 使用了一个贪心的算法来减小 GC 开销，思想是回收有最大数量的无效值的无序的 vTables。具体的，DiffKV 在 compaction 中监控了每个无序 vTables 的无效数据比例，并维护了一个内存中的 GC 队列来追踪所有候选的 vTables，候选条件即为无效值比例大于阈值。需要注意的是 GC 队列只维护每个无序 vTable 的元数据，它根据无效值的比率按降序进行维护。GC 触发的时候，DiffKV 简单地选择队列头的无序 vTables，如图 9 中的 t1 t2，然后将有效的 values 追加到 cold vLog 的 GC 头。出于性能考虑，DiffKV 使用后台进程多线程来实现 GC。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618205333.png" alt="20210618205333" loading="lazy"></figure>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618211219.png" alt="20210618211219" loading="lazy"></figure>
<figure data-type="image" tabindex="3"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618213715.png" alt="20210618213715" loading="lazy"></figure>
<figure data-type="image" tabindex="4"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618213702.png" alt="20210618213702" loading="lazy"></figure>
<figure data-type="image" tabindex="5"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210618213857.png" alt="20210618213857" loading="lazy"></figure>
]]></content>
    </entry>
</feed>