<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://blog.shunzi.tech</id>
    <title>Elvis Zhang</title>
    <updated>2021-06-12T03:32:18.829Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://blog.shunzi.tech"/>
    <link rel="self" href="https://blog.shunzi.tech/atom.xml"/>
    <subtitle>The easy way or the right way.</subtitle>
    <logo>https://blog.shunzi.tech/images/avatar.png</logo>
    <icon>https://blog.shunzi.tech/favicon.ico</icon>
    <rights>All rights reserved 2021, Elvis Zhang</rights>
    <entry>
        <title type="html"><![CDATA[REMIX: Efficient Range Query for LSM-trees]]></title>
        <id>https://blog.shunzi.tech/post/REMIX/</id>
        <link href="https://blog.shunzi.tech/post/REMIX/">
        </link>
        <updated>2021-06-05T04:21:02.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST21 REMIX: Efficient Range Query for LSM-trees</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST21 REMIX: Efficient Range Query for LSM-trees</li>
</ul>
</blockquote>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>LSM 本身是为高速写操作优化的，而范围查询在传统的 LSM-tree 上需要 seek 且归并排序来自多个 Table 的数据，开销是很大的且经常导致较差的读性能。为了提升范围查询的性能，我们提出了一个空间高效的的 KV 索引数据结构 REMIX，记录跨多个表文件的KV数据的全局排序视图。对多个 REMIX 索引数据文件的范围查询可以使用二分搜索快速定位目标键，并在不进行键比较的情况下按排序顺序检索后续键。基于此构建了 RemixDB，一个采用了一个更高效的压缩策略并使用 REMIX 来进行快速的点查询和范围查询的 KV 存储。实验结果表明，在基于写优化 LSM 树的 KV-store 中，REMIX 可以显著提高范围查询性能。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>LSM 代表的是更新开销和读开销的一种权衡，相比于 B+tree 保证了更小的写开销以及更大的读开销。关于读开销的优化就有，驻留在内存中的为每个 Table 维护的 BloomFilters，来减小不必要的 Table 访问。但是 BloomFilter 不能处理范围查询，所以出现了 Range Filters，如 SIGMOD18-SuRF、SIGMOD20-Rosetta 来在范围查询时过滤掉 Tables。<strong>但是当范围查询内的 Key 位于很多个候选的 Tables 中时</strong>，filtering 就很难提升查询性能了，特别是大范围查询，而且当查询请求可以在缓存中处理时，访问 Filters 的计算开销可能导致性能表现一般，这是真实负载中比较普遍的情况。</li>
<li>LSM 本身是有 Compaction 来减少查询时检索的 SSTable 数量的，选择 Table 的策略又分为了 leveled 和 tiering。
<ul>
<li>LevelDB、RocksDB 采用的 Leveled 的策略就是把小的 sorted run 合并一个更大的 sorted run 来保证重叠的 Tables 数量小于阈值，该策略却是保证了较好的读性能但是因为归并排序的方式导致写放大比较严峻</li>
<li>Tiered 则是等待多个相近大小的 sorted runs 达到阈值后合并到一个更大的 runs，从而提供更小的写放大以及更高的写吞吐。Cassandra、ScyllaDB 就采用了这样的方式。但是没有限制重叠的 Tables 的数量从而导致较大的查询开销。</li>
<li>SIGMOD18 Dostoevsky 和 SIGMOD19 The Log-Structured Merge-Bush &amp; the Wacky Continuum 提出的 compaction 策略虽然做了一定程度上的读写的平衡，但是还是没能同时实现最好的读和写。</li>
</ul>
</li>
<li>问题的关键其实在于限制 sorted runs 的数量以及 KV 存储不得不归并排序且重写现有的数据。如今的硬件技术使得随机访问的效率也很高了，因此 KV 存储不再说必须保证物理上的有序，而可以只保证逻辑有序同时避免大量的重写。</li>
<li>为此，我们设计了REMIX，现有的范围查询解决方案很难在物理重写数据和动态执行昂贵的排序合并之间进行改进，与此不同的是，REMIX 使用了一个空间效率高的数据结构来记录跨多个表文件的 KV 数据的全局排序视图。使用 REMIX，基于 LSM 树的 KV-store 可以利用高效写压缩策略，而不会牺牲搜索性能。基于此我们还构建了 RemixDB，和高效的 Tiered 压缩策略以及分区的布局集成，同时实现了较低的写放大和快速的查询。</li>
</ul>
<h2 id="background">Background</h2>
<ul>
<li>了解几个概念就好：
<ul>
<li><strong>minor compaction</strong>：其实就是我们说的 flush 过程，数据从内存持久化到存储设备。这个过程写是比较快的，因为是顺序批量写入且不需要合并存储中的现有数据，但也就意味着查询操作需要检索所有的重叠的 Tables，查询开销较大，所以出现了 major compaction。</li>
<li><strong>major compaction</strong>：其实就是把几个重叠的 runs 归并排序排序成更少的的 runs。也就是我们更为熟知的 compaction 操作。常说的 compaction 策略也是指 major compaction 过程中使用的策略。示例如下图<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210610211420.png" alt="20210610211420" loading="lazy"></li>
<li><strong>Range Query</strong>：范围查询是在 LevelDB/RocksDB 中是通过迭代器来定位多个 Tables 实现的，首先初始化一个迭代器，对一个 key 进行 seek，也就是范围查询的下届，seek 操作则定位对应的迭代器让其指向大于等于该 Key 的最小位置，然后 next 操作则相应地按顺序移动迭代器指向下一个 key，直到遇到了范围查询的边界。由于 sorted run 是按时间顺序生成的，所以目标键可以驻留在任何 runs 中。因此，迭代器必须跟踪所有 sorted runs。如上图所示的查询过程，找到了对应的 Keys 之后构建一个小顶堆进行归并排序，从而得到结果。</li>
</ul>
</li>
</ul>
<h2 id="remix">REMIX</h2>
<ul>
<li>范围查询其实依赖全局有序视图，而全局有序视图其实本身是从 SSTables 的不变性继承过来的，也就是说该全局有序视图本身可以保证很长一段时间有效，直到 SSTables 结构发生了变化，被删除了重写了之类的操作。<strong>现有的方案没有利用到不变性的这个优势，而是在每次范围查询过程中重复构建该全局有序视图</strong>，因为大量的计算开销和 I/O 而导致较差的读性能。所以 REMIX 就是想利用 table files 的不变性来维护一个全局有序视图，从而加速后续的查询操作。</li>
<li>为了让 I/O 更高效，LSM 的 KV 存储常使用一些内存高效的元数据格式，譬如稀疏索引和布隆过滤器，如果我们记录了全局有序视图，势必也需要保证内存的空间高效。不能因为存储更多的元数据而导致读写性能损失。</li>
</ul>
<h3 id="the-remix-data-structure">The REMIX Data Structure</h3>
<ul>
<li>图示很好理解，简单解释一下概念。下图所示例子包含了三个 sorted runs，顺序对应地由箭头表示，共计 15 个 Keys，为了构建 REMIX，首先进行了个划分，划分为一定数量的 segments，每个分段包含的 Keys 数量相等。每个 Segment 对应包含一个起始 Key，也就是 Anchor Key，包含一组游标偏移，对应就是记录每个 Sorted Runs 现在的指针位置，也就是大于等于起始锚点的指针位置，还有一个 Run Selectors，包含了真正的顺序，即下一个 Key 所在的 runs 编号。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210610214450.png" alt="20210610214450" loading="lazy"></li>
<li>范围查询的过程就变成了：
<ul>
<li>首先检索稀疏索引，也就是 Anchor Key，二分查找找到所属的 segment</li>
<li>迭代器对应被 seek 到该 segment 对应的 Anchor Key，然后使用该 segment 的游标偏移来移动迭代器指针，根据 selectors 中的顺序来进行 seek</li>
<li>最后通过在全局有序的视图上找到了对应的目标 Key</li>
</ul>
</li>
<li>例子：图示中找到 Key 17，首先二分找到 Segment2。然后游标从 11 开始移动，根据 Seletors 发现 11 在 R0 上，以及 Offset 的结果，即在 R0 的索引为 1 的地方开始，因为 11 &lt; 17，那么要继续移动游标，直到找到大于等于 17 的 Key，也就是图中的 17，这时候 offset 变成了 2 2 1，根据 Selectors 那么找到了 R1，根据 offset 也就找到了 17。</li>
</ul>
<h3 id="efficient-search-in-a-segment">Efficient Search in a Segment</h3>
<ul>
<li>Segement 的划分是一个可调的配置，Size 太大，Anchor Keys 就少了，二分查找更快，但是 Segment 内平均访问次数增多，因此在目标的 Segment 内也使用二分查找。</li>
</ul>
<h4 id="binary-search">Binary Search</h4>
<ul>
<li>为了在一个段中执行二分搜索，我们必须能够随机访问段中的每个键。段中的键属于 run，由相应的 run 选择器指示。要访问一个键，我们需要将 run 的游标放在正确的位置。这可以通过计算相同的 run 选择器在键之前的段中出现的次数，并将相应的游标向前移动相同的次数来实现。可以使用现代 CPU 上的 SIMD 指令快速计算出现次数。搜索范围可以通过对段的一些随机访问快速缩小，直到识别出目标键。为了结束查找操作，我们使用每个 run 选择器在目标键之前出现的次数初始化所有游标。</li>
<li>图示很清楚，很好理解就不解释了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210610220752.png" alt="20210610220752" loading="lazy"></li>
</ul>
<h4 id="io-optimization">I/O Optimization</h4>
<ul>
<li>执行段内二分查找当然是为了减少比较的次数，但是，搜索路径上的键可能驻留在不同的 runs 中，如果各自的数据块没有被缓存，则必须通过单独的 I/O 请求来检索。上图所示就比较了四次 Key，对应访问了 3 个 runs，但是像 41 43 这两个 Key 其实属于一个 Run，甚至可能属于一个数据块。因此，在进行键比较之后，搜索可以利用相同数据块中的其余键，在必须访问不同的 Run 之前进一步缩小搜索范围。这样，R3 中的每个键都可以在不访问任何其他 Run 的情况下找到。比如为了查找 79，访问 R3 可以将搜索范围缩小到键 43 和键 83 之间，这时候二分查找找到了 71，也就是 R0，然后在 R0 中找到了 79。</li>
</ul>
<h3 id="search-efficiency">Search Efficiency</h3>
<ul>
<li>总结下来，具体的提升体现在下面三个方面
<ul>
<li><strong>REMIXes find the target key using one binary search</strong>：说白了就是执行一次二分查找就能找到 Key（这个一次其实是指完整的一次，包括段内的，因为全局有序视图，就花一次也很好理解）</li>
<li><strong>REMIXes move the iterator without key comparisons</strong>：这个就是说不用再比较很多个 runs 的 Key 之后再移动了，这个还是因为全局有序，可以跳过一些没必要的比较操作。</li>
<li><strong>REMIXes skip runs that are not on the search path</strong>：就是说可以跳过一些 run，但这个其实是看数据重叠的情况的，总是是因为读一个数据块的粒度导致可能读上来的数据块恰好就包含了这个 key，或者减小了查询的范围。</li>
</ul>
</li>
<li>作者表明对点查询也是有优化的。</li>
</ul>
<h3 id="remix-storage-cost">REMIX Storage Cost</h3>
<ul>
<li>具体公式就不列了，看个数据就行。</li>
<li>其实空间开销不小，特别是那个快 10% 的 USR。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210610223144.png" alt="20210610223144" loading="lazy"></li>
</ul>
<h2 id="remixdb">RemixDB</h2>
<ul>
<li>RemixDB 使用了 Tiered Compaction 加一个分区的布局。因为有很多研究表明真实负载大多有很强的空间局部性，然后分区存储的话可以很好地降低压缩开销。所以 RemixDB 将键空间划分为不重叠键范围的分区。REMIX 对每个分区中的表文件进行索引，提供分区的排序视图。通过这种方式，RemixDB 本质上是一个使用分层压缩的单层 LSM 树。RemixDB 不仅继承了 Tiered 压缩的写效率，而且在 REMIX 的帮助下实现了高效的读取。RemixDB 的点查询操作(GET)执行一个 seek 操作，如果它与目标键匹配，则返回迭代器下的键。RemixDB 不使用Bloom过滤器。</li>
<li>下图展示了 RemixDB 的结构，内存组件和 LevelDB 这些是一样的。分区中的压缩创建分区的新版本，其中包含新旧表文件的混合以及一个新的 REMIX 文件。旧版本在压缩后进行垃圾收集。</li>
<li>在多级 LSM-tree 设计中，MemTable 的大小通常只有几十 MB，接近于默认的 SSTable 大小。在分区存储布局中，较大的 Memtable 在触发压缩之前可以积累更多的更新，这有助于减少 WA。MemTables 和 WAL 的空间成本几乎不变，考虑到当今数据中心的大内存和存储容量，这是合理的。在 RemixDB 中，MemTable 的最大大小被设置为 4GB。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210610223337.png" alt="20210610223337" loading="lazy"></li>
</ul>
<h3 id="the-structures-of-remixdb-files">The Structures of RemixDB Files</h3>
<h4 id="table-files">Table Files</h4>
<ul>
<li>图 6 显示了 RemixDB 中的表文件格式。数据块默认为 4KB。一个大的 KV-pair 如果不能容纳在一个 4KB 的块中，就会独占一个 4K 的倍数的巨型块。每个数据块在块的开始包含一个小数组的 KV 对的块偏移量，用于随机访问单独的 KV 对。</li>
<li>元数据块是一个 8bit value 的数组，每个都记录着一个 4KB block 中的 Keys 的数量。一个块可以包含 255 个 KV，在一个大 Block 中，除了第一个 4KB 外，其余的都将其对应的数字设为0，这样一个非零的数字总是对应于一个块的头部。使用偏移数组和元数据块，搜索可以快速到达任何相邻的块，并跳过任意数量的键，而不访问数据块。因为 KV 对是由 REMIX 索引的，所以表文件不包含索引或过滤器。</li>
</ul>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611115230.png" alt="20210611115230" loading="lazy"></figure>
<h4 id="remix-files">REMIX Files</h4>
<ul>
<li>图 7 展示了 REMIX 文件格式，anchor key 是在一个不可变的类似于 B+Tree 的索引中维护的，相当于 LevelDB/RocksDB 的块索引，来辅助二分查找。每个 Anchor Key 和一个 Segment ID 关联，对应关联游标偏移和 run selectors。游标偏移由 16 位的块索引和 8 位的 key 索引组成，即图示中的 blk-id 和 key-id。块索引可以索引 65536 个 4KB 的块，也就是 256MB，每个块可以包含 256 个 KV 对。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611115712.png" alt="20210611115712" loading="lazy"></li>
<li>一个 Key 的多个版本可以存在于一个分区的不同 table 文件中，一个范围查询操作必须跳过老版本并返回对应的最新的数据，所以 REMIX 构建的全局有序视图按照了从最新到最老的顺序排序，每个 run 选择器的最高位被保留来区分新旧版本。forward scan operation 将总是最先遇到最新版本的 Key，然后通过检查每个 run selector 的保留位来跳过老版本，而不用进行 Key 的比较。</li>
<li>如果一个 Key 有多版本，这些版本可能分布在两个 Segments，查询操作可能需要检索两个 Segments 来获得最新的版本。为了简化查询，当构造一个 REMIX 时，通过在第一个段中插入特殊的 run 选择器作为占位符，我们将键的所有版本向前移动到第二个段。还需要确保一个 Segment 中的 Key 的最大数量等于或大于由 REMIX 索引的 runs 的个数，才能保证每个 Segment 足够大以容纳一个 Key 的所有版本的数据。</li>
<li>为了容纳上面描述的特殊值，每个 run selector 占据了一个 byte，run selector 的第 8 位和第 7 位 (0x80 and 0x40) 分别表示老版本和删除的 Key。特殊值 63 (0x3f) 意味 placeholder，通过这种方式，RemixDB 可以在一个分区中管理多达 63 个 sorted tuns。</li>
</ul>
<h3 id="compaction">Compaction</h3>
<ul>
<li>在每个分区中，compaction 进程基于进入分区的新数据的大小和现有的 tables 的布局来估计 compaction cost，基于估算出来的开销，可能执行下列操作：
<ul>
<li><strong>Abort</strong>：取消分区 compaction 并保留 Memtable 和 WAL 中的新数据</li>
<li><strong>Minor Compaction</strong>：写新数据到一个或多个 tables 而不用重写现有的 tables</li>
<li><strong>Major Compaction</strong>：将新数据和一些甚至全部的现有数据合并</li>
<li><strong>Split Compaction</strong>：将新数据和所有的现有数据合并，并拆分分区到几个新分区上</li>
</ul>
</li>
<li><strong>Abort</strong>：Compaction 之后，一个分区如果有文件将重建该分区的 REMIX 索引，当一个小的表文件因为 minor compacion 在分区中创建，重建 REMIX 可能导致比较大的 I/O 开销。例如，表1 中的 USR 负载就有最高的空间开销比例。写 100MB 新数据到一个 1GB 的分区，将创建一个大约 100M 的索引。为了最小化 I/O 开销，RemixDB 可能丢弃一个分区的 compaction，如果估算出的 I/O 开销超过了阈值。该场景下，新的 KV 数据应该保留在 MemTables 和 WAL 中直到下一次 Compaction。
<ul>
<li>但是还有一个极端例子，如果一个 uniform 的负载，当所有分区都有 compaction 被Aborted 的时候，compaction 进程不能高效地当移动数据到分区。为了避免这个问题，我们限制了可以驻留在 Memtables 和 WAL 中的新数据的大小，不能超过最大 MemTable 大小的 15%，compaction 进程可以丢弃掉那些 I/O 开销最大的 compactions 如果达到了限制的话。</li>
</ul>
</li>
<li><strong>Minor Compaction</strong>：minor compaction 就是从 immutable memtable 中把数据写入到分区中，而不用重写分区中原有的数据，但是需要重建 REMIX 索引。根据新写入的数据大小，minor compaction 创建一个或者多个新的 table files，Minor Compaction 只有在 compaction 后的预期 table 文件数量低于阈值 T 的时候才会执行，我们的实现中该阈值设定为了 10。下图展示了 minor compaction 的例子。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611145307.png" alt="20210611145307" loading="lazy"></li>
<li><strong>Major Compaction</strong>：当一个分区中的预期文件数量达到了阈值 T 将执行 major 或者 split compaction。major compaction 将归并排序现有的 table files 合并成更少的 tables。随着 tables 数量的减少，minor compaction 的效率也就可以根据输入表文件的数量与输出表文件的数量的比率来估计。下图展示了 major compaction 的过程，该例子中，新数据合并到了三个小的 tables，只有一个新的 table 在 compaction 之后被创建，比例为 3/1，如果整个分区被归并排序，compactions 需要重写更多的数据并且仍然输出三个 tables，比例 5/3，因为 table file 大小的限制。相应的，major compaction 选择能够产生最高比率的输入文件的数量。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611145910.png" alt="20210611145910" loading="lazy"></li>
<li><strong>Split Compaction</strong>：major compaction 可能不会很快地减少填满了大 table 的分区中 tables 的数量，这可以通过一个较低的估计输入/输出比率(例如10/9)来预测。该例子下，分区需要被分成多个分区，从而保证每个分区中的 tables 的数量可以持续减少。Split Compaction 归并排序了新数据和现有的 table fils，产生新的 tables 来组成几个新的分区。下图展示了过程。为了避免创建太多的小分区，compaction process 在一个分区里创建了 M 个新的 table files，M 默认为 2，这样来保证创建了 E/M 个分区，E 为新产生的 tables 的数量。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611150559.png" alt="20210611150559" loading="lazy"></li>
</ul>
<h3 id="rebuilding-remixes">Rebuilding REMIXes</h3>
<ul>
<li>分区存储布局可以通过利用较强的空间局部性来有效最小化真实负载 compaction cost，具体的，RemixDB 可以在几个分区中吸收大多数更新，在接受少量的分区中的 compaction 可以被避免。但是如果负载没有空间局部性，许多分区不可避免地必须执行压缩并进行少量的更新。Tiered Compaction 可以最小化这些分区的写操作，但是在一个分区中重建 REMIX 仍然需要读取已经存在的 tables。在我们的实现中，RemixDB 利用了现有的 REMIX 索引并使用了一个搞笑的合并算法来最小化重建过程的 I/O 开销。</li>
<li>当重建分区的 REMIX，现有的表已经被 REMIX 索引，这些表可以被视为一个 Sorted Run，相应的，重建过程相当于归并排序两个 sorted runs，一个来自现有的数据，另一个来自新数据，当现有的这个 sorted run 比新的大很多的时候，generalized binary merging algorithm 算法通过使用小顶堆实现了更少的 Key 比较，相比于归并排序、算法根据两个 sorted run 之间的 size ratio 估计出了下一次合并点的 location 信息，并在相邻的范围中进行搜索。在RemixDB中，我们通过使用锚键来定位包含合并点的目标段，最后在该段中应用二叉搜索。在这个过程中，访问锚定键不会产生任何I/O，因为它们存储在 REMIX 中。在目标段中进行二分搜索，最多读取 <span class="katex"><span class="katex-mathml"><math><semantics><mrow><mi>l</mi><mi>o</mi><msub><mi>g</mi><mn>2</mn></msub><mi>D</mi></mrow><annotation encoding="application/x-tex">log_2D</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathdefault" style="margin-right:0.01968em;">l</span><span class="mord mathdefault">o</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">g</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.03588em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.02778em;">D</span></span></span></span> 键来找到合并点。现有表的所有 run 选择器和游标偏移量都可以从现有的 REMIX 中派生出来，而不需要任何 I/O。要为新的分段创建锚定键，我们最多需要在新的排序视图上对每个分段访问一个键。</li>
<li>重建 REMIX 的读开销被分区中的所有表的大小限制，重建过程导致对现有表的读 I/O，为了降低 WA 并提升未来的读性能。构建 REMIX 是否是成本高效的取决于想要节省多少个写 I/O 以及未来想提升多少读性能。实际上，在 SSDs 中的写操作通常比读操作更慢而且可能对设备造成永久的破坏。因此读相比于写来说更经济，特别是有空闲 I/O 带宽的系统。在期望具有弱空间局部性的密集写操作的系统中，采用多层 tiered 压缩策略或延迟在单个分区中重建 REMIX 可以降低以拥有更多级别已排序视图为代价的重建成本。调整 REMIX 与不同的存储布局超出了本文的范围。我们对 RemixDB 在不同工作负载下的重建成本进行了实证评估</li>
</ul>
<h2 id="evaluations">Evaluations</h2>
<ul>
<li>实验搭建：
<ul>
<li>在每个实验中，我们首先创建一组 H 个表文件(1≤H≤16)，它们类似于RemixDB中的一个分区或LSM-tree中的一个级别，使用 tiered compaction。每个表文件包含 64MB 的KV-pairs，其中键和值的大小分别为 16B 和 100B。当 H≥2 时，KV 对可以用两种不同的模式分配到表中:
<ul>
<li><strong>Weak locality</strong>：每个键被分配给一个随机选择的表，这提供了弱访问局域性，因为逻辑上连续的键经常驻留在不同的表中</li>
<li><strong>Strong locality</strong>：每64个逻辑上连续的键被分配给一个随机选择的表，这提供了强访问局域性，因为一个范围查询可以从几个表中检索多个连续的键</li>
</ul>
</li>
<li>D=32，段内二分查找可以开启或关闭，对应图中的 full/partial<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162158.png" alt="20210611162158" loading="lazy"></li>
</ul>
</li>
<li>单次 Seek：Merge Iterator 在只有一个 Table 的时候表现更好，因为和 REMIX 需要执行相同次数的二分查找，但是 REMIX 需要动态地计算出现次数，并将迭代器从段的开头移动到一个键进行比较，开销相对更大。随着 Tables 数量增加，REMIX 的性能优势渐渐体现出来了。</li>
<li>Seek+Next50 整体性能低于 Seek 因为要拷贝数据到 Buffer。开启段内二分查找影响不大，因为 next 操作主要影响执行时间，在段中对寻道操作的线性扫描预热了块缓存，这使得未来的操作更快。</li>
<li>点查询：当少于 14 个 tables 的时候，REMIX 都不如带布隆过滤器的点查询，因为搜索可以只需要检查Bloom过滤器来有效地缩小到一个表文件。其次，在SSTable中搜索比在管理更多的键的 REMIX 中要快。在最坏的情况下，REMIX的吞吐量比Bloom过滤器(有3个表)低20%。不出所料，在没有Bloom过滤器的情况下，使用两个以上sstable的搜索速度会慢得多</li>
<li>强局部性的时候在 range query 的结果上差距不大，通常，改进的局部性允许更快的二分搜索，因为在这种情况下，最后几个键比较通常可以使用相同数据块中的键。但是，合并迭代器的吞吐量仍然很低，因为密集的键比较操作占据了搜索时间。带有部分二分搜索的 REMIX 比完全二分搜索的改进更多，是因为局部性的提升减少了在目标 segment 里 scan 的开销，导致更少的缓存缺失。</li>
<li>REMIX 点查询性能也得到了改善，因为强大的局域性加快了底层查找操作的速度。同时，Bloom 过滤器的结果保持不变，因为搜索代价主要由假阳性率和对单个表的搜索代价决定。因此，当包含超过9个表时，remix的性能可以超过Bloom过滤器。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162216.png" alt="20210611162216" loading="lazy"></li>
<li>8 个 tables，不同的 segment size，如果关闭了段内二分查找，只 seek 的负载下， D 的大小影响最大，这是因为段中的线性扫描显著增加了较大的 D 值的成本。由于段内的随机访问速度较慢，较大的段大小仍然会导致较高的开销。在 Seek+Next50 实验中，数据复制在执行时间中占主导地位，使用不同的 D 时没有显著差异<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162225.png" alt="20210611162225" loading="lazy"></li>
<li>不同的 range query 展现出了大约相同的趋势，实验中顺序加载相应的数据，所以在每种存储系统中都不会有重叠的文件，也就意味着 seek 操作只会访问一个文件。然而一个合并迭代器必须检查每个 sorted run 即便他们没有重叠的键范围，所以如果有多个 sorted runs 的话检索每个 run 的操作将占据 seek 时间的很大一部分。具体的，每一个在 LevelDB 和 RocksDB 中的 L0 表都是一个独立的 run，但是每一层 Li 又只包含一个 run。PebblesDB 允许一个 Level 有多个 runs，话虽如此，LevelDB 的性能至少比RocksDB高出2倍，尽管它们都使用了 Leveled 压缩。观察发现 RocksDB 在顺序加载过程中，在 L0 层保留了好几个 tables，总共八个，而没有把这些文件移动到更深的层次。相反，LevelDB 直接把这个 Table 推向了更深的层次 （L2 或 L3），如果这个 Table 和别的 Tables 不重叠的话，从而让 LevelDB 的 L0 层总是为空。因此，RocksDB 中的一个 seek 操作需要动态地排序-合并至少 12 个 sorted run，而这个数字在 LevelDB 中只有 3 或 4。</li>
<li>查找性能对访问的局部性是敏感的。较弱的局部性会增加搜索路径上的CPU和I/O开销。在每个特定 value 大小的实验中，采用 uniform 访问模式的吞吐量比顺序访问低50%左右。同时，顺序访问的性能对值大小不太敏感，因为内存复制成本不显著<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162232.png" alt="20210611162232" loading="lazy"></li>
<li>另外的实验测试了不同的存储总量大小下 query 长度对范围扫描的性能影响。每个实验以随机的顺序将具有 120B 值大小的固定大小的 KV 数据集加载到存储中，然后使用 Zipfian 访问模式使用四个线程执行范围扫描。REMIX 表现最好，但随着范围查询的长度增加，性能差异逐渐变小，这是因为长范围的查询在每个 sorted run 上展示出了顺序访问的特性，也就意味着在 scan 过程中更多的数据已经被预取，同时内存拷贝也给每个存储增加了固定的开销。</li>
<li>还有观察发现 LevelDB 在 256GB 的时候性能下降到和 RocksDB 一样。因为实验中配置了 4GB 的 block cache，缓存丢失导致大量的 I/O 占据了查询时间。与此同时 RocksDB 展示出了极大的计算开销，因为在数据量较小的时候 L0 层包含了太多 Tables。在更大数据量的存储中，过多的 I/O 掩盖了开销。与此同时 REMIX 保证了最好的访问局部性，因为导致了最少的随机访问和缓存丢失。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162249.png" alt="20210611162249" loading="lazy"></li>
<li>单线程随机插入 256GB 的数据，数据集有 20 个 KV 对，value 大小是 120B，负载有 uniform 的访问特征，代表着最差的情况。REMIX 和 PebblesDB 吞吐量最高，因为使用了利于写的 tiered compaction 策略，对应写放大为 4.99、9.26。比 LevelDB/RocksDB 小了很多。RocksDB 和 RemixDB 有更多的读 I/O，因为 RocksDB 使用了四个线程进行 Compaction 来充分利用 SSD 带宽，因为 Block Cache 和 Page Cache 的低效利用导致读 I/O 较多，LevelDB 只支持单线程压缩。尽管 RemixDB 读 I/O 多于 RocksDB 但是总的 I/O 还是小于 RocksDB 的。综上所述，RemixDB 以增加读 I/O 为代价实现了低 WA 和高写吞吐量。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611162259.png" alt="20210611162259" loading="lazy"></li>
<li>顺序的负载展现出最高的吞吐，因为每一轮压缩只影响很少的分区。写 I/O 主要包含日志和创建新的表，大约是用户写入量的 2 倍。读 I/O 主要是重建 REMIX 和数据本身的大小基本相同。相比之下，两个倾斜负载，在 Memtable 中重复 overwrites 导致写 I/O 大幅减少，但是会创建分散的更新，将导致 Memtable 中更新较慢以及更多的分区被 Compacted。Zipfian-Composite 有更差的空间局部性比 Zipfian，导致了更大的 Compaction I/O 开销。</li>
<li>YCSB 中，RemixDB 除了负载 D 表现都比其他的好。也就是 95% 的读请求访问最近的 5% 的写入，这种访问模式有很强的局部性，大部分请求直接是由存储中的 Memtables 来处理的，单线程压缩导致的缓慢插入阻碍了 LevelDB 的性能(1.1 MOPS)。</li>
<li>即便 REMIX 没有显示出相比于布隆过滤器足够的优势，但是在 YCSB-B，C 中比其他表现好，这两种负载下点查询占据大部分。是因为点查询在多级的 LSM 中在搜索路径上选择对应的 tables 的开销很大，具体而言就是每一个 L0 的 table，大约有两个键比较用于检查 seek 键是否被表覆盖，如果在 L0 中没找到，在每一个更深的层次中执行二分查找，直到 Key 被找到。布隆过滤器的大小大约 600KB，对于一个 64MB 的表而言，访问一个布隆过滤器会导致大约七次内存随机访问，在一个很大的存储中将导致严重的缓存缺失。REMIX 索引的分区形成了一个全局有序视图，基于此的二分查找可以很快得到应答。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210611195131.png" alt="20210611195131" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[C++ STL]]></title>
        <id>https://blog.shunzi.tech/post/cpp-std/</id>
        <link href="https://blog.shunzi.tech/post/cpp-std/">
        </link>
        <updated>2021-05-12T01:36:30.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>C++ STL 的相关资料记录</li>
<li>持续更新</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>C++ STL 的相关资料记录</li>
<li>持续更新</li>
</ul>
<!--more-->
<h2 id="stl">STL</h2>
<ul>
<li>C++ STL（标准模板库）是一套功能强大的 C++ 模板类，提供了通用的模板类和函数，这些模板类和函数可以实现多种流行和常用的算法和数据结构，如向量、链表、队列、栈。</li>
</ul>
<h3 id="参考链接">参考链接</h3>
<ul>
<li><a href="https://www.runoob.com/cplusplus/cpp-stl-tutorial.html">[1] 菜鸟教程：C++ STL 教程</a></li>
<li><a href="https://blog.csdn.net/u010183728/article/details/81913729">[2] CSDN - C++中STL用法超详细总结</a></li>
<li><a href="https://github.com/czs108/Cpp-Primer-5th-Notes-CN">[3] Github - 《C++ Primer中文版（第5版）》笔记</a></li>
</ul>
<h3 id="stl-容器">STL 容器</h3>
<h4 id="顺序容器-sequential-containers">顺序容器 Sequential Containers</h4>
<ul>
<li><strong>概览</strong></li>
</ul>
<table>
<thead>
<tr>
<th style="text-align:center">类型</th>
<th style="text-align:center">特性</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>vector</code></td>
<td style="text-align:center">可变大小数组。支持快速随机访问。在尾部之外的位置插入/删除元素可能很慢</td>
</tr>
<tr>
<td style="text-align:center"><code>deque</code></td>
<td style="text-align:center">双端队列。支持快速随机访问。在头尾位置插入/删除速度很快</td>
</tr>
<tr>
<td style="text-align:center"><code>list</code></td>
<td style="text-align:center">双向链表。只支持双向顺序访问。在任何位置插入/删除速度都很快</td>
</tr>
<tr>
<td style="text-align:center"><code>forward_list</code></td>
<td style="text-align:center">单向链表。只支持单向顺序访问。在任何位置插入/删除速度都很快</td>
</tr>
<tr>
<td style="text-align:center"><code>array</code></td>
<td style="text-align:center">固定大小数组。支持快速随机访问。不能添加/删除元素</td>
</tr>
<tr>
<td style="text-align:center"><code>string</code></td>
<td style="text-align:center">类似<code>vector</code>，但用于保存字符。支持快速随机访问。在尾部插入/删除速度很快</td>
</tr>
</tbody>
</table>
<ul>
<li><code>forward_list</code>和<code>array</code>是C++11新增类型。与内置数组相比，<code>array</code>更安全易用。<code>forward_list</code>没有<code>size</code>操作。</li>
</ul>
<p><strong>容器选择原则</strong></p>
<ul>
<li>除非有合适的理由选择其他容器，否则应该使用<code>vector</code>。</li>
<li>如果程序有很多小的元素，且空间的额外开销很重要，则不要使用<code>list</code>或<code>forward_list</code>。</li>
<li>如果程序要求随机访问容器元素，则应该使用<code>vector</code>或<code>deque</code>。</li>
<li>如果程序需要在容器头尾位置插入/删除元素，但不会在中间位置操作，则应该使用<code>deque</code>。</li>
<li>如果程序只有在读取输入时才需要在容器中间位置插入元素，之后需要随机访问元素。则：
<ul>
<li>先确定是否真的需要在容器中间位置插入元素。当处理输入数据时，可以先向<code>vector</code>追加数据，再调用标准库的<code>sort</code>函数重排元素，从而避免在中间位置添加元素。</li>
<li>如果必须在中间位置插入元素，可以在输入阶段使用<code>list</code>。输入完成后将<code>list</code>中的内容拷贝到<code>vector</code>中。</li>
</ul>
</li>
<li>不确定应该使用哪种容器时，可以先只使用<code>vector</code>和<code>list</code>的公共操作：使用迭代器，不使用下标操作，避免随机访问。这样在必要时选择<code>vector</code>或<code>list</code>都很方便。</li>
</ul>
<h4 id="关联容器">关联容器</h4>
<p>关联容器支持高效的关键字查找和访问操作。2个主要的关联容器（associative-container）类型是<code>map</code>和<code>set</code>。</p>
<ul>
<li><code>map</code>中的元素是一些键值对（key-value）：关键字起索引作用，值表示与索引相关联的数据。</li>
<li><code>set</code>中每个元素只包含一个关键字，支持高效的关键字查询操作：检查一个给定关键字是否在<code>set</code>中。</li>
</ul>
<p>标准库提供了8个关联容器，它们之间的不同体现在三个方面：</p>
<ul>
<li>是<code>map</code>还是<code>set</code>类型。</li>
<li>是否允许保存重复的关键字。</li>
<li>是否按顺序保存元素。</li>
</ul>
<p>允许重复保存关键字的容器名字都包含单词<code>multi</code>；无序保存元素的容器名字都以单词<code>unordered</code>开头。</p>
<h5 id="使用关联容器using-an-associative-container">使用关联容器（Using an Associative Container）</h5>
<p><code>map</code>类型通常被称为关联数组（associative array）。</p>
<p>从<code>map</code>中提取一个元素时，会得到一个<code>pair</code>类型的对象。<code>pair</code>是一个模板类型，保存两个名为<code>first</code>和<code>second</code>的公有数据成员。<code>map</code>所使用的<code>pair</code>用<code>first</code>成员保存关键字，用<code>second</code>成员保存对应的值。</p>
<pre><code class="language-c++">// count the number of times each word occurs in the input
map&lt;string, size_t&gt; word_count;     // empty map from string to size_t
string word;
while (cin &gt;&gt; word)
    ++word_count[word];     // fetch and increment the counter for word
for (const auto &amp;w : word_count)    // for each element in the map
    // print the results
    cout &lt;&lt; w.first &lt;&lt; &quot; occurs &quot; &lt;&lt; w.second
        &lt;&lt; ((w.second &gt; 1) ? &quot; times&quot; : &quot; time&quot;) &lt;&lt; endl;
</code></pre>
<p><code>set</code>类型的<code>find</code>成员返回一个迭代器。如果给定关键字在<code>set</code>中，则迭代器指向该关键字，否则返回的是尾后迭代器。</p>
<h6 id="pair">pair</h6>
<p><code>pair</code>定义在头文件<em>utility</em>中。一个<code>pair</code>可以保存两个数据成员，分别命名为<code>first</code>和<code>second</code>。</p>
<pre><code class="language-c++">pair&lt;string, string&gt; anon;        // holds two strings
pair&lt;string, size_t&gt; word_count;  // holds a string and an size_t
pair&lt;string, vector&lt;int&gt;&gt; line;   // holds string and vector&lt;int&gt;
</code></pre>
<p><code>pair</code>的默认构造函数对数据成员进行值初始化。</p>
<h3 id="stl-常用算法">STL 常用算法</h3>
<h4 id="搜索">搜索</h4>
<h5 id="二分查找">二分查找</h5>
<ul>
<li><strong>upper_bound()</strong></li>
</ul>
<pre><code class="language-C++">//查找[first, last)区域中第一个大于 val 的元素。
ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last,
                             const T&amp; val);
//查找[first, last)区域中第一个不符合 comp 规则的元素
ForwardIterator upper_bound (ForwardIterator first, ForwardIterator last,
                             const T&amp; val, Compare comp);
</code></pre>
<ul>
<li><strong>lower_bound()</strong></li>
</ul>
<pre><code class="language-C++">//在 [first, last) 区域内查找不小于 val 的元素
ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last,
                             const T&amp; val);
//在 [first, last) 区域内查找第一个不符合 comp 规则的元素
ForwardIterator lower_bound (ForwardIterator first, ForwardIterator last,
                             const T&amp; val, Compare comp);
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[HashKV: Enabling Efficient Updates in KV Storage via Hashing]]></title>
        <id>https://blog.shunzi.tech/post/HashKV/</id>
        <link href="https://blog.shunzi.tech/post/HashKV/">
        </link>
        <updated>2021-05-11T04:21:02.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>HashKV: Enabling Efficient Updates in KV Storage via Hashing</li>
<li>http://adslab.cse.cuhk.edu.hk/software/hashkv/</li>
<li>ATC18 &amp; TOS19</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>HashKV: Enabling Efficient Updates in KV Storage via Hashing</li>
<li>http://adslab.cse.cuhk.edu.hk/software/hashkv/</li>
<li>ATC18 &amp; TOS19</li>
</ul>
</blockquote>
<!--more-->
<h2 id="个人总结">个人总结</h2>
<h2 id="abstract">Abstract</h2>
<ul>
<li>持久键值(KV)存储主要构建在日志结构的合并树(LSM)上，以获得较高的写性能，但是LSM树存在固有的较高的I/O放大问题。KV分离通过在LSM-tree中只存储键和在单独存储中的值来减轻I/O放大。<strong>然而，当前的KV分离设计在更新密集型工作负载下仍然是低效的，因为它在值存储方面的垃圾收集(GC)开销很大</strong>。我们提出了HashKV，它的目标是在更新密集型工作负载下，在KV分离的基础上提高更新性能。HashKV使用基于哈希的数据分组，它确定地将值映射到存储空间，从而提高更新和GC的效率。我们通过简单但有用的设计扩展进一步放宽了这种确定性映射的限制。通过大量的实验，我们将HashKV与最先进的KV存储进行了比较，并表明与当前的KV分离设计相比，HashKV实现了4.6×吞吐量和减少了53.4%的写流量。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>持久性 KV 存储应用广泛，用于存储海量结构化数据：Bigtable, Dynamo, Atlas。虽然现实中的KV存储工作负载主要是读密集型的(例如，在Facebook的Memcached工作负载中，Get/Update的比率可以达到30:1)，但更新密集型的工作负载在许多存储场景中也占主导地位（例如,雅虎报告其低延迟工作负载越来越多地从读转移到写）</li>
<li>现代的针对写操作优化的 KV 存储大多是基于 LSM 树，思想源于最初的 LSF，LSM-tree设计不仅通过避免小的随机更新(这也有害于固态硬盘(SSD)的寿命)来提高写性能，而且通过在每个节点中保留已排序的KV对来提高范围扫描性能。<strong>但是写放大严重，读放大更严重</strong>。</li>
<li><strong>已有方案</strong>：WiscKey 采用 KV 分离来减少 Compaction 操作带来的影响，<strong>但是我们发现 KV 分离还是无法在更新密集型工作负载下完全实现高性能</strong>。</li>
<li><strong>根本原因</strong>在于用于值存储的循环日志需要频繁的垃圾收集(GC)，以从被删除或被新更新取代的KV对中回收空间。然而，由于循环日志的两个限制，GC开销实际上是昂贵的。
<ul>
<li>首先，循环日志保持严格的GC顺序，因为它总是在日志的开始处执行GC，即最近写入的KV对所在的位置。这可能会导致<strong>大量不必要的数据重定位</strong>(例如，当最近写的KV对仍然有效时)。</li>
<li>其次，<strong>GC 操作需要查询LSM-tree</strong>，检查每个KV对的有效性。这些查询具有<strong>很高的延迟</strong>，特别是当LSM-tree在大工作负载下变得相当大时。</li>
</ul>
</li>
<li>所以提出了 HashKV，为更新密集型工作负载量身定制的高性能KV存储。HashKV建立在KV分离的基础上，并使用一种新的基于哈希的数据分组设计来存储值。其思想是将值存储划分为固定大小的分区，并通过散列其键确定地将每个写入KV对的值映射到一个分区。基于哈希的数据分组支持<strong>基于确定性映射的轻量级更新</strong>。更重要的是，它显著地减轻了GC开销，因为每个GC操作<strong>不仅具有选择一个分区来回收空间的灵活性，而且还消除了为了检查KV对的有效性而对LSM-tree的查询</strong>。</li>
<li>另一方面，基于哈希的数据分组的确定性限制了KV对的存储位置。因此，我们提出了三种新的设计扩展来放松基于哈希的数据分组的限制：
<ul>
<li>(i)  <strong>动态预留空间分配</strong>，在给定大小限制的情况下，动态分配预留空间给额外的写操作</li>
<li>(ii)  <strong>热感知</strong>，受现有SSD设计的启发，将热KV和冷KV对的存储分开，以提高GC效率</li>
<li>(iii) <strong>有选择性的KV分离</strong>，在LSM-tree中保持较小的KV对的完整，以简化查找</li>
</ul>
</li>
<li>基于 LevelDB 实现了 HashKV 的原型，通过测试实验表明，在更新密集型的工作负载下，HashKV 的吞吐量达到 4.6×，与 wisckey 中的循环日志设计相比，写流量减少了 53.4%。此外，在各种情况下，与现代KV存储(如LevelDB和RocksDB)相比，HashKV通常能够实现更高的吞吐量和更少的写流量。</li>
<li>我们的工作只是增加 KV 分离与一个新的Value管理设计的案例。虽然HashKV的密钥和元数据管理现在建立在LevelDB上，但它也可以采用带有新的LSM树设计的其他KV存储。在KV分离条件下，哈希KV如何影响各种基于lsm树的KV存储的性能是未来研究的重点。</li>
</ul>
<h2 id="motivation">Motivation</h2>
<ul>
<li>这一章节主要介绍 LevelDB 的读写放大问题，以及 WiscKey 这种 KV 分离方案带来的改善，同时分析普通的 KV 分离无法在写密集负载中实现高性能的原因。</li>
</ul>
<h3 id="leveldb">LevelDB</h3>
<ul>
<li>原理不过多介绍，重点是读写放大问题。
<ul>
<li>首先，压缩过程不可避免地产生额外的读写。在最坏的情况下，要将一个SSTable从Li−1合并到Li，它需要读取并排序10个SSTable，然后写回所有SSTable。先前的研究表明，LevelDB可以有至少50倍的总体写放大，因为在大的工作负载下，它可能会触发不止一次的压缩，将一个KV对向下移动多个级别。</li>
<li>查找操作可以在多个级别搜索一个KV对，并导致多个磁盘访问。原因是，每个级别的搜索都需要读取相关SSTable中的索引元数据和Bloom过滤器，虽然使用了Bloom过滤器，但它可能会引入误报。在这种情况下，即使KV对实际上不存在，仍然需要从磁盘读取SSTable。因此，每次查找通常会导致多次磁盘访问。这种读放大在大的工作负载下会进一步恶化，因为LSMtree会逐级累积。测量结果表明，在最坏的情况下，读放大可达300倍以上<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324103632.png" alt="20210324103632" loading="lazy"></li>
</ul>
</li>
</ul>
<h3 id="kv-separation">KV Separation</h3>
<h4 id="wisckey-原理">WiscKey 原理</h4>
<ul>
<li>KV分离，由 Wisckey 提出，对键和值的管理进行解耦，以减轻写和读的放大。其基本原理是，在 LSM-tree 中存储值对于索引来说是不必要的。因此，在LSM-tree中，wisckey 只存储键和元数据(例如键/值的大小、值的位置等)，而将值存储在一个单独的仅追加的循环日志vLog 中。KV 分离有效地减轻了LevelDB的写和读放大，因为它显著地减少了 lsm 树的大小，从而同时减少了压缩和查找开销。</li>
<li>由于vLog遵循日志结构设计，KV分离在vLog中实现轻量级的垃圾收集(GC)是至关重要的，即在有限的开销下从无效值中回收空闲空间。具体来说，wisckey跟踪的是vLog头部和vLog尾部，分别对应于vLog的结束和开始。它总是向vLog头插入新的值。当它执行GC操作时，它从vLog尾部读取一大块KV对。它首先查询lsm树，查看每个KV对是否有效。然后丢弃无效KV对的值，并将有效值写回vLog头。它最后更新LSM-tree以获取有效值的最新位置。为了在GC期间支持有效的LSM-tree查询，wisckey还将相关的键和元数据与值一起存储在vLog中。请注意，vLog经常为减少GC开销而提供额外的预留空间。<br>
<img src="https://github.com/zjs1224522500/files-and-images/blob/master/blog/pic/papers/WiscKey%20Garbage%20Collection.png?raw=true" alt="image" loading="lazy"></li>
</ul>
<h4 id="limitations">Limitations</h4>
<ul>
<li>虽然KV分离降低了压缩和查找开销，但我们认为它受到了vLog中大量GC开销的影响。另外，如果预留空间有限，GC开销会变得更加严重。原因有两方面:
<ul>
<li>首先，由于它的循环日志设计，vLog只能从它的vLog尾部回收空间。这个约束可能会导致不必要的数据移动。特别是，真实世界的KV存储往往表现出很强的局部性，其中一小部分的热KV对经常更新，而其余的冷KV对只接收到很少甚至没有更新。在vLog中保持严格的顺序不可避免地会多次重新定位冷KV对，从而增加GC开销。</li>
<li>此外，每个GC操作都查询LSM-tree，以检查vLog尾部chunk中每个KV对的有效性。由于KV对的键可能分散在整个LSM-tree中，查询开销很高，并增加了GC操作的延迟。虽然KV分离已经减少了LSM-tree的大小，但是LSM-tree在大的工作负载下仍然是相当大的，这加剧了查询成本。</li>
</ul>
</li>
<li><strong>分别总结一下这两个问题</strong>：
<ul>
<li>GC 过程中需要对有效数据进行拷贝，<strong>热数据进行 GC 是必须的，确保频繁更新的数据的老旧版本的空间能够及时回收，冷数据 GC 其实意义不大，反而因为 GC 过程中的数据迁移引入了开销</strong>。GC 最好的情况就是遇到需要删除和回收的数据，因为在 WiscKey 这样的设计背景下，无需进行回收的数据就肯定会进行数据的拷贝来回收对应的这一段空间（为了避免碎片或者说hole 的产生，因为碎片相应地容易引发随机 I/O）</li>
<li><strong>GC 操作每个都需要检查数据的有效性，需要去 LSM 树中进行查询，相当于 GC 操作引入了额外的查询开销，并增加了 GC 操作的延迟</strong>（这个是因为 LSM 树本身读的一部分原因，造成了读操作需要读很多层，延迟就比较高，本来 LSM 树读性能就不是特别好）
<ul>
<li>那 KV 分离为啥是用 LSM + Value Log 的方案，而不是 B tree + Value Log 呢？思想就是找一个小写读写性能比 LSM 相对更好的数据结构来代替 LSM 存储 Key。Key 大小甚至可以固定，使用一些定长编码的策略。<strong>换一个更高效的索引结构似乎就能解决查询延迟的问题？但仿佛又会牺牲写</strong></li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="实验验证">实验验证</h4>
<ul>
<li>为了验证KV分离的局限性，我们实现了一个基于vLog的KV存储原型(见§3.8)，并评估其写入放大。我们考虑两个阶段:加载和更新。在加载阶段，将40GiB的1-KiB KV对插入到初始为空的vLog中;在更新阶段，我们基于Zipf分布，Zipfian常数为0.99，发起了40GiB对现有KV对的更新。我们为视频日志提供40GiB的空间，并额外预留30% (12GiB)的空间。我们也在原型中禁用了写缓存(见§3.2)。图2显示了加载和更新阶段vLog的写放大结果，即由于插入或更新导致的总设备写大小与实际写大小的比例。</li>
<li>为了进行比较，我们还考虑两个现代KV存储，LevelDB和RocksDB，基于它们的默认参数。在 Load 阶段，vLog有足够的空间容纳所有KV对，不触发GC，由于KV分离，它的写入放大只有1.6×。但是，在更新阶段，更新会填满保留的空间并开始触发GC。我们可以看到，vLog的写放大倍数为19.7×，接近LevelDB (19.1×)，高于RocksDB (7.9×)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324111831.png" alt="20210324111831" loading="lazy"></li>
<li>为了减轻vLog的GC开销，一种方法是将vLog划分成段，并根据成本-收益策略或其变体 [如下参考文献 ]  选择最佳的候选段来减少GC开销。但是，热KV和冷KV对仍然可以在vLog中混合在一起，所以为GC选择的段可能仍然包含冷KV对，并且不必要地移动。
<ul>
<li>[SOSP1997  Improving the Performance of Log-Structured File Systems with Adaptive Methods]</li>
<li>[TOCS1992 The Design and Implementation of a Log-structured File System]</li>
<li>[FAST14 Logstructured Memory for DRAM-based Storage]</li>
</ul>
</li>
<li>为了解决热数据和冷数据的混合问题，更好的方法是像SSD设计中那样执行热-冷数据分组，其中，我们将热KV和冷KV的存储分成两个区域，并分别对每个区域应用GC(更多的GC操作将应用于热KV的存储区域)。然而，直接实现热-冷数据分组不可避免地增加了KV分离过程中的更新延迟。由于KV对可能存储在热或冷数据区，每次更新都需要首先查询LSM-tree以获得KV对的确切存储位置。因此，我们工作的一个关键动机是在不使用 LSM 树查找的情况下启用热状态识别。</li>
</ul>
<h2 id="design">Design</h2>
<ul>
<li>HashKV是一个持久的KV存储，专门针对更新密集型工作负载。在KV分离的基础上改进了 Value 存储的管理，实现了高更新性能。它支持标准的KV操作:PUT(即写入一个KV对)、GET(即检索一个键的值)、DELETE(即删除一个KV对)和SCAN(即检索一个键范围的值)。</li>
</ul>
<h3 id="main-idea">Main Idea</h3>
<ul>
<li>在KV分离之上，HashKV引入了几个核心设计元素来实现高效的 Value 存储管理：
<ul>
<li><strong>Hash-based data grouping</strong>: 回想一下，vLog在值存储方面会招致大量的GC开销。相反，HashKV通过散列相关的键将值映射到值存储中固定大小的分区中。这种设计实现了:
<ul>
<li>(i)分区隔离，在这种情况下，所有版本的值更新都必须被写入到相同的分区中，</li>
<li>(ii)确定性分组，在这种情况下，一个值应该存储在哪个分区中是通过哈希来确定的。我们利用这种设计来实现灵活和轻量级的GC</li>
</ul>
</li>
<li><strong>Dynamic reserved space allocation</strong>: 由于我们将值映射到固定大小的分区中，一个挑战是一个分区接收的更新可能多于它能容纳的更新。HashKV通过在值存储中分配部分保留空间，允许分区动态增长，超过其大小限制。</li>
<li><strong>Hotness awareness</strong>：由于确定性分组，分区可能会被来自热KV对和冷KV对的混合值填满，在这种情况下，GC操作会不必要地读取和回写冷KV对的值。HashKV使用标记方法将冷KV对的值重新定位到不同的存储区域，并将热KV对和冷KV对分开，这样我们就可以只对热KV对使用GC，避免重复复制冷KV对。</li>
<li><strong>Selective KV separation</strong>：HashKV通过其值大小来区分KV对，小的KV对可以直接存储在LSM-tree中，而不需要分离KV。这节省了访问小KV对的LSM-tree和值存储的开销，而在LSM-tree中存储小KV对的压缩开销是有限的。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324112600.png" alt="20210324112600" loading="lazy"></li>
</ul>
</li>
<li>HashKV维护一个单一的LSM-tree来进行索引(而不是像值存储中那样对LSM-tree进行哈希分区)，以保持键的顺序和范围扫描性能。由于基于哈希的数据分组将KV对分散到值存储中，因此会导致随机写操作;相反，vLog使用日志结构的存储布局来维护顺序写入。我们的HashKV原型(见§3.8)利用<strong>多线程和批处理写来限制随机写开销</strong>。</li>
</ul>
<h3 id="storage-management">Storage Management</h3>
<ul>
<li>将 value store 的逻辑地址空间划分成固定大小的单元，称之为 main segments，此外，它还过度规定了保留空间的固定部分，将其再次划分为固定大小的单元，称为 log segments。注意，主段和日志段的大小可能不同;缺省情况下，分别设置为64MiB和1MiB。</li>
<li>对于每一个KV对的插入或更新，HashKV将其密钥散列到一个主要段中。如果主段没有被填满，HashKV会把这个值添加到主段的末尾，以一种日志结构的方式存储这个值;另一方面，如果主段已满，HashKV会动态分配一个空闲的日志段，以日志结构的方式存储额外的值。同样，如果当前日志段已满，它将进一步分配额外的空闲日志段。我们统称一个主段和它的所有关联的日志段为段组。</li>
<li>此外，HashKV更新LSM-tree以获得最新的值位置。为了跟踪段组和段的存储状态，HashKV使用一个内存中的全局段表来存储每个段组的当前结束位置，以供后续的插入或更新，以及与每个段组相关的日志段列表。我们的设计<strong>确保了每个插入或更新都可以直接映射到正确的写位置，而无需在写路径上执行LSM-tree查找</strong>（直接 HASH 得到对应的分区，然后在分区中进行追加写），从而实现了较高的写性能。另外，与同一个键相关联的值的更新必须转到同一个段组，这简化了GC。<strong>为了容错，HashKV checkpoints 段表到持久存储</strong>（内存中的段表数据恢复方式）。</li>
<li>为了便于GC, HashKV还存储键和元数据(例如,键/值大小)，和 WiscKey 一样和值存储在一起(参见图3)。这使GC操作能够在 scan value store 时，快速识别关联到某一个值的键。然而，我们的GC设计与 WiscKey 使用的 vLog 有本质上的不同。</li>
<li>为了提高写性能，<strong>HashKV在内存中保存了一个写缓存，以降低可靠性为代价来存储最近写的KV对。如果在写缓存中找到了要写的新KV对的键，HashKV就直接就地更新缓存的键的值，而不用向LSM-tree和值存储区发出写操作</strong>。它还可以从写缓存中返回KV对用于读取。如果写缓存已满，HashKV将所有缓存的KV对刷新到lsm 树和值存储中。注意，写缓存是一个可选组件，可以出于可靠性考虑禁用它。</li>
<li>HashKV 通过将冷值保存在单独的冷数据日志 cold data log 中来支持热感知(见§3.4)。它还通过在 write journal 和 GC journal 中跟踪更新来解决崩溃一致性问题(见3.7)。</li>
</ul>
<h3 id="garbage-collection-gc">Garbage Collection (GC)</h3>
<ul>
<li>HashKV要求GC回收值存储中无效值所占用的空间。在HashKV中，GC以段组为单位进行操作，当保留空间中的空闲日志段用完时将触发GC。在较高的级别上，<strong>GC操作首先选择一个候选段组，并识别组中所有有效的KV对(即最新版本的KV对)。然后，它以日志结构的方式将所有有效的KV对写回主段，或者在需要时写到附加的日志段。它还释放任何未使用的日志段，这些日志段以后可以被其他段组使用</strong>。最后，它更新LSM-tree中最新的值位置。这里，GC操作需要解决两个问题:
<ul>
<li>(i)  应该为GC选择哪个段组;</li>
<li>(ii) GC操作如何快速识别所选段组中的有效KV对。</li>
</ul>
</li>
<li>不同于vLog，它要求GC操作遵循严格的顺序，HashKV可以灵活地选择执行GC的段组。<strong>目前采用的是贪婪的方法，选择写量最大的段组</strong>。我们的基本原理是，所选的段组通常保存有许多更新，因此有大量写操作的hot KV对。因此，为GC选择这个段组可能会回收最多的空闲空间。<strong>为了实现贪婪的方法，HashKV跟踪内存段表中每个段组的写量</strong>(见§3.2)，<strong>并使用堆 heap 来快速识别哪个段组收到的写量最大</strong></li>
<li>为了检查所选段组中KV对的有效性，HashKV顺序扫描段组中的KV对，而不查询LSM-tree(注意，它还检查写缓存，以查找段组中最新的KV对)。由于KV对以日志结构的方式写入段组，所以必须按照更新的顺序依次放置KV对。对于一个有多个版本更新的KV对，最接近段组末尾的版本必须是最新的版本，并且对应于有效的KV对，而其他版本是无效的。因此，<strong>每个GC操作的运行时间只取决于需要扫描的段组的大小</strong>。相反，vLog中的GC操作从vLog尾部读取一大块KV对(见§2.2)。它查询LSM-tree(基于与值一起存储的键)以获取每个KV对的最新存储位置，以检查KV对是否为有效的。在大的工作负载下，查询LSM-tree的开销会变得很大。</li>
<li>在段组的GC操作期间，<strong>HashKV构造一个临时内存哈希表(按键索引)来缓冲在段组中找到的有效KV对的地址</strong>。由于键和地址的大小通常比较小，并且一个段组中的 KV 对数量有限，所以哈希表的大小有限，可以全部存储在内存中。</li>
</ul>
<h3 id="hotness-awareness">Hotness Awareness</h3>
<ul>
<li>冷热数据分离提高了日志结构存储中的GC性能。事实上，当前基于散列的数据分组设计实现了某种形式的冷热数据分离，因为对热KV对的更新必须散列到同一段组，而我们当前的GC策略总是选择可能存储热KV对的段组。然而，不可避免的是，一些冷KV对被散列到为GC选择的段组中，导致不必要的数据重写。因此，充分实现冷热数据分离，进一步提高GC性能是一个挑战。</li>
<li>HashKV通过标记方法放松了基于哈希的数据分组的限制(见图4)。具体来说，当HashKV对一个段组<strong>执行GC操作时，它将段组中的每个KV对划分为热的或冷的</strong>。目前，我们将自<strong>最后一次插入以来至少更新过一次</strong>的KV对视为热的，否则是冷的(可以使用更精确的热-冷数据识别方法）
<ul>
<li><strong>对于热KV对，HashKV仍然通过散列将它们的最新版本写回同一个段组。</strong></li>
<li>对于冷KV对，它现在将它们的值写入一个单独的存储区域，并在段组中只保留它们的元数据(即没有值)。此外，<strong>它在每个冷KV对的元数据中添加一个标记，以表明它在段组中的存在</strong>。</li>
</ul>
</li>
<li>因此，如果一个冷KV对后来被更新，我们直接从标签(不查询LSM-tree)知道冷KV对已经被存储，因此我们可以根据我们的分类策略将其视为热；标记的KV对也将失效。最后，<strong>在GC操作结束时，HashKV更新LSM-tree中最新的值位置，这样冷KV对的位置（LSM存储的地址）就指向单独的区域</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324165745.png" alt="20210324165745" loading="lazy"></li>
<li>通过标记，HashKV避免了将冷KV对的值存储在段组中，并在GC期间重写它们。而且，<strong>标记只在GC期间触发，不会给写路径增加额外的开销</strong>。目前，我们为冷KV对实现了单独的存储区域，作为值存储中的一个仅追加的日志(称为冷数据日志)，并像vLog一样对冷数据日志执行gc（这里就和 WiscKey 一样了）。如果冷KV对很少被访问，冷数据日志也可以放在容量更大的二级存储中(如硬盘)。
<ul>
<li><strong>其实这里有三个问题：</strong>
<ul>
<li>热数据特别少，冷数据特别多的时候，空间分配会是个问题。</li>
<li>所有的数据第一次访问的时候都是冷数据，第二次访问就成了热数据了，要是冷数据变热数据之间的时间较长，中间进行了 GC，就会导致数据拷贝（至少一次 segment group 到 cold Value Log），然后再引入一次 Cold Value Log 中的 GC 操作，这时候就退化成了 WiscKey 的 GC，需要检查 LSM Tree。</li>
<li>这种过于简单的冷热数据识别，应对不了热数据变冷的问题，热数据变冷会一直在 segment group 中进行拷贝，还是会一直进行数据的拷贝，还是退化成了 WiscKey 的 GC。（<strong>这种简单的冷热数据识别只能对 只访问一次的冷数据为主要组成部分的负载 产生比较好的效果</strong>）</li>
</ul>
</li>
</ul>
</li>
<li><strong>本质是将冷数据只进行一次拷贝操作，减少了冷数据的数据迁移，同时验证冷数据的有效性直接通过 tag 验证，不用查询 LSM 树，热数据的有效性的话还是需要通过内存中临时内存哈希表来进行验证。</strong></li>
<li>我们评估了热度感知对HashKV更新性能的影响。我们考虑两个Zipfian常量，0.9和0.99，以捕获工作负载中的不同偏度。图10显示了禁用和启用热感知功能时的结果。当启用热感知功能时，更新吞吐量增加了113.1%和121.3%，而对于Zipfian常数0.9和0.99，写大小分别减少了42.8%和42.5%。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324215235.png" alt="20210324215235" loading="lazy"></li>
</ul>
<h3 id="selective-kv-separation">Selective KV Separation</h3>
<ul>
<li>HashKV支持具有一般值大小的工作负载。我们的理论基础是，KV分离降低了压缩开销，特别是对于大型KV对，但它对小型KV对的好处是有限的，而且它会导致访问 LSM 树和值存储的额外开销。因此，我们提出了选择KV分离的方法，<strong>即对较大值的KV对仍然采用KV分离，而较小值的KV对则全部存储在LSM-tree中</strong>。选择KV分离的一个关键挑战是选择区分小尺寸和大尺寸KV对的KV对大小阈值(假设键大小不变)。我们认为选择取决于部署环境。在实践中，我们可以对不同的值大小进行性能测试，看看什么时候选择性KV分离的吞吐量增益显著。</li>
<li>我们观察到，由于在KV分离下存储的小KV对的高更新开销，当小KV对的比例较高时，此时工作负载下，KV分离的性能增益更高。同时 40B-1KB 的组合相比于 40B-4KB 的组合优化效果更明显。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324214027.png" alt="20210324214027" loading="lazy"></li>
</ul>
<h3 id="range-scans">Range Scans</h3>
<ul>
<li>使用LSM-tree进行索引的一个关键原因是它对范围扫描的有效支持。由于LSMtree按键存储和排序KV对，因此它可以通过顺序读取返回一系列键的值。然而，KV分离现在将值存储在单独的存储空间中，因此它会招致额外的值读取。在HashKV中，这些值分散在不同的段组中，因此范围扫描将触发许多随机读取，从而降低性能。<strong>HashKV目前利用预读机制通过将值预取到页面缓存中来加速范围扫描</strong>。对于每个扫描请求，HashKV遍历LSM-tree中排序键的范围，并(通过posix fadvise)向每个值发出预读请求。然后读取所有值并返回排序的KV对。</li>
<li>在KV对大小上，HashKV具有与vLog相似的扫描性能。然而，对于256-B和1-KiB KV对，HashKV的扫描吞吐量分别比LevelDB低70.0%和36.3%，主要是因为HashKV需要向LSM-tree和值存储发出读操作，而且通过随机读操作从值存储中检索小值的开销也很大。然而，对于4KiB或更大的KV对，HashKV的性能优于LevelDB，例如，4KiB KV对的性能为94.2%。</li>
<li>注意，预读机制(见§3.6)是使HashKV实现高范围扫描性能的关键。例如，与没有预读的情况相比，256-B KV对的HashKV的范围扫描吞吐量增加了81.0%<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324214413.png" alt="20210324214413" loading="lazy"></li>
</ul>
<h3 id="crash-consistency">Crash Consistency</h3>
<ul>
<li>
<p>当HashKV问题写入持久性存储时，可能会发生崩溃。HashKV基于元数据日志记录解决崩溃一致性问题，主要关注两个方面:</p>
<ul>
<li>(i) 刷新写缓存</li>
<li>(ii) GC操作。</li>
</ul>
</li>
<li>
<p>刷新写缓存涉及将KV对写入值存储并更新LSM-tree中的元数据。<strong>HashKV维护一个写日志 write journal 来跟踪每次 flush 操作</strong>。它在刷新写缓存时执行以下步骤：</p>
<ul>
<li>(i) 将缓存的KV对刷新到值存储中;</li>
<li>(ii) 在 write journal 中追加写入元数据更新;</li>
<li>(iii) 在 journal end 写入一个提交记录;</li>
<li>(iv) 更新 LSM-tree 中的 keys 和元数据;</li>
<li>(v) 在日志中标记 flush 操作为 free 状态( free 的日志记录可以稍后回收)。</li>
</ul>
</li>
<li>
<p>如果在步骤(iii)完成后发生崩溃，HashKV在写日志中回放更新，并确保LSMtree和值存储是一致的。</p>
</li>
<li>
<p>GC 操作中崩溃一致性的处理是不同的，因为它们可能会覆盖现有的有效 KV 对。因此，我们还需要保护现有的有效 KV 对，防止在GC 期间崩溃。<strong>HashKV 维护一个GC日志来跟踪每个GC操作。</strong> 在GC操作中识别出所有有效的KV对后，它将执行以下步骤:</p>
<ul>
<li>(i)  将被覆盖的有效KV对以及元数据更新一起添加到GC日志中;</li>
<li>(ii) 将所有有效KV对写回段组;</li>
<li>(iii) 更新LSM-tree中的元数据;</li>
<li>(iv) 在日志中标记GC操作对应的记录为 free 状态。</li>
</ul>
</li>
<li>
<p>我们研究了崩溃一致性机制对HashKV性能的影响。表1显示了结果。当启用了崩溃一致性机制时，P3阶段的HashKV的更新吞吐量减少了6.5%，总写大小增加了4.2%，这表明崩溃一致性机制的影响仍然有限。请注意，我们在运行时通过代码注入和意外终止来使HashKV崩溃，从而验证崩溃一致性机制的正确性。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324221131.png" alt="20210324221131" loading="lazy"></p>
</li>
</ul>
<h3 id="implementation-details">Implementation Details</h3>
<ul>
<li>基于 LevelDB v1.20 实现</li>
<li><strong>Storage organization</strong>: 我们目前将HashKV部署在具有多个ssd的RAID阵列上，以获得更高的I/O性能。我们使用 mdadm 创建一个软件RAID卷，并将RAID卷挂载为Ext4文件系统，在该文件系统上运行LevelDB和值存储。特别地，HashKV将值存储管理为一个大文件。它根据预先配置的段大小，将value store文件划分为两个区域，一个用于主段，另一个用于日志段。所有的段在value store文件中对齐，这样每个 main/log 段的起始偏移量是 main 段大小的倍数。<strong>如果启用了热感知功能(见§3.4)，HashKV会在值存储文件中为冷数据日志添加一个单独的区域。此外，为了解决崩溃一致性(见3.7)，HashKV使用单独的文件来存储写和GC日志。</strong></li>
<li><strong>Multi-threading</strong>：<strong>HashKV通过线程池实现了多线程，当把写缓存中的KV对刷新到不同的段时(见§3.2)，并且在GC(见§3.3）过程中从段组中并行检索段，从而提高I/O性能</strong>。</li>
<li>为了减轻确定性分组带来的随机写开销(见§3.1)，HashKV实现了批量写。<strong>当HashKV在写缓存中刷新KV对时，它首先识别并缓冲一批被散列到同一个分段组中的KV对，然后(通过线程)发出一个顺序写来刷新批处理。更大的批处理大小减少了随机写开销，但它也降低了并行性。</strong> 目前，我们配置了一个批写入阈值，在批中添加KV对后，如果批大小达到或超过批大小阈值，批将被刷新;换句话说，如果一个KV对的大小大于批写阈值，HashKV就直接刷新它。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>我们评估了LevelDB (LDB)、RocksDB (RDB)、HyperLevelDB (HDB)、PebblesDB (PDB)、vLog和HashKV (HKV)在更新密集型工作负载下的性能。我们首先比较LevelDB, RocksDB, vLog和HashKV;之后，我们还将HyperLevelDB和pebblesdb纳入比较中。</li>
<li>图5(a)显示了每个阶段的性能。对于vLog和HashKV，加载阶段的吞吐量比更新阶段的吞吐量高，因为更新阶段主要由GC开销控制。在 load 阶段，hashkv的吞吐量分别比LevelDB和RocksDB大17.1×和3.0×。HashKV的吞吐量比vLog慢7.9%，这是由于通过哈希来分发KV对而引入了随机写。在更新阶段，HashKV的吞吐量在LevelDB、RocksDB和vLog上分别为6.3-7.9×、1.3-1.4×和3.7-4.6×。由于压缩开销很大，LevelDB的吞吐量在所有KV存储中是最低的，而vLog的GC开销也很高。</li>
<li>图5(b)和5(c)显示了总写大小和所有加载和更新请求发出后，不同KV存储的大小。HashKV将LevelDB、RocksDB和vLog的总写大小分别减少了71.5%、66.7%和49.6%。而且，它们有非常相似的 KV 存储大小。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210324221302.png" alt="20210324221302" loading="lazy"></li>
<li>对于 HyperLevelDB 和 peblesdb，两者都是如此，由于压缩开销较低，因此具有较高的负载和更新吞吐量。例如，PebblesDB将sstable的片段从较高的级别添加到较低的级别，而不重写较低级别的sstable。<strong>HyperLevelDB和PebblesDB都至少实现了HashKV的两倍吞吐量，同时产生的写大小也小于HashKV。</strong> 另一方面，<strong>它们的存储开销很大，其最终KV存储大小分别为2.2× HashKV和1.7× HashKV。主要原因是HyperLevelDB和PebblesDB都只对选定范围的键进行压缩，以减少写放大，这样在压缩后可能仍然会有很多无效的KV对。它们触发压缩操作的频率也低于LevelDB。这两个因素都会导致较高的存储开销。</strong> 在接下来的实验中，我们关注LevelDB、RocksDB、vLog和HashKV，因为它们的存储开销相当。</li>
<li><strong>写放大 PebblesDB/HyperLevelDB 做得更好，但空间放大 HASHKV 更好</strong></li>
</ul>
<h2 id="related-work">Related Work</h2>
<ul>
<li><strong>General KV stores</strong>:
<ul>
<li>DRAM: Redis, MemC3, NSDI13 Distributed Caching with Memcached, NSDI14 MICA</li>
<li>SSDs: FlashStore, SIGMOD11 SkimpyStash, SOSP11 SILT, FAST16 WiscKey</li>
<li>NVM: ATC15 NVMKV, ATC17 HiKV</li>
</ul>
</li>
<li><strong>LSM-tree-based KV stores</strong>
<ul>
<li>bLSM</li>
<li>VT-tree</li>
<li>LSM-trie</li>
<li>LWC-store</li>
<li>SkipStore</li>
<li>PebblesDB</li>
</ul>
</li>
<li><strong>KV separation</strong>
<ul>
<li>WiscKey</li>
<li>Atlas</li>
<li>Cocytus</li>
</ul>
</li>
<li><strong>Hash-based data organization</strong>
<ul>
<li>Dynamo</li>
<li>Kinesis</li>
<li>Ceph</li>
<li>NVMKV</li>
</ul>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>本文提出了HashKV算法，它能够在更新密集型工作负载下对KV存储进行有效的更新。它的新颖之处在于利用基于哈希的数据分组进行确定性数据组织，从而减轻GC开销。我们通过几个扩展进一步增强了HashKV，包括动态预留空间分配、热度感知和选择性KV分离。试验台实验表明，HashKV实现了较高的更新吞吐量，并减少了总写大小。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[C++ 多线程]]></title>
        <id>https://blog.shunzi.tech/post/cpp-multi-thread/</id>
        <link href="https://blog.shunzi.tech/post/cpp-multi-thread/">
        </link>
        <updated>2021-05-06T01:36:30.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>C++ 多线程封装，以及一些并发处理机制的资料</li>
<li>持续更新ing</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>C++ 多线程封装，以及一些并发处理机制的资料</li>
<li>持续更新ing</li>
</ul>
<!-- more -->
<h2 id="参考链接">参考链接</h2>
<ul>
<li><a href="https://blog.csdn.net/deng821776892/article/details/106984687/">[1] CSDB: C++多线程之旅-atomic原子类型</a></li>
<li><a href="https://www.runoob.com/cplusplus/cpp-multithreading.html">[2] 菜鸟教程：C++ 多线程</a></li>
<li><a href="http://www.cplusplus.com/reference/atomic/">[3] cplusplus.com - atomic</a></li>
<li><a href="http://shouce.jb51.net/cpp_concurrency_in_action/content/chapter5/5.2-chinese.html">[4] C++ 并发编程</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/24983412">[5] 知乎：C++11原子操作与无锁编程</a></li>
<li><a href="https://www.jianshu.com/p/8c1bb012d5f8">[6] 简书：c++11 多线程（3）atomic 总结</a></li>
<li><a href="https://www.cnblogs.com/haippy/p/3301408.html">[7] 博客园：C++11 并发指南六( <atomic> 类型详解二 std::atomic )</a></li>
<li><a href="https://support.huaweicloud.com/prtg-kunpengdbs/kunpengdbsolution_12_0002.html">[8] 华为云：常见原子操作（C语言）</a></li>
<li><a href="http://events.jianshu.io/p/a25e0754e2b9">[9] 简书：LevelDB 中的跳表实现</a></li>
</ul>
<h3 id="atomic">atomic</h3>
<ul>
<li>所谓原子操作，就是多线程程序中“最小的且不可并行化的”操作。对于在多个线程间共享的一个资源而言，这意味着同一时刻，多个线程中有且仅有一个线程在对这个资源进行操作，即互斥访问。</li>
<li>而关于原子性，我们应当具有一个基本的认知：<strong>高级语言层面，单条语句并不能保证对应的操作具有原子性</strong>。</li>
</ul>
<h4 id="举例说明">举例说明</h4>
<ul>
<li>在使用 C、C++、Java 等各种高级语言编写代码时，不少人会下意识的认为一条不可再分的单条语句具有原子性，例如常见 i++。</li>
</ul>
<pre><code class="language-C">// 伪码

int i = 0;

void increase() {
  i++;
}

int main() {
  /* 创建两个线程，每个线程循环进行 100 次 increase */
  // 线程 1
  Thread thread1 = new Thread(
    run() {
      for (int i = 0; i &lt; 100; i++) increase();
    }
  );
  // 线程 2
  Thread thread2 = new Thread(
    run() {
      for (int i = 0; i &lt; 100; i++) increase();
    }
  );
}
</code></pre>
<ul>
<li>如果 i++ 是原子操作，则上述伪码中的 i 最终结果为 200。但实际上每次运行结果可能都不相同，且通常小于 200。</li>
<li>之所以出现这样的情况是因为 <strong>i++ 在执行时通常还会继续划分为多条 CPU 指令</strong>。以 Java 为例，i++ 编译将形成四条字节码指令，以下四条指令是 Java 虚拟机中的字节码指令，字节码指令是 JVM 执行的指令。实际每条字节码指令还可以继续划分为更底层的机器指令。但字节码指令已经足够演示原子性的含义了。如下所示：
<ul>
<li><a href="https://blog.csdn.net/tanggao1314/article/details/53260891">CSDN：JVM指令集及各指令的详细使用说明</a></li>
</ul>
</li>
</ul>
<pre><code>// Java 字节码指令
0:  getstatic     # 获取静态属性指令，获取指定类的静态域，并将其值压入栈顶  
1:  iconst_1      # 当int取值-1~5时，JVM采用iconst指令将常量压入栈中，也就是将 1 压入栈中
2:  iadd          # 将栈顶两 int 型数值相加并将结果压入栈顶
3:  putstatic     # 为指定的类的静态域赋值
</code></pre>
<ul>
<li>而上述四条指令的执行并不保证原子性，即执行过程可被打断。考虑如下 CPU 执行序列：
<ol>
<li>线程 1 执行 getstatic 指令，获得 i = 1</li>
<li>CPU 切换到线程 2，也执行了 getstatic 指令，获得 i = 1。</li>
<li>CPU 切回线程 1 执行剩下指令，此时 i = 2</li>
<li>CPU 切到线程 2，由于步骤 2 读到的是 i = 1，固执行剩下指令最终只会得到 i = 2</li>
</ol>
</li>
</ul>
<h3 id="c-stdatomic">C++ std::atomic</h3>
<ul>
<li>std::atomic 为C++11封装的原子数据类型。原子数据类型不会发生数据竞争，能直接用在多线程中而不必我们用户对其进行添加互斥资源锁的类型。从实现上，大家可以理解为这些原子类型内部自己加了锁。</li>
<li>原子类型在头文件 <atomic> 中，使用atomic有两套命名模式：一种是使用替代名称，一种是使用atomic的特化。
<ul>
<li>atomic_bool &lt;=&gt; atomic &lt; bool &gt;</li>
<li>atomic_char &lt;=&gt; atomic&lt; char &gt;</li>
<li>atomic_int  &lt;=&gt; atomic&lt; int &gt;</li>
<li>...<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210513164522.png" alt="20210513164522" loading="lazy"></li>
</ul>
</li>
<li>并非所有的类型都能提供原子操作，这是因为原子操作的可行性取决于 CPU 的架构以及所实例化的类型结构是否满足该架构对内存对齐条件的要求，因而我们总是可以通过 std::atomic<T>::is_lock_free来检查该原子类型是否需支持原子操作。这个函数让用户可以查询某原子类型的操作是直接用的原子指令(x.is_lock_free()返回true)， 还是编译器和库内部用了一个锁(x.is_lock_free()返回false)。</li>
</ul>
<pre><code class="language-C++">template &lt; class T &gt; struct atomic {
    bool is_lock_free() const volatile;
    bool is_lock_free() const;
    void store(T, memory_order = memory_order_seq_cst) volatile;
    void store(T, memory_order = memory_order_seq_cst);
    T load(memory_order = memory_order_seq_cst) const volatile;
    T load(memory_order = memory_order_seq_cst) const;
    operator  T() const volatile;
    operator  T() const;
    T exchange(T, memory_order = memory_order_seq_cst) volatile;
    T exchange(T, memory_order = memory_order_seq_cst);
    bool compare_exchange_weak(T &amp;, T, memory_order, memory_order) volatile;
    bool compare_exchange_weak(T &amp;, T, memory_order, memory_order);
    bool compare_exchange_strong(T &amp;, T, memory_order, memory_order) volatile;
    bool compare_exchange_strong(T &amp;, T, memory_order, memory_order);
    bool compare_exchange_weak(T &amp;, T, memory_order = memory_order_seq_cst) volatile;
    bool compare_exchange_weak(T &amp;, T, memory_order = memory_order_seq_cst);
    bool compare_exchange_strong(T &amp;, T, memory_order = memory_order_seq_cst) volatile;
    bool compare_exchange_strong(T &amp;, T, memory_order = memory_order_seq_cst);
    atomic() = default;
    constexpr atomic(T);
    atomic(const atomic &amp;) = delete;
    atomic &amp; operator=(const atomic &amp;) = delete;
    atomic &amp; operator=(const atomic &amp;) volatile = delete;
    T operator=(T) volatile;
    T operator=(T);
};
</code></pre>
<h4 id="主要操作">主要操作</h4>
<h5 id="load">load()</h5>
<ul>
<li>加载原子对象中存入的值，等价于直接使用原子变量。</li>
</ul>
<pre><code class="language-C++">T load (memory_order sync = memory_order_seq_cst) const volatile noexcept;
T load (memory_order sync = memory_order_seq_cst) const noexcept;
</code></pre>
<ul>
<li>读取被封装的值，参数 sync 设置内存序(Memory Order)，可能的取值如下：
<ul>
<li>memory_order_relaxed</li>
<li>memory_order_consume</li>
<li>memory_order_acquire</li>
<li>memory_order_seq_cst</li>
</ul>
</li>
</ul>
<h5 id="store">store()</h5>
<ul>
<li>存储一个值到原子对象，等价于使用等号。</li>
</ul>
<pre><code class="language-C++">void store (T val, memory_order sync = memory_order_seq_cst) volatile noexcept;
void store (T val, memory_order sync = memory_order_seq_cst) noexcept;
</code></pre>
<ul>
<li>修改被封装的值，std::atomic::store 函数将类型为 T 的参数 val 复制给原子对象所封装的值。T 是 std::atomic 类模板参数。另外参数 sync 指定内存序(Memory Order)，可能的取值:
<ul>
<li>memory_order_relaxed</li>
<li>memory_order_release</li>
<li>memory_order_seq_cst</li>
</ul>
</li>
</ul>
<h5 id="exchange-原子操作">exchange() 原子操作</h5>
<ul>
<li>返回原来里面存储的值，然后在存储一个新的值，相当于将上面两个 load() 和 store() 合成起来的参数。</li>
</ul>
<pre><code class="language-C++">T exchange (T val, memory_order sync = memory_order_seq_cst) volatile noexcept;
T exchange (T val, memory_order sync = memory_order_seq_cst) noexcept;
</code></pre>
<ul>
<li>读取并修改被封装的值，exchange 会将 val 指定的值替换掉之前该原子对象封装的值，并返回之前该原子对象封装的值，整个过程是原子的(因此exchange 操作也称为 read-modify-write 操作)。sync参数指定内存序(Memory Order)，可能的取值如下：
<ul>
<li>memory_order_relaxed</li>
<li>memory_order_consume</li>
<li>memory_order_acquire</li>
<li>memory_order_release</li>
<li>memory_order_acq_rel</li>
<li>memory_order_seq_cst</li>
</ul>
</li>
</ul>
<h5 id="compare_exchange_weak">compare_exchange_weak()</h5>
<ul>
<li>交换-比较操作是比较原子变量值和所提供的期望值，如果二者相等，则存储提供的期望值，如果不等则将期望值更新为原子变量的实际值，更新成功则返回true反之则返回false。</li>
</ul>
<pre><code class="language-C++">atomic&lt;bool&gt; b;
b.compare_exchange_weak(expected, new_value);
</code></pre>
<ul>
<li>当存储的值和expected相等时则将则更新为new_value，如果不等时则不变。其中expected必须是类型变量，而不能是常量。</li>
</ul>
<h5 id="compare_exchange_strong">compare_exchange_strong()</h5>
<ul>
<li>不像compare_exchange_weak，此版本必须始终true在预期确实与所包含的对象相等时返回，不允许出现虚假故障。但是，在某些计算机上，对于某些在循环中进行检查的算法，compare_exchange_weak 可能会明显改善性能。</li>
<li>其余使用方法和compare_exchange_weak完全一致。</li>
<li>这两个版本的区别是：Weak版本如果数据符合条件被修改，其也可能返回false，就好像不符合修改状态一致；而Strong版本不会有这个问题，但在某些平台上Strong版本比Weak版本慢 [注:在x86平台我没发现他们之间有任何性能差距]；绝大多数情况下，我们应该优先选择使用Strong版本；</li>
</ul>
<h4 id="memory-order">memory order</h4>
<ul>
<li>在介绍 Memory Order 之前首先介绍 <strong>有序性</strong></li>
</ul>
<h5 id="有序性">有序性</h5>
<ul>
<li>上述已经提到 CPU 的一条指令执行时，通常会有多个步骤，如取指IF 即从主存储器中取出指令、ID 译码即翻译指令、EX 执行指令、存储器访问 MEM 取数、WB 写回。</li>
<li>即指令执行将经历：<strong>IF、ID、EX、MEM、WB</strong> 阶段。</li>
<li>现在考虑 CPU 在执行一条又一条指令时该如何完成上述步骤？最容易想到并是顺序串行，指令 1 依次完成上述五个步骤，完成之后，指令 2 再开始依次完成上述步骤。这种方式简单直接，但执行效率显然存在很大的优化空间。</li>
<li>思考一种流水线工作：</li>
</ul>
<pre><code>指令1   IF ID EX MEM WB
指令2      IF ID EX MEM WB
指令3         IF ID EX MEM WB
</code></pre>
<ul>
<li>采用这种流水线的工作方式，将避免 CPU 、存储器中各个器件的空闲，从而充分利用每个器件，提升性能。同时注意到由于每条指令执行的情况有所不同，指令执行的先后顺序将会影响到这条流水线的负载情况，而我们的目标则是让整个流水线满载紧凑的运行。</li>
<li>为此 CPU 又实现了「指令重排」技术，CPU 将有选择性的对部分指令进行重排来提高 CPU 执行的性能和效率。例如：</li>
</ul>
<pre><code class="language-C">x = 100;    // #1
y = 200;    // #2
z = x + y;  // #3
</code></pre>
<ul>
<li>虽然上述高级语言的语句会编译成多条机器指令，多条机器指令还会进行「指令重排」，#1 语句与 #2 语句完全有可能被 CPU 重新排序，所以程序实际运行时可能会先执行 y = 200; 然后再执行 x = 100;</li>
<li>但另一方面，指令重排的前提是不会影响线程内程序的串行语义，CPU 在重排指令时必须保证线程内语义不变，例如：</li>
</ul>
<pre><code class="language-C">x = 0; // #1
x = 1; // #2
y = x; // #3
</code></pre>
<ul>
<li>上述的 y 一定会按照正常的串行逻辑被赋值为 1。</li>
<li>但不幸的是，CPU 只能保证线程内的串行语义。在多线程的视角下，「指令重排」造成的影响需要程序员自己关注。</li>
</ul>
<pre><code class="language-C">
// 公共资源
int x = 0;
int y = 0;
int z = 0;

Thread_1:             Thread_2:
x = 100;              while (y != 200);
y = 200;              print x
z = x + y;
</code></pre>
<ul>
<li>如果 CPU 不进行「乱序优化」执行，那么 y = 200 时，x 已经被赋值为 100，此时线程 2 输出 x = 200。但实际运行时，线程 1 可能先执行 y = 200，此时 x 还是初始值 0。线程 2 观察到 y = 200 后，退出循环，输出 x = 0;</li>
</ul>
<h5 id="memory-order-2">memory order</h5>
<ul>
<li>C++ 提供了 std::atomic 类模板，以保证操作原子性。同时也提供了内存顺序模型 memory_order指定内存访问，以便提供有序性和可见性。
<ul>
<li><strong>memory_order_relaxed</strong>: 只保证原子操作的原子性，不提供有序性的保证</li>
<li><strong>memory_order_consume</strong>	当前线程中依赖于当前加载的该值的读或写不能被重排到此加载前</li>
<li><strong>memory_order_acquire</strong>	在其影响的内存位置进行获得操作：当前线程中读或写不能被重排到此加载前</li>
<li><strong>memory_order_release</strong>	当前线程中的读或写不能被重排到此存储后</li>
<li><strong>memory_order_acq_rel</strong>	带此内存顺序的读修改写操作既是获得操作又是释放操作</li>
<li><strong>memory_order_seq_cst</strong>	有此内存顺序的加载操作进行获得操作，存储操作进行释放操作，而读修改写操作进行获得操作和释放操作，再加上存在一个单独全序，其中所有线程以同一顺序观测到所有修改</li>
</ul>
</li>
<li>组合出四种顺序：
<ul>
<li><strong>Relaxed ordering 宽松顺序</strong>：宽松顺序只保证原子变量的原子性（变量操作的机器指令不进行重排序），但无其他同步操作，不保证多线程的有序性。</li>
</ul>
<pre><code class="language-C">Thread1: 
y.load(std::memory_order_relaxed);

Thread2:
y.store(h, std::memory_order_relaxed);
</code></pre>
<ul>
<li><strong>Release-Acquire ordering 释放获得顺序</strong>，store 使用 memory_order_release，load 使用 memory_order_acquire，CPU 将保证如下两点：
<ul>
<li>store 之前的语句不允许被重排序到 store 之后（例子中的 #1 和 #2 语句一定在 store 之前执行）</li>
<li>load 之后的语句不允许被重排序到 load 之前（例子中的 #3 和 #4 一定在 load 之后执行）</li>
</ul>
</li>
</ul>
<pre><code class="language-C++">std::atomic&lt;std::string*&gt; ptr;
int data;

void producer()
{
    std::string* p  = new std::string(&quot;Hello&quot;); // #1
    data = 42;  // #2
    ptr.store(p, std::memory_order_release);
}

void consumer()
{
    std::string* p2;
    while (!(p2 = ptr.load(std::memory_order_acquire)))
        ;
    assert(*p2 == &quot;Hello&quot;); // 绝无问题 #3
    assert(data == 42); // 绝无问题 #4
}

int main()
{
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join(); t2.join();
}
</code></pre>
<ul>
<li>同时 CPU 将保证 store 之前的语句比 load 之后的语句「先行发生」，即先执行 #1、#2，然后执行 #3、#4。这实际上就意味着线程 1 中 store 之前的读写操作对线程 2 中 load 执行后是可见的。<strong>注意是所有操作都同步了，不管 #3 是否依赖了 #1 或 #2</strong></li>
<li>值得关注的是这种顺序模型在一些强顺序系统例如 x86、SPARC TSO、IBM 主框架上是自动进行的。但在另外一些系统如 ARM、Power PC 等需要额外指令来保障。</li>
<li><strong>Release-Consume ordering 释放消费顺序</strong>:store 使用 memory_order_release，load 使用 memory_order_consume。其效果与 Release-Acquire ordering 释放获得顺序类似，唯一不同的是并不是所有操作都同步（不够高效），而是只对依赖操作进行同步，保证其有序性上例就是 #3 一定发生在 #1 之后，因为这两个操作依赖于 ptr。但不会保证 #4 一定发生在 #2 之后（注意「释放获得顺序」可以保证这一点）。</li>
</ul>
<pre><code class="language-C++">std::atomic&lt;std::string*&gt; ptr;
int data;

void producer()
{
    std::string* p  = new std::string(&quot;Hello&quot;); // #1
    data = 42; // #2
    ptr.store(p, std::memory_order_release);
}

void consumer()
{
    std::string* p2;
    while (!(p2 = ptr.load(std::memory_order_consume)))
        ;
    assert(*p2 == &quot;Hello&quot;); // #3 绝无出错： *p2 从 ptr 携带依赖
    assert(data == 42); // #4 可能也可能不会出错： data 不从 ptr 携带依赖
}

int main()
{
    std::thread t1(producer);
    std::thread t2(consumer);
    t1.join(); t2.join();
}
</code></pre>
<ul>
<li><strong>Sequential consistency 序列一致顺序</strong>: 「释放获得顺序」是对某一个变量进行同步，Sequential consistency 序列一致顺序则是对所有变量的所有操作都进行同步。store 和 load 都使用 memory_order_seq_cst，可以理解对每个变量都进行 Release-Acquire 操作。所以这也是最慢的一种顺序模型。</li>
</ul>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[FloDB: Unlocking Memory in Persistent Key-Value Stores]]></title>
        <id>https://blog.shunzi.tech/post/FloDB/</id>
        <link href="https://blog.shunzi.tech/post/FloDB/">
        </link>
        <updated>2021-04-09T04:21:02.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>EuroSys17: FloDB: Unlocking Memory in Persistent Key-Value Stores</li>
<li>https://dcl.epfl.ch/site/flodb</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>EuroSys17: FloDB: Unlocking Memory in Persistent Key-Value Stores</li>
<li>https://dcl.epfl.ch/site/flodb</li>
</ul>
</blockquote>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>日志结构合并(LSM)数据存储允许存储和处理大量数据，同时保持良好的性能。它们通过吸收内存层中的更新并按顺序分批地将它们传输到磁盘层来缓解I/O瓶颈。然而，LSM体系结构基本上要求元素按排序顺序排列。随着内存中数据量的增长，维护这种排序顺序的成本越来越高。与直觉相反，现有的LSM系统在使用较大的内存组件时实际上可能会损失吞吐量。</li>
<li>在本文中，我们介绍了FloDB，一种LSM内存组件架构，它允许吞吐量在具有充足内存大小的现代多核机器上扩展。<strong>FloDB的主要思想本质上是通过在内存组件上添加一个小的内存缓冲层来引导传统的LSM体系结构</strong>。该缓冲区提供低延迟操作，屏蔽排序内存组件的写延迟。将这个缓冲区集成到经典的LSM内存组件中以变成 FloDB 并非易事，需要重新访问面向用户的 LSM 操作(搜索、更新、扫描)的算法。FloDB 的两层可以用最先进的、高并发的数据结构来实现。通过这种方式，正如我们在本文中所展示的那样，FloDB 消除了经典 LSM 设计中显著的同步瓶颈，同时提供了一个丰富的 LSM API。</li>
<li>我们实现FloDB作为LevelDB的扩展，谷歌流行的LSM键值存储。我们将FloDB的性能与最先进的LSMs进行比较。简而言之，在各种多线程工作负载下，FloDB的性能比性能仅次于它的竞争对手高出一个数量级。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>键值存储是许多系统的关键组件，这些系统需要对大量数据进行低延迟的访问。这些存储的特点是扁平的数据组织和简化的接口，这允许有效的实现。然而，键值存储的目标数据量通常大于主存;因此，通常需要持久化存储。由于访问持久存储的速度比 CPU 慢，直接在磁盘上更新数据产生了一个严重的瓶颈。所以很多 KV 存储系统使用了 LSM 结构。LSM 数据存储适用于需要低延迟访问的应用程序，例如需要进行大量更新的消息队列，以及在面向用户的应用程序中维护会话状态。基本上，LSM 体系结构一方面通过缓存读，另一方面通过在内存中吸收写并稍后批量写入磁盘，掩盖了磁盘访问瓶颈。<strong>尽管 LSM 键值存储在解决I/O瓶颈带来的挑战方面发挥了很大的作用，但它们的性能不会随着内存组件的大小而扩展，也不会随着线程的数量而扩展</strong>。换句话说，可能令人惊讶的是，<strong>增加现有lsm的内存部分只能在相对较小的尺寸下提高性能。类似地，添加线程并不能提高许多现有lsm的吞吐量，因为它们使用了全局阻塞同步</strong>。</li>
<li>上面描述的两种限制在 LSM 的传统设计中是固定存在的。我们通过引入 FloDB 来规避这些限制，FloDB 是一种新颖的 LSM 内存组件，其设计目的是随线程数量及其在内存中的大小而扩展。传统上，LSMs 一直采用两层存储层次结构，一层在内存中，一层在磁盘上。我们建议增加一个内存级别。换句话说，<strong>FloDB 是一个位于磁盘组件之上的两级内存组件(图1)，每个in-memory 级别都是一个并发数据结构。内存顶层是一个小而快速的数据结构，而内存底层是一个更大的、排序的数据结构</strong>。在存储到磁盘之前，新条目被插入到顶层，然后在后台被排到底层。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210330151553.png" alt="20210330151553" loading="lazy"></li>
<li>这个方案有几个优点：
<ul>
<li>首先，它允许扫描和写入并行进行，分别在底层和顶层。</li>
<li>其次，无论内存组件大小如何，使用小型、快速的顶级级别都可以实现低延迟的更新，而现有的 LSM 会随着内存组件大小的增加而性能下降。更大的内存组件可能在峰值吞吐量时造成更长的写突发。</li>
<li>第三，维护底层内存层的排序允许在不进行额外的(昂贵的)排序步骤的情况下对磁盘进行刷新。</li>
</ul>
</li>
<li>我们使用一个<strong>小型的高性能并发哈希表来实现 FloDB</strong>，并使用一个<strong>较大的并发跳表</strong>来实现底部的内存级别。乍一看，实现 FloDB 似乎只需要在现有的 LSM 体系结构上添加一个额外的基于哈希表的缓冲区级别。然而，这个看似很小的步骤却带来了微妙的技术挑战。
<ul>
<li>第一个挑战是<strong>确保两个内存级别之间的有效数据流动</strong>，以便充分利用额外的缓冲区，同时不耗尽系统资源。为此，我们引入了多插入操作，这是一种用于并发跳表的新操作。其<strong>主要思想是在一个操作中在跳跃列表中插入 n 个排序的元素，使用前面插入元素的位置作为插入下一个元素的起点，从而重用已经遍历过的 hops</strong>。Skiplist 多插入是独立的，通过增加内存组件更新的吞吐量，它也可以使以前的单写入器 LSM 实现受益。</li>
<li>第二个挑战是<strong>确保面向用户的 LSM 操作的一致性</strong>，同时在这些操作之间启用高级别的并发性。特别是，FloDB 是第一个同时支持一致扫描和就地更新的LSM系统。</li>
</ul>
</li>
<li>我们的实验表明，FloDB的性能优于当前的键值存储解决方案，特别是在写密集型场景中。例如，在只写的工作负载中，FloDB 可以用一个工作线程来饱和磁盘组件的吞吐量，并且在最多有16个工作线程的情况下，它的性能继续超过性能第二好的系统，平均是后者的2倍。此外，对于倾斜的读写工作负载，FloDB 获得比性能最高的竞争对手高一个数量级的吞吐量：
<ul>
<li>FloDB，一个用于 log-structured merge 内存组件的两级架构。FloDB 可以根据主内存的大小进行扩展，并且具有丰富的API，读取、写入和扫描都可以并发进行。</li>
<li>在Xeon多核机器上，作为LevelDB的扩展，FloDB体系结构的一个公开的c++实现，以及一个高达192GB内存组件大小的实验。</li>
<li>一种用于并行跳过表的新型多插入操作的算法。除了FloDB，多插入还可以使任何使用skiplist 的 LSM 体系结构受益</li>
</ul>
</li>
<li><strong>尽管 FloDB 有很多优点，但我们并不认为它是一剂灵丹妙药;它确实有局限性</strong>。
<ul>
<li>首先，与所有lsm一样，稳态吞吐量受到层次结构中最慢组件的限制:将内存组件写入磁盘。FloDB在内存组件中所做的改进与潜在的磁盘级改进是正交的，这超出了本文的讨论范围。磁盘级别的改进可以与FloDB结合使用，进一步提高LSMs的总体性能。</li>
<li>其次，可以设计出对FloDB扫描有问题的工作负载。例如，在写密集型工作负载和严重争用中，长时间运行扫描的性能会下降。第三，对于比主存大得多的数据集，FloDB提高的读性能要小于写性能。这是因为在LSMs中，读取性能很大程度上取决于缓存的有效性，这也不在我们的论文讨论范围之内。</li>
</ul>
</li>
</ul>
<h2 id="shortcomings-of-current-lsms">Shortcomings of Current LSMs</h2>
<h3 id="lsm-key-value-store-overview">LSM Key-Value Store Overview</h3>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210330155255.png" alt="20210330155255" loading="lazy"></figure>
<h3 id="limitationscalability-with-number-of-threads">Limitation—Scalability with Number of Threads</h3>
<ul>
<li>多个处理核心的存在可以提高LSM数据存储的性能。虽然现有的LSM系统允许某种程度的并发，但它们仍留有大量的并行机会。例如，
<ul>
<li>leveldb——许多LSM键值存储的基础——支持多个写线程，但通过让线程将其预期的写操作存储在一个并发队列中来序列化写操作;这个队列中的写操作由单个线程一个接一个地应用到键值存储。此外，LevelDB还要求读取器在每次操作中获取全局锁，以便访问或更新元数据</li>
<li>HyperLevelDB 建立在LevelDB之上，通过允许并发更新来提高并发性;然而，为了通过版本号来维护更新的顺序，仍然需要昂贵的同步</li>
<li>RocksDB 也是源自LevelDB的一个键值存储，它通过引入磁盘组件的多线程合并来提高并发性。虽然多线程压缩确实提高了整体性能，但RocksDB仍然保持全局同步来访问内存结构和更新版本号，这与HyperLevelDB类似</li>
<li>cLSM 甚至更进一步，从只读路径上删除了任何阻塞的同步，但是使用全局共享独占锁来协调更新和后台磁盘写操作，仍然削弱了系统的可伸缩性。正如我们在第5节中所展示的，这些可伸缩性瓶颈确实在实践中表现出来</li>
</ul>
</li>
</ul>
<h3 id="limitationscalability-with-memory-size">Limitation—Scalability with Memory Size</h3>
<ul>
<li>现有的LSM内存组件可以排序(例如，skiplist)或未排序的(例如，哈希表)。这两种选择都有各自的优点和缺点，但令人惊讶的是，它们都不能扩展到大型内存组件。</li>
<li>一方面，当使用skiplist时，顺序扫描是自然的。而且，压缩阶段只不过是将组件直接复制到磁盘;因此它的开销很低。然而，写需要数据结构大小的对数时间来维持排序顺序(直接从内存组件读取也需要数据结构大小的对数时间。然而，对于大型数据集，大多数读操作都是从磁盘组件上读取的，因此<strong>内存组件数据结构的选择对读延迟的影响小于写延迟</strong>)。因此，从图3中可以看出，分配更多的内存实际上增加了写延迟。该图显示了作为RocksDB(最流行的最先进的lsm之一)中内存组件大小的函数的读和写延迟的中值。在第99百分位，我们观察到类似的读写延迟趋势。延迟使用RocksDB的读写基准测试，有八个读线程和一个写线程在一个有100万个条目的数据库上运行。密钥大小为8字节，值大小为256字节。延迟被规范化为128 MB内存组件。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210330161949.png" alt="20210330161949" loading="lazy"></li>
<li>另一方面，对于哈希表，写操作在常量时间内完成，但是顺序扫描是不实用的，而且压缩阶段更复杂。压缩在将内存组件写入磁盘之前需要对其进行完全排序，以保存LSM结构。实际上，我们的测量结果表明，基于哈希表的内存组件的平均压缩时间至少比相同大小的基于skiplist的内存组件的平均压缩时间高一个数量级：因为随着哈希表变得越来越大，排序和持久化到磁盘所需的时间也越来越长。在对不可变内存组件进行排序和持久化时，活动(可变)内存组件也可能被填满。在这种情况下，writer 被延迟，因为在内存中没有空间来完成它们的操作。结果，端到端写延迟随着内存大小而增加，如图4所示，图4展示了与图3相同的实验，使用的是哈希表而不是跳过列表。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210330163009.png" alt="20210330163009" loading="lazy"></li>
<li>因此，对于跳过列表和哈希表，<strong>端到端系统吞吐量随着内存的增加而趋于稳定，甚至下降。这种限制是传统 LSM 的单级内存组件固有的，限制了 LSM 用户利用现代多核中的丰富内存</strong>。在接下来的部分中，我们将展示将哈希表和跳过表的优点结合起来是可能的，从而提高 LSMs的写吞吐量，同时仍然允许 inorder 扫描。</li>
</ul>
<h2 id="flodb-design">FloDB Design</h2>
<ul>
<li>现有的 LSM 存在固有的可伸缩性问题，包括内存大小和线程数量方面的问题。后者是由可伸缩性瓶颈引起的，而前者则源于大小-延迟的权衡。</li>
<li>这种权衡体现在排序和未排序的内存组件上。有序的组件允许扫描，并且可以直接持久化到磁盘，但是随着内存组件变大，其内存组件对应就会有显著更高的访问次数。未排序组件的速度可能与大小无关，但对于扫描来说并不实用，并且需要在被刷新到磁盘之前排序，需要耗费线性时间，这可能会延迟 writers。FloDB 的内存组件体系结构就是为了避免这些问题而设计的。FloDB 的主要目标是:
<ul>
<li>(1) 根据给定的内存量进行扩展;</li>
<li>(2) 使用最小的同步(以扩大规模);</li>
<li>(3) 利用内存组件提高写性能，而不牺牲读性能。</li>
</ul>
</li>
<li>下面，我们将概述FloDB的内存组件架构，以及利用这个新结构的主要操作:Get、Put、Delete和Scan。</li>
</ul>
<h3 id="in-memory-levels">In-memory Levels</h3>
<ul>
<li>简而言之，FloDB的基本思想是使用两层内存组件，其中一层是小型、快速的数据结构，第二层是大型、排序的数据结构。这种设计允许FloDB打破大小延迟的平衡，并最小化同步。</li>
<li>第一级称为 Membuffer，它又小又快，但不一定要排序。第二级称为Memtable，规模更大，以便捕获更大的工作集，从而更好地屏蔽高I/O延迟。此外，Memtable 保持元素的排序，所以它可以直接刷新到磁盘。<strong>这两个级别都是并发的数据结构，使得操作可以并行进行</strong>。与其他LSMs 类似，数据从最小的组件(Membuffer)流向最大的组件(磁盘)，因为各个级别都满了。</li>
<li>磁盘级组件不是我们的重点，也不在本文的讨论范围之内。由于磁盘组件和内存中对数据的处理在 LSM 键值存储中是相互正交的，所以我们展示的内存中优化方法可以与任何持久化到磁盘的机制一起使用。例如，FloDB的内存组件可以与类似于 RocksDB 的多线程压缩方案相结合，或者像LSM-trie中那样，可以减少写放大，从而获得更好的磁盘结构。</li>
</ul>
<h3 id="operations">Operations</h3>
<ul>
<li>我们给出了FloDB主要操作的高层设计。在第4.4节中，我们将描述该设计的具体实现。FloDB的两层内存组件允许非常简单的基本操作(即Get、Put、Delete)，但在扫描的情况下会带来额外的复杂性。</li>
</ul>
<h4 id="get">Get</h4>
<ul>
<li>除了LSM数据结构中的同步外，FloDB中的Get操作不需要同步。首先搜索的是Membuffer，然后是Memtable，最后是磁盘。如果在某个级别找到所需的元素，则read可以立即返回。显然，一旦找到了元素，就没有必要在较低的层次中进行搜索，因为层次结构中的较高层次总是包含最新的值。</li>
</ul>
<h4 id="update">Update</h4>
<ul>
<li>Put和Delete操作本质上是相同的。删除是通过插入一个特殊的tombstone值来完成的。从现在开始，我们将把Put和Delete操作都称为Update。首先，<strong>在Membuffer中尝试更新。如果Membuffer中没有空间，则直接在Memtable中进行更新。如果键已经存在于Membuffer或Memtable中，相应的值将就地更新</strong>。</li>
<li>就地更新的另一种方法是多版本化:保留相同键的多个版本，仅在压缩阶段丢弃旧版本。所有现有的 LSMs 都使用多版本控制。然而，多版本控制方法不能利用倾斜工作负载的局部性。事实上，不断地更新一个键就足以填满内存组件并触发频繁刷新磁盘。相比之下，在就地更新中，重复写同一个键不会占用额外的内存，因此内存中的存储是按照数据大小的顺序排列的。就地更新，结合一个大型内存组件，允许我们有效地捕获大型的、写密集型的、倾斜的工作负载，如第5节所示。</li>
</ul>
<h4 id="scan">Scan</h4>
<ul>
<li>我们的扫描算法背后的<strong>主要思想是扫描Memtable和磁盘的同时允许在Membuffer中完成并发更新</strong>。这种方法的一个挑战是，扫描Memtable 可能返回一个过时的键值，当扫描开始时，这个键仍然在Membuffer 中。我们通过在扫描前清空 Memtable 中的 MemBuffer 来解决这个问题。</li>
<li>另一个挑战是，如果扫描需要很长时间才能完成，如果经常调用扫描，或者在扫描期间有很多线程执行更新，那么吸收更新的 Membuffer 可能会被填满。在这种情况下，我们允许写入器和扫描器在 Memtable 中并行进行。然而，如果允许写入者在扫描 A 期间天真地更新 Memtable，那么在 A 的范围内的一个条目可能会在 A 进行时被修改，导致不一致。我们<strong>通过在 Memtable 级别引入每个条目的序列号来解决这个问题</strong>。通过这种方式，扫描 A 可以验证自 A 启动以来是否在其范围内写入了 Memtable 中的新值；如果是这种情况，扫描 A 将失效并重新启动。A 的 fallback 机制来确保 liveness(即，扫描不会被 writers 无限期重启)。值得注意的是，我们使用序列号的方式与上面提到的多版本控制不同;在我们的算法中，当 Memtable 中存在的键 k 发生更新时，k 的值和序列号就地更新 （<strong>这里感觉讲的不是很清楚，有些含糊</strong>）</li>
</ul>
<h4 id="summary">Summary</h4>
<ul>
<li>FloDB 的两级设计有几个好处。
<ul>
<li>首先，很大一部分写操作是在Membuffer 中完成的，因此 FloDB 从快速内存组件(通常为整体未排序的内存组件保留，如哈希表)中获得好处。</li>
<li>第二，排序的底部组件允许扫描，并可以直接刷新到磁盘。</li>
<li>第三，级别的分离使写和读能够与扫描并行进行。</li>
<li>两层层次结构的另一个好处是，可以增加内存组件的总体大小，以获得更好的性能(与现有系统相比)。</li>
<li>最后，我们的设计允许就地更新，同时支持一致扫描</li>
</ul>
</li>
</ul>
<h2 id="flodb-implementation">FloDB Implementation</h2>
<ul>
<li>我们在谷歌的 LevelDB 上实现FloDB。LevelDB的内存组件完全被FloDB架构所取代。我们保留了 LevelDB 的持久化和压缩机制。</li>
</ul>
<blockquote>
<p>LevelDB中的原始方法是在内存中保留 thread-local 版本和文件描述符缓存(fd-cache)的一个共享版本，并在必要时获取一个全局锁来访问fd-cache的共享版本。在我们的初步测试中，我们发现这个全局锁是一个主要的可伸缩性瓶颈。为了消除这个瓶颈，作为内存优化的一部分，我们将LevelDB fd-cache实现替换为一个更可伸缩、并发的哈希表</p>
</blockquote>
<ul>
<li>在下面,我们将讨论的关键实现细节FloDB:
<ul>
<li>(1) 对于 Memtable 和 Membuffer 数据结构的选择</li>
<li>(2) 机制将用于 level 之间的数据移动</li>
<li>(3) 我们的新型 multi-insert 操作用于简化在 Memtable 和 Membuffer 之间的数据流</li>
<li>(4) 面向用户的操作的实现</li>
</ul>
</li>
</ul>
<h3 id="memory-component-implementation">Memory Component Implementation</h3>
<ul>
<li>在实现FloDB体系结构时，需要解决的一个重要问题是，在内存级别上如何选择良好的数据结构。为了使写操作尽可能快，第一级的合适选择是哈希表。如图5所示，一个现代哈希表可以提供超过100 Mops/s的吞吐量，即使有10亿个条目的工作负载。然而，即使哈希表很快，它们也不会对它们的条目进行排序，这意味着不能直接对数据进行顺序迭代。由于这个原因，保持数据有序且易于迭代的数据结构(例如在传统LSM实现中已经用作内存组件的skiplist)是第二级的良好选择。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331115947.png" alt="20210331115947" loading="lazy"></li>
<li>因此，FloDB的内存层次结构由一个小型的高性能并发哈希表和一个较大的并发跳表组成。哈希表存储键-值元组。skiplist存储用于扫描操作的键-值元组和序列号。</li>
<li><strong>Membuffer和Memtable之间的大小比例是一种权衡</strong>。一方面，相对较小的Membuffer会更快地填满，迫使更多的更新在Memtable中完成。这是有问题的，因为Memtable较慢，但也因为Memtable绑定的更新可能会迫使更多的扫描重新启动。另一方面，一个大的Membuffer将花费更长的时间来进入Memtable，这可能会延迟扫描。</li>
</ul>
<h3 id="interaction-between-levels">Interaction Between Levels</h3>
<ul>
<li><strong>Background threads</strong>：因为我们不希望数据是静态的(也就是说，数据不断地被插入或更新)，所以FloDB有两种机制来跨层次结构的级别移动数据:<strong>持久化和draining</strong>。
<ul>
<li>持久化通过一个专门的后台线程将项从Memtable移动到磁盘——这是LSM实现中的一种既定技术。当Memtable被填满时，持久化被触发。</li>
<li>Draining 将条目从Membuffer移动到Memtable，这是由一个或多个专门的后台线程完成的。Draining 是一个持续不断的过程，因为人们希望保持尽可能低的 Membuffer 占用率。<strong>实际上，只有在 Membuffer 中完成写操作时，才会从两级层次结构中获益</strong>。</li>
</ul>
</li>
<li><strong>Persisting</strong>：在LSMs中，将内存组件持久化到磁盘的一种标准技术是使该组件不可变，安装一个新的、可变的组件，并在后台将旧组件写入磁盘。通过这种方式，当旧组件被持久化时，写入者仍然可以继续处理新组件，并且不可变组件中的数据对读者仍然是可见的。然而，内存组件之间的切换通常是使用锁定完成的。<strong>FloDB有一个更高效的RCU（Read-Copy-Update）方法来切换内存组件，它不会阻塞任何更新或读取</strong>。当持久化的时候，RCU 是用来确保在后台线程将Memtable复制到磁盘之前，所有对不可变Memtable的待处理的更新都已经完成。第二，在Memtable被复制到磁盘之后，我们使用RCU来确保在后台线程可以继续释放不可变Memtable之前，没有reader线程正在读取它。</li>
<li><strong>Draining</strong>：Draining(图6)是由一个或多个专门的后台线程与更新、读取或其他Draining同时进行的，并按照以下步骤进行。要将一个key-value 条目 e 从 Membuffer 移动到 Memtable 中，e 首先被检索并在 Membuffer 中标记。这样做是为了确保没有其他后台线程也试图将 e 移动到 Memtable 中。然后，e 被分配一个序列号(通过原子递增操作获得)，并被后台线程插入到 Memtable 中。最后，e 从 Membuffer 中移除。有一种特殊情况，扫描开始时发生 Draining，为了使扫描能够只在Memtable和磁盘级别上进行，但仍然包括Membuffer中最近的更新，在扫描开始时将Membuffer中所有的内存都 Draining 到Memtable中。<strong>这种类型的 Draining 是通过使当前的 Membuffer 不可变，安装一个新的 Membuffer(使用RCU)，然后将旧的 Membuffer 中的所有条目移动到 Memtable 中，类似于Memtable是如何持久化到磁盘上的</strong>。</li>
<li><strong>RCU 的本质是转变数据可写状态，然后创建新的空的数据结构</strong><br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331144829.png" alt="20210331144829" loading="lazy"></li>
</ul>
<h3 id="skiplist-multi-inserts">Skiplist Multi-inserts</h3>
<ul>
<li>图5和图7显示了一个并发跳表大约比一个相同大小的并发哈希表慢一到两个数量级。因此，为了使大量的更新可以直接在哈希表中进行，我们需要尽可能快地在各级之间移动项。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331144936.png" alt="20210331144936" loading="lazy"></li>
</ul>
<h4 id="intuition">Intuition</h4>
<ul>
<li>我们为并发跳过表引入了一种新的多插入操作，以增加 Draining 线程的吞吐量。多插入操作背后的直观感觉很简单。在跳过列表中插入 n 个元素，而不是调用 n 次插入操作，只需在一次多插入操作中插入这些元素。<strong>n个元素按升序插入，使用已经完成的进度(即所经过的跃点)插入前一个元素，作为插入下一个元素的起点</strong>。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331161934.png" alt="20210331161934" loading="lazy"></li>
<li>除了增加FloDB中的吞吐量，多插入还可以使使用并发跳过表的其他应用程序受益。例如，像LevelDB这样的lsm使用了来进行更新(第2.2节)，它们可以通过以下方式加速更新。组合线程可以在一个多插入操作中应用所有更新，而不是一个接一个地分别应用它们。更重要的是，几个组合线程可以通过多次插入并发地应用更新。</li>
</ul>
<h4 id="pseudocode">Pseudocode</h4>
<ul>
<li>多插入操作的伪代码见算法1。操作输入是一个由(键、值)元组组成的数组。首先，输入元组按升序排序。然后，对于每个元组，使用FindFromPreds定位其在跳过列表中的位置。FindFromPreds从前面插入的元素的前身开始搜索当前元素在跳过列表中的位置(第5-8行)。如果当前 level 中存储的前一个元素的键大于要插入的当前元素的键，则可以直接从前一个元素跳转到存储的前一个元素。这是多插入操作的核心，其中应用了路径重用的思想。在跳过列表中找到一个元组的位置后，对该元组的插入操作类似于普通的插入操作。</li>
</ul>
<pre><code class="language-C++">FindFromPreds(key , preds , succs): 
// returns true iff key was found
// side -effect: updates preds and succs 
  pred = root
  for level from levelmax downto 0: 
    if (preds[level].key &gt; pred.key):
      pred = preds[level]
  curr = pred.next[level] 
  while(true):
    succ = curr.next[level] 
    if (curr.key &lt; key): 
      pred = curr
      curr = succ 
    else: 
      break
  preds[level] = pred 
  succs[level] = curr
  return (curr.key == key)

MultiInsert(keys , values): 
  sortByKey(keys , values)
  for i from 0 to levelmax: 
    preds[i] = root
  for each key -value pair (k,v): 
    node = new node(k,v) 
    while(true):
      if (FindFromPreds(k, preds , succs)): 
        SWAP(succs [0].val , v)
        break 
      else:
        for lvl from 0 to node.toplvl: 
          node.next[lvl] = succs[lvl]
      if (CAS(preds [0]. next[0], succs[0], node)):
        for lvl from 1 to node.toplvl: 
          while(true):
            if (CAS(preds[lvl].next[lvl], succs[lvl], node)):
              break 
            else: 
              FindFromPreds(k, preds , succs) 
        break
</code></pre>
<h4 id="concurrency">Concurrency</h4>
<ul>
<li>多个插入相互并发，简单的插入和读取也是如此。然而，多插入的正确性依赖于这样一个事实，即元素不会同时从跳过列表中删除。从设计上讲，这在 FloDB 中不是一个问题;只有当条目持久化到磁盘时，才会从跳过列表中删除它们。此外，虽然多重插入中的每个插入都是原子的，但多重插入本身是不能线性化的，也就是说，整个元素数组不会被视为在单个时间点插入(即中间状态是可见的)。</li>
</ul>
<h4 id="neighborhoods">Neighborhoods</h4>
<ul>
<li><strong>键的邻近性是多插入性能的一个主要因素。直观地说，如果在跳跃列表中多次插入的键最终会在跳跃列表中靠近，路径重用就会最大化</strong>。图8描述了一个实验的结果，这个实验比较了简单插入和5个键多插入的吞吐量，这是一个只进行更新的测试中键接近度的函数。在本实验中，如果键范围的邻域大小为n，则一次多插入中的所有键之间的距离最大为2n。可以清楚地看到，随着邻域大小的减小，多插入的效率也会提高。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331155201.png" alt="20210331155201" loading="lazy"></li>
<li>在FloDB中，我们<strong>在哈希表中创建分区，以利用在多次插入中键靠得更近的性能优势(邻域效应)</strong>。当一个key-value元组插入到FloDB中时，该键的最有效位 ℓ 将用于确定该元组应该插入到哈希表的哪个分区。然后，键的其余位被散列，以确定分区(即桶)中的位置。因为 ℓ 是一个参数，所以可以很容易地控制邻居的大小。</li>
<li><strong>虽然我们的分区方案利用了多插入的性能优势，但它也使哈希表容易受到数据倾斜的影响</strong>。如果存在具有公共前缀的流行键(如果数据倾斜涉及某个键范围)，那么与流行键对应的桶将比与流行度较低的键对应的桶更快地满满。这反过来会导致在 Membuffer 中能够完成的更新比例更小。倾斜工作负载的这种影响将在第5节中讨论。</li>
</ul>
<h4 id="implementation-of-flodb-operations">Implementation of FloDB Operations</h4>
<ul>
<li>算法2给出了Get, Put和Delete操作的伪代码(为了提高可读性，我们省略了进入和退出RCU临界区的代码)。</li>
<li><strong>Get</strong>: Get操作简单地在每一层上搜索一个键，顺序如下:Membuffer (MBF)，不可变Membuffer (IMM_MBF)，如果有的话，Memtable (MTB)，不可变Memtable (IMM_MTB)，如果有的话，最后在磁盘上搜索。Get返回它遇到的第一个键，这保证是最新的一个，因为级别是按照与数据流相同的顺序检查的。</li>
<li><strong>Update</strong>: 如前所述，Delete操作是一个带有特殊tombstone值的Put操作，因此我们只需要描述后一个操作。实际上，Put操作是通过尝试在Membuffer中插入键值对e来进行的(第10行);如果e的目标哈希表桶没有满，添加到Membuffer成功，操作返回(第11行);否则，e将被插入Memtable中(第20行)。此外，算法2中的完整Put伪代码还包括与持久化线程和并发扫描器同步的机制。首先，如果不成功地将e插入到Membuffer中，并且设置了pauseWriters标志，那么writer将在必要时帮助 Draining 不可变的Membuffer，或者等待标志被取消设置(第12-16行)。正如我们在下面解释的那样，扫描使用pauseWriters标志向写入者发出信号:Membuffer已经被完全清空到memtable中，为扫描做准备，写入者应该等待或者帮助完成清空。第二，写入者会等到Memtable中有空间时才开始执行Put(第17-18行)。这通常是一个非常短的等待，即当前 Memtable 被填满后，持久化线程准备新 Memtable 的时间。</li>
</ul>
<pre><code class="language-C++">Get(key):
  for c in MBF IMM_MBF MTB IMM_MTB DISK: 
    if (c != NULL):
      value = c.search(key) 
      if (value != not_found):
        return value 
      return not_found 

Put(key , value):
  if (MBF.add(key , value) == success): 
    return
  while pauseWriters:
    if MBFNeedsDraining (): 
      IMM_MBF.helpDrain ()
    else: 
      wait()
  while MTB.size &gt; MTB_TARGET_SIZE: 
    wait() 
  seq = globalSeqNumber.fetchAndIncrement() 
  MTB.add(key , value , seq)

Delete(key):
  Put(key , TOMBSTONE)
</code></pre>
<h5 id="scan-2">Scan</h5>
<ul>
<li><strong>Scan</strong>：扫描操作以两个参数作为输入:lowKey和highKey。它返回一个数组，其中包含数据存储中介于低输入键和高输入键之间的所有键和对应值。为清楚起见，我们将扫描算法的介绍分为两部分。首先，我们提出了一种扫描算法，该算法可以并行进行读和写操作，但不能与另一个扫描一起进行。然后，我们介绍必要的额外部分，即允许多线程扫描。</li>
<li><strong>Single-threaded scans, multithreaded reads and writes</strong>：单线程扫描的伪代码在算法3中显示(为了更清晰，我们再次忽略了RCU临界区边界)。第一步是冻结对当前Membuffer和Memtable的更新，并将当前Membuffer的内容释放到Memtable中。为此，我们暂停后台 Draining(第4行)，通知写入者停止直接在Memtable中进行更新(第5行)，并等待所有正在进行的Memtable写入完成(第9行)。注意，<strong>扫描从来不会完全阻塞写入器;即使写入者不能在扫描的第5行和第13行之间更新Memtable，他们也可以尝试更新Membuffer或帮助处理Draining。帮助确保即使扫描程序线程很慢，也能完成 Draining，从而减少写入程序不允许更新Memtable的时间。</strong></li>
<li>然后，将当前的Membuffer变为不可变的，并创建一个新的Membuffer，用于吸收未来的更新(第6-8行)。然后，发起扫描的线程开始将Membuffer清空到Memtable(第10行)。在Membuffer被清空后，扫描将获得一个序列号(通过原子递增操作)(第12行)。现在，允许后台从新的Membuffer中抽取数据，并允许写入者在Memtable上进行更新(如有必要，第13-14行)是安全的，因为扫描的序列号将用于确保一致性。实际的扫描操作(第15-28行)现在开始，首先是在Memtable和不可变Memtable(如果存在的话)上，最后是在磁盘上。当遇到扫描范围内的键时，将检查其序列号。如果小于扫描序列号，则保存key-value元组。如果遇到已经保存的键，则保留与小于扫描序列号的最大序列号对应的值。否则，如果键序列号高于扫描序列号，扫描将重启，因为序列号高于扫描序列号可能意味着该键对应的值被并发操作覆盖。最后，对保存的键和值数组进行排序并返回。</li>
<li>重启是昂贵的，因为它们需要完全 re-drain Membuffer.。<strong>为了避免在写密集型工作负载中任意多次重启扫描，我们添加了一种称为 fallbackScan 的回退机制，当扫描被迫重启过多次时将触发该机制</strong>。fallbackScan 的工作原理是阻止 Memtable 的写入者，直到完成扫描。在我们的实验中(第5节)，回退扫描很少被触发，并且不会增加显著的开销。</li>
</ul>
<pre><code class="language-C++">Scan(lowKey , highKey): 
  restartCount = 0
restart:
  pauseDrainingThreads = true 
  pauseWriters = true 
  IMM_MBF = MBF
  MBF = new MemBuffer () 
  MemBufferRCUWait () 
  MemTableRCUWait () 
  IMM_MBF.drain() 
  IMM_MBF.destroy ()
  seq = globalSeqNumber.fetchAndIncrement () 
  pauseWriters = false
  pauseDrainingThreads = false 
  results = ∅
for dataStructure in MTB IMM_MTB DISK: 
  iter = dataStructure.newIterator () 
  iter.seek(lowKey)
  while (iter.isValid () and iter.key &lt; highKey):
    if iter.seq &gt; seq: 
      restartCount += 1
      if restartCount &lt; RESTART_THRESHOLD: 
        goto restart
      else:
      return fallbackScan(lowKey , highKey) 
    results.add(iter.key , iter.value) 
    iter.next()
  results.sort() 
  return results
</code></pre>
<h5 id="multithreaded-scans">Multithreaded scans</h5>
<ul>
<li>如果多个线程同时进行扫描，则需要进行额外的同步，以避免多个线程各自创建一个Membuffer的副本并试图耗尽它的情况。为此，我们区分了两种类型的扫描:主扫描和 piggybacking 扫描。<strong>主扫描是在没有其他扫描并发运行时启动的扫描。piggybacking扫描是在其他扫描同时运行时启动的扫描</strong>。在任何给定的时间，只有一个主扫描可能正在运行。我们确保主扫描执行算法3的第4-14行，所有扫描执行第2行和第15-30行。piggyback扫描将等待，直到主扫描程序发布第12行获得的序列号，然后继续进行第15-28行中的实际扫描。请注意，如果在主扫描运行时启动的另一个piggyback扫描与主扫描并发，则在主扫描未运行时启动的piggyback扫描也可以启动。这个过程可以自我重复，创建长长的 piggybacking 扫描链，在链的开始处重用相同的主扫描序列号。我们通过一个系统参数来限制这些链的长度，以避免由于使用陈旧的序列号而重新启动的大量扫描。</li>
<li>当多个扫描并发运行时，上述方案会产生良好的性能，这是由于负载机制将耗尽的开销分散到多个扫描调用上，并且负载扫描在重新启动时不会重新耗尽Membuffer。针对低并发情况的一种优化方法是允许主扫描(除了附带扫描之外)重用前一个主扫描的序列号。这样可以避免过于频繁地完全耗尽内存缓冲区。</li>
</ul>
<h5 id="correctness">Correctness</h5>
<ul>
<li>在安全方面，建立一个新的扫描序列号的主扫描对于更新来说是线性化的。线性化点在算法3的第7行，在安装新的可变Membuffer的指令上。Draining 不可变的Membuffer可以确保线性化点之前的所有更新都包含在扫描中，并且第12行中获得的序列号可以确保线性化点之后的更新都不包含在扫描中。然而，piggyback扫描(以及重用现有序列号的主扫描)对于更新是不能线性化的(但它们是可序列化的)，因为它们可能会错过在它们的序列号建立之后发生的更新。如果在应用程序级别需要更严格的扫描一致性，可以指示扫描等待，直到它能够建立一个新的序列号，或者可以完全禁用扫描 piggyback。因此，FloDB中的所有扫描在更新方面都可以线性化，但要以性能为代价(每次扫描在继续之前都必须耗尽Membuffer，这是一个昂贵的操作)。就活性而言，所有的扫描最终都会完成，这是由于 fallbackScan 机制，该机制不能被写入器作废，因此保证会返回。</li>
</ul>
<h4 id="implementation-trade-offs">Implementation Trade-offs</h4>
<ul>
<li>如第5节所示，在各种工作负载下，FloDB比它的竞争对手获得更好的性能。尽管如此，FloDB的性能是有代价的:我们用资源来换取性能。连接两个inmemory级别需要至少一个后台线程(即引流线程)在写密集型工作负载下几乎持续运行。此外，FloDB的扫描操作存在以下局限性。虽然理论上可以调用范围(−∞，+∞)上的扫描(这将返回整个数据库的快照)，但大型扫描可能会重启多次，为了成功完成，会触发针对所有 writer 的 block 操作，开销较大。该扫描算法仅适用于中小型扫描。</li>
</ul>
<h2 id="evaluation">Evaluation</h2>
<ul>
<li>测试结果重点是测扩展性，就是说在不同的内存大小或者线程数上性能的提升，因为以往是到了一定程度性能会有所下降的，同时对比其他几个方案表明 FloDB 其实在扩展性这一块确实做的更好，从而把这个故事讲的更完整。</li>
<li>但是看下图结果很容易发现对于只读负载，在线程数大于等于 16 的时候，FloDB 就不如 RocksDB 及其变种 cLSM 了。作者给的解释是 FloDB 比 LevelDB 及其变体要好是因为毕竟采用了简化后的并发性支持更好的 Get 操作，但是比 RocksDB 差是因为在磁盘组件上没有像 RocksDB 那样做这么多优化，而大数据量的读操作很多都是在磁盘组件上最终进行的。这地方其实前面读者也有埋伏笔，就是说内存组件对于写的影响比对读的影响可能更大，所以才有 MemBuffer，然后当时也降到了和其他磁盘组件优化是正交的，所以也算是自圆其说吧。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331225435.png" alt="20210331225435" loading="lazy"></li>
<li>其他测试就看原文吧，没啥特别想讲的了。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210331230132.png" alt="20210331230132" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[C 基础]]></title>
        <id>https://blog.shunzi.tech/post/c-basic/</id>
        <link href="https://blog.shunzi.tech/post/c-basic/">
        </link>
        <updated>2021-04-06T01:36:30.000Z</updated>
        <summary type="html"><![CDATA[<ul>
<li>C 语言基础汇总，主要包括一些容易混淆的、容易忘记的知识点，经常需要查询的。好吧，其实就是菜~</li>
<li>持续更新ing</li>
</ul>
]]></summary>
        <content type="html"><![CDATA[<ul>
<li>C 语言基础汇总，主要包括一些容易混淆的、容易忘记的知识点，经常需要查询的。好吧，其实就是菜~</li>
<li>持续更新ing</li>
</ul>
<!-- more -->
<h2 id="struct-union">struct &amp; union</h2>
<ul>
<li><a href="https://www.runoob.com/cprogramming/c-structures.html">菜鸟教程 - C 结构体</a></li>
<li><a href="https://www.runoob.com/cprogramming/c-unions.html">菜鸟教程 - C 共用体</a></li>
</ul>
<h3 id="union">union</h3>
<h4 id="定义">定义</h4>
<pre><code class="language-C">union [union tag]
{
   member definition;
   member definition;
   ...
   member definition;
} [one or more union variables];
</code></pre>
<h4 id="示例">示例</h4>
<ul>
<li>Data 类型的变量可以存储一个整数、一个浮点数，或者一个字符串。这意味着一个变量（相同的内存位置）可以存储多个多种类型的数据。您可以根据需要在一个共用体内使用任何内置的或者用户自定义的数据类型。</li>
</ul>
<pre><code class="language-C">union Data
{
   int i;
   float f;
   char  str[20];
} data;
</code></pre>
<ul>
<li>共用体占用的内存应足够存储共用体中最大的成员。例如，在上面的实例中，Data 将占用 20 个字节的内存空间，因为在各个成员中，字符串所占用的空间是最大的。下面的实例将显示上面的共用体占用的总内存大小：</li>
</ul>
<pre><code class="language-c">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
 
union Data
{
   int i;
   float f;
   char  str[20];
};
 
int main( )
{
   union Data data;     
   // Memory size occupied by data : 20
   printf( &quot;Memory size occupied by data : %d\n&quot;, sizeof(data));
   
 
   data.i = 10;
   data.f = 220.5;
   strcpy( data.str, &quot;C Programming&quot;);
 
   printf( &quot;data.i : %d\n&quot;, data.i);
   printf( &quot;data.f : %f\n&quot;, data.f);
   printf( &quot;data.str : %s\n&quot;, data.str);
 
   return 0;
}

data.i : 1917853763
data.f : 4122360580327794860452759994368.000000
data.str : C Programming
</code></pre>
<ul>
<li>共用体的 i 和 f 成员的值有损坏，因为最后赋给变量的值占用了内存位置，这也是 str 成员能够完好输出的原因。</li>
</ul>
<h4 id="作用">作用</h4>
<ul>
<li>节省内存，有两个很长的数据结构，不会同时使用，比如一个表示老师，一个表示学生，如果要统计教师和学生的情况用结构体的话就有点浪费了！用共用体的话，只占用最长的那个数据结构所占用的空间，就足够了！</li>
<li>通信中的数据包会用到共用体:因为不知道对方会发一个什么包过来，用共用体的话就很简单了，定义几种格式的包，收到包之后就可以直接根据包的格式取出数据。</li>
</ul>
<h4 id="大小分配">大小分配</h4>
<ul>
<li>结构体变量所占内存长度是其中最大字段大小的整数倍（参考：结构体大小的计算）。</li>
<li>共用体变量所占的内存长度等于最长的成员变量的长度。例如，教程中定义的共用体Data各占20个字节（因为char str[20]变量占20个字节）,而不是各占4+4+20=28个字节。</li>
<li>union的长度取决于其中的长度最大的那个成员变量的长度。即union中成员变量是重叠摆放的，其开始地址相同。</li>
</ul>
<pre><code class="language-C">  union   mm{   
   char   a;//元长度1   
   int   b[5];//元长度4   
   double   c;//元长度8   
   int   d[3]; //元长度4
  };   
</code></pre>
<ul>
<li>本来mm的空间应该是sizeof(int)*5=20;但是如果只是20个单元的话,那可以存几个double型(8位)呢?两个半?当然不可以,所以mm的空间延伸为既要大于20,又要满足其他成员所需空间的整数倍,,因为含有double元长度8，故大小为24。</li>
</ul>
<h3 id="struct">struct</h3>
<h4 id="定义-2">定义</h4>
<pre><code class="language-C">struct tag { 
    member-list
    member-list 
    member-list  
    ...
} variable-list ;
</code></pre>
<h4 id="示例-2">示例</h4>
<ul>
<li>在一般情况下，tag、member-list、variable-list 这 3 部分至少要出现 2 个。以下为实例：</li>
</ul>
<pre><code class="language-C">//此声明声明了拥有3个成员的结构体，分别为整型的a，字符型的b和双精度的c
//同时又声明了结构体变量s1
//这个结构体并没有标明其标签
struct 
{
    int a;
    char b;
    double c;
} s1;
 
//此声明声明了拥有3个成员的结构体，分别为整型的a，字符型的b和双精度的c
//结构体的标签被命名为SIMPLE,没有声明变量
struct SIMPLE
{
    int a;
    char b;
    double c;
};
//用SIMPLE标签的结构体，另外声明了变量t1、t2、t3
struct SIMPLE t1, t2[20], *t3;
 
//也可以用typedef创建新类型
typedef struct
{
    int a;
    char b;
    double c; 
} Simple2;
//现在可以用Simple2作为类型声明新的结构体变量
Simple2 u1, u2[20], *u3;
</code></pre>
<ul>
<li>在上面的声明中，第一个和第二声明被编译器当作两个完全不同的类型，即使他们的成员列表是一样的，如果令 t3=&amp;s1，则是非法的。</li>
<li>结构体的成员可以包含其他结构体，也可以包含指向自己结构体类型的指针，而通常这种指针的应用是为了实现一些更高级的数据结构如链表和树等。</li>
</ul>
<pre><code class="language-C">//此结构体的声明包含了其他的结构体
struct COMPLEX
{
    char string[100];
    struct SIMPLE a;
};
 
//此结构体的声明包含了指向自己类型的指针
struct NODE
{
    char string[100];
    struct NODE *next_node;
};
</code></pre>
<ul>
<li>如果两个结构体互相包含，则需要对其中一个结构体进行不完整声明，如下所示：</li>
</ul>
<pre><code class="language-C">struct B;    //对结构体B进行不完整声明
 
//结构体A中包含指向结构体B的指针
struct A
{
    struct B *partner;
    //other members;
};
 
//结构体B中包含指向结构体A的指针，在A声明完后，B也随之进行声明
struct B
{
    struct A *partner;
    //other members;
};
</code></pre>
<ul>
<li><strong>实际使用示例</strong></li>
</ul>
<pre><code class="language-C">#include &lt;stdio.h&gt;
#include &lt;string.h&gt;
 
struct Books
{
   char  title[50];
   char  author[50];
   char  subject[100];
   int   book_id;
};
 
/* 函数声明 */
void printBook( struct Books *book );
int main( )
{
   struct Books Book1;        /* 声明 Book1，类型为 Books */
   struct Books Book2;        /* 声明 Book2，类型为 Books */
 
   /* Book1 详述 */
   strcpy( Book1.title, &quot;C Programming&quot;);
   strcpy( Book1.author, &quot;Nuha Ali&quot;); 
   strcpy( Book1.subject, &quot;C Programming Tutorial&quot;);
   Book1.book_id = 6495407;
 
   /* Book2 详述 */
   strcpy( Book2.title, &quot;Telecom Billing&quot;);
   strcpy( Book2.author, &quot;Zara Ali&quot;);
   strcpy( Book2.subject, &quot;Telecom Billing Tutorial&quot;);
   Book2.book_id = 6495700;
 
   /* 通过传 Book1 的地址来输出 Book1 信息 */
   printBook( &amp;Book1 );
 
   /* 通过传 Book2 的地址来输出 Book2 信息 */
   printBook( &amp;Book2 );
 
   return 0;
}
void printBook( struct Books *book )
{
   printf( &quot;Book title : %s\n&quot;, book-&gt;title);
   printf( &quot;Book author : %s\n&quot;, book-&gt;author);
   printf( &quot;Book subject : %s\n&quot;, book-&gt;subject);
   printf( &quot;Book book_id : %d\n&quot;, book-&gt;book_id);
}

Book title : C Programming
Book author : Nuha Ali
Book subject : C Programming Tutorial
Book book_id : 6495407
Book title : Telecom Billing
Book author : Zara Ali
Book subject : Telecom Billing Tutorial
Book book_id : 6495700
</code></pre>
<h4 id="大小分配-2">大小分配</h4>
<ul>
<li><a href="https://www.cnblogs.com/lykbk/archive/2013/04/02/krtmbhrkhoirtj9468945.html">博客园：结构体大小的计算</a></li>
<li><strong>结构体中成员变量分配的空间是按照成员变量中占用空间最大的来作为分配单位</strong>,同样成员变量的存储空间也是不能跨分配单位的,如果当前的空间不足,则会存储到下一个分配单位中。</li>
<li>结构体变量的首地址能够被其最宽基本类型成员的大小所整除。</li>
<li>结构体每个成员相对于结构体首地址的偏移量(offset)都是成员大小的整数倍，如有需要编译器会在成员之间加上填充字节(internal adding)。即结构体成员的末地址减去结构体首地址(第一个结构体成员的首地址)得到的偏移量都要是对应成员大小的整数倍。</li>
<li>结构体的总大小为结构体最宽基本类型成员大小的整数倍，如有需要编译器会在成员末尾加上填充字节。</li>
</ul>
<h2 id="弹性数组">弹性数组</h2>
<ul>
<li>定义数组时，没有指明其长度，此为弹性数组。</li>
<li>弹性数组只能存在于结构体中，并且必须满足如下<strong>条件</strong>：
<ul>
<li>弹性数组必须为<strong>结构体的最后一个成员</strong>；</li>
<li>该结构体必须<strong>包含一个非弹性数组的成员</strong>；</li>
<li>编译器需要支持 <strong>C99</strong> 标准。</li>
</ul>
</li>
</ul>
<h3 id="示例-3">示例</h3>
<pre><code class="language-C"> #include &lt;stdio.h&gt;
 #include &lt;stdlib.h&gt;
 #include &lt;string.h&gt;
 #include &lt;stdint.h&gt;
 ​
 typedef struct {
     int32_t id;
     int32_t grade;
     int8_t name[];
 }student_info_struct;
 ​
 int main() {
     int32_t *tmp = 0;
     int8_t *name = &quot;sdc&quot;;
     student_info_struct *si = NULL;
 ​
     printf(&quot;sizeof(struct) = %d\n&quot;, sizeof(student_info_struct));
 ​
     si = (student_info_struct *)malloc(sizeof(student_info_struct) + strlen(name) + 1); // +1 是为了存储 '\0'
     if(NULL == si)
     {
         printf(&quot;malloc failed\n&quot;);
         return -1;
     }
     memset((void *)si, 0, sizeof(student_info_struct) + strlen(name) + 1);
 ​
     si-&gt;id = 123;
     si-&gt;grade = 6;
     memcpy((void *)si-&gt;name, name, strlen(name));
 ​
     printf(&quot;addr:\n&quot;);
     printf(&quot;si: 0x%p\n&quot;, si);
     printf(&quot;si-&gt;grade: 0x%p\n&quot;, &amp;si-&gt;grade);
     printf(&quot;si-&gt;name: 0x%p\n&quot;, &amp;si-&gt;name);
     printf(&quot;si-&gt;name: 0x%p\n&quot;, si-&gt;name);
 ​
     return 0;
 }
</code></pre>
<ul>
<li>输出结果：</li>
</ul>
<pre><code> sizeof(struct) = 8 // 弹性数组不占空间
 addr: 
 &amp;si: 0x00000000003C21C0
 &amp;si-&gt;grade: 0x00000000003C21C4
 &amp;si-&gt;name: 0x00000000003C21C8
 si-&gt;name: 0x00000000003C21C8
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience]]></title>
        <id>https://blog.shunzi.tech/post/RocksDB-Experience/</id>
        <link href="https://blog.shunzi.tech/post/RocksDB-Experience/">
        </link>
        <updated>2021-04-05T04:21:02.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>FAST21: Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>FAST21: Evolution of Development Priorities in Key-value Stores Serving Large-scale Applications: The RocksDB Experience</li>
</ul>
</blockquote>
<!--more-->
<h2 id="abstract">Abstract</h2>
<ul>
<li>RocksDB是一个针对大规模分布式系统的键值存储，并针对固态硬盘(ssd)进行了优化。本文描述了RocksDB在过去8年中开发重点的变化。这种演变是硬件发展趋势的结果，也是许多公司大规模生产RocksDB的丰富经验的结果。RocksDB在多个组织进行规模化生产。我们将描述RocksDB的资源优化目标是如何以及为什么从写放大、到空间放大、到CPU利用率迁移的。大规模应用的运行经验告诉我们，需要跨不同的RocksDB实例管理资源分配，数据格式需要保持向后和向前兼容，以允许增量软件上线，还需要对数据库复制和备份提供适当的支持。失败处理的教训告诉我们，需要更早地在系统的每一层检测到数据损坏错误。</li>
</ul>
<h2 id="introduction">Introduction</h2>
<ul>
<li>RocksDB 起源于 LevelDB，针对 SSD 的一些特性进行了优化，以分布式应用程序为目标，同时也被设计成可以被嵌入到一些高级应用程序（如 Ceph）的 KV 库。每个RocksDB实例只管理单个节点上的数据，没有处理任何节点之间的操作（比如 replication、 loadbalancing），也不执行高级操作，比如 checkpoints，让上层应用来实现，底层只是提供一些支持。</li>
<li>RocksDB 可以根据负载以及性能需求进行定制和调优，主要包括对 WAL 的处理、压缩策略的选择。</li>
<li>RocksDB 应用广泛：
<ul>
<li>Database: MySQL, Rocksandra, CockroachDB, MongoDB, TiDB</li>
<li>Stream processing：Apache Flink, Kafka Stream, Samza, and Facebook’s Stylus.</li>
<li>Logging/queuing services: Facebook’s LogDevice, Uber’s Cherami, Iron.io</li>
<li>Index services: Facebook’s Dragon, Rockset</li>
<li>Caching on SSD: Caching on SSD, Qihoo’s Pika, Redis</li>
</ul>
</li>
<li>对不同应用特性进行了简单总结：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301101401.png" alt="20210301101401" loading="lazy"></li>
<li>FAST20 之前的一篇介绍 Facebook 实际的 RocksDB 负载特性也得出了一些数据信息：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301101630.png" alt="20210301101630" loading="lazy"></li>
<li>使用如 RocksDB 这种公共的存储引擎有利有弊：
<ul>
<li>劣势在于每个系统需要基于 RocksDB 构建自己的子系统，那么就涉及到一系列复杂的崩溃一致性的保证操作（但比较好的公共存储引擎一般会 cover 这一点）</li>
<li>优势在于很多基础功能是可以复用的</li>
</ul>
</li>
<li>其实就是因为 RocksDB 的应用广泛，以及一些硬件技术的发展，导致 RocksDB 的开发重点也在不断地变化。</li>
</ul>
<h2 id="background">Background</h2>
<ul>
<li>flash的特性对RocksDB的设计产生了深刻的影响。读写性能的不对称和有限的寿命给数据结构和系统架构的设计带来了挑战和机遇。因此，RocksDB采用了flash友好的数据结构，并针对现代硬件进行了优化。</li>
</ul>
<h3 id="embedded-storage-on-flash-based-ssds">Embedded storage on flash based SSDs</h3>
<ul>
<li>低延迟高吞吐的高性能 SSD 的出现促进了相应设计的变化，在一些场景里，以往的 IO 瓶颈可能转移到了网络上，无论是吞吐量还是延迟。所以这时候数据存储到本地的 SSD 相比于存储到远端性能要好很多，这时候嵌入式的存储引擎的需求就增加了。</li>
<li>RocksDB 就基于该场景产生了，并基于 LSM 开始了相关设计。</li>
</ul>
<h3 id="rocksdb-architecture">RocksDB architecture</h3>
<ul>
<li>LSM 树是 RocksDB 存储数据的主要数据结构</li>
</ul>
<h4 id="主要操作">主要操作</h4>
<h5 id="写">写</h5>
<ul>
<li>步骤：
<ul>
<li>首先写入到名为 MemTable 的内存 Buffer 和磁盘上的 WAL 中
<ul>
<li>MemTable 基于跳表实现有序，插入和查询的复杂度均为 logN</li>
<li>WAL 用于故障恢复，但其实不是强制的。</li>
</ul>
</li>
<li>然后持续写入。一旦 MemTable 达到一定的大小（设定的阈值），当前 MemTable 和 WAL 就不可修改了</li>
<li>分配一个新的 MemTable 和 WAL 来接受后续的写入</li>
<li>不可修改的 Memtable 刷入到磁盘上的 SSTable 文件中</li>
<li>刷入后的 Memtable 和 WAL 相应地被丢弃</li>
</ul>
</li>
<li>每个SSTable按排序顺序存储数据，并将其划分为大小相同的块。每个SSTable也有一个索引块，每个SSTable块有一个索引项用于二分查找。</li>
</ul>
<h5 id="read">Read</h5>
<ul>
<li>在读取路径中，在每个后续级别都进行键查找，直到找到键为止。它首先搜索所有memtable，然后搜索所有级别为0的sstable，然后依次搜索更高级别的sstable。在每一个级别上，都使用二分查找。Bloom过滤器用于消除SSTable文件中不必要的搜索。扫描要求搜索所有级别。</li>
</ul>
<h5 id="compaction">Compaction</h5>
<ul>
<li>最新的sstable由MemTable刷新创建，并放置在0级。高于0级的级别是由称为压缩的流程创建的。给定级别上sstable的大小受到配置参数的限制。当超过level-L的size目标时，选择level-L中的一些sstable与level-(L+1)中的重叠sstable合并。这样做，deleted 和 overwritten 的数据被删除，并优化表的读性能和空间效率。这个过程将写数据从0级逐渐迁移到最后一级。压缩I/O是高效的，因为它可以并行化，并且只涉及对整个文件的批量读写。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301114859.png" alt="20210301114859" loading="lazy"></li>
<li>Level-0 sstable有重叠的键范围，因为SSTable覆盖了完整的 sorted run。后面的每个级别只包含一个sorted run，因此这些级别中的sstable包含其级别 sorted run 的一个分区。</li>
<li>RocksDB支持多种不同类型的压缩。
<ul>
<li><em>Leveled Compaction</em> 是从LevelDB继承的，然后进行了改进，如上图所示，Level 的 target 大小呈指数级增长</li>
<li><em>Tiered Compaction</em> （又称 <em>Universal Compaction</em>）是和 Cassandra 以及 HBase 使用的策略类似的。多个 sorted run 被延迟压缩在一起，或者当有太多的 sorted run，或者DB总大小与最大 sorted run 的大小之比超过了一个可配置的阈值。</li>
<li><em>FIFO Compaction</em> 当 DB 大小达到某个阈值限制时直接丢弃以前的文件并只执行轻量压缩，主要用于内存缓存应用。</li>
</ul>
</li>
<li>通过使用不同的压缩策略，RocksDB 可以被配置为 读友好/写友好/对某些特殊缓存负载非常写友好。然而，应用程序所有者将需要考虑他们特定用例的不同指标之间的权衡。
<ul>
<li>一个 lazy 的压缩策略可以提升写吞吐量和写放大，但读性能将下降</li>
<li>虽然更积极的压缩会牺牲写性能，但允许更快的读取。</li>
</ul>
</li>
<li>日志和流处理服务需要进行大量的写操作，而数据库应用更喜欢进行权衡。表3通过微基准测试结果描述了这种灵活性。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301142503.png" alt="20210301142503" loading="lazy"></li>
</ul>
<h3 id="rocksdb-history">RocksDB History</h3>
<figure data-type="image" tabindex="1"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301160214.png" alt="20210301160214" loading="lazy"></figure>
<h2 id="evolution-of-resource-optimization-targets">Evolution of resource optimization targets</h2>
<ul>
<li>资源优化目标的转变过程：从写放大到空间放大再到 CPU 利用率</li>
</ul>
<h3 id="write-amplification">Write amplification</h3>
<ul>
<li>当初开发 RockDB 时主要关注节省 Flash 颗粒的擦除次数来减小 SSD 内部的写放大，这是当时社区的普遍看法，这个对于很多写密集型的应用确实是个极大的挑战，现在也仍然是个问题。</li>
<li>写放大实际出现在两个层次：
<ul>
<li>SSD 本身的写放大，观察发现在 1.1 到 3 之间。</li>
<li>存储和数据库软件也会产生写放大，有时候甚至可能达到 100（比如当对小于100字节的更改写入整个4KB/8KB/16KB页时）</li>
</ul>
</li>
<li><em>Leveled Compaction</em> 在 RocksDB 中通常显示出了 10-30 的写放大，在很多情况下，比使用 b 树要好好几倍。不过，10-30范围内的写放大对于写密集的应用程序来说还是太高了。</li>
<li><em>Tiered Compaction</em> 将写放大降低到了 4-10，但也降低了读性能。如下图以及上表所示的对比结果：<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301145605.png" alt="20210301145605" loading="lazy"></li>
<li>RocksDB应用程序所有者通常会选择一种压缩方法，在写速率高的时候减少写放大，在写速率低的时候更积极地压缩，以实现空间效率和读性能的目标。</li>
</ul>
<h3 id="space-amplification">Space amplification</h3>
<ul>
<li>经历了几年的开发之后发现，考虑到闪存写周期和写开销都没有限制，Space amplification 比 Write amplification 更重要，实际生产环境中表现出的 IOPS 是要比 SSD 本身可以提供的性能要低的，因此，我们将资源优化目标转移到磁盘空间。</li>
<li>幸运的是，由于lsm -tree的非碎片数据布局，它在优化磁盘空间时也可以很好地工作。然而，我们看到了通过减少 LSM 树中的老数据(即删除和覆盖的数据)的数量来改进 <em>Leveled Compaction</em> 的机会，因此开发了 <em>Dynamic Leveled Compaction</em>，树中每个 Level 的大小会根据最后一个 Level 的实际大小自动调整(而不是静态地设置每个 Level 的大小)。该策略相比于 <em>Leveled Compaction</em> 实现了更好的以及更稳定的空间效率。</li>
</ul>
<h3 id="cpu-utilization">CPU utilization</h3>
<ul>
<li>很多人认为对于高速 SSD，已经足够快，性能瓶颈已经从 SSD 转移到了 CPU。基于我们的经验，我们并不认为这是一个问题，我们也不希望它成为未来基于 SSD 的 NAND 闪存的问题，原因有二。
<ul>
<li>只有少部分应用被 SSD 提供的 IOPS 给限制，大多数应用还是被空间给限制。</li>
<li>其次，我们发现任何具有高端 CPU 的服务器都有足够的计算能力来饱和一个高端 SSD，在我们的环境中，RocksDB在充分利用SSD性能方面从来没有遇到过问题。
<ul>
<li>当然，可以配置一个系统，使 CPU 变成一个瓶颈（比如一个 CPU，多个 SSD）。然而，高效的系统通常是那些配置为良好平衡的系统，这是今天的技术所允许的</li>
<li>密集的写为主导的工作负载也可能导致CPU成为瓶颈。对于某些情况，可以通过配置RocksDB使用更轻量级的压缩选项来缓解这种问题。</li>
<li>对于其他情况，工作负载可能根本不适合SSD，因为它将超过允许SSD持续2-5年的典型闪存耐久预算。</li>
</ul>
</li>
</ul>
</li>
<li>为了证实我们的观点，我们调查了生产中 ZippyDB 和 MyRocks 的 42 种不同部署，它们分别服务于不同的应用程序。下图测试结果表明 <strong>大多数工作负载都受到空间限制</strong>。有些主机的确CPU利用率很高，但是主机通常没有得到充分的利用，没有为增长和处理数据中心或区域级故障留出空间(或由于错误配置)。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301151148.png" alt="20210301151148" loading="lazy"></li>
<li>然而，减少 CPU 开销已经成为一个重要的优化目标，因为减少空间放大已经做的差不多了。减少CPU开销可以提高CPU确实受到限制的少数应用程序的性能，更重要的是，减少CPU开销的优化允许更经济有效的硬件配置，从成本的角度讲该优化也挺有必要的。</li>
<li>早期降低CPU开销的努力包括引入前缀bloom过滤器、在索引查找之前应用bloom过滤器，以及其他bloom过滤器改进。还有进一步改善的空间。</li>
</ul>
<h3 id="adapting-to-newer-technologies">Adapting to newer technologies</h3>
<ul>
<li>SSD 架构相关的改进可能很容易瓦解 RocksDB 的相关性，比如 OC-SSD，Multi-Stream SSD，ZNS 承诺改善查询延迟和节省flash擦除周期。然而，这些新技术只能使使用 RocksDB 的少数应用程序受益，因为大多数应用程序都受到空间限制，而没有消除周期或延迟限制。此外，如果让RocksDB直接使用这些技术，将会对RocksDB的经验构成挑战，一个可能的方法是将这些技术的适应能力委托给底层文件系统，也许RocksDB会给底层文件系统提供一定的额外的提示。</li>
<li>存储内计算可能会带来显著的收益，但目前还不清楚有多少RocksDB应用能从中受益。我们认为，RocksDB 要适应存储内计算将是一个挑战，可能需要对整个软件堆栈进行API更改，以充分利用。我们期待未来关于如何最好地做到这一点的研究。
<ul>
<li><a href="https://www.zhihu.com/question/305643823">计算型存储/存算一体如何实现？</a></li>
</ul>
</li>
<li>分类(远程)存储似乎是一个更有趣的优化目标，并且是当前的优先级，到目前为止，我们的优化假设flash是本地连接的，因为我们的系统基础设施主要是这样配置的。然而，目前更快的网络可以提供更多的远程 I/O，因此在越来越多的应用程序中，使用远程存储运行RocksDB的性能是可行的。使用远程存储，可以更容易地同时充分利用 CPU 和 SSD 资源，因为它们可以根据需要分别提供(使用本地附加的SSD很难实现这一点)，因此，优化 RocksDB 的远程闪存存储已成为当务之急。我们目前正在通过尝试合并和并行 I/O 来解决长 I/O 延迟的挑战。我们已经对RocksDB进行了改造，以处理瞬时故障，将QoS要求传递给底层系统，并报告分析信息。然而，还需要做更多的工作。</li>
<li>存储级内存(SCM)是一种很有前途的技术。我们正在研究如何最好地利用它。以下几种可能性值得考虑: (NVM) <a href="https://zhuanlan.zhihu.com/p/72564047">SCM介绍</a>
<ul>
<li>使用 SCM 作为 DRAM 的延申，这就提出了这样的问题:如何用混合的 DRAM 和 SCM 来最好地实现关键数据结构(例如，块缓存或 memtable)，以及在尝试利用所提供的持久性时会引入哪些开销</li>
<li>使用 SCM 作为数据库的主存储，但是我们注意到RocksDB往往会受到空间或CPU的瓶颈，而不是 I/O</li>
<li>为 WAL 使用 SCM，但是，这就提出了这样一个问题:考虑到我们只需要在转移到 SSD 之前的一小块 staging 区域，这个用例是否单独证明了 SCM 的成本是合理的。</li>
</ul>
</li>
</ul>
<h3 id="main-data-structure-revisited">Main Data Structure Revisited</h3>
<ul>
<li>我们不断地重新讨论lsm -tree是否仍然合适的问题，但仍然得出它们确实合适的结论。SSD的价格还没有下降到足以改变大多数用例的空间和闪存续航瓶颈的程度，用CPU或DRAM交换SSD的使用只对少数应用程序有意义，虽然主要结论是一致的，但我们经常听到用户对写放大的要求低于RocksDB所能提供的。然而，我们注意到，当对象大小很大时，可以通过分离键和值来减少写放大，比如 WiscKey 和 ForrestDB，所以在 RocksDB 中也添加了这种支持 BlobDB</li>
</ul>
<h2 id="lessons-on-serving-large-scale-systems">Lessons on serving large-scale systems</h2>
<ul>
<li>RocksDB是各种需求各异的大型分布式系统的基石。随着时间的推移，我们了解到需要在资源管理、WAL处理、批处理文件删除、数据格式兼容性和配置管理方面进行改进。</li>
</ul>
<h3 id="resource-management">Resource management</h3>
<ul>
<li>分布式系统常常对数据进行分片，分片成 shards，shards 分布在多个服务器存储节点上，大小通常有限，因为要进行备份以及负载均衡，同时也会作为原子粒度进行一些一致性的保证。因此每个节点大概会有几十上百个 shards，一个 RocksDB 实例通常只服务于一个 shard，所以一个服务节点上可能通常会运行多个 RocksDB 实例。可能共享地址空间，也可能使用自己独立的地址空间。</li>
<li>一台主机可能会运行多个RocksDB实例，这对资源管理有一定影响。假设这些实例共享主机的资源，那么需要对资源进行全局(每个主机)和本地(每个实例)管理，以确保公平和有效地使用它们。当运行在单个进程模式下,拥有全局资源限制是很重要的,包括
<ul>
<li>(1) 内存写入缓冲器和块缓存,</li>
<li>(2) Compaction 的 I/O 带宽、</li>
<li>(3) Compaction 线程数</li>
<li>(4) 总磁盘使用情况</li>
<li>(5) 文件删除率</li>
</ul>
</li>
<li>还需要本地资源限制，例如，确保单个实例不能使用过多的任何资源。RocksDB允许应用程序为每种类型的资源创建一个或多个资源控制器(作为传递给不同DB对象的c++对象实现)，也可以在每个实例的基础上这样做。同时需要支持一些优先级策略，来保证一些资源可以优先被分配给一些最需要该资源的实例。</li>
<li>在一个进程中运行多个实例时得到的另一个教训是:大量地生成非池线程可能会有问题，特别是当线程是长期存在的时候。拥有太多的线程会增加CPU的概率，导致过多的上下文切换开销，并使调试变得极其困难，导致I/O strike。如果一个RocksDB实例需要使用一个可能会休眠或等待某个条件的线程来执行一些工作，那么最好使用一个线程池，这样可以方便地限制线程的大小和资源使用。</li>
<li>考虑到每个shard只有本地信息，当RocksDB实例运行在单独的进程中时，全局(每台主机)资源管理变得更加具有挑战性。可以采用两种策略。
<ul>
<li>首先，将每个实例配置为谨慎地使用资源，而不是贪婪地使用资源。例如，使用压缩，每个实例可以启动比“正常”更少的压缩，只有在压缩落后时才会增加。这种策略的缺点是，全局资源可能无法得到充分利用，导致资源使用不理想。</li>
<li>第二种，在操作上更具挑战性的策略是让实例之间共享资源使用信息，并相应地进行调整，试图在更大范围内优化资源使用。RocksDB还需要更多的工作来改善主机范围内的资源管理。</li>
</ul>
</li>
</ul>
<h3 id="wal-treatment">WAL treatment</h3>
<ul>
<li>传统的数据库倾向于在每一个写操作上强制执行write-ahead-log (WAL)，以确保持久性。相比之下，大型分布式存储系统通常为了性能和可用性复制数据，并且它们通过各种一致性保证来做到这一点。例如，当同一数据存在多个副本，其中一个副本损坏或无法访问时，存储系统会使用其他未受影响主机的有效副本重建故障主机的副本。对于这样的系统，RocksDB WAL 写的就不那么重要了。此外，分布式系统通常有自己的复制日志(例如Paxos日志)，在这种情况下，根本不需要RocksDB WAL。</li>
<li>我们了解到，提供调优WAL同步行为的选项，以满足不同应用程序的需要是很有帮助的。具体来说，我们介绍了不同的WAL操作模式:(i)同步WAL写，(i)缓冲WAL写，和(i)根本没有WAL写。对于buffered WAL处理，为了不影响RocksDB的流量延迟，在后台定期将WAL以低优先级写入磁盘。</li>
</ul>
<h3 id="rate-limited-file-deletions">Rate-limited file deletions</h3>
<ul>
<li>RocksDB通常通过文件系统与底层存储设备交互。这些文件系统支持是能识别出 SSD 的;例如,XFS。每当一个文件被删除，使用实时丢弃，可以发出修剪命令 TRIM 到 SSD。修剪命令通常被认为可以改善 SSD 性能和 Flash 寿命，这是经过我们的生产经验验证的。然而也会产生性能问题，TRIM 破坏性很大：除了更新地址映射(通常在SSD的内部内存中)之外，SSD固件还需要将这些更改写入flash中的FTL的日志，这反过来可能触发SSD的内部垃圾收集，导致相当大的数据迁移，并对前台IO延迟造成负面影响。为了避免 TRIM 活动 spike 和I/O延迟的相关增加，我们引入了文件删除的速率限制，以防止多个文件同时被删除(在压缩后发生)。</li>
</ul>
<h3 id="data-format-compatibility">Data format compatibility</h3>
<ul>
<li>型分布式应用程序在许多主机上运行它们的服务，它们期望零停机时间。结果，软件升级在主机之间逐步推出;当出现问题时，更新将被回滚。由于持续部署，这些软件升级频繁发生;RocksDB每个月都会发布一个新版本。因此，磁盘上的数据在不同的软件版本之间保持向后和向前兼容是很重要的。新升级(或回滚)的RocksDB实例必须能够理解前一个实例存储在磁盘上的数据。此外，为了构建副本或平衡负载，可能需要在分布式实例之间复制RocksDB数据文件，而这些实例可能运行不同的版本。由于缺少前向兼容性保障，在一些部署中，RocksDB的操作出现了困难，因此我们增加了这一保障。</li>
<li>RocksDB竭尽全力确保数据的前后兼容(除了新特性)。这在技术上和过程上都具有挑战性，但我们发现付出的努力是值得的。为了向后兼容，RocksDB必须能够理解之前写入磁盘的所有格式;这增加了软件和维护的复杂性。为了向前兼容，需要理解未来的数据格式，我们的目标是保持至少一年的向前兼容性。这可以通过使用通用技术部分实现，例如协议Buffer或Thrift所使用的技术。对于配置文件条目，RocksDB需要能够识别新的字段，并尽可能地猜测如何应用配置或何时放弃配置。我们不断地用不同版本的数据对不同版本的RocksDB进行测试。</li>
</ul>
<h3 id="managing-configurations">Managing configurations</h3>
<ul>
<li>RocksDB具有高度的可配置性，可以优化应用程序的工作负载。然而，我们发现配置管理是一个挑战。最初，RocksDB继承了LevelDB的参数配置方法，将参数选项直接嵌入到代码中。这造成了两个问题。
<ul>
<li>首先，参数选项通常与存储在磁盘上的数据绑定在一起，当使用一个选项创建的数据文件不能被新配置了另一个选项的RocksDB实例打开时，可能会产生兼容性问题。</li>
<li>其次，代码中没有明确指定的配置选项会自动设置为RocksDB的默认值。当RocksDB软件更新包括改变默认配置参数(例如，增加内存使用量或压缩并行度)时，应用程序有时会遇到意想不到的后果</li>
</ul>
</li>
<li>为了解决这些问题，RocksDB首先引入了RocksDB实例使用包含配置选项的字符串参数打开数据库的能力。后来，RocksDB引入了对随数据库存储选项文件的可选支持。我们还引入了两个工具:(i)验证工具，验证打开数据库的选项是否与目标数据库兼容;(ii)迁移工具重写数据库以兼容所需的选项(尽管这个工具是有限的)</li>
<li>RocksDB配置管理的一个更严重的问题是大量的配置选项。在RocksDB的早期，我们的设计选择是支持高度定制:我们引入了许多新的旋钮，并引入了可插拔组件的支持，所有这些都允许应用程序实现其性能潜力。这被证明是一种成功的策略，能够在早期获得最初的吸引力。然而，现在一个常见的抱怨是，选择太多了，很难理解它们的影响;也就是说，它变得非常困难</li>
<li>除了要调优许多配置参数之外，更令人生畏的是，最优配置不仅取决于嵌入了RocksDB的系统，还取决于上面应用程序产生的工作负载。以ZippyDB为例，ZippyDB是一个内部开发的大型分布式键值存储系统，在其节点上使用了RocksDB。ZippyDB服务于许多不同的应用程序，有时是单独的，有时是在多租户设置中。尽管在尽可能跨所有ZippyDB用例使用统一配置方面进行了大量努力，但不同用例的工作负载是如此不同，当性能很重要时，统一配置实际上是不可行的。表5显示了我们抽样的39个ZippyDB部署的25种不同配置<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301163015.png" alt="20210301163015" loading="lazy"></li>
<li>对于已交付给第三方的嵌入式RocksDB系统，调整配置参数也尤其具有挑战性。考虑第三方在其应用程序中使用MySQL或ZippyDB等数据库。第三方通常对RocksDB知之甚少，也不知道如何对其进行最佳调整。数据库所有者也不太愿意对客户的系统进行调优</li>
<li>这些真实的经验教训触发了我们配置支持策略的变化。我们在改进开箱即用性能和简化配置上花费了大量的精力。我们目前的重点是提供自动适应性，同时继续支持广泛的显式配置，考虑到RocksDB继续服务于专门的应用程序。我们注意到在追求自适应性的同时保持显式的可配置性会造成大量的代码维护开销，我们相信拥有一个统一的存储引擎的好处要大于代码的复杂性。</li>
</ul>
<h3 id="replication-and-backup-support">Replication and backup support</h3>
<ul>
<li>RocksDB是一个单节点库。如果需要，使用RocksDB的应用程序将负责复制和备份。每个应用程序都以自己的方式实现这些函数(出于合理的原因)，因此RocksDB对这些函数提供适当的支持是很重要的。</li>
<li>通过复制现有副本的所有数据来引导一个新的副本可以通过两种方式完成。
<ul>
<li>首先，可以从源副本读取所有键，然后将其写入目标副本(逻辑复制)。在源端，RocksDB支持数据扫描操作，能够最大限度地减少对并发在线查询的影响;例如，通过提供选项来不缓存这些操作的结果，从而防止缓存垃圾化。在目标端，支持散装装载，并针对此场景进行了优化。</li>
<li>第二，可以通过直接复制sstable和其他文件(物理复制)来引导一个新的副本。RocksDB通过识别当前时间点的现有数据库文件，并防止它们被删除或更改，从而帮助物理复制。支持物理复制是RocksDB将数据存储在底层文件系统上的一个重要原因，因为它允许每个应用程序使用自己的工具。我们认为，RocksDB直接使用块设备接口或与FTL进行深度集成所带来的潜在性能提升，不会超过上述所述的好处。</li>
</ul>
</li>
<li>备份是大多数数据库和其他应用程序的重要特性。对于备份，应用程序具有与复制相同的逻辑与物理选择。备份和复制之间的一个区别是，应用程序通常需要管理多个备份。虽然大多数应用程序都实现了自己的备份(以满足自己的需求)，但如果应用程序的备份需求很简单，RocksDB会提供一个备份引擎供其使用。</li>
<li>在这方面，我们看到有两个方面需要进一步改进，但都需要对key-value API进行更改;
<ul>
<li>第一个是在不同副本上以一致的顺序应用更新，这带来了性能挑战。</li>
<li>第二个问题涉及到每次发出一个写请求的性能问题，以及副本可能会落后，而应用程序可能希望这些副本能够更快地赶上</li>
</ul>
</li>
<li>不同的应用程序实现了不同的解决方案来解决这些问题，但是它们都有局限性. 挑战在于，应用程序不能乱序写数据，也不能用它们自己的序列号读取快照，因为RocksDB目前不支持使用用户定义的时间戳进行多版本控制</li>
</ul>
<h2 id="lessons-on-failure-handling">Lessons on failure handling</h2>
<ul>
<li>通过生产经验，我们学到了三个主要的教训，关于故障处理。首先，需要尽早检测数据损坏，以最小化数据不可用或丢失的风险，并通过这样做来查明错误的起源。其次，完整性保护必须覆盖整个系统，以防止静默的破坏暴露给RocksDB客户或传播到其他副本(见图4)。第三，错误需要以不同的方式处理.<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301163522.png" alt="20210301163522" loading="lazy"></li>
</ul>
<h3 id="frequency-of-silent-corruptions">Frequency of silent corruptions</h3>
<ul>
<li>由于性能原因，RocksDB用户通常不使用SSD(如DIF/DIX)的数据保护，存储介质损坏由RocksDB块校验和检测，这是所有成熟数据库的常规特性，因此我们在这里跳过分析。CPU/内存损坏很少发生，而且很难准确量化。使用RocksDB的应用程序通常会进行数据一致性检查，比较副本的完整性。这可以捕获错误，但这些错误可能是由RocksDB或客户端应用程序引入的(例如，在复制、备份或恢复数据时)。</li>
<li>我们发现，通过比较MyRocks数据库表中的主索引和二级索引，可以估计在RocksDB级别引入的损坏频率;任何不一致都可能在RocksDB级别上出现，包括CPU或内存损坏。根据我们的测量，大约每三个月，每100PB的数据就会出现一次RocksDB级别的故障。更糟糕的是，在这些案件中，有40%的损坏已经传播到其他副本。</li>
<li>在传输数据时也会发生数据损坏，通常是由于软件错误。例如，在处理网络故障时，底层存储系统中的错误导致我们在一段时间内看到，传输的每拍字节的物理数据大约有17个校验和不匹配。</li>
</ul>
<h3 id="multi-layer-protection">Multi-layer protection</h3>
<ul>
<li>需要尽早检测到数据损坏，以减少停机时间和数据丢失。大多数RocksDB应用都将数据复制到多个主机上;当检测到校验和不匹配时，将丢弃损坏的副本，并用正确的副本替换。然而，只有在正确的副本仍然存在时，这才是可行的选择。</li>
<li>如今，RocksDB在多个层次上对文件数据进行校验和，以识别底层的损坏情况。这些以及计划的应用层校验和如图4所示。多层校验和是重要的，主要是因为它们有助于及早发现错误，也因为它们可以防止不同类型的威胁。从LevelDB继承的块校验和可以防止文件系统或文件系统以下的数据损坏暴露给客户端。文件校验和是在2020年增加的，用于防止底层存储系统传播到其他副本造成的损坏，以及防止在通过网络传输SSTable文件时造成的损坏。对于WAL文件，切换校验和能够在写时有效地早期检测损坏。</li>
</ul>
<h4 id="block-integrity">Block integrity</h4>
<ul>
<li>每个SSTable块或WAL片段都附加了一个校验和，在数据创建时生成。与仅在文件移动时才验证的文件校验和不同，这个校验和在每次读取数据时都要验证，因为它的作用域更小。这样做可以防止存储层损坏的数据暴露给RocksDB客户端。</li>
</ul>
<h4 id="file-integrity">File integrity</h4>
<ul>
<li>文件内容特别有可能在传输操作期间被破坏;例如，用于备份或分发SSTable文件。为了解决这个问题，sstable被它们自己的校验和保护，这些校验和是在创建表时生成的。SSTable的校验和记录在元数据的SSTable文件条目中，并在传输时使用SSTable文件进行验证。然而，我们注意到其他文件，如WAL文件，仍然没有采用这种方式保护。</li>
</ul>
<h4 id="handoffintegrity">Handoffintegrity</h4>
<ul>
<li>早期检测写损坏的一种已建立的技术是对将要写入底层文件系统的数据生成一个切换校验和，并将其与数据一起传递下去，由底层进行验证。我们希望使用这样的write API来保护WAL的写操作，因为与sstable不同的是，WAL可以从每次追加的增量验证中获益。不幸的是，本地文件系统很少支持这一点——但是，一些专门的堆栈，比如Oracle ASM就支持。</li>
<li>另一方面，在远程存储上运行时，write API可以更改为接受校验和，并与存储服务的内部ECC挂钩。RocksDB可以在现有的WAL片段校验和上使用校验和组合技术来高效地计算写切换校验和。由于我们的存储服务执行写时验证，我们希望将损坏检测延迟到读时的情况非常罕见。</li>
</ul>
<h3 id="end-to-end-protection">End-to-end protection</h3>
<ul>
<li>虽然上面描述的保护层在许多情况下防止客户端暴露于损坏的数据，但它们并不全面。到目前为止提到的保护的一个不足是数据在文件I/O层以上是不受保护的;例如，MemTable中的数据和块缓存。在此级别损坏的数据将无法检测，因此最终将暴露给用户。此外，刷新或压缩操作可以持久化已损坏的数据，从而使损坏永久存在。</li>
</ul>
<h4 id="key-value-integrity">Key-value integrity</h4>
<ul>
<li>为了解决这个问题，我们目前正在实现每个键值校验和，以检测在文件I/O层之上发生的损坏。这个校验和将与键/值一起被复制，尽管我们将从已经存在替代完整性保护的文件数据中删除它。</li>
</ul>
<h3 id="severity-based-error-handling">Severity-based error handling</h3>
<ul>
<li>RocksDB遇到的大多数错误是底层存储系统返回的错误。这些错误可能源于许多问题，从严重的问题(如只读文件系统)到短暂的问题(如全磁盘或访问远程存储的网络错误)。早期，RocksDB对此类问题的反应要么是简单地向客户端返回错误消息，要么是永久停止所有写操作。</li>
<li>今天，我们的目标是在错误不能在本地恢复的情况下中断RocksDB的操作;例如，临时网络错误不需要用户干预重启RocksDB实例。为了实现这一点，我们对RocksDB进行了改进，使其在遇到被归类为瞬态错误的错误后，能够周期性地重试恢复操作。因此，我们获得了运营上的优势，因为客户不需要为发生的大部分故障手动缓解RocksDB。</li>
</ul>
<h2 id="lessons-on-the-key-value-interface">Lessons on the key-value interface</h2>
<ul>
<li>核心键值(KV)接口的用途惊人地多。几乎所有的存储工作负载都可以通过一个带有KV API的数据存储来提供;我们很少看到应用程序不能使用这个接口实现功能。这也许就是kv如此受欢迎的原因。KV接口是通用的。键和值都是可变长度的字节数组。应用程序在决定将哪些信息打包到每个键和值中方面具有很大的灵活性，而且它们可以从丰富的编码方案集中自由选择。因此，是应用程序负责解析和解释键和值。KV接口的另一个优点是可移植性。从一个键值系统迁移到另一个键值系统是相对容易的。然而，尽管许多用例通过这个简单的接口实现了最佳性能，但我们注意到它可能会限制一些应用程序的性能。</li>
<li>例如，在RocksDB之外构建并发控制是可能的，但很难提高效率，特别是在需要支持两阶段提交的情况下，在提交事务之前需要一些数据持久性。为此我们添加了事务支持，MyRocks (MySQL+RocksDB)使用了事务支持。我们会继续添加新功能;例如，gap/next 键锁定和大事务支持。</li>
<li>在其他情况下，这种限制是由key-value接口本身造成的。因此，我们已经开始研究基本键-值接口的可能扩展。其中一个扩展就是支持用户定义的时间戳</li>
</ul>
<h3 id="versions-and-timestamps">Versions and timestamps</h3>
<ul>
<li>在过去的几年里，我们已经认识到了数据版本控制的重要性。我们得出的结论是，版本信息应该成为RocksDB的头等公民，以适当地支持功能，如多版本并发控制(MVCC)和时间点读取。为了实现这一目标，RocksDB需要能够有效地访问不同的版本。</li>
<li>到目前为止，RocksDB内部一直在使用56位序列号来识别不同版本的kv -对。序列号由RocksDB生成，并在每次写入客户端时递增(因此，所有数据在逻辑上按排序顺序排列)。客户端应用程序不能影响序列号。然而，RocksDB允许应用程序对数据库进行快照，在此之后，RocksDB保证快照时存在的所有KV对将持续存在，直到应用程序显式地释放快照。因此，具有相同密钥的多个kv -对可能共存，并根据它们的序列号进行区分。</li>
<li>这种版本控制方法是不充分的，因为它并没有满足多种应用的要求。要从过去的状态读取数据，必须在前一个时间点已经拍摄了快照。RocksDB不支持对过去进行快照，因为没有API来指定时间点。此外，支持时间点读取是低效的。最后，每个RocksDB实例分配自己的序列号，并且只能根据每个实例获取快照。这使得具有多个(可能是复制的)分片(每个分片都是一个RocksDB实例)的应用程序的版本控制变得复杂。总之，创建提供跨分片一致读取的数据版本基本上是不可能的</li>
<li>应用程序可以通过在键或值中编码时间戳来绕开这些限制。但是，在这两种情况下，它们的性能都会下降。在键内进行编码会影响点查找的性能，而在值内进行编码会影响对相同键的无序写入的性能，并使读取旧版本的键变得复杂。我们相信应用程序指定的时间戳可以更好地解决这些限制，应用程序可以用可以全局理解的时间戳标记其数据，并且在键或值之外这样做</li>
<li>我们已经添加了对应用程序指定的时间戳的基本支持，并使用DB-Bench评估了这种方法。结果如表6所示。每个工作负载有两个步骤:第一步填充数据库，第二步测量性能。例如，在“fill_seq + read_random”中，我们通过按升序写一些键来填充初始数据库，在步骤2中执行随机读操作。相对于基线，应用程序将时间戳编码为密钥的一部分(对于RocksDB来说是透明的)，应用程序指定的时间戳API可以带来1.2倍或更好的吞吐量增益。这些改进来自于将时间戳作为元数据与用户键分开处理，因为这样就可以使用点查找而不是迭代器来获取键的最新值，并且Bloom过滤器可以识别不包含该键的sstable。此外，SSTable覆盖的时间戳范围可以存储在其属性中，这可以用来排除只能包含陈旧值的SSTable。</li>
</ul>
<figure data-type="image" tabindex="2"><img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301165053.png" alt="20210301165053" loading="lazy"></figure>
<ul>
<li>我们希望这个特性能够让用户更容易地在系统中实现单节点MVCC、分布式事务的多版本控制，或者解决多主复制中的冲突。然而，更复杂的API使用起来不那么简单，而且可能容易被误用。此外，与不存储时间戳相比，数据库将消耗更多的磁盘空间，而且对其他系统的可移植性也较差。</li>
</ul>
<h2 id="related-work">Related Work</h2>
<h3 id="storage-engine-libraries">Storage Engine Libraries</h3>
<ul>
<li>BerkeleyDB</li>
<li>SQLite</li>
<li>Hekaton</li>
</ul>
<h3 id="key-value-stores-for-ssds">Key-value stores for SSDs</h3>
<ul>
<li>SILT: 在内存效率、CPU和性能之间进行平衡的键值存储</li>
<li>ForestDB: 使用 HB+ tree 在日志上建立索引</li>
<li>TokuDB：和其他数据库使用 FractalTree/Bε 树。</li>
<li>LOCS/NoFTL-KV/FlashKV 瞄准开放通道ssd以提高性能</li>
</ul>
<h3 id="lsm-tree-improvements">LSM-tree improvements</h3>
<ul>
<li>一些系统也使用LSM树并改进了它们的性能。</li>
<li>写放大通常是最主要的优化目标：
<ul>
<li>WiscKey</li>
<li>PebblesDB</li>
<li>IAM-tree</li>
<li>TRIAD</li>
</ul>
</li>
<li>这些系统在优化写放大方面比RocksDB做得更好，后者更关注不同指标之间的权衡。
<ul>
<li>SlimDB 优化 LSM 树空间效率</li>
<li>Monkey 试图在 DRAM 和 IOPs 之间实现平衡。</li>
<li>bLSM/VT-tree/cLSM 优化LSM树的一般性能</li>
</ul>
</li>
</ul>
<h3 id="large-scale-storage-systems">Large-scale storage systems</h3>
<h2 id="future-work-and-open-questions">Future Work and Open Questions</h2>
<ul>
<li>除了完成上面提到的改进，包括对分类聚合存储、键值分离、多层校验和和应用程序指定的时间戳的优化，我们计划统一 leveled 和 tiered 压缩并提高自适应能力。然而，一些开放的问题可以从进一步的研究中受益。
<ul>
<li>How can we use SSD/HDD hybrid storage to improve efficiency?</li>
<li>How can we mitigate the performance impact on readers when there are many consecutive deletion markers?</li>
<li>How should we improve our write throttling algorithms?</li>
<li>Can we develop an efficient way of comparing two replicas to ensure they contain the same data?</li>
<li>How can we best exploit SCM? Should we still use LSM tree and how to organize storage hierarchy?</li>
<li>Can there be a generic integrity API to handle data handoff between RocksDB and the file system layer?</li>
</ul>
</li>
</ul>
<h2 id="conclusion">Conclusion</h2>
<ul>
<li>RocksDB已经从一个服务于小众应用的键值存储平台发展到目前广泛应用于众多工业大规模分布式应用的地位。作为主要数据结构的LSM树很好地服务于RocksDB，因为它表现出良好的写入和空间放大能力。然而，我们对业绩的看法是随着时间的推移而演变的。虽然写和空间放大仍然是主要关注的问题，但更多的焦点已经转移到CPU和DRAM效率，以及远程存储上。</li>
<li>运行大规模应用程序给我们带来了教训资源分配需要跨不同的RocksDB实例进行管理，数据格式需要保持向后和向前兼容，以允许增量软件部署，需要对数据库复制和备份提供适当的支持，配置管理需要简单且最好是自动化。失败处理的教训告诉我们，需要更早地在系统的每一层检测到数据损坏错误。key-value接口因其简单性和性能上的一些限制而广受欢迎。对界面进行一些简单的修改可能会产生更好的平衡。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210301170127.png" alt="20210301170127" loading="lazy"></li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Series Three of Basic of Concurrency - Condition Variables]]></title>
        <id>https://blog.shunzi.tech/post/basic-of-concurrency-one/</id>
        <link href="https://blog.shunzi.tech/post/basic-of-concurrency-one/">
        </link>
        <updated>2021-04-05T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第三篇（Condition Variables），条件变量。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第三篇（Condition Variables），条件变量。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="chapter-index">Chapter Index</h2>
<ul>
<li><a href="">Series One of Basic of Concurrency - Concurrency and Threads</a></li>
<li><a href="../lock/">Series Two of Basic of Concurrency - Lock</a></li>
<li><a href="">Series Three of Basic of Concurrency - Condition Variables</a></li>
<li><a href="">Series Four of Basic of Concurrency - Semaphores</a></li>
<li><a href="">Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency</a></li>
</ul>
<h2 id="condition-variables">Condition Variables</h2>
<ul>
<li>到目前为止，我们已经形成了锁的概念，看到了如何通过硬件和操作系统支持的正确组合来实现锁。然而，锁并不是并发程序设计所需的唯一原语。</li>
<li>具体来说，在很多情况下，线程需要检查某一条件（condition）满足之后，才会继续运行。例如，父线程需要检查子线程是否执行完毕 [这常被称为 join()]。这种等待如何实现呢？我们来看如下所示的代码。</li>
</ul>
<pre><code class="language-C">1 void *child(void *arg) {
2   printf(&quot;child\n&quot;);
3   // XXX how to indicate we are done?
4   return NULL;
5 }
6
7 int main(int argc, char *argv[]) {
8   printf(&quot;parent: begin\n&quot;);
9   pthread_t c;
10  Pthread_create(&amp;c, NULL, child, NULL); // create child
11  // XXX how to wait for child?
12  printf(&quot;parent: end\n&quot;);
13  return 0;
14 }
</code></pre>
<ul>
<li>我们期望能看到这样的输出：</li>
</ul>
<pre><code>parent: begin
child
parent: end 
</code></pre>
<ul>
<li>我们可以尝试用一个共享变量，如下所示。这种解决方案一般能工作，但是效率低下，因为主线程会自旋检查，浪费 CPU 时间。我们希望有某种方式让父线程休眠，直到等待的条件满足（即子线程完成执行）。</li>
</ul>
<pre><code class="language-C">1 volatile int done = 0;
2
3 void *child(void *arg) {
4   printf(&quot;child\n&quot;);
5   done = 1;
6   return NULL;
7 }
8
9 int main(int argc, char *argv[]) {
10  printf(&quot;parent: begin\n&quot;);
11  pthread_t c;
12  Pthread_create(&amp;c, NULL, child, NULL); // create child
13  while (done == 0)
14      ; // spin
15  printf(&quot;parent: end\n&quot;);
16  return 0;
17 }
</code></pre>
<ul>
<li><strong>CRUX: 多线程程序中，一个线程等待某些条件是很常见的。简单的方案是自旋直到条件满足，这是极其低效的，某些情况下甚至是错误的。那么，线程应该如何等待一个条件？</strong></li>
</ul>
<h3 id="definition-and-routines">Definition and Routines</h3>
<ul>
<li>线程可以使用条件变量（condition variable），来等待一个条件变成真。条件变量是一个显式队列，当某些执行状态（即条件，condition）不满足时，线程可以把自己加入队列，等待（waiting）该条件。另外某个线程，当它改变了上述状态时，就可以唤醒一个或者多个等待线程（通过在该条件上发信号），让它们继续执行。Dijkstra 最早在“私有信号量”中提出这种思想。Hoare 后来在关于观察者的工作中，将类似的思想称为条件变量。</li>
<li>要声明这样的条件变量，只要像这样写：pthread_cond_t c;，这里声明 c 是一个条件变量（注意：还需要适当的初始化）。条件变量有两种相关操作：wait() 和 signal()。线程要睡眠的时候，调用 wait()。当线程想唤醒等待在某个条件变量上的睡眠线程时，调用 signal()。具体来说，POSIX 调用如下所示。</li>
</ul>
<pre><code class="language-C">pthread_cond_wait(pthread_cond_t *c, pthread_mutex_t *m);
pthread_cond_signal(pthread_cond_t *c);
</code></pre>
<ul>
<li>我们常简称为 wait()和 signal()。你可能注意到一点，wait()调用有一个参数，它是互斥量。它假定在 wait()调用时，这个互斥量是已上锁状态。wait()的职责是释放锁，并让调用线程休眠（原子地）。当线程被唤醒时（在另外某个线程发信号给它后），它必须重新获取锁，再返回调用者。这样复杂的步骤也是为了避免在线程陷入休眠时，产生一些竞态条件。我们观察一下如下所示代码中 join 问题的解决方法，以加深理解。</li>
</ul>
<pre><code class="language-C">1 int done = 0;
2 pthread_mutex_t m = PTHREAD_MUTEX_INITIALIZER;
3 pthread_cond_t c = PTHREAD_COND_INITIALIZER;
4
5 void thr_exit() {
6   Pthread_mutex_lock(&amp;m);
7   done = 1;
8   Pthread_cond_signal(&amp;c);
9   Pthread_mutex_unlock(&amp;m);
10 }
11
12 void *child(void *arg) {
13  printf(&quot;child\n&quot;);
14  thr_exit();
15  return NULL;
16 }
17
18 void thr_join() {
19  Pthread_mutex_lock(&amp;m);
20  while (done == 0)
21      Pthread_cond_wait(&amp;c, &amp;m);
22  Pthread_mutex_unlock(&amp;m);
23 }
24
25 int main(int argc, char *argv[]) {
26  printf(&quot;parent: begin\n&quot;);
27  pthread_t p;
28  Pthread_create(&amp;p, NULL, child, NULL);
29  thr_join();
30  printf(&quot;parent: end\n&quot;);
31  return 0;
32 }
</code></pre>
<ul>
<li>有两种情况需要考虑。
<ul>
<li>第一种情况是父线程创建出子线程，但自己继续运行（假设只有一个处理器），然后马上调用 thr_join()等待子线程。在这种情况下，它会先获取锁，检查子进程是否完成（还没有完成），然后调用 wait()，让自己休眠。子线程最终得以运行，打印出“child”，并调用 thr_exit()函数唤醒父进程，这段代码会在获得锁后设置状态变量 done，然后向父线程发信号唤醒它。最后，父线程会运行（从 wait()调用返回并持有锁），释放锁，打印出“parent:end”。</li>
<li>第二种情况是，子线程在创建后，立刻运行，设置变量 done 为 1，调用 signal 函数唤醒其他线程（这里没有其他线程），然后结束。父线程运行后，调用 thr_join()时，发现 done 已经是 1 了，就直接返回。</li>
</ul>
</li>
<li>最后一点说明：你可能看到父线程使用了一个 while 循环，而不是 if 语句来判断是否需要等待。虽然从逻辑上来说没有必要使用循环语句，但这样做总是好的（后面我们会加以说明）。</li>
<li>为了确保理解 thr_exit()和 thr_join()中每个部分的重要性，我们来看一些其他的实现。首先，你可能会怀疑状态变量 done 是否需要。代码像下面这样如何？正确吗？</li>
</ul>
<pre><code class="language-C">1 void thr_exit() {
2   Pthread_mutex_lock(&amp;m);
3   Pthread_cond_signal(&amp;c);
4   Pthread_mutex_unlock(&amp;m);
5 }
6
7 void thr_join() {
8   Pthread_mutex_lock(&amp;m);
9   Pthread_cond_wait(&amp;c, &amp;m);
10  Pthread_mutex_unlock(&amp;m);
11 }

</code></pre>
<ul>
<li>这段代码是有问题的。假设子线程立刻运行，并且调用 thr_exit()。在这种情况下，子线程发送信号，但此时却没有在条件变量上睡眠等待的线程。父线程运行时，就会调用 wait 并卡在那里，没有其他线程会唤醒它。通过这个例子，你应该认识到变量 done 的重要性，它记录了线程有兴趣知道的值。睡眠、唤醒和锁都离不开它。</li>
<li>下面是另一个糟糕的实现。在这个例子中，我们假设线程在发信号和等待时都不加锁。会发生什么问题？想想看！</li>
</ul>
<pre><code class="language-C">1 void thr_exit() {
2   done = 1;
3   Pthread_cond_signal(&amp;c);
4 }
5
6 void thr_join() {
7   if (done == 0)
8       Pthread_cond_wait(&amp;c);
9 }

</code></pre>
<ul>
<li>这里的问题是一个微妙的竞态条件。具体来说，如果父进程调用 thr_join()，然后检查完 done 的值为 0，然后试图睡眠。但在调用 wait 进入睡眠之前，父进程被中断。子线程修改变量 done 为 1，发出信号，同样没有等待线程。父线程再次运行时，就会长眠不醒，这就惨了。</li>
</ul>
<blockquote>
<ul>
<li><strong>提示：发信号时总是持有锁</strong></li>
<li>尽管并不是所有情况下都严格需要，但有效且简单的做法，还是在使用条件变量发送信号时持有锁。虽然上面的例子是必须加锁的情况，但也有一些情况可以不加锁，而这可能是你应该避免的。因此，为了简单，请在调用 signal 时持有锁（hold the lock when calling signal）。</li>
<li>这个提示的反面，即调用 wait 时持有锁，不只是建议，而是 wait 的语义强制要求的。因为 wait 调用总是假设你调用它时已经持有锁、调用者睡眠之前会释放锁以及返回前重新持有锁。因此，这个提示的一般化形式是正确的：调用 signal 和 wait 时要持有锁（hold the lock when calling signal or wait），你会保持身心健康的。</li>
</ul>
</blockquote>
<ul>
<li>希望通过这个简单的 join 示例，你可以看到使用条件变量的一些基本要求。为了确保你能理解，我们现在来看一个更复杂的例子：生产者/消费者（producer/consumer）或有界缓冲区（bounded-buffer）问题。</li>
</ul>
<h3 id="the-producerconsumer-bounded-buffer-problem">The Producer/Consumer (Bounded Buffer) Problem</h3>
<ul>
<li>本章要面对的下一个问题，是生产者/消费者（producer/consumer）问题，也叫作有界缓冲区（bounded buffer）问题。这一问题最早由 Dijkstra 提出。实际上也正是通过研究这一问题，Dijkstra 和他的同事发明了通用的信号量（它可用作锁或条件变量）。</li>
<li>假设有一个或多个生产者线程和一个或多个消费者线程。生产者把生成的数据项放入缓冲区；消费者从缓冲区取走数据项，以某种方式消费。很多实际的系统中都会有这种场景。例如，在多线程的网络服务器中，一个生产者将 HTTP 请求放入工作队列（即有界缓冲区），消费线程从队列中取走请求并处理。</li>
<li>我们在使用管道连接不同程序的输出和输入时，也会使用有界缓冲区，例如 grep foo file.txt | wc -l。这个例子并发执行了两个进程，grep 进程从 file.txt 中查找包括“foo”的行，写到标准输出；UNIX shell 把输出重定向到管道（通过 pipe 系统调用创建）。管道的另一端是 wc 进程的标准输入，wc 统计完行数后打印出结果。因此，grep 进程是生产者，wc 是进程是消费者，它们之间是内核中的有界缓冲区，而你在这个例子里只是一个开心的用户。</li>
<li>因为有界缓冲区是共享资源，所以我们必须通过同步机制来访问它，以免产生竞态条件。为了更好地理解这个问题，我们来看一些实际的代码。- 首先需要一个共享缓冲区，让生产者放入数据，消费者取出数据。简单起见，我们就拿一个整数来做缓冲区（你当然可以想到用一个指向数据结构的指针来代替），两个内部函数将值放入缓冲区，从缓冲区取值。</li>
</ul>
<pre><code class="language-C">1 int buffer;
2 int count = 0; // initially, empty
3
4 void put(int value) {
5   assert(count == 0);
6   count = 1;
7   buffer = value;
8 }
9
10 int get() {
11  assert(count == 1);
12  count = 0;
13  return buffer;
14 }

</code></pre>
<ul>
<li>很简单，不是吗？put()函数会假设缓冲区是空的，把一个值存在缓冲区，然后把 count 设置为 1 表示缓冲区满了。get()函数刚好相反，把缓冲区清空后（即将 count 设置为 0），并返回该值。不用担心这个共享缓冲区只能存储一条数据，稍后我们会一般化，用队列保存更多数据项，这会比听起来更有趣。</li>
<li>现在我们需要编写一些函数，知道何时可以访问缓冲区，以便将数据放入缓冲区或从缓冲区取出数据。条件是显而易见的：仅在 count 为 0 时（即缓冲器为空时），才将数据放入缓冲器中。仅在计数为 1 时（即缓冲器已满时），才从缓冲器获得数据。如果我们编写同步代码，让生产者将数据放入已满的缓冲区，或消费者从空的数据获取数据，就做错了（在这段代码中，断言将触发）。</li>
<li>这项工作将由两种类型的线程完成，其中一类我们称之为生产者（producer）线程，另<br>
一类我们称之为消费者（consumer）线程。下面展示了一个生产者的代码，它将一个整<br>
数放入共享缓冲区 loops 次，以及一个消费者，它从该共享缓冲区中获取数据（永远不停），每次打印出从共享缓冲区中提取的数据项。</li>
</ul>
<pre><code class="language-C">1 void *producer(void *arg) {
2   int i;
3   int loops = (int) arg;
4   for (i = 0; i &lt; loops; i++) {
5       put(i);
6   }
7 }
8
9 void *consumer(void *arg) {
10  while (1) {
11      int tmp = get();
12      printf(&quot;%d\n&quot;, tmp);
13  }
14 }
</code></pre>
<h4 id="a-broken-solution">A Broken Solution</h4>
<ul>
<li>假设只有一个生产者和一个消费者。显然，put()和 get()函数之中会有临界区，因为 put() 更新缓冲区，get()读取缓冲区。但是，给代码加锁没有用，我们还需别的东西。不奇怪，别的东西就是某些条件变量。在这个（有问题的）首次尝试中，我们用了条件变量 cond 和相关的锁 mutex。</li>
</ul>
<pre><code class="language-C">1 int loops; // must initialize somewhere...
2 cond_t cond;
3 mutex_t mutex;
4
5 void *producer(void *arg) {
6   int i;
7   for (i = 0; i &lt; loops; i++) {
8       Pthread_mutex_lock(&amp;mutex); // p1
9       if (count == 1) // p2
10          Pthread_cond_wait(&amp;cond, &amp;mutex); // p3
11      put(i); // p4
12      Pthread_cond_signal(&amp;cond); // p5
13      Pthread_mutex_unlock(&amp;mutex); // p6
14  }
15 }
16
17 void *consumer(void *arg) {
18  int i;
19  for (i = 0; i &lt; loops; i++) {
20      Pthread_mutex_lock(&amp;mutex); // c1
21      if (count == 0) // c2
22          Pthread_cond_wait(&amp;cond, &amp;mutex); // c3
23      int tmp = get(); // c4
24      Pthread_cond_signal(&amp;cond); // c5
25      Pthread_mutex_unlock(&amp;mutex); // c6
26      printf(&quot;%d\n&quot;, tmp);
27  }
28 }

</code></pre>
<ul>
<li>来看看生产者和消费者之间的信号逻辑。当生产者想要填充缓冲区时，它等待缓冲区变空（p1～p3）。消费者具有完全相同的逻辑，但等待不同的条件——变满（c1～c3）。当只有一个生产者和一个消费者时，上图中的代码能够正常运行。但如果有超过一个线程（例如两个消费者），这个方案会有两个严重的问题。哪两个问题？</li>
<li>……（暂停思考一下）……</li>
<li>我们来理解第一个问题，它与等待之前的 if 语句有关。假设有两个消费者（Tc1 和 Tc2），一个生产者（Tp）。首先，一个消费者（Tc1）先开始执行，它获得锁（c1），检查缓冲区是否可以消费（c2），然后等待（c3）（这会释放锁）。</li>
<li>接着生产者（Tp）运行。它获取锁（p1），检查缓冲区是否满（p2），发现没满就给缓冲区加入一个数字（p4）。然后生产者发出信号，说缓冲区已满（p5）。关键的是，这让第一个消费者（Tc1）不再睡在条件变量上，进入就绪队列。Tc1 现在可以运行（但还未运行）。生产者继续执行，直到发现缓冲区满后睡眠（p6,p1-p3）。</li>
<li>这时问题发生了：另一个消费者（Tc2）抢先执行，消费了缓冲区中的值（c1,c2,c4,c5,c6，跳过了 c3 的等待，因为缓冲区是满的）。现在假设 Tc1 运行，在从 wait 返回之前，它获取了锁，然后返回。然后它调用了 get() (p4)，但缓冲区已无法消费！断言触发，代码不能像预期那样工作。显然，我们应该设法阻止 Tc1 去消费，因为 Tc2 插进来，消费了缓冲区中之前生产的一个值。下表展示了每个线程的动作，以及它的调度程序状态（就绪、运行、睡眠）随时间的变化。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405165844.png" alt="20210405165844" loading="lazy"></li>
<li>问题产生的原因很简单：在 Tc1 被生产者唤醒后，但在它运行之前，缓冲区的状态改变了（由于 Tc2）。发信号给线程只是唤醒它们，暗示状态发生了变化（在这个例子中，就是值已被放入缓冲区），但并不会保证在它运行之前状态一直是期望的情况。信号的这种释义常称为Mesa 语义（Mesa semantic），为了纪念以这种方式建立条件变量的首次研究。另一种释义是 Hoare 语义（Hoare semantic），虽然实现难度大，但是会保证被唤醒线程立刻执行。实际上，几乎所有系统都采用了 Mesa 语义。</li>
</ul>
<h4 id="better-but-still-broken-while-not-if">Better, But Still Broken: While, Not If</h4>
<ul>
<li>幸运的是，修复这个问题很简单：把 if 语句改为 while。当消费者 Tc1 被唤醒后，立刻再次检查共享变量（c2）。如果缓冲区此时为空，消费者就会回去继续睡眠（c3）。生产者中相应的 if 也改为 while（p2）。</li>
</ul>
<pre><code class="language-C">1 int loops; // must initialize somewhere...
2 cond_t cond;
3 mutex_t mutex;
4
5 void *producer(void *arg) {
6   int i;
7   for (i = 0; i &lt; loops; i++) {
8       Pthread_mutex_lock(&amp;mutex); // p1
9       while (count == 1) // p2
10          Pthread_cond_wait(&amp;cond, &amp;mutex); // p3
11      put(i); // p4
12      Pthread_cond_signal(&amp;cond); // p5
13      Pthread_mutex_unlock(&amp;mutex); // p6
14  }
15 }
16
17 void *consumer(void *arg) {
18  int i;
19  for (i = 0; i &lt; loops; i++) {
20      Pthread_mutex_lock(&amp;mutex); // c1
21      while (count == 0) // c2
22          Pthread_cond_wait(&amp;cond, &amp;mutex); // c3
23      int tmp = get(); // c4
24      Pthread_cond_signal(&amp;cond); // c5
25      Pthread_mutex_unlock(&amp;mutex); // c6
26      printf(&quot;%d\n&quot;, tmp);
27  }
28 }

</code></pre>
<ul>
<li>由于 Mesa 语义，我们要记住一条关于条件变量的简单规则：总是使用 while 循环（always use while loop）。虽然有时候不需要重新检查条件，但这样做总是安全的，做了就开心了。</li>
<li>但是，这段代码仍然有一个问题，也是上文提到的两个问题之一。你能想到吗？它和我们只用了一个条件变量有关。尝试弄清楚这个问题是什么，再继续阅读。想一下！</li>
<li>……（暂停想一想，或者闭一下眼）……</li>
<li>我们来确认一下你想得对不对。假设两个消费者（Tc1 和 Tc2）先运行，都睡眠了（c3）。生产者开始运行，在缓冲区放入一个值，唤醒了一个消费者（假定是 Tc1），并开始睡眠。现在是一个消费者马上要运行（Tc1），两个线程（Tc2 和 Tp）都等待在同一个条件变量上。问题马上就要出现了</li>
<li>消费者 Tc1 醒过来并从 wait()调用返回（c3），重新检查条件（c2），发现缓冲区是满的，消费了这个值（c4）。这个消费者然后在该条件上发信号（c5），唤醒一个在睡眠的线程。但是，应该唤醒哪个线程呢？</li>
<li>因为消费者已经清空了缓冲区，很显然，应该唤醒生产者。但是，如果它唤醒了 Tc2（这绝对是可能的，取决于等待队列是如何管理的），问题就出现了。具体来说，消费者 Tc2 会醒过来，发现队列为空（c2），又继续回去睡眠（c3）。生产者 Tp 刚才在缓冲区中放了一个值，现在在睡眠。另一个消费者线程 Tc1 也回去睡眠了。3 个线程都在睡眠，显然是一个缺陷。由表可以看到这个可怕灾难的步骤。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405170637.png" alt="20210405170637" loading="lazy"></li>
<li>信号显然需要，但必须更有指向性。消费者不应该唤醒消费者，而应该只唤醒生产者，反之亦然。</li>
</ul>
<h4 id="the-single-buffer-producerconsumer-solution">The Single Buffer Producer/Consumer Solution</h4>
<ul>
<li>解决方案也很简单：使用两个条件变量，而不是一个，以便正确地发出信号，在系统状态改变时，哪类线程应该唤醒。</li>
</ul>
<pre><code class="language-C">1 cond_t empty, fill;
2 mutex_t mutex;
3
4 void *producer(void *arg) {
5 int i;
6 for (i = 0; i &lt; loops; i++) {
7   Pthread_mutex_lock(&amp;mutex);
8   while (count == 1)
9       Pthread_cond_wait(&amp;empty, &amp;mutex);
10  put(i);
11  Pthread_cond_signal(&amp;fill);
12  Pthread_mutex_unlock(&amp;mutex);
13  }
14 }
15
16 void *consumer(void *arg) {
17  int i;
18  for (i = 0; i &lt; loops; i++) {
19      Pthread_mutex_lock(&amp;mutex);
20      while (count == 0)
21          Pthread_cond_wait(&amp;fill, &amp;mutex);
22      int tmp = get();
23      Pthread_cond_signal(&amp;empty);
24      Pthread_mutex_unlock(&amp;mutex);
25      printf(&quot;%d\n&quot;, tmp);
26  }
27 }
</code></pre>
<ul>
<li>在上述代码中，生产者线程等待条件变量 empty，发信号给变量 fill。相应地，消费者线程等待 fill，发信号给 empty。这样做，从设计上避免了上述第二个问题：消费者再也不会唤醒消费者，生产者也不会唤醒生产者。</li>
</ul>
<h4 id="the-correct-producerconsumer-solution">The Correct Producer/Consumer Solution</h4>
<ul>
<li>我们现在有了可用的生产者/消费者方案，但不太通用。我们最后的修改是提高并发和效率。具体来说，增加更多缓冲区槽位，这样在睡眠之前，可以生产多个值。同样，睡眠之前可以消费多个值。单个生产者和消费者时，这种方案因为上下文切换少，提高了效率。多个生产者和消费者时，它甚至支持并发生产和消费，从而提高了并发。幸运的是，和现有方案相比，改动也很小。</li>
<li>第一处修改是缓冲区结构本身，以及对应的 put() 和 get()方法。</li>
</ul>
<pre><code class="language-C">1 int buffer[MAX];
2 int fill_ptr = 0;
3 int use_ptr = 0;
4 int count = 0;
5
6 void put(int value) {
7   buffer[fill_ptr] = value;
8   fill_ptr = (fill_ptr + 1) % MAX;
9   count++;
10 }
11
12 int get() {
13  int tmp = buffer[use_ptr];
14  use_ptr = (use_ptr + 1) % MAX;
15  count--;
16  return tmp;
17 }

</code></pre>
<ul>
<li>我们还稍稍修改了生产者和消费者的检查条件，以便决定是否要睡眠。展示了最终的等待和信号逻辑。生产者只有在缓冲区满了的时候才会睡眠（p2），消费者也只有在队列为空的时候睡眠（c2）。至此，我们解决了生产者/消费者问题。</li>
</ul>
<pre><code class="language-C">1 cond_t empty, fill;
2 mutex_t mutex;
3
4 void *producer(void *arg) {
5   int i;
6   for (i = 0; i &lt; loops; i++) {
7       Pthread_mutex_lock(&amp;mutex); // p1
8       while (count == MAX) // p2
9           Pthread_cond_wait(&amp;empty, &amp;mutex); // p3
10      put(i); // p4
11      Pthread_cond_signal(&amp;fill); // p5
12      Pthread_mutex_unlock(&amp;mutex); // p6
13  }
14 }
15
16 void *consumer(void *arg) {
17  int i;
18  for (i = 0; i &lt; loops; i++) {
19      Pthread_mutex_lock(&amp;mutex); // c1
20      while (count == 0) // c2
21          Pthread_cond_wait(&amp;fill, &amp;mutex); // c3
22      int tmp = get(); // c4
23      Pthread_cond_signal(&amp;empty); // c5
24      Pthread_mutex_unlock(&amp;mutex); // c6
25      printf(&quot;%d\n&quot;, tmp);
26  }
27 }

</code></pre>
<blockquote>
<ul>
<li><strong>提示：对条件变量使用 while（不是 if）</strong></li>
<li>多线程程序在检查条件变量时，使用 while 循环总是对的。if 语句可能会对，这取决于发信号的语义。因此，总是使用 while，代码就会符合预期。</li>
<li>对条件变量使用 while 循环，这也解决了假唤醒（spurious wakeup）的情况。某些线程库中，由于实现的细节，有可能出现一个信号唤醒两个线程的情况。再次检查线程的等待条件，假唤醒是另一个原因。</li>
</ul>
</blockquote>
<h3 id="covering-conditions">Covering Conditions</h3>
<ul>
<li>现在再来看条件变量的一个例子。这段代码摘自 Lampson 和 Redell 关于飞行员的论文，同一个小组首次提出了上述的 Mesa 语义（Mesa semantic，他们使用的语言是 Mesa，因此而得名）。</li>
<li>他们遇到的问题通过一个简单的例子就能说明，在这个例子中，是一个简单的多线程内存分配库。</li>
</ul>
<pre><code class="language-C">1 // how many bytes of the heap are free?
2 int bytesLeft = MAX_HEAP_SIZE;
3
4 // need lock and condition too
5 cond_t c;
6 mutex_t m;
7
8 void *
9 allocate(int size) {
10  Pthread_mutex_lock(&amp;m);
11  while (bytesLeft &lt; size)
12      Pthread_cond_wait(&amp;c, &amp;m);
13  void *ptr = ...; // get mem from heap
14  bytesLeft -= size;
15  Pthread_mutex_unlock(&amp;m);
16  return ptr;
17 }
18
19 void free(void *ptr, int size) {
20  Pthread_mutex_lock(&amp;m);
21  bytesLeft += size;
22  Pthread_cond_signal(&amp;c); // whom to signal??
23  Pthread_mutex_unlock(&amp;m);
24 }

</code></pre>
<ul>
<li>从代码中可以看出，当线程调用进入内存分配代码时，它可能会因为内存不足而等待。相应的，线程释放内存时，会发信号说有更多内存空闲。但是，代码中有一个问题：应该唤醒哪个等待线程（可能有多个线程）？</li>
<li>考虑以下场景。假设目前没有空闲内存，线程 Ta 调用 allocate(100)，接着线程 Tb 请求较少的内存，调用 allocate(10)。Ta 和 Tb 都等待在条件上并睡眠，没有足够的空闲内存来满足它们的请求。这时，假定第三个线程 Tc调用了 free(50)。遗憾的是，当它发信号唤醒等待线程时，可能不会唤醒申请 10 字节的 Tb 线程。而 Ta 线程由于内存不够，仍然等待。因为不知道唤醒哪个（或哪些）线程，所以图中代码无法正常工作。</li>
<li>Lampson 和 Redell 的解决方案也很直接：用 pthread_cond_broadcast()代替上述代码中的 pthread_cond_signal()，唤醒所有的等待线程。这样做，确保了所有应该唤醒的线程都被唤醒。当然，不利的一面是可能会影响性能，因为不必要地唤醒了其他许多等待的线程，它们本来（还）不应该被唤醒。这些线程被唤醒后，重新检查条件，马上再次睡眠。</li>
<li>Lampson 和 Redell 把这种条件变量叫作覆盖条件（covering condition），因为它能覆盖所有需要唤醒线程的场景（保守策略）。成本如上所述，就是太多线程被唤醒。聪明的读者可能发现，在单个条件变量的生产者/消费者问题中，也可以使用这种方法。但是，在这个例子中，我们有更好的方法，因此用了它。一般来说，如果你发现程序只有改成广播信号时才能工作（但你认为不需要），可能是程序有缺陷，修复它！但在上述内存分配的例子中，广播可能是最直接有效的方案。</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>我们看到了引入锁之外的另一个重要同步原语：条件变量。当某些程序状态不符合要求时，通过允许线程进入休眠状态，条件变量使我们能够漂亮地解决许多重要的同步问题，包括著名的（仍然重要的）生产者/消费者问题，以及覆盖条件。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Lock of Programming]]></title>
        <id>https://blog.shunzi.tech/post/lock/</id>
        <link href="https://blog.shunzi.tech/post/lock/">
        </link>
        <updated>2021-04-01T08:37:41.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>编程中的锁。</li>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第二篇（Locks），锁。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>编程中的锁。</li>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第二篇（Locks），锁。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="before">Before</h2>
<ul>
<li>开始之前先简单阐述背景，最近项目测试发现在对于底层对象存储的并发访问过程中由于未进行并发控制而产生了一些问题，所以想趁此机会回顾“锁”的相关基础知识，并在此总结，看看能不能在总结的过程中顺便实现项目里的并发访问控制。</li>
<li>还是和往常一样，会贴很多链接，只是尝试着吸收网络上已经存在的优秀的教程，然后再进行一下转化（讲道理确实没啥新意，但主要是为了之后查阅复习起来比较方便）</li>
</ul>
<h2 id="参考链接">参考链接</h2>
<blockquote>
<ul>
<li>先贴参考链接主要是为了膜拜一下，并以表尊敬，因为绝大数情况下，参考链接里的文献就已经写的很好了，我的下文就其实没啥营养了大概，主要是为了节省大家的时间，看完早点去嗨皮~</li>
<li>参考链接也会持续地更新补充，文章的思路将主要沿袭教科书 Three Easy Pieces（其实我是偷懒顺便把书看了~）</li>
</ul>
</blockquote>
<ul>
<li><a href="https://zhuanlan.zhihu.com/p/71156910">[1] 知乎：通俗易懂 悲观锁、乐观锁、可重入锁、自旋锁、偏向锁、轻量/重量级锁、读写锁、各种锁及其Java实现！</a></li>
<li><a href="https://pages.cs.wisc.edu/~remzi/OSTEP/threads-locks.pdf">[2] Operating Systems: Three Easy Pieces - Concurrency - Locks </a></li>
<li><a href="https://www.zhihu.com/question/66733477/answer/1267625567">[3] 知乎：如何理解互斥锁、条件锁、读写锁以及自旋锁？</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/57354304">[4] 知乎：漫画|Linux 并发、竞态、互斥锁、自旋锁、信号量都是什么鬼？</a></li>
<li><a href="http://itmyhome.com/java-concurrent-programming/java-concurrent-programming.pdf">[5] Java 并发编程的艺术</a></li>
<li><a href="https://zhuanlan.zhihu.com/p/194283005">[6] 知乎：2w字 + 40张图带你参透并发编程！</a></li>
<li><a href="https://github.com/xlxing/personalLearn/blob/master/%E5%A4%9A%E5%A4%84%E7%90%86%E5%99%A8%E7%BC%96%E7%A8%8B%E7%9A%84%E8%89%BA%E6%9C%AF.pdf">[7] 多处理器编程的艺术</a>
<ul>
<li><a href="https://github.com/MottoX/TAOMP">Github Repo</a></li>
</ul>
</li>
<li><a href="https://zhuanlan.zhihu.com/p/138819184">[8] 知乎：volatile 关键字，你真的理解吗？</a> （这个参考可能有点例外，更多是针对缓存一致性的，但在并发场景也常使用）</li>
</ul>
<h2 id="locks">Locks</h2>
<ul>
<li>并发编程中的一个基本问题:我们希望原子地执行一系列指令，但由于单个处理器上出现中断(或多个线程同时在多个处理器上执行)，我们无法执行。我们通过引入所谓的锁来直接解决这个问题。程序员用锁注释源代码，把它们放在临界区周围，从而确保任何这样的临界区都像执行单个原子指令一样执行。</li>
</ul>
<h3 id="locks-the-basic-idea">Locks: The Basic Idea</h3>
<ul>
<li>作为一个例子，假设我们的临界区是这样的，共享变量的规范更新:</li>
</ul>
<pre><code class="language-C">balance = balance + 1;
</code></pre>
<ul>
<li>当然，也可能有其他关键部分，比如向链表添加元素，或者对共享结构进行其他更复杂的更新，但我们现在只讨论这个简单的示例。为了使用锁，我们在临界区周围添加如下代码:</li>
</ul>
<pre><code class="language-C">1 lock_t mutex; // some globally-allocated lock ’mutex’
2 ...
3 lock(&amp;mutex);
4 balance = balance + 1;
5 unlock(&amp;mutex);
</code></pre>
<ul>
<li>锁只是一个变量，因此要使用一个锁，必须声明某种类型的锁变量(比如上面的互斥锁)。这个锁变量(或直接简称“lock”)在任何时刻保持锁的状态。它是可用的(或未锁定的或空闲的)，因此没有线程持有该锁，也没有线程获得该锁(或锁定或持有)，因此只有一个线程持有该锁，并且可能处于临界区。我们也可以存储一些额外的信息在数据类型里，例如哪个线程持有当前锁。或者一个预定锁获取的队列，但是这样的信息对锁的使用者是隐藏的。</li>
<li>lock() 和 unlock() 例程的语义很简单。调用例程lock()试图获取锁;如果没有其他线程持有这个锁(即，它是空闲的)，这个线程将获得这个锁并进入临界区;这个线程有时被称为锁的所有者。如果另一个线程在这个锁变量上调用lock()(在本例中是互斥)，当锁被另一个线程持有时，它不会返回;这样，当持有锁的第一个线程还在临界区中时，其他线程就无法进入临界区。</li>
<li>一旦锁的所有者调用unlock()，锁现在就可用了。如果没有其他线程在等待锁(也就是说，没有其他线程调用了lock()并被困在锁中)，锁的状态就会被更改为free。如果有正在等待的线程(卡在lock()中)，其中一个将(最终)通知(或被告知)锁状态的改变，获取锁，并进入临界区</li>
<li>锁为程序员提供了对调度的最低限度的控制。通常，我们将线程视为程序员创建但由操作系统调度的实体，以操作系统选择的任何方式。锁将部分控制权交还给程序员;通过在一段代码周围加一个锁，程序员可以保证在该代码中不会有超过一个线程处于活动状态。因此，锁有助于将传统操作系统调度的混乱转变为一种更可控的活动</li>
</ul>
<h3 id="pthread-locks">Pthread Locks</h3>
<ul>
<li>POSIX库用于锁的名称是互斥锁，因为它用于提供线程之间的互斥，例如，如果一个线程处于临界区，它将排除其他线程进入，直到它完成临界区。因此，当你看到下面的POSIX线程代码时，你应该明白它正在做和上面一样的事情(我们再次使用我们的包装器在锁定和解锁时检查错误):</li>
</ul>
<pre><code class="language-C">1 pthread_mutex_t lock = PTHREAD_MUTEX_INITIALIZER;
2
3 Pthread_mutex_lock(&amp;lock); // wrapper; exits on failure
4 balance = balance + 1;
5 Pthread_mutex_unlock(&amp;lock);

// Keeps code clean; only use if exit() OK upon failure
void Pthread_mutex_lock(pthread_mutex_t *mutex) {
  int rc = pthread_mutex_lock(mutex);
  assert(rc == 0);
}
</code></pre>
<ul>
<li>您可能还注意到，POSIX版本将一个变量传递给lock和unlock，因为我们可能使用不同的锁来保护不同的变量。这样做可以增加并发性:与在访问任何关键部分时使用一个大锁(一种粗粒度的锁定策略)不同，通常使用不同的锁来保护不同的数据和数据结构，从而允许更多的线程同时处于锁定代码中(更细粒度的方法)。</li>
</ul>
<h3 id="building-a-lock">Building A Lock</h3>
<ul>
<li>我们如何建立一个高效的锁?高效的锁以较低的成本提供互斥，还可能获得我们下面讨论的一些其他属性。需要什么硬件支持?操作系统支持什么?</li>
<li>为了构建一个有效的锁，我们需要我们的老朋友——硬件，以及我们的好朋友——操作系统的帮助。多年来，许多不同的硬件原语被添加到各种计算机体系结构的指令集中;虽然我们不会研究这些指令是如何实现的(毕竟，这是计算机架构类的主题)，但我们将研究如何使用它们来构建像锁一样的互斥原语。我们还将研究操作系统是如何参与进来的，从而使我们能够构建一个复杂的锁库。</li>
</ul>
<h3 id="evaluating-locks">Evaluating Locks</h3>
<ul>
<li>在构建任何锁之前，我们应该首先理解我们的目标是什么，因此我们应该询问如何评估特定锁实现的有效性。要评价锁是否有效(以及是否有效)，我们应该建立一些基本的标准。第一个问题是锁是否执行它的基本任务，即提供互斥。基本上，锁是否有效，防止多个线程进入一个临界区?</li>
<li>第二是公平。是否每个争用锁的线程都有机会在锁空闲时获取它?另一种考虑这个问题的方法是检查更极端的情况:是否有任何争用锁的线程在争用锁的时候饿死，从而永远得不到锁?</li>
<li>最后一个标准是性能，特别是使用锁所增加的时间开销。这里有几个不同的情况值得考虑。一种是没有争用的情况;当一个线程正在运行并获取和释放锁时，这样做的开销是什么?另一种情况是多个线程争夺单个CPU上的锁;在这种情况下，是否存在性能问题?最后，当涉及多个cpu，并且每个cpu上的线程都争用该锁时，该锁是如何执行的?通过比较这些不同的场景，我们可以更好地理解使用各种锁定技术对性能的影响，如下所述。</li>
</ul>
<h3 id="controlling-interrupts">Controlling Interrupts</h3>
<ul>
<li>最早用于提供互斥的解决方案之一是禁用临界区中断;这种解决方案是为单处理器系统而发明的。代码如下所示:</li>
</ul>
<pre><code class="language-C">1 void lock() {
2   DisableInterrupts();
3 }
4 void unlock() {
5   EnableInterrupts();
6 }
</code></pre>
<ul>
<li>假设我们运行在这样一个单处理器系统上。通过在进入临界区之前关闭中断(使用某种特殊的硬件指令)，我们可以确保临界区内的代码不会被中断，从而可以像原子一样执行。当我们完成时，我们重新启用中断(同样是通过硬件指令)，这样程序就像往常一样继续运行。这种方法的主要优点是简单。你当然不需要绞尽脑汁来弄明白为什么这是可行的。在没有中断的情况下，线程可以确保它执行的代码将被执行，并且不会有其他线程干扰它。</li>
<li>不幸的是，负面的东西很多。
<ul>
<li>首先，这种方法要求我们允许任何调用线程执行特权操作(打开和关闭中断)，因此相信这种功能不会被滥用。正如你已经知道的，任何时候我们被要求信任一个任意的程序，我们可能会遇到麻烦。在这里，问题表现在许多方面:贪婪的程序可能会在执行开始时调用lock()，从而垄断处理器;更糟糕的是，一个错误的或恶意的程序可能会调用lock()并进入一个无穷循环。在后一种情况下，操作系统永远无法恢复对系统的控制，只有一种方法:重启系统。使用中断禁用作为通用的同步解决方案需要对应用程序有太多的信任。</li>
<li>第二，这种方法不能在多处理器上工作。如果多个线程运行在不同的cpu上，并且每个线程都试图进入相同的临界区，那么是否禁用中断是无关紧要的;线程可以在其他处理器上运行，因此可以进入临界区。由于多处理器现在很普遍，我们的一般解决方案必须做得比这更好。</li>
<li>第三，长时间关闭中断可能会导致中断丢失，从而导致严重的系统问题。例如，想象一下，如果CPU没有注意到磁盘设备已经完成了一个读请求。操作系统如何知道要唤醒等待读取的进程?</li>
<li>最后，也可能是最不重要的，这种方法可能效率低下。与普通的指令执行相比，屏蔽或取消中断的代码在现代cpu中执行得比较慢。</li>
</ul>
</li>
<li>由于这些原因，关闭中断只在有限的上下文中用作互斥原语。例如，在某些情况下，操作系统本身将使用中断屏蔽来保证访问自己的数据结构时的原子性，或者至少防止出现某些混乱的中断处理情况。这种用法是有意义的，因为信任问题在操作系统内部消失了，操作系统无论如何总是信任自己执行特权操作。</li>
</ul>
<h3 id="a-failed-attempt-just-using-loadsstores">A Failed Attempt: Just Using Loads/Stores</h3>
<ul>
<li>要超越基于中断的技术，我们将不得不依赖CPU硬件以及它提供给我们的构建正确锁的指令。让我们首先尝试通过使用单个标记变量来构建一个简单的锁。在这次失败的尝试中，我们将了解构建锁所需的一些基本思想，并(希望如此)了解为什么仅使用单个变量并通过正常的 load 和 store 访问它是不够的。</li>
<li>在如下第一次尝试中，想法非常简单:使用一个简单的变量(标志)来指示某个线程是否拥有锁。进入临界区的第一个线程将调用lock()，它将测试该标志是否等于1(在本例中，它不是)，然后将该标志设置为1，以表明该线程现在持有锁。当临界区结束时，线程调用unlock()并清除标志，从而表明锁不再被持有。</li>
<li>如果另一个线程碰巧在第一个线程处于临界区时调用了lock()，它将在while循环中简单地spin-wait，以便该线程调用unlock()并清除标志。一旦第一个线程这样做了，等待的线程将退出while循环，为自己设置标志为1，并继续进入临界区。</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t { int flag; } lock_t;
2
3 void init(lock_t *mutex) {
4   // 0 -&gt; lock is available, 1 -&gt; held
5   mutex-&gt;flag = 0;
6 }
7
8 void lock(lock_t *mutex) {
9   while (mutex-&gt;flag == 1) // TEST the flag
10      ; // spin-wait (do nothing)
11  mutex-&gt;flag = 1; // now SET it!
12 }
13
14 void unlock(lock_t *mutex) {
15  mutex-&gt;flag = 0;
16 }

</code></pre>
<ul>
<li>不幸的是，代码有两个问题:一个是正确性，另一个是性能。一旦您习惯了并行编程，正确性问题就很容易看到。想象一下如果代码交错;假设flag=0开始。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210401172110.png" alt="20210401172110" loading="lazy"></li>
<li>正如您从这个交错中看到的，通过及时(不及时?)中断，我们可以很容易地产生这样一种情况，即两个线程都将标志设置为1，从而两个线程都能够进入临界区。这种行为被专业人士称为“bad”——我们显然没有提供最基本的要求:提供互斥。</li>
<li>性能问题(我们将在后面详细讨论)是这样一个事实:线程等待获取已经持有的锁的方式:它无休止地检查flag的值，这种技术称为旋转等待。旋转等待浪费了等待另一个线程释放锁的时间。在单处理器上，这种浪费是非常高的，服务端正在等待的线程甚至不能运行(至少在发生上下文切换之前)!因此，当我们向前推进并开发更复杂的解决方案时，我们也应该考虑避免这种浪费的方法。</li>
</ul>
<blockquote>
<p>ASIDE: DEKKER’S AND PETERSON’S ALGORITHMS</p>
<ul>
<li>在20世纪60年代，Dijkstra向他的朋友们提出了并发问题，其中一位名叫Theodorus Jozef Dekker的数学家提出了一个解决方案。我们这里讨论的解决方案使用特殊的硬件指令甚至操作系统支持，而Dekker的算法只使用 load 和 store(假设它们彼此之间是原子的，这在早期的硬件上是正确的)。</li>
<li>Dekker的方法后来被Peterson改进了。同样，只使用load和store，这样做是为了确保两个线程不会同时进入临界区。下面是Peterson的算法(适用于两个线程);看看你能不能理解代码。flag 和 turn 的用途是什么? <strong>如下代码其实同时使用了 flag 和 turn 来调度线程，对应线程的 flag 置为了 1 说明该线程即将持有锁，和前面的代码类似，但是还使用了一个 turn 来表明下一个需要调度的线程的 ID，此时两个线程都运行到了 while 处时，条件 1 必定为真，条件二必定其中一个线程在执行时为真，该线程相应地进行自旋</strong></li>
<li><strong>flag 表示哪个线程想要占用临界区状态，就是举手表示想要访问临界区</strong></li>
<li><strong>turn 用于标识当前允许谁进入，就是谁的轮次</strong></li>
</ul>
</blockquote>
<pre><code class="language-C">int flag[2];
int turn;
void init() {
    // indicate you intend to hold the lock w/ ’flag’
    flag[0] = flag[1] = 0;
    // whose turn is it? (thread 0 or 1)
    turn = 0;
}
void lock() {
    // ’self’ is the thread ID of caller
    // 首先举手表示当前线程要访问临界区，eg 线程 1，即 flag[1]=1
    flag[self] = 1;
    // make it other thread’s turn
    // 这里其实有一个“礼让”的逻辑
    // 就是说虽然是当前线程在执行，但是会把轮次让给另外的线程
    // 如果另外的那个线程确实也举手了，
    //        然后另外的线程还没来得及礼让，那么就由另外的线程执行
    //        要是另外的线程也刚好礼让了，那么就由当前线程执行
    turn = 1 - self;
    // 首先看另外的线程举手了没？
    // case0. 没举手那就只有当前线程访问，直接运行就完事了
    // case1. 另外的线程举手了，那这时候就要看该谁的轮次了
    //   case1.0 如果这时候的轮次确实该另外的线程 1-self，那当前线程就自旋等待
    //   case2.0 如果这时候的轮次该自个儿 self，那当前线程就执行
    while ((flag[1-self] == 1) &amp;&amp; (turn == 1 - self))
        ;  // spin-wait while it’s not your turn
}
void unlock() {
    // simply undo your intent
    flag[self] = 0;
}
</code></pre>
<blockquote>
<ul>
<li>由于某些原因，开发不需要特殊硬件支持就能工作的锁在一段时间内变得非常流行，这给理论类型带来了许多问题。当然，当人们意识到假定有一点硬件支持会容易得多(事实上，这种支持早在多处理的早期就存在了)时，这一行的工作就变得毫无用处了。此外，上面的算法不能在现代硬件上运行(由于内存一致性模型不严格)，因此它们比以前更没用了。然而，更多的研究被扔进了历史的垃圾箱</li>
<li>Peterson 算法结合了 LockOne 和 LockTwo 两种算法
<ul>
<li>LockOne 只满足互斥，无法避免死锁。</li>
<li>LockTwo 也只满足互斥，无法避免死锁</li>
</ul>
</li>
<li>https://zh.wikipedia.org/wiki/Peterson%E7%AE%97%E6%B3%95#cite_note-3</li>
<li><a href="https://zhuanlan.zhihu.com/p/125739705">并发编程的艺术02-过滤锁算法</a></li>
</ul>
</blockquote>
<pre><code class="language-Java">class LockOne implements Lock {
    private boolean[] flag = new boolean[2];
    
    // 因为 Lock 本身不互斥，所以两个线程可能同时都将对方的 flag 标识设置为 true
    // 导致两个线程都被阻塞在 while(true) {} 。
    public void lock() {
        int i = ThreadId.get();
        int j = 1 - i;
        flag[i] = true;
        while(flag[j]) {}
    }
    
    public void unlock() {
        int i = ThreadID.get();
        flag[i] = false;
    }
}

class LockTwo implements Lock {
    private volatile int victim;
    
    // 当一个线程先执行，而另一个线程迟迟不执行
    // 这时候第一个线程就被锁住无法进行向下执行
    public void lock() {
        int i = ThreadId.get();

        // 阻塞率先执行 victim = i 的线程（礼让执行）
        victim = i;
        while(victim == i) {}
    }
    
    public void unlock() {
    }
}

class PetersonLock implements Lock {
    private boolean[] flag = new boolean[2];
    private volatile int victim;
    
    public void lock() {
        int i = ThreadId.get();
        int j = 1 - i;
        flag[i] = true;
        victim = i;
       while(flag[j] &amp;&amp; victim == i) {}
    }
    
    public void unlock() {
        int i = ThreadID.get();
        flag[i] = false;
    }
}

class FilterLock implements Lock {
    int[] level;
    int[] victim;
    int n;
    
    public FilterLock(int n) {
        level = new int[n];
        victim = new int[n];
        this.n = n;
        for (int i = 0;i &lt; n; i++) {
            level[i] = 0;
       }
    }
    
    public void lock() {
        int me = ThreadID.get();
        for (int i = 0; i &lt; n; i++) {
            level[me] = i;
            victim[i] = me;
            for (int k = 0; k &lt; n;k++) {
                while ((k != me) &amp;&amp; (level[k] &gt;= i &amp;&amp; victim[i] == me))) {
                    
                }
            }
        }
    }
    
    public void unlock() {
        int me = ThreadID.get();
        level[me] = 0;
    }
}
</code></pre>
<h3 id="building-working-spin-locks-with-test-and-set">Building Working Spin Locks with Test-And-Set</h3>
<blockquote>
<ul>
<li>当使用TAS实现TASLock （Test And Set Lock）测试-设置锁，它的特点是自旋时，每次尝试获取锁时，底层还是使用CAS操作，不断的设置锁标志位的过程会一直修改共享变量的值（回写），会引发缓冲一致性流量发风暴。【因为每一次CAS操作都会发出广播通知其他处理器，从而影响程序的性能。】</li>
<li><a href="https://en.wikipedia.org/wiki/Test-and-set">Wikipedia: Test and Set</a></li>
</ul>
</blockquote>
<ul>
<li>因为禁用中断不能在多个处理器上工作，而且使用load和store(如上所示)的简单方法也不能工作，所以系统设计人员开始发明对锁定的硬件支持。最早的多处理器系统，如20世纪60年代早期的Burroughs B5000，就有这样的支持;如今，所有系统都提供这种类型的支持，即使是单CPU系统。</li>
<li>需要理解的最简单的硬件支持是 test-and-set (或 atomic exchange) 指令。我们通过下面的C代码片段来定义 test-and-set 指令的作用:</li>
</ul>
<pre><code class="language-C">1 int TestAndSet(int *old_ptr, int new) {
2   int old = *old_ptr; // fetch old value at old_ptr
3   *old_ptr = new; // store ’new’ into old_ptr
4   return old; // return the old value
5 }

1 typedef struct __lock_t {
2   int flag;
3 } lock_t;
4
5 void init(lock_t *lock) {
6   // 0: lock is available, 1: lock is held
7   lock-&gt;flag = 0;
8 }
9
10 void lock(lock_t *lock) {
11  while (TestAndSet(&amp;lock-&gt;flag, 1) == 1)
12      ; // spin-wait (do nothing)
13 }
14
15 void unlock(lock_t *lock) {
16  lock-&gt;flag = 0;
17 }
</code></pre>
<ul>
<li><strong>test-and-set 指令的作用如下。它返回旧 ptr 所指向的旧值，并同时将该值更新为new。当然，关键是这个操作序列是以原子的方式执行的。</strong> 之所以称之为 “test-and-set”，是因为它使您能够同时“test”旧值(即返回的值)“set”内存位置为一个新值;事实证明，这个稍微强大一些的指令足以构建一个简单的旋转锁。</li>
<li>我们先弄清楚为什么这把锁可以用。首先想象一下这样一种情况:
<ul>
<li>一个线程调用了lock()，而当前没有其他线程持有该锁;因此，flag应该是0。当线程调用 TestAndSet(flag，1)，线程返回旧的 flag 值，即 0; 因此，正在 testing flag 值的调用线程将不会在 while 循环中被捕获，并将获得锁。线程也会自动地将该值设置为 1，从而表明现在已持有锁。当线程完成它的临界区时，它调用unlock()将标志设为0。</li>
<li>我们可以想象的第二种情况是，当一个线程已经持有锁(即，flag是1)。在这种情况下，这个线程将调用lock()，然后调用 TestAndSet(flag, 1)。这一次，teststandset()将返回旧的flag值，即1(因为锁被持有)，同时再次将其设置为1。只要锁被另一个线程持有，TestAndSet()就会反复返回1，因此这个线程就会不停地旋转，直到锁最终被释放。当标志最终被其他线程设置为0时，该线程将再次调用 TestAndSet()，它现在将返回0，同时原子地将值设置为1，从而获得锁并进入临界区。</li>
</ul>
</li>
<li><strong>通过将 Test(对旧锁的值)和 Set(对新值的值)都设置为单个原子操作，我们确保只有一个线程获得锁。这就是如何构建一个有效的互斥原语</strong>!</li>
<li>现在可能也理解了为什么这种类型的锁通常称为自旋锁。它是需要构建的最简单的锁类型，只是使用CPU周期旋转，直到锁可用为止。为了在单个处理器上正常工作，它需要一个抢占式调度程序(即，为了不时地运行另一个线程，它会通过计时器中断一个线程)。如果没有抢占，旋转锁在单个CPU上没有多大意义，因为在CPU上旋转的线程永远不会放弃它。</li>
<li>汇编代码实现：</li>
</ul>
<pre><code>enter_region:        ; A &quot;jump to&quot; tag; function entry point.

  tsl reg, flag      ; Test and Set Lock; flag is the
                     ; shared variable; it is copied
                     ; into the register reg and flag
                     ; then atomically set to 1.

  cmp reg, #0        ; Was flag zero on entry_region?

  jnz enter_region   ; Jump to enter_region if
                     ; reg is non-zero; i.e.,
                     ; flag was non-zero on entry.

  ret                ; Exit; i.e., flag was zero on
                     ; entry. If we get here, tsl
                     ; will have set it non-zero; thus,
                     ; we have claimed the resource
                     ; associated with flag.

leave_region:
  move flag, #0      ; store 0 in flag
  ret                ; return to caller
</code></pre>
<h3 id="evaluating-spin-locks">Evaluating Spin Locks</h3>
<ul>
<li>有了我们的基本自旋锁，我们现在可以评估它在我们前面描述的轴上的有效性。锁最重要的方面是<strong>正确性</strong>:它是否提供了互斥?答案是肯定的:旋转锁一次只允许一个线程进入临界区。这样，我们就有了一个正确的锁。</li>
<li>下一个轴是<strong>公平</strong>。旋转锁对等待的线程有多公平?你能保证等待的线程会进入临界区吗?不幸的是，这里的答案是坏消息:<strong>旋转锁不提供任何公平性保证。的确，在争用的情况下，在旋转等待的线程可能一直旋转。简单的旋转锁(到目前为止已经讨论过)是不公平的，可能会导致线程饿死</strong>。</li>
<li>最后一个轴是<strong>性能</strong>。使用旋转锁的成本是多少?为了更仔细地分析这个问题，我们建议考虑几个不同的案例。在第一种情况下，想象线程在单个处理器上竞争锁;在第二种情况下，考虑分布在多个cpu上的线程。
<ul>
<li>对于自旋锁，在单一CPU的情况下，性能开销是非常痛苦的;想象一下持有锁的线程在一个临界区中被抢占的情况。调度程序可能会运行其他每一个线程(假设有N−1个其他线程)，每个线程都试图获取锁。在这种情况下，每个线程都将在一个时间片的持续时间内旋转，然后放弃CPU，这是对CPU周期的浪费。</li>
<li>但是，在多个cpu上，旋转锁工作得相当好(如果线程的数量大致等于cpu的数量)。假设CPU 1上的线程A和CPU 2上的线程B都争用一个锁。如果线程(CPU 1)持有锁,线程B试图加锁,B将自旋(CPU 2)。然而,假定关键的部分较短,因此很快锁就变得可用,线程B将获得锁。旋转等待锁在另一个处理器不会浪费很多的周期在本例中,从而可以很有效。</li>
</ul>
</li>
</ul>
<blockquote>
<ul>
<li><strong>更多避免旋转的理由:优先级反转</strong></li>
<li>避免旋转锁的一个很好的理由是性能:正如正文中所描述的，如果一个线程在持有一个锁时被中断，其他使用旋转锁的线程将花费大量的CPU时间来等待锁可用。然而，在某些系统上避免自旋锁还有另一个有趣的原因:正确性。要警惕的问题是所谓的优先级反转。</li>
<li>现在,这个问题。假设T2由于某种原因被阻塞。所以T1运行，抓住旋转锁，进入一个临界区。T2现在变得畅通(可能是因为一个I/O完成了)，CPU调度器会立即调度它(从而重新调度T1)。T2现在尝试获取锁，因为它不能(T1持有锁)，所以它只是继续旋转。因为锁是自旋锁，T2永远自旋，系统就挂了。</li>
<li>不幸的是，仅仅避免使用旋转锁并不能避免反转的问题(唉)。假设有三个线程，T1, T2和T3。T3的优先级最高，T1的优先级最低。现在想象一下，T1抓住了一个锁。然后T3启动，由于它的优先级高于T1，所以它立即运行(抢占T1)。T3试图获取T1所持有的锁，但是由于T1仍然持有锁，所以被困在等待中。如果T2开始运行，它将拥有比T1更高的优先级，因此它将运行。T3的优先级高于T2，它被困在等待T1，由于T2正在运行，T1可能永远不会运行。强大的T3不能运行，而不起眼的T2控制着CPU ?高优先级已经不像以前那么重要了。</li>
<li>您可以通过多种方式解决优先级反转问题。在自旋锁导致问题的特定情况下，可以避免使用自旋锁(后面将详细描述)。一般来说，高优先级线程等待低优先级线程可以暂时提高低优先级线程的优先级，从而使其能够运行并克服反转，这种技术称为优先级继承。最后一个解决方案是最简单的:确保所有线程具有相同的优先级。</li>
</ul>
</blockquote>
<h3 id="compare-and-swap">Compare-And-Swap</h3>
<blockquote>
<ul>
<li>在计算机科学中，比较与交换(CAS)是多线程中用来实现同步的原子指令。它将内存位置的内容与给定值进行比较，只有当它们相同时，才将该内存位置的内容修改为新的给定值。这是作为单个原子操作完成的</li>
<li><a href="https://en.wikipedia.org/wiki/Compare-and-swap">Wikipedia: CAS</a></li>
</ul>
</blockquote>
<ul>
<li>一些系统提供的另一个硬件原语是 Compare-And-Swap 指令(例如，SPARC上这样称呼它)或compare-and-exchange 指令(x86上这样称呼它)。<strong>也就是我们常说的 CAS 操作</strong>。这条指令的C伪代码如下：</li>
</ul>
<pre><code class="language-C">1 int CompareAndSwap(int *ptr, int expected, int new) {
2   int original = *ptr;
3   if (original == expected)
4       *ptr = new;
5   return original;
6 }
</code></pre>
<ul>
<li>其基本思想是通过  compare-and-swap 来测试ptr指定的地址上的值是否等于预期值;如果是，用新值更新ptr所指向的内存位置。如果不是，那就什么都不做。在这两种情况下，返回该内存位置的原始值，从而允许调用 compare-and-swap 的代码知道它是否成功。</li>
<li>CAS 操作其实就比 TAS 多了一个参数，<code>expected</code> 预期值，预期值 E 主要用于和当前值的比对，从而来决定是否更新新的值。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210404172916.png" alt="20210404172916" loading="lazy"></li>
<li>使用 compare-and-swap 指令，我们可以以类似于使用 test-and-set 的方式构建锁。例如，我们可以用以下代码替换上面的lock()例程:</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (CompareAndSwap(&amp;lock-&gt;flag, 0, 1) == 1)
3       ; // spin
4 }
</code></pre>
<ul>
<li>其余的代码与上面的测试集示例相同。这段代码的工作原理非常相似;它只是检查标志是否为0，如果是，就自动交换1，从而获得锁。当锁被持有时，试图获取锁的线程将被困在旋转中，直到锁最终被释放。</li>
<li>如果您想了解如何真正制作一个 c 可调用的x86版本的 compare-and-swap，看以下链接：<br>
https://github.com/remzi-arpacidusseau/ostep-code/tree/master/threads-locks</li>
</ul>
<pre><code class="language-C">#include &lt;stdio.h&gt;

int global = 0;

char compare_and_swap(int *ptr, int old, int new) {
    unsigned char ret;
    // Note that sete sets a ’byte’ not the word
    __asm__ __volatile__ (
	&quot; lock\n&quot;
	&quot; cmpxchgl %2,%1\n&quot;
	&quot; sete %0\n&quot;
	: &quot;=q&quot; (ret), &quot;=m&quot; (*ptr)
	: &quot;r&quot; (new), &quot;m&quot; (*ptr), &quot;a&quot; (old)
	: &quot;memory&quot;);
    return ret;
}

int main(int argc, char *argv[]) {
    printf(&quot;before successful cas: %d\n&quot;, global);
    int success = compare_and_swap(&amp;global, 0, 100);
    printf(&quot;after successful cas: %d (success: %d)\n&quot;, global, success);
    
    printf(&quot;before failing cas: %d\n&quot;, global);
    success = compare_and_swap(&amp;global, 0, 200);
    printf(&quot;after failing cas: %d (old: %d)\n&quot;, global, success);

    return 0;
}
</code></pre>
<ul>
<li>最后，您可能已经感觉到，compare_and_swap 指令比 test-and-set 指令更强大。我们将在以后简要地研究诸如无锁同步等主题时使用这种能力。然而，如果我们只是用它构建一个简单的自旋锁，它的行为与我们上面分析的自旋锁相同。</li>
</ul>
<h3 id="load-linked-and-store-conditional">Load-Linked and Store-Conditional</h3>
<ul>
<li>一些平台提供了一对协同工作的指令来帮助构建关键部分。例如，在MIPS架构中，load-linked 和 store-conditional 指令可以串联使用来构建锁和其他并发结构。这些指令的 C 伪代码如下。Alpha、PowerPC和ARM提供了类似的说明</li>
</ul>
<pre><code class="language-C">1 int LoadLinked(int *ptr) {
2   return *ptr;
3 }
4
5 int StoreConditional(int *ptr, int value) {
6   if (no update to *ptr since LoadLinked to this address) {
7       *ptr = value;
8       return 1; // success!
9   } else {
10      return 0; // failed to update
11  }
12 }
</code></pre>
<ul>
<li>load-linked的操作与典型的load指令非常相似，它只是从内存中获取一个值并将其放入寄存器中。关键的不同在于store-conditional，它<strong>只在没有对该地址进行存储的情况下才成功(并更新存储在刚刚进行load-linked的地址上的值)</strong>。在成功的情况下，store-conditional返回1并将ptr处的值更新为value;如果失败，则ptr处的值不会更新，返回0。</li>
<li>lock()代码是唯一有趣的部分。首先，线程旋转，等待标志被设置为0(从而表明锁没有被持有)。一旦这样，线程试图通过store条件获取锁;如果成功，线程就自动地将标志的值更改为1，因此可以进入临界区</li>
<li>请注意 store-conditional 是如何发生故障的。一个线程调用lock()并执行LoadLinked，当锁未被持有时返回0。在它尝试 store-conditional 之前，它被中断，另一个线程进入 lock代码，也执行 load-linked 指令，得到一个0，然后继续。此时，两个线程分别执行了load-linked，并且都准备尝试store-conditional。这些指令的<strong>关键特性是，只有其中一个线程会成功地将标志更新为1</strong>，从而获得锁;第二个尝试 store-conditional 的线程将失败(因为另一个线程更新了它的load-linked和store-conditional之间的标志值)，因此必须再次尝试获取锁。</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (1) {
3       while (LoadLinked(&amp;lock-&gt;flag) == 1)
4           ; // spin until it’s zero
5       if (StoreConditional(&amp;lock-&gt;flag, 1) == 1)
6           return; // if set-it-to-1 was a success: all done
7                   // otherwise: try it all over again
8   }
9 }
10
11 void unlock(lock_t *lock) {
12  lock-&gt;flag = 0;
13 }
</code></pre>
<ul>
<li>简化一下如上代码</li>
</ul>
<pre><code class="language-C">1 void lock(lock_t *lock) {
2   while (LoadLinked(&amp;lock-&gt;flag) ||
3           !StoreConditional(&amp;lock-&gt;flag, 1))
4       ; // spin
5 }
</code></pre>
<h3 id="fetch-and-add">Fetch-And-Add</h3>
<ul>
<li>最后一个硬件原语是“Fetch-And-Add”指令，它在返回特定地址的旧值时自动增加一个值。Fetch-And-Add指令的C伪代码是这样的:</li>
</ul>
<pre><code class="language-C">1 int FetchAndAdd(int *ptr) {
2   int old = *ptr;
3   *ptr = old + 1;
4   return old;
5 }
</code></pre>
<ul>
<li>在本例中，我们将使用取加(fetch-and-add)来构建一个更有趣的 ticket lock，。锁定和解锁代码如下：</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t {
2   int ticket;
3   int turn;
4 } lock_t;
5
6 void lock_init(lock_t *lock) {
7   lock-&gt;ticket = 0;
8   lock-&gt;turn = 0;
9 }
10
11 void lock(lock_t *lock) {
12  int myturn = FetchAndAdd(&amp;lock-&gt;ticket);
13  while (lock-&gt;turn != myturn)
14      ; // spin
15 }
16
17 void unlock(lock_t *lock) {
18  lock-&gt;turn = lock-&gt;turn + 1;
19 }

</code></pre>
<ul>
<li>这个解决方案不是使用单个值，而是组合使用 ticket 和 turn 变量来构建一个锁。基本操作非常简单:当线程希望获取锁时，它首先对票据值执行原子的取加操作;这个值现在被认为是这个线程的“回合”(myturn)。然后使用全局共享的 lock-&gt;turn 来确定线程的回合;当(myturn == turn)对于一个给定的线程，它是线程进入临界区的回合。解锁可以通过增加回合数来实现，这样下一个等待线程(如果有的话)就可以进入临界区。</li>
<li>注意这个解决方案与我们之前尝试的一个重要区别:它确保了所有线程的执行。一旦给一个线程分配了它的票证值，它将在未来的某个时间点被调度(一旦它前面的线程通过了临界区并释放了锁)。在我们以前的尝试中，并不存在这样的保证;在 test-and-set 上自旋的线程(例如)可能会永远自旋，即使其他线程获得和释放锁。</li>
</ul>
<h3 id="too-much-spinning-what-now">Too Much Spinning: What Now?</h3>
<ul>
<li>我们的简单的基于硬件的锁很简单(只有几行代码)，而且它们可以工作(如果您愿意，甚至可以通过编写一些代码来证明这一点)，这是任何系统或代码的两个优秀特性。然而，在某些情况下，这些解决方案可能非常低效。假设您在一个处理器上运行两个线程。现在，假设有一个线程(线程0)处于临界区，因此持有一个锁，不幸的是被中断。第二个线程(线程1)现在尝试获取锁，但发现锁被持有。因此，它开始自旋，接着自旋。</li>
<li>然后它继续自旋。最后，时钟中断产生，线程0再次运行，释放锁，最后(比如，下一次运行时)，线程1不必自选了，可以获得锁。因此，任何时候一个线程在这种情况下被捕获，它都浪费了整个时间片，自旋只检查一个不会改变的值! 当有N个线程争用一个锁时，这个问题会变得更糟;N−1个时间片也可能以类似的方式浪费，只是自旋并等待一个线程释放锁。因此，我们的下一个问题:<strong>我们怎样才能开发出一个不需要浪费时间在CPU上自旋的锁呢</strong>？</li>
<li>仅靠硬件支持无法解决这个问题。我们也需要操作系统的支持!现在让我们来看看它是如何工作的。</li>
</ul>
<h3 id="a-simple-approach-just-yield-baby">A Simple Approach: Just Yield, Baby</h3>
<ul>
<li>硬件支持让我们走得很远:工作锁，甚至(就像票据锁的情况)获取锁的公平性。然而，我们仍然有一个问题:当临界区发生上下文切换，并且线程开始无休止地旋转，等待被中断时，该怎么办让(持有锁的)线程再次运行?</li>
<li>我们的第一个尝试是一个简单而友好的方法:当要自旋时，将CPU让给另一个线程。如下演示了这种方法：</li>
</ul>
<pre><code class="language-C">1 void init() {
2   flag = 0;
3 }
4
5 void lock() {
6   while (TestAndSet(&amp;flag, 1) == 1)
7       yield(); // give up the CPU
8 }
9
10 void unlock() {
11  flag = 0;
12 }

</code></pre>
<ul>
<li>在这种方法中，我们假设一个操作系统原语 <code>yield()</code>，当线程想要放弃CPU并让另一个线程运行时，可以调用它。线程可以处于以下三种状态中的一种(运行、准备或阻塞); <code>yield</code>只是一个系统调用，它将调用者从运行状态移动到就绪状态，从而使另一个线程运行。因此，<strong>yield 的线程本质上是对自己进行了重新调度</strong>。</li>
<li>考虑一个CPU上有两个线程的例子;在这种情况下，我们基于 yield 的方法非常有效。如果一个线程碰巧调用了lock()，并发现一个锁被持有，它就会释放CPU，这样另一个线程就会运行并完成它的临界区。在这个简单的例子中，yielding 方法很有效。</li>
<li>现在让我们考虑一下有许多线程(比如100个)反复争夺一个锁的情况。在这种情况下，如果一个线程获得了锁，并在释放它之前被抢占，其他99个线程将调用lock()，找到所持有的锁，并交出CPU。假设有某种轮询调度器，在持有锁的线程再次运行之前，这99个调度器中的每一个都将执行这个run-and-yield模式。虽然比我们的旋转方法更好(这将浪费99个时间切片旋转)，<strong>但这种方法仍然是昂贵的;上下文切换的成本可能很高，因此会产生大量的浪费</strong>。</li>
<li>更糟糕的是，我们根本没有解决饥饿问题。当其他线程反复进入和退出临界区时，一个线程可能会陷入一个无穷 yield 循环。我们显然需要一种直接解决这一问题的方法。</li>
</ul>
<h3 id="using-queues-sleeping-instead-of-spinning">Using Queues: Sleeping Instead Of Spinning</h3>
<ul>
<li>我们之前方法的真正问题是，它们留给了太多的机会。调度程序决定下一个线程运行;如果调度程序做出了错误的选择，那么运行的线程要么必须自旋，等待锁(我们的第一种方法)，要么立即 yield CPU (我们的第二种方法)。无论哪种方式，都有浪费的可能，而且无法防止饥饿。</li>
<li>因此，在当前持有者释放锁之后，必须显式地对下一个获得锁的线程施加一些控制。为此，我们需要更多的操作系统支持，以及<strong>一个队列来跟踪哪些线程正在等待获取锁</strong>。</li>
<li>为简单起见，我们将使用 Solaris 提供的支持，包括两种调用： <strong><code>park()</code> 将调用线程置于睡眠状态，<code>unpark(threadID)</code> 唤醒由threadID指定的特定线程。</strong> 这两个例程可以一起使用来构建一个锁，如果调用者试图获取一个被占用的锁，那么这个锁将使调用者处于休眠状态，当锁空闲时将其唤醒。如下代码所示</li>
</ul>
<pre><code class="language-C">1 typedef struct __lock_t {
2   int flag;
3   int guard;
4   queue_t *q;
5 } lock_t;
6
7 void lock_init(lock_t *m) {
8   m-&gt;flag = 0;
9   m-&gt;guard = 0;
    // 初始化锁的等待队列
10  queue_init(m-&gt;q);
11 }
12
13 void lock(lock_t *m) {
    // 尝试获取锁，将 guard 设置为 1，如果已经为 1 自旋等待
14  while (TestAndSet(&amp;m-&gt;guard, 1) == 1)
15      ; //acquire guard lock by spinning
16  if (m-&gt;flag == 0) {
        // 获取到了锁，将 guard 置为 0
17      m-&gt;flag = 1; // lock is acquired
18      m-&gt;guard = 0;
19  } else {
        // 未获取到锁，加入该锁的等待队列中
20      queue_add(m-&gt;q, gettid());
        // 将 guard 置为 0
21      m-&gt;guard = 0;
        // 将该线程休眠，等待唤醒
22      park();
23  }
24 }
25
26 void unlock(lock_t *m) {
    // 将 guard 设置为 1，如果已经为 1 自选等待
27  while (TestAndSet(&amp;m-&gt;guard, 1) == 1)
28      ; //acquire guard lock by spinning
29  if (queue_empty(m-&gt;q))
        // 如果该锁的等待队列为空，那就标记锁为空闲态
30      m-&gt;flag = 0; // let go of lock; no one wants it
31  else
        // 如果该锁的等待队列不为空，相应的出队一个线程，唤醒该线程去获取锁
32      unpark(queue_remove(m-&gt;q)); // hold lock
33                                  // (for next thread!)
    // 将 guard 设置为 0
34  m-&gt;guard = 0;
35 }

</code></pre>
<ul>
<li>在这个例子中，我们做了一些有趣的事情。首先，我们将旧的 test-and-set 思想与一个显式的锁等待队列相结合，以获得更高效的锁。其次，我们<strong>使用队列来帮助控制谁下一个获得锁，从而避免饥饿</strong>。</li>
<li>您可能会注意到 <strong>Guard 是如何使用的，它基本上是围绕着锁所使用的标志和队列操作的自旋锁</strong>。因此，<strong>这种方法不能完全避免旋转等待;一个线程可能在获取或释放锁时被中断，从而导致其他线程旋转——等待这个线程再次运行。然而，旋转所花费的时间非常有限</strong>(锁定和解锁代码中只有几条指令，而不是用户定义的关键部分)，因此这种方法可能是合理的。</li>
<li>您可能还会注意到，在lock()中，当线程无法获得锁(它已经被持有)时，我们会小心地将自己添加到队列中(通过调用gettid()函数来获取当前线程的线程ID)，将guard设置为0，并 yield CPU。给读者一个问题:如果在park()之后，而不是之前，释放守卫锁会发生什么?提示: something bad。（<strong>park之后设置guard的话，就会出现guard一直为1的情况，然后一直陷入自旋等待</strong>）</li>
<li>您还可以进一步检测到，当另一个线程被唤醒时，该标志没有被设回0。这是为什么呢?好吧，这不是错误，而是必须的!当一个线程被唤醒时，它将像从park()返回一样;然而，它在代码的那一点上并没有保持保护，因此甚至不能尝试将标志设置为1。因此，我们只需将释放锁的线程直接传递给获取锁的下一个线程;flag在两者之间没有设置为0</li>
<li>最后，您可能会注意到解决方案中在调用park()之前出现了可感知的竞争条件。在错误的时间（比如 park 调用之前），一个线程将会被暂停，假设它应该休眠，直到锁不再被持有。此时切换到另一个线程(例如，持有锁的线程)可能会导致问题，例如，如果该线程随后释放了锁。第一个线程之后的park 将永远休眠(潜在的)，这个问题有时被称为wakeup/waiting race。为了解决这个问题，需要做一些额外的工作。（<strong>其实就是在无限等待 park</strong>）</li>
<li>Solaris通过添加第三个<strong>系统调用 setpark() 解决了这个问题。通过调用这个例程，线程可以指示它将要 park。如果它恰好被中断了，刚好另一个线程被调度，并且另一个线程在实际调用 park 之前调用unpark，那么随后的 park 将立即返回，而不是休眠</strong>。lock() 内部的代码修改非常小</li>
</ul>
<pre><code class="language-C">1 queue_add(m-&gt;q, gettid());
2 setpark(); // new code
3 m-&gt;guard = 0;
</code></pre>
<ul>
<li>另一种解决方案可以将 guard 传递到内核中。在这种情况下，内核可以采取预防措施，原子地释放锁并使正在运行的线程退出队列。</li>
</ul>
<h3 id="different-os-different-support">Different OS, Different Support</h3>
<ul>
<li>到目前为止，我们已经看到了操作系统为了在线程库中构建更有效的锁而提供的一种支持。其他操作系统也提供了类似的支持;细节有所不同。</li>
<li>例如，Linux提供了一个futex，它类似于Solaris接口，但提供了更多的内核功能。具体来说，每个futex都有一个特定的物理内存位置，以及一个每个futex的内核队列。调用者可以根据需要使用futex调用(如下所述)来睡眠和唤醒。</li>
<li>具体来说，有两个调用可用。对 futex_wait(address, expected) 的调用将调用线程置于睡眠状态，假设 address 上的值等于 expected。如果不相等，则调用立即返回。对例程futex_wake(address) 的调用唤醒一个正在队列中等待的线程。这些调用在 Linux 互斥中的用法如下所示：</li>
</ul>
<pre><code class="language-C">1 void mutex_lock (int *mutex) {
2   int v;
3   /* Bit 31 was clear, we got the mutex (the fastpath) */
    // 自旋锁
4   if (atomic_bit_test_set (mutex, 31) == 0)
5       return;
    // 请求锁，相应的 mutex+1
6   atomic_increment (mutex);
7   while (1) {
        // 被unlock唤醒了！！获取锁然后维护等待队列长度
8       if (atomic_bit_test_set (mutex, 31) == 0) {
9           atomic_decrement (mutex);
10          return;
11      }
12      /* We have to waitFirst make sure the futex value
13         we are monitoring is truly negative (locked). */
14      v = *mutex;
        // 判断整数的正负，正则为被持有
15      if (v &gt;= 0)
16          continue;
        // 为负，即该锁将被持有，会有竞争发生，休眠线程
        // 原子性的检查 mutex 中计数器的值是否为val,如果是则让进程休眠，
        // 直到FUTEX_WAKE或者超时(time-out)。也就是把进程挂到 mutex 相对应的等待队列上去。
17      futex_wait (mutex, v);
18  }
19 }
20
21 void mutex_unlock (int *mutex) {
22  /* Adding 0x80000000 to counter results in 0 if and
23  only if there are not other interested threads */
    // 解锁，如果等待队列长度是0就不用唤醒！
    // 不把这个逻辑放futex_wake是为了减少sys call的开销。
24  if (atomic_add_zero (mutex, 0x80000000))
25      return;
26
27  /* There are other threads waiting for this mutex,
28     wake one of them up. */
    // 唤醒等待 mutex 的进程
29  futex_wake (mutex);
30 }
</code></pre>
<ul>
<li>这段来自nptl库(gnu libc库的一部分)中的 <code>lowlevellock.h</code> 的代码片段很有趣，原因有几个。首先，它使用单个整数来跟踪锁是否被持有(整数的高位)和锁上的等待者数量(所有其他位)。因此，如果锁是负的，它将被持有(因为设置了高位，该位决定了整数的符号)。</li>
<li>其次，代码片段展示了如何针对常见情况进行优化，特别是当没有争用锁的时候;只有一个线程获取和释放锁，完成的工作很少(获取锁时 TestAndSet 原子位运算，释放锁原子位加法)。</li>
<li>看看您能否解开这个“现实世界”锁的其余部分，以理解它是如何工作的。做到这一点，成为Linux锁的大师，或者至少是当一本书告诉你要做什么的时候倾听的人。</li>
</ul>
<h3 id="two-phase-locks">Two-Phase Locks</h3>
<ul>
<li>最后一点:Linux方法有一种老方法，这种方法已经断断续续地使用了多年，至少可以追溯到 Dahm 锁在1960年代早期，现在被称为两阶段锁。两阶段锁意识到自旋是有用的，特别是当锁即将被释放的时候。所以在<strong>第一阶段，锁会自旋一段时间，希望它能获得锁</strong>。</li>
<li>但是，<strong>如果在第一个旋转阶段没有获得锁，那么就会进入第二个阶段，在这个阶段中调用者被置于睡眠状态，直到后来锁被释放时才会被唤醒</strong>。上面的Linux锁就是这种锁的一种形式，但它只旋转一次;<strong>更常见的方式是在循环中自旋固定的次数，然后使用 futex 睡眠。</strong></li>
<li>两阶段锁是混合方法的另一个实例，在这种方法中，结合两个好的想法可能会产生一个更好的想法。当然，是否这样做很大程度上取决于许多因素，包括硬件环境、线程数量和其他工作负载细节。像往常一样，制作一个适用于所有可能用例的通用锁是一个很大的挑战。</li>
</ul>
<h3 id="summary">Summary</h3>
<ul>
<li>上面的方法展示了目前如何构建真正的锁:一些硬件支持(以更强大的指令的形式)加上一些操作系统支持(例如，在Solaris上以park()和unpark()原语的形式，或在Linux上以futex的形式)。当然，细节是不同的，执行这种锁定的确切代码通常是经过高度调整的。如果您想了解更多细节，请查看Solaris或Linux代码库;它们是一本引人入胜的读物。也可以参阅David等人关于现代多处理器上的锁策略比较的优秀工作。</li>
</ul>
<h2 id="lock-based-concurrent-data-structures">Lock-based Concurrent Data Structures</h2>
<ul>
<li>对于特定数据结构，如何加锁才能让该结构功能正确？进一步，如何对该数据结构加锁，能够保证高性能，让许多线程同时访问该结构，即并发访问（concurrently）？</li>
<li>这里会简单介绍一些并发的数据结构，后期补充一个 <strong>并发跳表</strong>。</li>
</ul>
<h3 id="concurrent-counters">Concurrent Counters</h3>
<ul>
<li>如下先定义一个非并发的计数器，操作很简单，主要三种操作。+ / - / r</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int value;
3 } counter_t;
4
5 void init(counter_t *c) {
6   c-&gt;value = 0;
7 }
8
9 void increment(counter_t *c) {
10  c-&gt;value++;
11 }
12
13 void decrement(counter_t *c) {
14  c-&gt;value--;
15 }
16
17 int get(counter_t *c) {
18  return c-&gt;value;
19 } 
</code></pre>
<ul>
<li>为了实现并发的效果，简单粗暴地加锁就完事了。其实很好理解，就是在相应的非线程安全函数执行之前对应的加锁和释放锁就 OK。显然简单粗暴多半就意味着性能是有一定问题的。</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int value;
3   pthread_mutex_t lock;
4 } counter_t;
5
6 void init(counter_t *c) {
7   c-&gt;value = 0;
8   Pthread_mutex_init(&amp;c-&gt;lock, NULL);
9 }
10
11 void increment(counter_t *c) {
12  Pthread_mutex_lock(&amp;c-&gt;lock)
13  c-&gt;value++;
14  Pthread_mutex_unlock(&amp;c-&gt;lock);
15 }
16
17 void decrement(counter_t *c) {
18  Pthread_mutex_lock(&amp;c-&gt;lock);
19  c-&gt;value--;
20  Pthread_mutex_unlock(&amp;c-&gt;lock);
21 }
22
23 int get(counter_t *c) {
24  Pthread_mutex_lock(&amp;c-&gt;lock);
25  int rc = c-&gt;value;
26  Pthread_mutex_unlock(&amp;c-&gt;lock);
27  return rc;
28 } 

</code></pre>
<ul>
<li>为了理解简单方法的性能代价，我们运行了一个基准测试，其中每个线程更新一个共享计数器固定次数;然后我们改变线程的数量。下图显示了在一到四个线程处于活动状态时所花费的总时间;每个线程更新计数器一百万次。这个实验是在装有四个Intel 2.7 GHz i5 cpu的iMac上进行的;随着更多的cpu处于活动状态，我们希望在单位时间内完成更多的工作</li>
<li>从图上方的曲线（标为“Precise”）可以看出，同步的计数器扩展性不好。单线程完成 100 万次更新只需要很短的时间（大约 0.03s），而两个线程并发执行，每个更新 100 万次，性能下降很多（超过 5s！）。线程更多时，性能更差。</li>
<li>理想情况下，你会看到多处理上运行的多线程就像单线程一样快。达到这种状态称为完美扩展（perfect scaling）。虽然总工作量增多，但是并行执行后，完成任务的时间并没有增加。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210404221747.png" alt="20210404221747" loading="lazy"></li>
</ul>
<h4 id="scalable-counting">Scalable Counting</h4>
<ul>
<li>令人吃惊的是，关于如何实现可扩展的计数器，研究人员已经研究了多年。更令人吃惊的是，最近的操作系统性能分析研究表明，可扩展的计数器很重要。没有可扩展的计数，一些运行在 Linux 上的工作在多核机器上将遇到严重的扩展性问题。</li>
<li>尽管人们已经开发了多种技术来解决这一问题，我们将介绍一种特定的方法。这个方法是最近的研究提出的，称为懒惰计数器（sloppy counter）</li>
<li>懒惰计数器通过多个局部计数器和一个全局计数器来实现一个逻辑计数器，其中每个CPU 核心有一个局部计数器。具体来说，在 4 个 CPU 的机器上，有 4 个局部计数器和 1 个全局计数器。除了这些计数器，还有锁：每个局部计数器有一个锁，全局计数器有一个。</li>
<li>懒惰计数器的基本思想是这样的。如果一个核心上的线程<strong>想增加计数器，那就增加它的局部计数器，访问这个局部计数器是通过对应的局部锁同步的</strong>。因为每个 CPU 有自己的局部计数器，不同 CPU 上的线程不会竞争，所以计数器的更新操作可扩展性好。但是，<strong>为了保持全局计数器更新（以防某个线程要读取该值），局部值会定期转移给全局计数器，方法是获取全局锁，让全局计数器加上局部计数器的值，然后将局部计数器置零</strong>。</li>
<li>这种局部转全局的频度，取决于一个阈值，这里称为 S（表示 sloppiness）。S 越小，懒惰计数器则越趋近于非扩展的计数器。S 越大，扩展性越强，但是全局计数器与实际计数的偏差越大。我们可以抢占所有的局部锁和全局锁（以特定的顺序，避免死锁），以获得精确值，但这种方法没有扩展性。</li>
<li>为了弄清楚这一点，来看一个例子（见表）。在这个例子中，阈值 S 设置为 5，4 个 CPU 上分别有一个线程更新局部计数器 L1,…, L4。随着时间增加，全局计数器 G 的值也会记录下来。每一段时间，局部计数器可能会增加。如果局部计数值增加到阈值 S，就把局部值转移到全局计数器，局部计数器清零。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405134342.png" alt="20210405134342" loading="lazy"></li>
<li>上面的曲线图中接近 X 轴的是阈值 S 为 1024 时懒惰计数器的性能。性能很高，4 个处理器更新 400 万次的时间和一个处理器更新 100 万次的几乎一样。</li>
<li>下图展示了了阈值 S 的重要性，在 4 个 CPU 上的 4 个线程，分别增加计数器 100 万次。如果 S 小，性能很差（但是全局计数器精确度高）。如果 S 大，性能很好，但是全局计数器会有延时。懒惰计数器就是在准确性和性能之间折中。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405153232.png" alt="20210405153232" loading="lazy"></li>
<li>下面代码就是懒惰计数器的基本实现。</li>
</ul>
<pre><code class="language-C">1 typedef struct counter_t {
2   int global; // global count
3   pthread_mutex_t glock; // global lock
4   int local[NUMCPUS]; // local count (per cpu)
5   pthread_mutex_t llock[NUMCPUS]; // ... and locks
6   int threshold; // update frequency
7 } counter_t;
8
9  // init: record threshold, init locks, init values
10 // of all local counts and global count
11 void init(counter_t *c, int threshold) {
12  c-&gt;threshold = threshold;
13
14  c-&gt;global = 0;
15  pthread_mutex_init(&amp;c-&gt;glock, NULL);
16
17  int i;
18  for (i = 0; i &lt; NUMCPUS; i++) {
19      c-&gt;local[i] = 0; 
20      pthread_mutex_init(&amp;c-&gt;llock[i], NULL);
21  }
22 }
23
24 // update: usually, just grab local lock and update local amount
25 // once local count has risen by 'threshold', grab global
26 // lock and transfer local values to it
27 void update(counter_t *c, int threadID, int amt) {
28  pthread_mutex_lock(&amp;c-&gt;llock[threadID]);
29  c-&gt;local[threadID] += amt; // assumes amt &gt; 0
30  if (c-&gt;local[threadID] &gt;= c-&gt;threshold) { // transfer to global
31      pthread_mutex_lock(&amp;c-&gt;glock);
32      c-&gt;global += c-&gt;local[threadID];
33      pthread_mutex_unlock(&amp;c-&gt;glock);
34      c-&gt;local[threadID] = 0;
35  }
36  pthread_mutex_unlock(&amp;c-&gt;llock[threadID]);
37 }
38
39 // get: just return global amount (which may not be perfect)
40 int get(counter_t *c) {
41  pthread_mutex_lock(&amp;c-&gt;glock);
42  int val = c-&gt;global;
43  pthread_mutex_unlock(&amp;c-&gt;glock);
44  return val; // only approximate!
45 } 
</code></pre>
<h3 id="concurrent-linked-lists">Concurrent Linked Lists</h3>
<ul>
<li>接下来看一个更复杂的数据结构，链表。同样，我们从一个基础实现开始。简单起见，<br>
我们只关注链表的插入操作，其他操作比如查找、删除等就交给读者了。</li>
</ul>
<pre><code class="language-C">1 // basic node structure
2 typedef struct node_t {
3   int key;
4   struct node_t *next;
5 } node_t;
6
7 // basic list structure (one used per list)
8 typedef struct list_t {
9   node_t *head;
10  pthread_mutex_t lock;
11 } list_t;
12
13 void List_Init(list_t *L) {
14  L-&gt;head = NULL; 
15  pthread_mutex_init(&amp;L-&gt;lock, NULL);
16 }
17  
    // 从链表头插入
18 int List_Insert(list_t *L, int key) {
19  pthread_mutex_lock(&amp;L-&gt;lock);
20  node_t *new = malloc(sizeof(node_t));
21  if (new == NULL) {
22      perror(&quot;malloc&quot;);
23      pthread_mutex_unlock(&amp;L-&gt;lock);
24      return -1; // fail
25  }
26  new-&gt;key = key;
27  new-&gt;next = L-&gt;head;
28  L-&gt;head = new;
29  pthread_mutex_unlock(&amp;L-&gt;lock);
30  return 0; // success
31 }
32
33 int List_Lookup(list_t *L, int key) {
34  pthread_mutex_lock(&amp;L-&gt;lock);
35  node_t *curr = L-&gt;head;
36  while (curr) {
37      if (curr-&gt;key == key) {
38          pthread_mutex_unlock(&amp;L-&gt;lock);
39          return 0; // success
40      }
41      curr = curr-&gt;next;
42  }
43  pthread_mutex_unlock(&amp;L-&gt;lock);
44  return -1; // failure
45 }
</code></pre>
<ul>
<li>从代码中可以看出，代码插入函数入口处获取锁，结束时释放锁。如果 malloc 失败（在极少的时候），会有一点小问题，在这种情况下，代码在插入失败之前，必须释放锁。事实表明，这种异常控制流容易产生错误。最近一个 Linux 内核补丁的研究表明，有40%都是这种很少发生的代码路径（实际上，这个发现启发了我们自己的一些研究，我们从 Linux 文件系统中移除了所有内存失败的路径，得到了更健壮的系统）。</li>
<li>因此，挑战来了：我们能够重写插入和查找函数，保持并发插入正确，但避免在失败情况下也需要调用释放锁吗？</li>
<li>在这个例子中，答案是可以。具体来说，我们调整代码，<strong>让获取锁和释放锁只环绕插入代码的真正临界区。</strong> 前面的方法有效是因为部分工作实际上不需要锁，<strong>假定 malloc()是线程安全的，每个线程都可以调用它，不需要担心竞争条件和其他并发缺陷。只有在更新共享列表时需要持有锁</strong>。</li>
</ul>
<pre><code class="language-C">1 void List_Init(list_t *L) {
2   L-&gt;head = NULL;
3   pthread_mutex_init(&amp;L-&gt;lock, NULL);
4 }
5
6 void List_Insert(list_t *L, int key) {
7   // synchronization not needed
8   node_t *new = malloc(sizeof(node_t));
9   if (new == NULL) {
10      perror(&quot;malloc&quot;);
11      return;
12  }
13  new-&gt;key = key;
14
15  // just lock critical section
16  pthread_mutex_lock(&amp;L-&gt;lock);
17  new-&gt;next = L-&gt;head;
18  L-&gt;head = new;
19  pthread_mutex_unlock(&amp;L-&gt;lock);
20 }
21
22 int List_Lookup(list_t *L, int key) {
23  int rv = -1;
24  pthread_mutex_lock(&amp;L-&gt;lock);
25  node_t *curr = L-&gt;head;
26  while (curr) {
27      if (curr-&gt;key == key) {
28          rv = 0;
29          break;
30      }
31      curr = curr-&gt;next;
32  }
33  pthread_mutex_unlock(&amp;L-&gt;lock);
34  return rv; // now both success and failure
35 } 
</code></pre>
<h4 id="scaling-linked-lists">Scaling Linked Lists</h4>
<ul>
<li>尽管我们有了基本的并发链表，但又遇到了这个链表扩展性不好的问题。研究人员发现的增加链表并发的技术中，有一种叫作过手锁（hand-over-hand locking，也叫作锁耦合，lock coupling）。</li>
<li>原理也很简单。每个节点都有一个锁，替代之前整个链表一个锁。遍历链表的时候，首先抢占下一个节点的锁，然后释放当前节点的锁。</li>
<li>从概念上说，过手锁链表有点道理，它增加了链表操作的并发程度。但是实际上，在遍历的时候，每个节点获取锁、释放锁的开销巨大，很难比单锁的方法快。即使有大量的线程和很大的链表，这种并发的方案也不一定会比单锁的方案快。也许某种杂合的方案（一定数量的节点用一个锁）值得去研究。</li>
</ul>
<blockquote>
<ul>
<li>如果方案带来了大量的开销（例如，频繁地获取锁、释放锁），那么高并发就没有什么意义。如果简单的方案很少用到高开销的调用，通常会很有效。增加更多的锁和复杂性可能会适得其反。话虽如此，有一种办法可以获得真知：实现两种方案（简单但少一点并发，复杂但多一点并发），测试它们的表现。毕竟，你不能在性能上作弊。结果要么更快，要么不快。</li>
<li>有一个通用建议，对并发代码和其他代码都有用，即注意控制流的变化导致函数返回和退出，或其他错误情况导致函数停止执行。因为很多函数开始就会获得锁，分配内存，或者进行其他一些改变状态的操作，如果错误发生，代码需要在返回前恢复各种状态，这容易出错。因此，最好组织好代码，减少这种模式。</li>
</ul>
</blockquote>
<h3 id="concurrent-queues">Concurrent Queues</h3>
<ul>
<li>你现在知道了，总有一个标准的方法来创建一个并发数据结构：添加一把大锁。对于一个队列，我们将跳过这种方法。我们来看看 Michael 和 Scott 设计的、更并发的队列。</li>
<li>仔细研究这段代码，你会发现有两个锁，一个负责队列头，另一个负责队列尾。这两个锁使得入队列操作和出队列操作可以并发执行，因为入队列只访问 tail 锁，而出队列只访问  head 锁。</li>
<li>Michael 和 Scott 使用了一个技巧，添加了一个假节点（在队列初始化的代码里分配的）。该假节点分开了头和尾操作。研究这段代码，或者输入、运行、测试它，以便更深入地理解它。</li>
<li>队列在多线程程序里广泛使用。然而，这里的队列（只是加了锁）通常不能完全满足这种程序的需求。更完善的有界队列，在队列空或者满时，能让线程等待。这是下一章探讨条件变量时集中研究的主题。读者需要看仔细了！</li>
</ul>
<pre><code class="language-C">1 typedef struct __node_t {
2   int value;
3   struct __node_t *next;
4 } node_t;
5
6 typedef struct __queue_t {
7   node_t *head;
8   node_t *tail;
9   pthread_mutex_t head_lock, tail_lock;
10 } queue_t;
11
12 void Queue_Init(queue_t *q) {
13  node_t *tmp = malloc(sizeof(node_t));
14  tmp-&gt;next = NULL;
15  q-&gt;head = q-&gt;tail = tmp;
16  pthread_mutex_init(&amp;q-&gt;head_lock, NULL);
17  pthread_mutex_init(&amp;q-&gt;tail_lock, NULL);
18 }
19
20 void Queue_Enqueue(queue_t *q, int value) {
21  node_t *tmp = malloc(sizeof(node_t));
22  assert(tmp != NULL);
23  tmp-&gt;value = value;
24  tmp-&gt;next = NULL;
25
26  pthread_mutex_lock(&amp;q-&gt;tail_lock);
27  q-&gt;tail-&gt;next = tmp;
28  q-&gt;tail = tmp;
29  pthread_mutex_unlock(&amp;q-&gt;tail_lock);
30 }
31
32 int Queue_Dequeue(queue_t *q, int *value) {
33  pthread_mutex_lock(&amp;q-&gt;head_lock);
34  node_t *tmp = q-&gt;head;
35  node_t *new_head = tmp-&gt;next;
36  if (new_head == NULL) {
37      pthread_mutex_unlock(&amp;q-&gt;head_lock);
38      return -1; // queue was empty
39  }
40  *value = new_head-&gt;value;
41  q-&gt;head = new_head;
42  pthread_mutex_unlock(&amp;q-&gt;head_lock);
43  free(tmp);
44  return 0;
45 }

</code></pre>
<h3 id="concurrent-hash-table">Concurrent Hash Table</h3>
<ul>
<li>我们讨论最后一个应用广泛的并发数据结构，散列表。我们只关注不需要调整大小的简单散列表。支持调整大小还需要一些工作，留给读者作为练习。</li>
<li>本例的散列表使用我们之前实现的并发链表，性能特别好。每个散列桶（每个桶都是一个链表）都有一个锁，而不是整个散列表只有一个锁，从而支持许多并发操作。</li>
</ul>
<pre><code class="language-C">1 #define BUCKETS (101)
2
3 typedef struct __hash_t {
4   list_t lists[BUCKETS];
5 } hash_t;
6
7 void Hash_Init(hash_t *H) {
8   int i;
9   for (i = 0; i &lt; BUCKETS; i++)
10      List_Init(&amp;H-&gt;lists[i]);
11 }
12
13 int Hash_Insert(hash_t *H, int key) {
14  return List_Insert(&amp;H-&gt;lists[key % BUCKETS], key);
15 }
16
17 int Hash_Lookup(hash_t *H, int key) {
18  return List_Lookup(&amp;H-&gt;lists[key % BUCKETS], key);
19 }

</code></pre>
<ul>
<li>下图展示了并发更新下的散列表的性能（同样在 4 CPU 的 iMac，4 个线程，每个线程分别执行 1 万～5 万次并发更新）。同时，作为比较，我们也展示了单锁链表的性能。可以看出，这个简单的并发散列表扩展性极好，而链表则相反。<br>
<img src="https://raw.githubusercontent.com/zjs1224522500/PicGoImages/master//img/blog/20210405160416.png" alt="20210405160416" loading="lazy"></li>
</ul>
<blockquote>
<ul>
<li><strong>建议：避免不成熟的优化（Knuth 定律）</strong></li>
<li>实现并发数据结构时，先从最简单的方案开始，也就是加一把大锁来同步。这样做，你很可能构建了正确的锁。如果发现性能问题，那么就改进方法，只要优化到满足需要即可。正如 Knuth 的著名说法“不成熟的优化是所有坏事的根源。”</li>
<li>许多操作系统，在最初过渡到多处理器时都是用一把大锁，包括 Sun 和 Linux。在 Linux 中，这个锁甚至有个名字，叫作 BKL（大内核锁，big kernel lock）。这个方案在很多年里都很有效，直到多 CPU 系统普及，内核只允许一个线程活动成为性能瓶颈。终于到了为这些系统优化并发性能的时候了。Linux 采用了简单的方案，把一个锁换成多个。Sun 则更为激进，实现了一个最开始就能并发的新系统，Solaris。读者可以通过 Linux 和 Solaris 的内核资料了解更多信息。</li>
</ul>
</blockquote>
<h3 id="summary-2">Summary</h3>
<ul>
<li>我们已经介绍了一些并发数据结构，从计数器到链表队列，最后到大量使用的散列表。同时，我们也学习到：控制流变化时注意获取锁和释放锁；增加并发不一定能提高性能；有性能问题的时候再做优化。关于最后一点，避免不成熟的优化（premature optimization），对于所有关心性能的开发者都有用。我们让整个应用的某一小部分变快，却没有提高整体性能，其实没有价值。</li>
<li>当然，我们只触及了高性能数据结构的皮毛。Moir 和 Shavit 的调查提供了更多信息，包括指向其他来源的链接。特别是，你可能会对其他结构感兴趣（比如 B 树），那么数据库课程会是一个不错的选择。你也可能对根本不用传统锁的技术感兴趣。这种非阻塞数据结构是有意义的，在常见并发问题的章节中，我们会稍稍涉及。但老实说这是一个广泛领域的知识，远非本书所能覆盖。感兴趣的读者可以自行研究。</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Series One of Basic of Concurrency - Concurrency and Threads]]></title>
        <id>https://blog.shunzi.tech/post/basic-of-concurrency-three/</id>
        <link href="https://blog.shunzi.tech/post/basic-of-concurrency-three/">
        </link>
        <updated>2021-03-28T03:40:00.000Z</updated>
        <summary type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第一篇（Concurrency and Threads），并发和线程。</li>
</ul>
</blockquote>
]]></summary>
        <content type="html"><![CDATA[<blockquote>
<ul>
<li>威斯康辛州大学操作系统书籍《Operating Systems: Three Easy Pieces》读书笔记系列之 Concurrency（并发）。本篇为并发技术的基础篇系列第一篇（Concurrency and Threads），并发和线程。</li>
</ul>
</blockquote>
<!-- more -->
<h2 id="chapter-index">Chapter Index</h2>
<ul>
<li><a href="">Series One of Basic of Concurrency - Concurrency and Threads</a></li>
<li><a href="../lock/">Series Two of Basic of Concurrency - Lock</a></li>
<li><a href="">Series Three of Basic of Concurrency - Condition Variables</a></li>
<li><a href="">Series Four of Basic of Concurrency - Semaphores</a></li>
<li><a href="">Series Five of Basic of Concurrency - Bugs and Event-Based Concurrency</a></li>
</ul>
]]></content>
    </entry>
</feed>